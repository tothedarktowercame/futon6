{"node_id": "p10-problem", "claim_verified": "plausible", "verification_notes": "The linear system is dimensionally consistent: Z⊗K_tau has shape (Mn)×(rn)=N×(nr), SS^T is N×N, and (I_r⊗K_tau) is (nr)×(nr), so both sides are in R^{nr}. With X:=Z⊗K_tau and P:=SS^T, the matrix is A=X^T P X + λ(I_r⊗K_tau). Since P is a projector/mask, X^T P X is symmetric PSD. If λ>0 and K_tau is symmetric PD, then λ(I_r⊗K_tau) is SPD, hence A is SPD and PCG is well-posed. PCG avoids O(N) explicit work by using implicit matvecs: apply v->V (reshape), compute K_tau V, use Kronecker identity (Z⊗K_tau)vec(V)=vec(K_tau V Z^T), apply sampling with S only on q observed entries, and back-project with sparse accumulations; this yields per-iteration cost in q,n,r (e.g., O(n^2 r + q r + n r^2)) rather than O(N).", "math_se_references": [], "suggested_improvement": "State assumptions explicitly in the node: K is symmetric PSD, τ>0 (or alternatively solve on Range(K) when τ=0), S is a selection matrix so SS^T is a diagonal projector, and therefore A is SPD so PCG applies. Also include one explicit implicit-matvec cost line to justify “no O(N)” rigorously.", "missing_assumptions": ["K should be symmetric PSD so that K_tau=K+τI is symmetric PD for τ>0.", "If τ=0 and K is singular, the solve must be restricted to Range(K) (or use pseudoinverse formulation).", "S should be defined as a selection/mask matrix (typically S^T S=I_q), making SS^T a projector."], "confidence": "high"}
{"node_id": "p10-s1", "claim_verified": "verified", "verification_notes": "The complexity statement is correct under the stated naive/dense interpretation. If A is treated as a dense (nr)×(nr) matrix, a direct factorization solve is Θ((nr)^3)=Θ(n^3 r^3) (e.g., Cholesky/LU up to constants). Also, Z⊗K_tau has shape (M n)×(r n)=N×(nr), so explicitly materializing it stores Nnr entries, i.e., Θ(Nnr) memory, and forming those entries naively is likewise Θ(Nnr) work.", "math_se_references": [], "suggested_improvement": "State assumptions explicitly: (1) no Kronecker/sparsity structure is exploited in the direct solve, and (2) the Kronecker factor is explicitly materialized rather than applied implicitly. You can tighten wording to: “dense direct solve costs Θ((nr)^3), explicit Kronecker construction costs Θ(Nnr) memory and at least Θ(Nnr) arithmetic.”", "missing_assumptions": ["A is handled as dense (not structure-exploiting).", "The “explicit route” means full materialization of Z⊗K_tau in memory.", "Complexity ignores constant factors and lower-order terms."], "confidence": "high"}
{"node_id": "p10-s2", "claim_verified": "plausible", "verification_notes": "Let A = (Z⊗K_tau)^T S S^T (Z⊗K_tau) + λ(I_r⊗K_tau). With K_tau ≻ 0 and λ>0, λ(I_r⊗K_tau) ≻ 0. Also S S^T is symmetric PSD (selection mask), so (Z⊗K_tau)^T S S^T (Z⊗K_tau) is PSD. Hence A is SPD, so CG is applicable; PCG is applicable provided the preconditioner is SPD.\n\nImplicit matvec Av for v=vec(V), V∈R^{n×r} can be done without forming N=nM objects:\n1) U=K_tau V, cost O(n^2 r).\n2) Forward sampled values: for each observed (i,j), y_ij = <U_{i,:}, Z_{j,:}>, total O(qr) via row-wise grouping (no nM materialization).\n3) Adjoint: accumulate GZ by adding y_ij Z_{j,:} to row i, cost O(qr), then multiply by K_tau: K_tau(GZ), cost O(n^2 r), yielding vec((Z^T⊗K_tau)w').\n4) Add regularizer λ vec(K_tau V)=λ vec(U), no extra asymptotic cost beyond step 1.\nTotal is O(n^2 r + qr) up to constants, independent of N.", "math_se_references": [], "suggested_improvement": "State explicitly: (i) A is SPD by PSD+SPD sum argument, (ii) PCG additionally needs an SPD preconditioner P, and (iii) MATVEC uses index set Ω of size q to compute forward/adjoint in O(n^2 r + qr) without constructing vec(K_tau V Z^T) or any N-length dense vector.", "missing_assumptions": ["Preconditioner definiteness is not stated (PCG requires P≻0, or use CG with P=I).", "S is assumed to be a selection/weighted sparse operator with at most q active entries and accessible observation indices.", "Cost model assumes dense K_tau-times-(n×r) multiply is O(n^2 r) and fits memory; no hidden O(N) data movement."], "confidence": "high"}
{"node_id": "p10-s2a", "claim_verified": "verified", "verification_notes": "The Kronecker-vectorization identity is correct under compatible dimensions: for A\\in\\mathbb{R}^{m\\times n}, B\\in\\mathbb{R}^{p\\times q}, X\\in\\mathbb{R}^{q\\times n}, (A\\otimes B)\\,\\mathrm{vec}(X)=\\mathrm{vec}(BXA^\\top). In this node, A=Z\\in\\mathbb{R}^{M\\times r}, B=K\\in\\mathbb{R}^{n\\times n}, X=V\\in\\mathbb{R}^{n\\times r}, so (Z\\otimes K)\\mathrm{vec}(V)=\\mathrm{vec}(KVZ^\\top) is dimensionally consistent (nM-vector both sides). For cost: compute U=KV in O(n^2 r) (dense K). Then for observed set \\Omega of size q, each selected entry is (KVZ^\\top)_{ij}=U_{i,:}\\cdot Z_{j,:} at O(r) per entry, totaling O(qr). Applying SS^\\top as masking/extraction adds at most O(q). Hence forward evaluation at observed entries is O(n^2 r+qr), avoiding O(Nr) full materialization.", "math_se_references": [], "suggested_improvement": "State S’s convention explicitly (e.g., S\\in\\mathbb{R}^{N\\times q} with SS^\\top a diagonal projector on observed indices) and define \\Omega={(i_t,j_t)}_{t=1}^q. Then write y_t=U_{i_t,:}\\cdot Z_{j_t,:} to make the O(qr) row-grouping argument explicit.", "missing_assumptions": ["K is treated as dense; otherwise complexity may differ.", "Observed indices are known and accessed in O(1) each (no heavy indexing overhead).", "q counts unique observed entries (or duplicates are allowed but still counted in q)."], "confidence": "high"}
{"node_id": "p10-s2b", "claim_verified": "verified", "verification_notes": "Using the standard Kronecker identity (A ⊗ B) vec(X) = vec(B X A^T) with A = Z^T, B = K_tau, and vec(W') = w, we get (Z^T ⊗ K_tau)w = vec(K_tau W' Z). If w comes from masking by S S^T, then it has at most q nonzeros, so s = nnz(W') = nnz(w) <= q (can be strictly smaller if masked values are numerically zero). Cost: sparse-dense multiply W'Z with W' in R^{n x M} and s nonzeros is O(sr); then dense multiply K_tau(W'Z) with K_tau in R^{n x n} and W'Z in R^{n x r} is O(n^2 r). Hence total O(sr + n^2 r) <= O(qr + n^2 r).", "math_se_references": [], "suggested_improvement": "State explicitly that w = vec(W') under the same column-major convention used in S and Kronecker identities, and that S selects q indices (not values), so nnz(w) <= q. Then present complexity as O(sr + n^2 r) first, followed by the bound O(qr + n^2 r).", "missing_assumptions": ["Vectorization/indexing convention is consistent: w = vec(W') and S acts on vec(.) in that same ordering.", "W' is stored/accessed in a sparse format so W'Z achieves O(sr) rather than O(nMr).", "K_tau is treated as dense for the O(n^2 r) bound (no extra structure exploited)."], "confidence": "high"}
{"node_id": "p10-s2c", "claim_verified": "verified", "verification_notes": "The identity is correct by the standard Kronecker-vec rule: vec(A X B) = (B^T ⊗ A) vec(X). Setting A = K_tau (n×n), X = V (n×r), and B = I_r gives (I_r^T ⊗ K_tau)vec(V) = (I_r ⊗ K_tau)vec(V) = vec(K_tau V). Multiplying by scalar λ yields λ(I_r⊗K_tau)vec(V) = λ·vec(K_tau V). The stated cost O(n^2 r) is correct for dense K_tau, since it is r matrix-vector products (or one n×n by n×r multiply).", "math_se_references": [], "suggested_improvement": "State dimensions explicitly (K_tau∈R^{n×n}, V∈R^{n×r}) and cite vec(AXB)=(B^T⊗A)vec(X) with B=I_r; note that O(n^2 r) assumes dense K_tau (can be lower with structure/sparsity).", "missing_assumptions": ["V has shape n×r so K_tau V is defined.", "K_tau is treated as dense for the O(n^2 r) complexity claim.", "λ is a scalar regularization parameter."], "confidence": "high"}
{"node_id": "p10-s2-total", "claim_verified": "verified", "verification_notes": "The claimed per-matvec cost is correct under the stated computational model. For input X in R^{n x r}: (1) forward sampled map S^T(Z⊗K_tau)vec(X) can be done as G=K_tau X (O(n^2 r)) plus q sampled dot-products with rows of Z (O(qr)); (2) adjoint map (Z⊗K_tau)^T S(·) can be done by sparse scatter-accumulation into an n x r buffer using q observations (O(qr)) then one K_tau multiply (O(n^2 r)); (3) regularization lambda(I_r⊗K_tau)vec(X)=lambda vec(K_tau X) is O(n^2 r). Summing terms gives O(n^2 r + qr) up to constants, with no N=nM factor in per-iteration work.", "math_se_references": [], "suggested_improvement": "State explicitly that S is implemented as an index list (gather/scatter), not formed as dense N x q or N x N masks, and that sampled (i,m) pairs are precomputed once. This makes the “no N dependence” claim airtight.", "missing_assumptions": ["S/SS^T is never materialized densely; only q sampled indices are used.", "Access to sampled rows Z[m_j,:] is O(1) (Z stored explicitly).", "K_tau-times-(n x r) uses dense cost O(n^2 r) with no hidden M or N term.", "Any preprocessing of sample indices is one-time, not charged per PCG matvec."], "confidence": "high"}
{"node_id": "p10-s3", "claim_verified": "verified", "verification_notes": "The step is correct under the stated dimensions. With T in R^{n x M} and Z in R^{M x r}, B = TZ is in R^{n x r}. If T has q nonzeros, sparse-dense multiplication costs O(nnz(T)*r)=O(qr). Next, (I_r \\otimes K_tau)vec(B)=vec(K_tau B), so compute K_tau B directly without forming the Kronecker matrix. For dense K_tau in R^{n x n} and B in R^{n x r}, this costs O(n^2 r). Total RHS cost is therefore O(qr + n^2 r), with no O(N)=O(nM) operation required.", "math_se_references": [], "suggested_improvement": "State explicitly: compute b as vec(K_tau(TZ)) and avoid constructing I_r \\otimes K_tau. Also define q := nnz(T) to make the O(qr) bound precise.", "missing_assumptions": ["q denotes nnz(T) (not just number of observed entries in another object).", "K_tau is treated as dense; otherwise O(n^2 r) may be reduced.", "Z is dense (or at least no extra sparsity/structure is exploited)."], "confidence": "high"}
{"node_id": "p10-s4", "claim_verified": "plausible", "verification_notes": "From A=(Z\\otimes K_\\tau)^TSS^T(Z\\otimes K_\\tau)+\\lambda(I_r\\otimes K_\\tau), with whitening y=(I_r\\otimes K_\\tau^{1/2})vec(W), the congruent matrix is A_w=(I\\otimes K_\\tau^{-1/2})A(I\\otimes K_\\tau^{-1/2})=(Z^T\\otimes K_\\tau^{1/2})SS^T(Z\\otimes K_\\tau^{1/2})+\\lambda I. Using SS^T\\approx cI (c=q/N) gives A_w\\approx c(Z^TZ\\otimes K_\\tau)+\\lambda I. Choosing H=cZ^TZ+\\lambda I_r and P=H\\otimes K_\\tau is consistent as a structured surrogate if one further approximates the whitened factor K_\\tau by I (or a scalar multiple of I / bounded spectrum). SPD: K_\\tau\\succ0 (\\tau>0). H\\succ0 if \\lambda>0 (or if \\lambda=0 and Z has full column rank with c>0). Then P=H\\otimes K_\\tau\\succ0.", "math_se_references": [], "suggested_improvement": "State the derivation explicitly in two steps: (1) whitening yields A_w\\approx c(Z^TZ\\otimes K_\\tau)+\\lambda I; (2) replace K_\\tau by I (or \\alpha I with spectral-equivalence bounds) to get \\tilde A_w=H\\otimes I, then map back to P=(I\\otimes K_\\tau^{1/2})\\tilde A_w(I\\otimes K_\\tau^{1/2})=H\\otimes K_\\tau. Also state conditions \\lambda>0, q>0, and sampling assumptions behind SS^T\\approx cI.", "missing_assumptions": ["An explicit justification for replacing K_\\tau in the whitened matrix by I (or \\alpha I).", "Sampling model (e.g., near-uniform random observations) under which SS^T\\approx (q/N)I is valid.", "Regularization/rank condition ensuring H is PD (typically \\lambda>0)."], "confidence": "medium"}
{"node_id": "p10-s4-hadamard", "claim_verified": "verified", "verification_notes": "The identity is correct for a Khatri-Rao product. If Z = A_a ⊙ ... ⊙ A_1 with A_i in R^{n_i x r}, then for any column indices p,q: (Z^T Z)_{pq} = <⊙_i A_i(:,p), ⊙_i A_i(:,q)> = product_i <A_i(:,p), A_i(:,q)> = product_i (A_i^T A_i)_{pq}. Hence Z^T Z = (A_1^T A_1) * ... * (A_a^T A_a), where * is Hadamard product. Cost: forming each Gram A_i^T A_i is O(n_i r^2), so total is O(sum_i n_i r^2), plus O(a r^2) to multiply the r x r Gram matrices elementwise. This avoids O(M r^2) with M = product_i n_i from forming/using Z explicitly.", "math_se_references": [], "suggested_improvement": "Tighten the statement to: O(sum_i n_i r^2 + a r^2), often simplified to O(sum_i n_i r^2). Also explicitly define Z as the column-wise Khatri-Rao product of factors with aligned r columns.", "missing_assumptions": ["Z is explicitly the Khatri-Rao product Z = A_a ⊙ ... ⊙ A_1 with common column count r.", "M equals product_i n_i (full Kronecker row dimension).", "Complexity assumes dense factors and standard matrix-multiply cost for each Gram.", "Hadamard-product accumulation cost O(a r^2) is negligible only when sum_i n_i is not tiny."], "confidence": "high"}
{"node_id": "p10-s4-solve", "claim_verified": "plausible", "verification_notes": "The Kronecker-inverse identity is correct under invertibility: for invertible H and K_tau, (H⊗K_tau)^(-1)=H^(-1)⊗K_tau^(-1) because (H⊗K_tau)(H^(-1)⊗K_tau^(-1))=(HH^(-1))⊗(K_tau K_tau^(-1))=I. Using vec(K_tau Y H^T)=(H⊗K_tau)vec(Y), applying P^(-1) to z'=vec(Z') is equivalent to solving K_tau Y H^T=Z'. If K_tau=L_k L_k^T and H=L_h L_h^T (Cholesky), each apply does: (1) left solve with K_tau for r RHS, O(n^2 r); (2) right solve with H^T (equivalently n RHS in r×r solves), O(n r^2). Total O(n^2 r + n r^2) per application, with precompute O(n^3)+O(r^3).", "math_se_references": [], "suggested_improvement": "Make assumptions explicit in the step: define P=H⊗K_tau, state H and K_tau are SPD (or at least nonsingular), and note Cholesky is valid only in the SPD case (otherwise LU is needed).", "missing_assumptions": ["H is invertible (SPD if PCG and Cholesky are intended).", "K_tau is SPD; this needs the stated kernel conditions (e.g., K symmetric PSD with tau>0, or sufficiently large tau).", "Z' and Y are shaped n×r so the vec/Kronecker identities are dimensionally consistent."], "confidence": "high"}
{"node_id": "p10-s5", "claim_verified": "plausible", "verification_notes": "The PCG part is standard and correct under the usual SPD assumptions. If A\\succ0 and P\\succ0, then for PCG on A with preconditioner P, with B:=P^{-1/2}AP^{-1/2},\n\\|e_t\\|_A \\le 2\\left(\\frac{\\sqrt{\\kappa}-1}{\\sqrt{\\kappa}+1}\\right)^t\\|e_0\\|_A, \\quad \\kappa=\\kappa(B),\nwhich implies t=O(\\sqrt{\\kappa}\\log(1/\\varepsilon)) (up to constants, e.g. t\\gtrsim \\tfrac12\\sqrt\\kappa\\log(2/\\varepsilon)). The spectral-equivalence implication is also correct: from (1-\\delta)P \\preceq A \\preceq (1+\\delta)P, premultiply/postmultiply by P^{-1/2} to get (1-\\delta)I \\preceq B \\preceq (1+\\delta)I, hence \\lambda_min(B)\\ge 1-\\delta and \\lambda_max(B)\\le 1+\\delta, so \\kappa(B)\\le (1+\\delta)/(1-\\delta). This requires 0\\le\\delta<1. The Tropp-based statement \\delta=O(\\sqrt{n\\log n/q}) is plausible but not fully verifiable from the local step alone because sampling model/normalization/probability parameters are not stated, and dimensional/coherence factors may be missing.", "math_se_references": [], "suggested_improvement": "State explicitly: (i) A and P are SPD; (ii) convergence is in A-norm (or P^{-1}A-induced equivalent form); (iii) 0\\le\\delta<1. For the concentration claim, give a high-probability form with constants and assumptions on S (independent sampling, scaling so E[SS^T]=cI, bounded leverage/coherence), e.g. q \\ge C\\,\\mu\\,d\\,\\log(d/\\eta)/\\delta^2 with d the effective matrix dimension used in the bound.", "missing_assumptions": ["P\\succ0 (so P^{-1/2} exists)", "A\\succ0 (or implied via (1-\\delta)P\\preceq A with \\delta<1)", "\\delta<1 to keep denominator positive and ensure spectral equivalence is nondegenerate", "Precise sampling model and normalization for S in the Tropp concentration step", "Failure probability parameter (e.g., 1-\\eta) and possible coherence/effective-dimension factor"], "confidence": "medium"}
{"node_id": "p10-s6", "claim_verified": "verified", "verification_notes": "The stated PCG complexity follows by summing the referenced components: one-time setup O(n^3+r^3) (e.g., dense K_tau factorization and r-scale preconditioner spectral/algebraic work), plus t iterations each costing matvec O(n^2 r + q r) and preconditioner solve O(n^2 r + n r^2), giving O(n^3 + r^3 + t(n^2 r + q r + n r^2)). Under n>=r, nr^2<=n^2r and r^3<=n^3, so this simplifies to O(n^3 + t(n^2 r + q r)). The direct baseline is consistent with dense solve on size nr (O((nr)^3)=O(n^3 r^3)) plus explicit N-scale data interaction O(Nnr). Hence the PCG route removes explicit N-dependence in per-subproblem solve, which is the key claim in regime n,r<<q<<N.", "math_se_references": [], "suggested_improvement": "State explicitly which terms are one-time (per mode-k subproblem) versus per-iteration, and clarify that N-independence assumes sampled/sparse access through S (cost O(qr), not O(Nr)). Also note that t must remain moderate (via bounded/preconditioned condition number) for practical advantage.", "missing_assumptions": ["K_tau is treated as dense for O(n^3) factorization; structured/sparse kernels would change this term.", "Preconditioner setup is bounded by O(r^3) (or equivalent r-scale cost) and does not hide N-dependent assembly.", "Iteration count t is analyzed independently of N (or at least does not grow enough to offset N-removal).", "Right-hand side/data products are formed from observed entries without full O(N) passes."], "confidence": "high"}
{"node_id": "p10-s7", "claim_verified": "plausible", "verification_notes": "The claim matches the stated linear system structure. From A = (Z⊗K_tau)^TSS^T(Z⊗K_tau) + lambda(I_r⊗K_tau) and b = (I_r⊗K_tau)vec(B), a SETUP phase naturally computes K_tau = K + tau I, forms b, and defines a preconditioner H. A PCG iteration can apply MATVEC implicitly as: x=vec(X) -> u=vec(K_tau X Z^T), mask with SS^T using only q observed indices, then back-project with (Z⊗K_tau)^T and add lambda(I_r⊗K_tau)x. This avoids explicit N=nM-sized dense operators. PRECOND_SOLVE via Kronecker factors is mathematically valid if H is chosen with Kronecker-separable SPD form (e.g., H_r⊗H_n), enabling solves through factor solves in each mode.", "math_se_references": [], "suggested_improvement": "State H explicitly (exact formula), and show one line proving PRECOND_SOLVE: (H_r⊗H_n)^{-1}vec(R)=vec(H_n^{-1} R H_r^{-T}). Also specify that SS^T is implemented as index masking/scatter on q observations, not as an N×N matrix.", "missing_assumptions": ["H is defined and has Kronecker-separable SPD structure required by PRECOND_SOLVE.", "lambda>0 (or equivalent) so the system and preconditioner are SPD for PCG.", "Observed-entry operator S is stored as index lists/sparse selector so MATVEC truly scales with q rather than N."], "confidence": "medium"}
{"node_id": "p10-synthesis", "claim_verified": "plausible", "verification_notes": "The core linear-algebraic argument is conditionally sound: with \\(\\lambda>0\\) and \\(K_\\tau\\succ0\\), \\(A_\\tau=\\Phi^T D\\Phi+\\lambda(I_r\\otimes K_\\tau)\\) is SPD, so PCG is valid. The matrix-free matvec and RHS construction avoid explicit \\(N\\)-scale tensors and correctly reduce dependence to \\(q\\) (observed entries), giving per-iteration cost \\(O(n^2r+qr)\\) plus preconditioner application \\(O(n^2r+nr^2)\\). The final complexity expression follows from summing setup and iteration costs; the simplified form \\(O(n^3+t(n^2r+qr))\\) is valid in the stated regime \\(n\\ge r\\). However, the fast-convergence claim remains conditional: the key spectral-equivalence step from sampling (\\(D\\approx cI\\) / restricted isometry on the relevant subspace) is stated but not proved quantitatively, so bounded \\(\\kappa\\) is an assumption, not a derived theorem.", "math_se_references": [{"question_id": 27466, "title": "iterative solvers for large systems in physics", "relevance": "Background motivation only; not required for the proof-critical derivations."}, {"question_id": 27556, "title": "preconditioning for elliptic PDEs", "relevance": "Background analogy for preconditioning; not a substitute for the sampling-specific spectral bound."}], "suggested_improvement": "Add a formal lemma that upper/lower bounds \\((Z\\otimes K_\\tau^{1/2})^T(D-cI)(Z\\otimes K_\\tau^{1/2})\\) under explicit sampling/coherence conditions and converts it directly into a numeric \\(\\delta\\) for \\((1-\\delta)P\\preceq A_\\tau\\preceq(1+\\delta)P\\); then state the resulting sample-complexity condition on \\(q\\). Also separate clearly the exact complexity (including \\(nr^2\\) terms) from the simplified regime statement.", "missing_assumptions": ["An explicit probabilistic sampling model (uniform/with replacement vs without replacement) for D.", "Concrete coherence/leverage assumptions on columns of \\(Z\\otimes K_\\tau^{1/2}\\) needed for concentration.", "A quantitative condition on q ensuring \\(\\delta<1\\) (and preferably \\(\\delta\\ll1\\)).", "Flop model for applying \\(K_\\tau\\) (dense \\(O(n^2r)\\) vs structured/approximate kernels).", "Handling of repeated observed indices and weighting in D/T when constructing sparse sampled operators."], "confidence": "high"}
