\chapter{Strategy Patterns Across Ten Problems}

The ten proofs in Part~II were not produced by a uniform method.
Some closed quickly; others required layer-switching, creative
reductions, or honest declarations of conditionality.
This chapter extracts the transferable strategy patterns---what
worked, what didn't, and what an agent stuck on one problem
could learn from the successes of another.

\section{Design Pattern: \textnormal{\textsc{Outcome Typing}} (Close, Reduce, Map)}

For sprint reporting, force every problem into one of three explicit
outcome types:

\begin{enumerate}
\item \textbf{Close.} A complete proof (possibly with minor caveats).
  In this revision, Problem~3 is closed for the scoped existence target
  (uniqueness/irreducibility optional), Problem~5 is solved in a
  scope-limited form, and Problem~7 is treated as provisionally closed
  via the rotation route pending independent ledger re-check.
\item \textbf{Reduce.} A conditional result with clearly stated
  assumptions.
\item \textbf{Map.} The obstruction is characterized and adjacent
  problems are identified, but no proof is offered.
\end{enumerate}

These labels describe \emph{mathematical status}. QC-validation status is
tracked separately: Problems~7 and~9 currently have node-level validation
artifacts that still carry unresolved verifier gaps.

All three are legitimate contributions.  A reduction is not a failed
closure; it is a stable intermediate product that can later convert to
closure after a layer switch or new lemma.  Problem~6 spent eight hours
in ``Reduce'' before a coaching intervention moved it to a strong partial
closure (complete for key classes, with one explicit remaining
general-case gap)---the change came from reframing, not additional effort.

\medskip
\noindent\textbf{Evidence ledger for Outcome Typing (top commits).}
\begin{center}
\small
\begin{tabular}{@{}p{1.3cm}p{4.8cm}p{7.0cm}@{}}
\toprule
Commit & Role in pattern & Primary artifacts \\
\midrule
\texttt{c557899} & Align closure language with evidence (separate math-status and QC-status axes) & \texttt{closure-validation-audit.md} + Part IV status text updates \\
\texttt{2c734cb} & Scope-aware closure typing for P3 (existence-scope clarified; QC status explicit) & P3 solution/writeup + \texttt{closure-validation-audit.md} \\
\texttt{cc11834} & ``Map'' outcome exemplar: counterexample + attack-path map instead of false closure & \texttt{problem6-gpl-h-counterexample.md} + \texttt{problem6-gpl-h-attack-paths.md} \\
\texttt{9aaf8fa} & Promote scoped result to ``Close'' after targeted gap closure and validation update & P3 prompts/results refresh + \texttt{closure-validation-audit.md} \\
\texttt{90b6dd6} & Reprocessed verifier summary results (15/15) to harden QC tracking independently of proof-status labels & \texttt{codex-unified-repair-results.json} + \texttt{codex-unified-repair-verification.md} \\
\bottomrule
\end{tabular}
\normalsize
\end{center}

\section{Design Pattern: \textnormal{\textsc{Layer-Switching}} (Problem 7)}

Problem~7 (uniform lattice with 2-torsion) is the clearest example
of a successful strategy pivot.

\begin{description}
\item[Initial approach:] Reflection lattice in even dimension.
  The E2 obligation (Fowler's criterion) was discharged.
\item[Obstruction:] The surgery obligation (S) was blocked.
  Equivariant surgery requires a codimension-2 gap hypothesis;
  reflections give codimension-1 fixed sets.  Blocked.
  (\texttt{0fa4e82}: ``restructure S-branch as open problem with three approaches'')
\item[Characterization:] Instead of pushing harder on reflections,
  the obstruction was characterized structurally: a dimension-parity
  tension between E2 (needs even $n$) and S (better in odd $n$).
\item[Layer switch:] Replace reflections (codimension~1) with rotations
  (codimension~2).  Now $n$ is odd, E2 still works (fixed set has
  odd dimension, $\chi = 0$), and the codimension-2 gap hypothesis
  is satisfied.
  (\texttt{287a41c}: ``rotation route (Approach~IV), paper reads, triage'';
   \texttt{620ed57}: ``resolve rotation lattice existence, discharge E2'')
\item[Closure:] The surgery obstruction vanishes (trivial normal
  bundle from the congruence condition), yielding a provisional closure
  chain pending independent theorem-ledger re-check.
  (\texttt{8ce9771}: ``flat-normal-bundle argument---rational obstruction vanishes'';
   \texttt{158bc4f}: ``integral obstruction vanishes---obligation~S discharged'')
\end{description}

\textbf{Transferable principle:} When blocked in one layer,
characterize the obstruction structurally before attempting to
overcome it.  The characterization may reveal that a different
layer avoids the obstruction entirely.

\medskip
\noindent\textbf{Evidence ledger for Layer-Switching (top commits).}
\begin{center}
\small
\begin{tabular}{@{}p{1.3cm}p{4.8cm}p{7.0cm}@{}}
\toprule
Commit & Role in pattern & Primary artifacts \\
\midrule
\texttt{0fa4e82} & Explicitly re-labels S as open and decomposes approaches & P7 solution + S-branch setup notes \\
\texttt{287a41c} & Rotation-route pivot and triage framing & Rotation-route wiring + P7 solution updates \\
\texttt{620ed57} & Discharges E2 under rotation route & Rotation-lattice construction note + P7 status update \\
\texttt{5c7388a} & Narrows to single remaining gap (G2) & Consolidated P7 complete-proof draft \\
\bottomrule
\end{tabular}
\normalsize
\end{center}

\section{Design Pattern: \textnormal{\textsc{Creative Reduction}} (Problem 4)}

Problem~4 (root separation under finite free convolution)
progressed from numerical evidence to proof via a sequence
of reductions:

\begin{enumerate}
\item Centering reduction (translation invariance of $\Phi_n$).
\item For $n = 3$: discovery of the identity
  $\Phi_3 \cdot \mathrm{disc} = 18 \, a_2^2$ (exact, verified symbolically).
\item Recognition that the surplus expression has the form
  required by Titu's lemma (Engel form of Cauchy--Schwarz).
\end{enumerate}

Step~2 was the creative step---an identity discovered numerically
and verified symbolically
(\texttt{9db6b4f}: ``Haar orbit exploration and key identity finding'';
 \texttt{0003300}: ``Prove P4 superadditivity for $n=3$ via $\Phi_3 \cdot \mathrm{disc}$
 identity + Cauchy--Schwarz'').
The key was that the identity was
\emph{sought} because the reduction strategy demanded an algebraic
relationship between $\Phi_n$ and known invariants.  The strategy
shaped the search.  The subsequent extension to $n = 4$ followed a
different route---algebraic elimination plus computational verification
(\texttt{84c0041}: ``harden PHC parsing and record $n = 4$ computational
verification''); full certification of one sub-case remains pending.

\textbf{Transferable principle:} Reductions create demand for
specific identities or lemmas.  The demand makes the search
targeted rather than exploratory.

\medskip
\noindent\textbf{Evidence ledger for Creative Reduction (top commits).}
\begin{center}
\small
\begin{tabular}{@{}p{1.3cm}p{4.8cm}p{7.0cm}@{}}
\toprule
Commit & Role in pattern & Primary artifacts \\
\midrule
\texttt{03c1f8d} & Discovers the key $\Phi_4 \cdot \mathrm{disc}$ identity & P4 invariant-exploration script \\
\texttt{269fddd} & 3-piece Cauchy--Schwarz reduction breakthrough & T2/R surplus proof script + P4 proof-state log \\
\texttt{9640d80} & Eliminates SOS route as global method (\(K\) not globally SOS) & Gram-matrix and \(K_8\)-factor analysis scripts \\
\texttt{fef7da1} & Path-2 closure claim for \(n=4\) via \(K_{\mathrm{red}}\ge 0\) & Path-2 structure artifact + handoff note \\
\bottomrule
\end{tabular}
\normalsize
\end{center}

\section{Design Pattern: \textnormal{\textsc{Structural Decomposition}} (Problem 8)}

Problem~8 (Lagrangian smoothing of polyhedral surfaces) was stuck
until the symplectic direct sum decomposition was discovered:
the 4-face Lagrangian condition forces $\mathbb{R}^4 = V_1 \oplus V_2$.
This single structural insight
(\texttt{09e23db}: ``symplectic direct sum forces Maslov index exactly~0'')
unlocked everything---Maslov index
vanishing, vertex smoothing via product structure
(\texttt{a5a4fbe}: ``replace crease smoothing + Lagrangian surgery with product smoothing''),
and the Hamiltonian isotopy property
(\texttt{de3e2ac}: ``justify Hamiltonian isotopy'').

\textbf{Transferable principle:} Look for structural decompositions
that simplify the problem globally, not just local fixes for individual
obstacles.

\medskip
\noindent\textbf{Evidence ledger for Structural Decomposition (top commits).}
\begin{center}
\small
\begin{tabular}{@{}p{1.3cm}p{4.8cm}p{7.0cm}@{}}
\toprule
Commit & Role in pattern & Primary artifacts \\
\midrule
\texttt{09e23db} & Structural decomposition step (\(\mathbb{R}^4 = V_1 \oplus V_2\), Maslov control) & P8 solution + wiring update \\
\texttt{a5a4fbe} & Replaces surgery framing with product smoothing & P8 solution + v3 diagram refresh \\
\texttt{de3e2ac} & Adds Hamiltonian-isotopy justification and consistency cleanup & P8 solution + wiring consistency patch \\
\bottomrule
\end{tabular}
\normalsize
\end{center}

\section{Design Pattern: \textnormal{\textsc{Layer Switch Under Coaching}} (Problem 6)}

Problem~6 (epsilon-light subsets) is the sprint's most instructive
case study because it exhibits both patterns: a TryHarder loop
\emph{and} the layer-switch that broke it.

\subsection{Phase 1: The TryHarder Loop}

The initial approach generated six or more handoff
and dispatch documents across multiple sessions, from the initial
dispatch (\texttt{5289ca8}: ``GPL-H attack dispatch'') through
Directions~A--D (\texttt{63a23ba}, \texttt{22c091f}, \texttt{b6a7625},
\texttt{d64fd13}) to closure attempts
(\texttt{78d94bc}: ``GPL-H closure attempt---all scores $< 1$'')
and counterexamples (\texttt{cc11834}: ``$K_{t,r}$ counterexample'').
The pattern:

\begin{enumerate}
\item Identify the gap (Assumption~V: vertex-induced selection).
\item Attempt closure via a specific attack vector.
\item Fail.
\item Generate a new handoff document dispatching another attack.
\item Repeat from step~2.
\end{enumerate}

All closure attempts targeted the sparsification layer (adapting
Batson--Spielman--Srivastava edge sparsification to vertex selection)
without recognizing that the obstruction there might be structural---not a
gap to fill but a genuine mismatch between edge and vertex selection.

\subsection{The coaching intervention}

A human coaching prompt forced layer enumeration:

\begin{quote}
\emph{What kind of problem is this?  What kind of proof applies?
How would you teach it to an undergraduate?  What kind of person
finds this easy?  Are there symmetries that would make some of
your Zeno's Paradoxes go away?}
\end{quote}

Each question targets a specific reframing:

\begin{description}
\item[``What kind of problem?''] Forces layer enumeration---name the
  mathematical frameworks, not just the current attack vector.
\item[``Teach it to an undergraduate?''] Forces identification of
  what is \emph{elementary} about the problem.  The elementary core
  is often the proof.
\item[``Who finds this easy?''] Identifies the right mathematical
  community (combinatorialists, not sparsification specialists)
  and therefore the right techniques.
\item[``Symmetries / Zeno's Paradoxes?''] Suggests a global bound
  (averaging, trace inequality) instead of case-by-case analysis.
\end{description}

\subsection{Phase 2: The elementary proof}

After 15 minutes of thinking, the agent found a proof chain that
bypasses the sparsification layer entirely:

\begin{enumerate}
\item \textbf{Tur\'an's theorem} gives an independent set $I_0$
  with $|I_0| \ge \varepsilon n/3$ in the heavy-edge subgraph.
  All edges internal to $I_0$ are light ($\tau_e \le \varepsilon$).
\item \textbf{Barrier greedy}: at each step, pick the vertex $v$
  with minimum $\|Y_t(v)\|$.
\item \textbf{PSD trace bound}: $\|Y\| \le \operatorname{tr}(Y)$
  for any PSD matrix~$Y$.
\item \textbf{Pigeonhole}: if the average trace
  $\bar{d}_t = \frac{1}{r_t}\sum_v \operatorname{tr}(Y_t(v)) < 1$,
  then some $v$ has $\operatorname{tr}(Y_t(v)) < 1$, therefore
  $\|Y_t(v)\| < 1$, therefore the barrier is maintained.
\end{enumerate}

For $K_n$: $\bar{d}_t = 2t/(n\varepsilon)$, so at $T = \varepsilon n/3$
steps, $\bar{d}_T = 2/3 < 1$.  This gives $|S| = \varepsilon n/3$
with $c = 1/3$, \textbf{proved exactly}.

\subsection{Phase 3: The partial averages breakthrough}

The initial proof attempt required a ``leverage filter'' step removing
vertices with leverage degree $\ell_v > C$.  This created an
irreconcilable tension: the Markov bound needed $C > 2$ to retain
enough vertices, but $\bar{d} < 1$ needed $C < 2$.  Testing on
$K_{a,b}$ confirmed this was not just a technical gap---up to $80\%$
of $I_0$ can have $\ell_v \ge 2$.

The resolution came from switching from a \emph{maximum-based} bound
to a \emph{sum-based} bound.  The \textbf{partial averages inequality}
observes that the average of the $T$ smallest leverage degrees cannot
exceed the overall average:
\[
  \frac{1}{T}\sum_{k=1}^{T} \ell_{(k)}
    \;\le\; \operatorname{avg}(\ell)
    \;<\; 2 \quad\text{(Foster on $I_0$)}.
\]
A ``min-$\ell$ greedy'' that selects vertices in order of increasing
leverage degree therefore accumulates $\sum \ell < 2T$, giving
\[
  \bar{d}_t \;\le\; \frac{2/3}{1 - \varepsilon/3} \;<\; 1
  \quad\text{for all $\varepsilon \in (0,1)$,\; at $M_t = 0$.}
\]

This eliminates the leverage filter entirely---no threshold $C$ needed,
no Markov bound, no structural assumption on maximum leverage degree.

\subsection{What remains}

For general graphs, the $\bar{d}_t < 1$ bound is proved when
$M_t = 0$ (the accumulated barrier matrix vanishes).  This covers
$K_n$, $K_{a,b}$ with $a \ne b$, cycles, grids, and sparse graphs.
For dense graphs where $M_t \ne 0$, the bound is verified
numerically at all 440 nontrivial greedy steps (max
$\bar{d} = 0.714$, amplification ratio $\le 1.30$) but one
quantitative estimate---bounding the $H_t^{-1}$ amplification
when $M_t \ne 0$---remains open.

\textit{Postscript (Feb 13 update):} the open bridge was further
localized via an E+F hybrid reduction
(\texttt{7e03174}): an E-regime (graph-adaptive transfer)
plus an F-regime (gain-loss balance) suffices for full stepwise
closure.  The reduction itself is proved; two regime lemmas remain.

No interlacing families.  No Borcea--Br\"and\'en.  No MSS theorem.
Just Foster's theorem $+$ partial averages $+$ PSD trace bound $+$
pigeonhole $+$ Tur\'an.  A three-line argument (PSD $\to$ pigeonhole
$\to$ existence) replaces the entire real stability machinery.

\subsection{The layer analysis}

\begin{description}
\item[Spectral layer:] Bound $\|L^{+/2} L_S L^{+/2}\| \le \varepsilon$.
  \emph{Status: set up correctly (unchanged).}
\item[Combinatorial layer:] Tur\'an $+$ Foster $+$ partial averages
  $+$ barrier greedy.
  \emph{Status: \textbf{complete} for $K_n$, $K_{a,b}$, cycles,
  grids, sparse graphs.  One quantitative gap ($M_t \ne 0$
  amplification) for dense general graphs.}
\item[Sparsification layer:] Adapt BSS from edges to vertices.
  \emph{Status: \textbf{bypassed entirely}.}
\end{description}

The breakthrough was not trying harder in the sparsification layer.
It was recognizing that the combinatorial layer---which was never
attempted---offers an elementary path that avoids the structural
obstruction entirely.  The \emph{second} breakthrough (partial averages)
came from recognizing that the leverage filter's Markov-based approach
was the wrong tool---the sum of selected leverage degrees matters, not
the maximum.

\textbf{Transferable principle:} When stuck, enumerate the layers
and assess status per layer.  Do not repeatedly attack the same
blocked layer.  Ask: ``What kind of problem is this?  Who would
find it easy?  What is the undergraduate proof?''

\begin{samepage}
\medskip
\noindent\textbf{Evidence ledger for Problem 6 Layer Switch (top commits).}
\begin{center}
\small
\begin{tabular}{@{}p{1.3cm}p{4.8cm}p{7.0cm}@{}}
\toprule
Commit & Role in pattern & Primary artifacts \\
\midrule
\texttt{5289ca8} & Starts the dispatch-driven GPL-H attack loop & Initial GPL-H dispatch note \\
\texttt{cc11834} & Counterexample + strategy split; blocks naive closure path & Counterexample note + verifier batch scripts \\
\texttt{73aa112} & Coaching-induced layer switch to elementary proof chain & P6 solution rewrite + layer-switch design note \\
\texttt{7e03174} & E+F hybrid reduction localizes remaining bridge & Direction E+F proof draft + MO evidence summary \\
\bottomrule
\end{tabular}
\normalsize
\end{center}
\end{samepage}

\chapter{Conclusion}

\section{Toward Real-Time Argumentation Structure}

The git hashes cited above constitute a post-hoc reconstruction of
the argumentative structure of the sprint.  Each pivotal commit
records a move: an obstruction identified, a layer switched, an
identity discovered, a closure achieved.  The connections between
moves---``this obstruction motivated that layer switch''---are
implicit in the temporal ordering but not represented explicitly.

This is a degenerate form of the Active Inference Framework
(AIF), in which argument moves are represented as typed states and
policy-relevant transitions (inferential, conflict, and preference edges).
The git log provides the nodes (commits) but not the edges
(argumentative relations).

The gap between post-hoc annotation and real-time capture is
precisely the gap between this chapter and a live AIF graph.
In a real-time system, each proof step, obstruction, and strategy
shift would be recorded as an AIF node at the moment it occurs,
with edges to its premises, targets, and alternatives.  The
``proof patterns'' analysis would then be a graph query, not
a retrospective essay.

Three infrastructure pieces converge on this:

\begin{enumerate}
\item \textbf{Arxana} (futon5) provides the typed-edge graph substrate.
\item \textbf{The peripheral model} (futon3c) provides scoped contexts
  in which moves are recorded as they happen.  A peripheral is not a
  separate process---it is a \emph{constrained execution environment}
  for the same agent, defined by a restricted tool set and scope.
  The agent hops between peripherals (explore, edit, test, deploy,
  reflect), and each hop transfers context while enforcing new
  constraints.  Real-time capture works by interleaving reflect
  hops with action hops: explore $\to$ reflect $\to$ edit $\to$
  reflect $\to$ test $\to$ reflect.  The reflect peripheral permits
  only log-reading---no editing, no deploying---so the agent
  produces argumentative annotation because that is the only
  permitted action.  The constraint is what makes reflection
  generative rather than reflexive.  This addresses the
  metacognitive overhead problem not by delegating to a separate
  scribe, but by repeatedly returning the agent to a mode where
  annotation is all it can do, while the work is still fresh.
\item \textbf{S-expression canonical form} provides the representation
  in which argument structure and mathematical content coexist
  without a separate annotation layer.
\end{enumerate}

To make this concrete, consider Problem~6's layer switch as an AIF
subgraph.  Five nodes suffice:

\begin{description}
\item[$n_1$:] \textsc{Claim}: ``BSS edge sparsification adapts to
  vertex selection.''
\item[$n_2$:] \textsc{Conflict}: ``Edge-to-vertex obstruction is
  structural: multi-rank atoms vs.\ rank-1 atoms.''
  (\texttt{cc11834})
\item[$n_3$:] \textsc{Preference}: ``Switch to the combinatorial
  layer---enumerate alternatives to sparsification.'' (coaching
  intervention)
\item[$n_4$:] \textsc{Claim}: ``Tur\'an $+$ Foster $+$ pigeonhole
  gives an elementary proof.'' (\texttt{73aa112})
\item[$n_5$:] \textsc{Inference}: $n_4$ resolves the problem that
  $n_2$ blocked in $n_1$.
\end{description}

\noindent The typed edges are: $n_2 \xrightarrow{\text{attacks}} n_1$,
$n_3 \xrightarrow{\text{preference}} n_4$ over $n_1$,
$n_5 \xrightarrow{\text{supports}} n_4$.  In S-expression form:

\begin{verbatim}
(aif:graph
 (node n1 :type claim
   :content "BSS adapts to vertex selection")
 (node n2 :type conflict :ref "cc11834"
   :content "edge-to-vertex obstruction is structural")
 (node n3 :type preference
   :content "switch to combinatorial layer")
 (node n4 :type claim :ref "73aa112"
   :content "Turan + Foster + pigeonhole proof")
 (node n5 :type inference
   :content "n4 resolves what n2 blocked in n1")
 (edge n2 n1 :type attacks)
 (edge n3 n4 :type preference :over n1)
 (edge n5 n4 :type supports))
\end{verbatim}

\noindent This is a five-node graph with three typed edges.  Arxana
stores it; the peripheral model would have produced these nodes
incrementally---$n_2$ during a reflect hop after the counterexample
commit, $n_3$ after the coaching intervention, $n_4$ after the
elementary proof landed---each time the agent hopped into reflect
and annotated its recent trace while the reasoning was still fresh.  The S-expression representation
makes the mathematical content (\texttt{:content}) and the
argumentative structure (\texttt{:type}, \texttt{:over}) cohabit
without a separate annotation layer.

With all three pieces, the sprint's argumentative structure would be
captured as it unfolds---not reconstructed from git hashes months
later.  The distance from the present chapter to that system is a
tooling gap, not a conceptual one---and the worked example above
shows that the gap is small.

This chapter is itself a case study for the infrastructure it
proposes.  Most of the ten problems were genuinely ``local''---they
yielded to focused work within a single layer, and the right
approach closed them quickly once found.  That track record creates
a disposition toward local play: attack the problem, find the trick,
close it.  Problem~6 was the exception that required whole-board
awareness, but by the time its hard phase began, nine successful
local closures had trained the expectation that persistence within
one approach would suffice.  The success pattern became the trap.

\columnratio{0.56}
\begin{paracol}{2}

Problem~6's later phase---after the elementary proof
landed---spent several hours exploring trajectory coupling,
amplification bounds, and spectral spread, all within a single
layer.  The $(\text{layer}, \text{status})$ monitor described in
Section~\ref{sec:learning} would have flagged stationarity and
forced a reflect hop that included external literature search.
When that search was finally performed (ad hoc, by dispatching a
second agent to scan MathOverflow), it returned several alternative
routes---including approaches that build on the existing partial
result but constitute a genuine layer switch.  The reflect hop
worked; it was just five hours late.  In Go, the analogous concept is \emph{whole-board play}: weaker
players get absorbed in local fights while stronger players
recognize when a local position is stalemated and play
\emph{tenuki}---elsewhere, where there are bigger points.  But in
true whole-board play there is no tenuki, because the player never
lost awareness of the whole board.  The same holds here: if reflect
hops are interleaved as the normal operating mode, there is no
TryHarder loop to break out of---the agent maintains whole-board
awareness throughout, and the layer switch happens as part of
ordinary play, not as a rescue.

\switchcolumn
\begin{processnote}[title={Sidebar: the layer switch that happened while writing this chapter}]

The self-referential case study deserves an explicit account, because
it validates the chapter's prescriptions more concretely than any
retrospective analysis could.

\medskip

\textbf{The stuck phase.}  After Problem~6's elementary proof landed
(Tur\'an $+$ Foster $+$ pigeonhole, proved for $K_n$), the remaining
gap was a single quantitative estimate: bounding the $H_t^{-1}$
amplification when the accumulated barrier matrix $M_t \ne 0$.  Two
agents spent several hours exploring this gap from within the same
layer---trajectory coupling, Neumann expansion bounds, spectral
spread analysis, amplification ratios.  Each cycle produced empirical
confirmation (440/440 greedy steps pass, amplification $\le 1.30$,
$\bar{d} \le 0.714$) but no proof.  The $(\text{layer},
\text{status})$ pair was stationary: barrier-greedy amplification,
open.

\textbf{The intervention.}  During the writing of this chapter---while
editing the very paragraphs about metacognitive interrupts and
whole-board play---a second agent was dispatched to scan
152{,}893 MathOverflow questions across six independent strategy tracks
(A--F), each with its own keyword bundle and contextual anchoring.

\textbf{The result.}  Track~E (expansion, conductance, spectral
geometry) dominated with 60 anchored-strong hits, versus 8 for the
next-best track.  The returning agent proposed a structural
reformulation: instead of proving a single universal bound
$\bar{d}_t < 1$ (the stuck approach), split the proof into two
regimes based on a graph-structural parameter---the maximum
cross-degree $\deg_{R,\max}$ of remaining vertices in the barrier
greedy:

\begin{description}
\item[E-regime] ($\deg_{R,\max} \le 2$): the local graph structure
  is sparse enough that the minimum spectral score $m_t$ stays below
  a universal threshold $\theta < 1$.  This is the regime where the
  existing Foster/partial-averages machinery is likely sufficient.
\item[F-regime] ($\deg_{R,\max} \ge 3$): high cross-degree gives
  structural control over the gain-loss decomposition.  A
  deterministic inequality $G_t > P_t$ (gain exceeds penalty) then
  certifies $m_t < 1$ via the proved ratio certificate and AR
  identity.
\end{description}

\noindent The formal reduction---if both regime lemmas hold at every
step, then GPL-H closes with a universal constant---is itself proved.
The open work is now two explicit lemmas tied to a concrete regime
separator, not an amorphous gap.  Empirical validation on graphs up
to $n = 40$ shows $\deg_{R,\max}$ has 0.973 correlation with the
hard transfer rows and separates them perfectly at threshold~3.

\textbf{What this demonstrates.}  The MathOverflow scan is
structurally a reflect hop: a constrained task (read external
literature, do not edit the proof) that returned with annotation
(here are the alternative approaches, ranked by precedent).  It
produced a genuine layer switch---from ``prove a universal
amplification bound'' to ``split by graph-adaptive regime and prove
two targeted lemmas.''  The new approach builds on the existing
partial result (the ratio certificate, the AR identity, the
pigeonhole mechanism) but reframes the open problem in a way that
the previous five hours of local play did not.

\medskip
\begin{wrapfigure}{r}{0.42\linewidth}
\vspace{-0.8\baselineskip}
\begin{tcolorbox}[
  title={Sub-sidebar: Postscript},
  colback=white,
  colframe=blue!45!black,
  boxrule=0.4pt,
  arc=1.5pt,
  left=3pt,
  right=3pt,
  top=2pt,
  bottom=2pt,
  fontupper=\scriptsize,
  fonttitle=\bfseries\sffamily\tiny,
  width=\linewidth
]
Cycle~7 falsified BMI ($\bar{d}_t<1$) in general graphs.
The construction still works; the target shifts to
vertex-level feasibility (No-Skip).

Operationally, the monitor should track \textsc{open}, \textsc{stuck},
and \textsc{falsified}.  A \textsc{falsified} state should trigger
layer abandonment, not just reframing.
\end{tcolorbox}
\vspace{-0.9\baselineskip}
\end{wrapfigure}

The layer switch happened \emph{during the writing of the chapter
that prescribes it}.  A reflect hop (dispatched ad hoc, but
functionally equivalent to a scheduled peripheral transition) broke
a multi-hour stationary phase and changed the proof policy quickly.
This is the chapter's claim: metacognitive
interrupts convert persistence-in-one-layer into layer switching.

\medskip
The same pattern appears in futon5 cellular automata: repeating
stripes trigger rule changes.  TryHarder loops are analogous.  Typed
layer/status annotation makes stationarity machine-visible, so the
monitor can trigger policy change automatically.
\par\WFclear
\end{processnote}

\switchcolumn
\section{Cross-Problem Learning}

An agent working on Problem~6 that could see Problem~7's history
would find: \emph{Problem~7 was blocked in one layer (codimension-1
surgery), characterized the obstruction, switched to a different
layer (codimension-2 surgery), and closed.}  This is directly
transferable advice.

More generally, the proof patterns across the ten problems suggest
a strategy checklist for open problems:

\begin{enumerate}
\item \textbf{Enumerate layers.}  What are the distinct mathematical
  frameworks in which this problem can be stated?
\item \textbf{Find the reduction in each layer.}  What does the problem
  reduce to from each perspective?
\item \textbf{Assess status per layer.}  Which reductions are complete,
  partial, or blocked?
\item \textbf{Characterize obstructions.}  For blocked layers, is the
  obstruction structural or technical?  Is there a known workaround?
\item \textbf{Look for analogues.}  Which other problems had a similar
  layer profile?  What strategy succeeded there?
\item \textbf{Decide the outcome.}  Is closure feasible from any layer?
  If not, state the conditional result and the open assumptions.
\end{enumerate}

This is case-based reasoning (CBR) applied to proof strategy.  The
``cases'' are the other problems in the sprint (or, with a larger
corpus, the indexed literature).  We call the key property
\textbf{strategy-level analogy}: the transfer operates at the level of
proof strategy, not mathematical content.  It is not that Problem~7's
rotation trick applies to Problem~6, but that Problem~7's
\emph{layer-switching strategy} might.  Strategy-level analogy is
what makes the checklist above actionable---a shared mathematical
technique would only transfer between cognate problems, but a shared
strategy pattern transfers between any problems with similar
obstruction profiles.

CBR has a well-known failure mode: \emph{superficial indexing}.  Within
a ten-problem sprint the case base is small enough that a human can
browse it exhaustively.  Scaling to the indexed literature requires a
retrieval mechanism, and the indexing question---what features of a
proof strategy are retrievable?---is itself a research problem.  The
checklist above implicitly proposes an index schema (layer structure,
obstruction type, resolution pattern), but whether these features
support effective retrieval at scale remains open.
\section{Learning from Success, Not Just Failure}
\label{sec:learning}

A recurring theme: the sessions that closed problems did so by
finding the right \emph{frame}, not by working harder within
the wrong one.  Problem~7 switched from reflections to rotations.
Problem~4 found an identity that restructured the algebra.
Problem~8 found a decomposition that made everything else fall out.
And Problem~6---after eight hours of TryHarder in the sparsification
layer---found an elementary combinatorial proof within fifteen minutes
of a coaching intervention that forced layer enumeration.

In every case the git log for the successful phase is a record of
\emph{reframing}, not persistence.  The git log for Problem~6's
failed phase is a record of persistence within the wrong frame.

For future sprints---human or AI---the design implications are:

\begin{enumerate}
\item \textbf{Make success histories as visible as failure histories.}
  An agent stuck on a problem should be prompted to consult the
  strategy traces of problems that closed, not just its own history
  of failed attempts.  Rumination is not learning.  Analogy is.
\item \textbf{Coach, don't dispatch.}  The intervention that broke
  Problem~6's TryHarder loop was not ``close the gap in Section~5''
  but ``what kind of problem is this?''  Dispatching generates another
  cycle.  Coaching generates a layer switch.
\end{enumerate}

\begin{enumerate}
\setcounter{enumi}{2}
\item \textbf{Ask pedagogical questions.}  ``How would you teach this
  to an undergraduate?'' forces identification of the elementary core.
  For Problem~6, the elementary core \emph{was} the proof.
\end{enumerate}

\end{paracol}

These three principles describe a \textbf{metacognitive interrupt}---a
mechanism that detects a TryHarder loop and triggers reframing.  In
reinforcement-learning terms, it is the difference between exploiting
within a fixed policy and switching policies.  The CS question is
whether the interrupt can fire automatically.  The git log provides
the detection signal: repeated commits within the same layer with no
status change is exactly the TryHarder signature.  Problem~6's failed
phase exhibits this pattern clearly---six dispatch-and-fail cycles
over eight hours, all targeting the sparsification layer, none
changing the layer assessment.  A monitor that tracks
$(\text{layer}, \text{status})$ pairs across commits and flags
stationarity after $k$ cycles would have triggered the reframing
prompt automatically.

\section{Summary}

\noindent\textbf{Thesis statement.}  Proof strategy should be
represented as a first-class typed graph object, because this makes
argument structure explicit, and supports effective validation.

\medskip

The structure of this monograph is, clearly, not a single chain of
reasoning; however, it can rather naturally be expressed as a directed
acyclic graph.  This summary necessarily linearises that structure on
the page; the associated wiring diagram
(Figure~\ref{fig:the-argument}) preserves more of it; the git
repository serves as the version of record.

\medskip

\noindent\textbf{Root.}  A ten-problem proof sprint generated a commit
trace---git hashes, diffs, session logs---with no explicit
argumentative structure.  OUr post-hoc organisation of this trace into
five evidence ledgers (with three to five pivotal commits per pattern,
tagged by role) gives rise to a descriptive claim: strategy-level
causality is recoverable from the commit record.

\medskip

\noindent\textbf{An illustrative episode} (Problem~6) grounds the
claim in a fully worked case: six dispatch-and-fail cycles over eight
hours in a single layer, followed by a coaching-induced layer switch
that produced an elementary proof within fifteen minutes.  Later, a
problem was found in that proof, and various new proofs were attempted,
eventually leading to an interesting-looking partial result.

\medskip

\noindent\textbf{Four independent paths} then lead from this
experience to the same prescriptive conclusion---to seek ways to
maintain continuity across a metacognitive interrupt.

\begin{enumerate}
\item \textbf{Formalisation.}  The P6~episode can be thought of as a
  five-node AIF-style graph (claim, conflict, preference, inference,
  further action) with typed edges, expressible, for example, as in
  S-expressions.  The typing makes the argumentative relations
  machine-queryable, and suggests a relationship to two developing
  infrastructure components: a typed-edge (hyper)graph store and a
  model of computational agency we refer to as peripherals, whereby
  processes of action and reflection produce argumentative structures
  driven by generation under constraint.  Agents interacting via
  peripherals can backed by a monitor that tracks $(\text{layer},
  \text{status})$ pairs across commits and flag when the same pair
  repeats without progress.  (In the current effort, we worked with a
  much reduced version of this architecture, in which the author
  played the role of monitor to agents with fewer native reflective
  abilities.)

\item \textbf{Live demonstrations.}  While writing about metacognitive
  interrupts, the agents relatively routinely produced them; one
  example we've cited above was a MathOverflow scan—an intuitive
  example of an as-if peripheral hop—which returned a regime split
  (E-track vs.\ F-track) that got the proof process unstuck from its
  single-bound target.  Prompting similarly produced changes in proof
  behaviour, as did the Lakatosian proofs-and-refutations style of
  working.  Our layer-switching claim was validated many times over
  during the sprint.

\item \textbf{Falsification.}  The same intervention led to the
  conclusion that the barrier-monotonicity inequality fails for
  general graphs---an exemplary \emph{challenge} in the wiring
  diagram, which stands in for a broader process language.  One
  possibility would be a three-state status code
  (Open\,/\,Stuck\,/\,Falsified) and a control rule:
  \textsc{Falsified} means abandon the layer entirely, not merely
  reframe.  The work as a whole retains failure as as a structurally
  load-bearing way of gaining evidence; naturally any successes in
  this project were built on many more failures!

\item \textbf{Cross-problem transfer.}  The patterns are not specific
  to Problem~6.  Problems~4, 7, and~8 exhibit similar
  dynamics---strategy transfers between problems at the strategy
  level, not the content level.  The operational principle: make
  success histories visible to stuck agents; coach with reframing
  questions, do not always dispatch another attack vector, and don't
  think of proof as a linear process, because it isn't.
\end{enumerate}

\noindent\textbf{Convergence.}  All four paths arrive at the
metacognitive interrupt, at which point the sprint mirrors the First
Proof provocation.  When argument structure is explicit and monitored,
the system can detect its own stuckness and intervene.  Two
conclusions follow.  A descriptive conclusion (\textbf{T0}): the
sprint's argumentative structure is recoverable by a structured
simplification across the the ten proof attempts.  Some
19~commit-attestations ground 27~typed edges across 23~nodes.  A
prescriptive conclusion (\textbf{T1}): build a system where this
capture happens in real time, not post-hoc.  That requires three
components converging: the argument we have developed, which becomes a
mission statement, the graph substrate which allows us to model
mathematical knowledge, and the peripheral model of computational
agency which allows us to work with such knowledge effectively.  The
distance between the two conclusions is a question of implementation.
