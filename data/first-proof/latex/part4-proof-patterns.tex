\chapter{Strategy Patterns Across Ten Problems}

The ten proofs in Part~II were not produced by a uniform method.
Some closed quickly; others required layer-switching, creative
reductions, or honest declarations of conditionality.
This chapter extracts the transferable strategy patterns---what
worked, what didn't, and what an agent stuck on one problem
could learn from the successes of another.

\section{Three Sprint Outcomes}

Every open problem in a sprint has one of three honest outcomes:

\begin{enumerate}
\item \textbf{Closed.} A complete proof (possibly with minor caveats).
  In the current QC pass, Problems 1, 2, 7, 8, and 9 have completed
  node-level validation runs but still carry unresolved verifier gaps
  (so they remain partial-validation at QC level); Problem~3 has a validation-complete
  existence path under a scoped criterion (uniqueness/irreducibility optional);
  Problem~10 is conditional under stated assumptions; Problem~5 is
  solved in a scope-limited form.
\item \textbf{Reduced.} A conditional result with clearly stated
  assumptions.
\item \textbf{Mapped.} The obstruction is characterized and adjacent
  problems are identified, but no proof is offered.
\end{enumerate}

All three are legitimate outcomes. Outcome~2 is not a defect of
outcome~1; it is a different kind of contribution.  The failure mode
is treating outcome~2 as something to repair rather than something
to state clearly. Problem~6 spent eight hours as outcome~2 before
a coaching intervention moved it to a strong partial closure (complete
for key classes, with one explicit remaining general-case gap)---the
layer switch, not additional effort, made the difference.

\section{Success Pattern: Layer-Switching (Problem 7)}

Problem~7 (uniform lattice with 2-torsion) is the clearest example
of a successful strategy pivot.

\begin{description}
\item[Initial approach:] Reflection lattice in even dimension.
  The E2 obligation (Fowler's criterion) was discharged.
\item[Obstruction:] The surgery obligation (S) was blocked.
  Equivariant surgery requires a codimension-2 gap hypothesis;
  reflections give codimension-1 fixed sets.  Blocked.
  (\texttt{0fa4e82}: ``restructure S-branch as open problem with three approaches'')
\item[Characterization:] Instead of pushing harder on reflections,
  the obstruction was characterized structurally: a dimension-parity
  tension between E2 (needs even $n$) and S (better in odd $n$).
\item[Layer switch:] Replace reflections (codimension~1) with rotations
  (codimension~2).  Now $n$ is odd, E2 still works (fixed set has
  odd dimension, $\chi = 0$), and the codimension-2 gap hypothesis
  is satisfied.
  (\texttt{287a41c}: ``rotation route (Approach~IV), paper reads, triage'';
   \texttt{620ed57}: ``resolve rotation lattice existence, discharge E2'')
\item[Closure:] The surgery obstruction vanishes (trivial normal
  bundle from the congruence condition).
  (\texttt{8ce9771}: ``flat-normal-bundle argument---rational obstruction vanishes'';
   \texttt{158bc4f}: ``integral obstruction vanishes---obligation~S discharged'')
\end{description}

\textbf{Transferable principle:} When blocked in one layer,
characterize the obstruction structurally before attempting to
overcome it.  The characterization may reveal that a different
layer avoids the obstruction entirely.

\section{Success Pattern: Creative Reduction (Problem 4)}

Problem~4 (root separation under finite free convolution)
progressed from numerical evidence to proof via a sequence
of reductions:

\begin{enumerate}
\item Centering reduction (translation invariance of $\Phi_n$).
\item For $n = 3$: discovery of the identity
  $\Phi_3 \cdot \mathrm{disc} = 18 \, a_2^2$ (exact, verified symbolically).
\item Recognition that the surplus expression has the form
  required by Titu's lemma (Engel form of Cauchy--Schwarz).
\end{enumerate}

Step~2 was the creative step---an identity discovered numerically
and verified symbolically
(\texttt{9db6b4f}: ``Haar orbit exploration and key identity finding'';
 \texttt{0003300}: ``Prove P4 superadditivity for $n=3$ via $\Phi_3 \cdot \mathrm{disc}$
 identity + Cauchy--Schwarz'').
The key was that the identity was
\emph{sought} because the reduction strategy demanded an algebraic
relationship between $\Phi_n$ and known invariants.  The strategy
shaped the search.  The subsequent extension to $n = 4$ followed a
different route---algebraic elimination plus computational verification
(\texttt{84c0041}: ``harden PHC parsing and record $n = 4$ computational
verification''); full certification of one sub-case remains pending.

\textbf{Transferable principle:} Reductions create demand for
specific identities or lemmas.  The demand makes the search
targeted rather than exploratory.

\section{Success Pattern: Structural Decomposition (Problem 8)}

Problem~8 (Lagrangian smoothing of polyhedral surfaces) was stuck
until the symplectic direct sum decomposition was discovered:
the 4-face Lagrangian condition forces $\mathbb{R}^4 = V_1 \oplus V_2$.
This single structural insight
(\texttt{09e23db}: ``symplectic direct sum forces Maslov index exactly~0'')
unlocked everything---Maslov index
vanishing, vertex smoothing via product structure
(\texttt{a5a4fbe}: ``replace crease smoothing + Lagrangian surgery with product smoothing''),
and the Hamiltonian isotopy property
(\texttt{de3e2ac}: ``justify Hamiltonian isotopy'').

\textbf{Transferable principle:} Look for structural decompositions
that simplify the problem globally, not just local fixes for individual
obstacles.

\section{From Failure to Progress: Problem 6's Layer Switch}

Problem~6 (epsilon-light subsets) is the sprint's most instructive
case study because it exhibits both patterns: a TryHarder loop
\emph{and} the layer-switch that broke it.

\subsection{Phase 1: The TryHarder Loop}

The initial approach generated six or more handoff
and dispatch documents across multiple sessions, from the initial
dispatch (\texttt{5289ca8}: ``GPL-H attack dispatch'') through
Directions~A--D (\texttt{63a23ba}, \texttt{22c091f}, \texttt{b6a7625},
\texttt{d64fd13}) to closure attempts
(\texttt{78d94bc}: ``GPL-H closure attempt---all scores $< 1$'')
and counterexamples (\texttt{cc11834}: ``$K_{t,r}$ counterexample'').
The pattern:

\begin{enumerate}
\item Identify the gap (Assumption~V: vertex-induced selection).
\item Attempt closure via a specific attack vector.
\item Fail.
\item Generate a new handoff document dispatching another attack.
\item Repeat from step~2.
\end{enumerate}

All closure attempts targeted the sparsification layer (adapting
Batson--Spielman--Srivastava edge sparsification to vertex selection)
without recognizing that the obstruction there might be structural---not a
gap to fill but a genuine mismatch between edge and vertex selection.

\subsection{The coaching intervention}

A human coaching prompt forced layer enumeration:

\begin{quote}
\emph{What kind of problem is this?  What kind of proof applies?
How would you teach it to an undergraduate?  What kind of person
finds this easy?  Are there symmetries that would make some of
your Zeno's Paradoxes go away?}
\end{quote}

Each question targets a specific reframing:

\begin{description}
\item[``What kind of problem?''] Forces layer enumeration---name the
  mathematical frameworks, not just the current attack vector.
\item[``Teach it to an undergraduate?''] Forces identification of
  what is \emph{elementary} about the problem.  The elementary core
  is often the proof.
\item[``Who finds this easy?''] Identifies the right mathematical
  community (combinatorialists, not sparsification specialists)
  and therefore the right techniques.
\item[``Symmetries / Zeno's Paradoxes?''] Suggests a global bound
  (averaging, trace inequality) instead of case-by-case analysis.
\end{description}

\subsection{Phase 2: The elementary proof}

After 15 minutes of thinking, the agent found a proof chain that
bypasses the sparsification layer entirely:

\begin{enumerate}
\item \textbf{Tur\'an's theorem} gives an independent set $I_0$
  with $|I_0| \ge \varepsilon n/3$ in the heavy-edge subgraph.
  All edges internal to $I_0$ are light ($\tau_e \le \varepsilon$).
\item \textbf{Barrier greedy}: at each step, pick the vertex $v$
  with minimum $\|Y_t(v)\|$.
\item \textbf{PSD trace bound}: $\|Y\| \le \operatorname{tr}(Y)$
  for any PSD matrix~$Y$.
\item \textbf{Pigeonhole}: if the average trace
  $\bar{d}_t = \frac{1}{r_t}\sum_v \operatorname{tr}(Y_t(v)) < 1$,
  then some $v$ has $\operatorname{tr}(Y_t(v)) < 1$, therefore
  $\|Y_t(v)\| < 1$, therefore the barrier is maintained.
\end{enumerate}

For $K_n$: $\bar{d}_t = 2t/(n\varepsilon)$, so at $T = \varepsilon n/3$
steps, $\bar{d}_T = 2/3 < 1$.  This gives $|S| = \varepsilon n/3$
with $c = 1/3$, \textbf{proved exactly}.

\subsection{Phase 3: The partial averages breakthrough}

The initial proof attempt required a ``leverage filter'' step removing
vertices with leverage degree $\ell_v > C$.  This created an
irreconcilable tension: the Markov bound needed $C > 2$ to retain
enough vertices, but $\bar{d} < 1$ needed $C < 2$.  Testing on
$K_{a,b}$ confirmed this was not just a technical gap---up to $80\%$
of $I_0$ can have $\ell_v \ge 2$.

The resolution came from switching from a \emph{maximum-based} bound
to a \emph{sum-based} bound.  The \textbf{partial averages inequality}
observes that the average of the $T$ smallest leverage degrees cannot
exceed the overall average:
\[
  \frac{1}{T}\sum_{k=1}^{T} \ell_{(k)}
    \;\le\; \operatorname{avg}(\ell)
    \;<\; 2 \quad\text{(Foster on $I_0$)}.
\]
A ``min-$\ell$ greedy'' that selects vertices in order of increasing
leverage degree therefore accumulates $\sum \ell < 2T$, giving
\[
  \bar{d}_t \;\le\; \frac{2/3}{1 - \varepsilon/3} \;<\; 1
  \quad\text{for all $\varepsilon \in (0,1)$,\; at $M_t = 0$.}
\]

This eliminates the leverage filter entirely---no threshold $C$ needed,
no Markov bound, no structural assumption on maximum leverage degree.

\subsection{What remains}

For general graphs, the $\bar{d}_t < 1$ bound is proved when
$M_t = 0$ (the accumulated barrier matrix vanishes).  This covers
$K_n$, $K_{a,b}$ with $a \ne b$, cycles, grids, and sparse graphs.
For dense graphs where $M_t \ne 0$, the bound is verified
numerically at all 440 nontrivial greedy steps (max
$\bar{d} = 0.714$, amplification ratio $\le 1.30$) but one
quantitative estimate---bounding the $H_t^{-1}$ amplification
when $M_t \ne 0$---remains open.

\textit{Postscript (Feb 13 update):} the open bridge was further
localized via an E+F hybrid reduction
(\texttt{7e03174}): an E-regime (graph-adaptive transfer)
plus an F-regime (gain-loss balance) suffices for full stepwise
closure.  The reduction itself is proved; two regime lemmas remain.

No interlacing families.  No Borcea--Br\"and\'en.  No MSS theorem.
Just Foster's theorem $+$ partial averages $+$ PSD trace bound $+$
pigeonhole $+$ Tur\'an.  A three-line argument (PSD $\to$ pigeonhole
$\to$ existence) replaces the entire real stability machinery.

\subsection{The layer analysis}

\begin{description}
\item[Spectral layer:] Bound $\|L^{+/2} L_S L^{+/2}\| \le \varepsilon$.
  \emph{Status: set up correctly (unchanged).}
\item[Combinatorial layer:] Tur\'an $+$ Foster $+$ partial averages
  $+$ barrier greedy.
  \emph{Status: \textbf{complete} for $K_n$, $K_{a,b}$, cycles,
  grids, sparse graphs.  One quantitative gap ($M_t \ne 0$
  amplification) for dense general graphs.}
\item[Sparsification layer:] Adapt BSS from edges to vertices.
  \emph{Status: \textbf{bypassed entirely}.}
\end{description}

The breakthrough was not trying harder in the sparsification layer.
It was recognizing that the combinatorial layer---which was never
attempted---offers an elementary path that avoids the structural
obstruction entirely.  The \emph{second} breakthrough (partial averages)
came from recognizing that the leverage filter's Markov-based approach
was the wrong tool---the sum of selected leverage degrees matters, not
the maximum.

\textbf{Transferable principle:} When stuck, enumerate the layers
and assess status per layer.  Do not repeatedly attack the same
blocked layer.  Ask: ``What kind of problem is this?  Who would
find it easy?  What is the undergraduate proof?''

\section{Cross-Problem Learning}

An agent working on Problem~6 that could see Problem~7's history
would find: \emph{Problem~7 was blocked in one layer (codimension-1
surgery), characterized the obstruction, switched to a different
layer (codimension-2 surgery), and closed.}  This is directly
transferable advice.

More generally, the proof patterns across the ten problems suggest
a strategy checklist for open problems:

\begin{enumerate}
\item \textbf{Enumerate layers.}  What are the distinct mathematical
  frameworks in which this problem can be stated?
\item \textbf{Find the reduction in each layer.}  What does the problem
  reduce to from each perspective?
\item \textbf{Assess status per layer.}  Which reductions are complete,
  partial, or blocked?
\item \textbf{Characterize obstructions.}  For blocked layers, is the
  obstruction structural or technical?  Is there a known workaround?
\item \textbf{Look for analogues.}  Which other problems had a similar
  layer profile?  What strategy succeeded there?
\item \textbf{Decide the outcome.}  Is closure feasible from any layer?
  If not, state the conditional result and the open assumptions.
\end{enumerate}

This is case-based reasoning (CBR) applied to proof strategy.  The
``cases'' are the other problems in the sprint (or, with a larger
corpus, the indexed literature).  We call the key property
\textbf{strategy-level analogy}: the transfer operates at the level of
proof strategy, not mathematical content.  It is not that Problem~7's
rotation trick applies to Problem~6, but that Problem~7's
\emph{layer-switching strategy} might.  Strategy-level analogy is
what makes the checklist above actionable---a shared mathematical
technique would only transfer between cognate problems, but a shared
strategy pattern transfers between any problems with similar
obstruction profiles.

CBR has a well-known failure mode: \emph{superficial indexing}.  Within
a ten-problem sprint the case base is small enough that a human can
browse it exhaustively.  Scaling to the indexed literature requires a
retrieval mechanism, and the indexing question---what features of a
proof strategy are retrievable?---is itself a research problem.  The
checklist above implicitly proposes an index schema (layer structure,
obstruction type, resolution pattern), but whether these features
support effective retrieval at scale remains open.

\section{Learning from Success, Not Just Failure}
\label{sec:learning}

A recurring theme: the sessions that closed problems did so by
finding the right \emph{frame}, not by working harder within
the wrong one.  Problem~7 switched from reflections to rotations.
Problem~4 found an identity that restructured the algebra.
Problem~8 found a decomposition that made everything else fall out.
And Problem~6---after eight hours of TryHarder in the sparsification
layer---found an elementary combinatorial proof within fifteen minutes
of a coaching intervention that forced layer enumeration.

In every case the git log for the successful phase is a record of
\emph{reframing}, not persistence.  The git log for Problem~6's
failed phase is a record of persistence within the wrong frame.

For future sprints---human or AI---the design implications are:

\begin{enumerate}
\item \textbf{Make success histories as visible as failure histories.}
  An agent stuck on a problem should be prompted to consult the
  strategy traces of problems that closed, not just its own history
  of failed attempts.  Rumination is not learning.  Analogy is.
\item \textbf{Coach, don't dispatch.}  The intervention that broke
  Problem~6's TryHarder loop was not ``close the gap in Section~5''
  but ``what kind of problem is this?''  Dispatching generates another
  cycle.  Coaching generates a layer switch.
\item \textbf{Ask pedagogical questions.}  ``How would you teach this
  to an undergraduate?'' forces identification of the elementary core.
  For Problem~6, the elementary core \emph{was} the proof.
\end{enumerate}

These three principles describe a \textbf{metacognitive interrupt}---a
mechanism that detects a TryHarder loop and triggers reframing.  In
reinforcement-learning terms, it is the difference between exploiting
within a fixed policy and switching policies.  The CS question is
whether the interrupt can fire automatically.  The git log provides
the detection signal: repeated commits within the same layer with no
status change is exactly the TryHarder signature.  Problem~6's failed
phase exhibits this pattern clearly---six dispatch-and-fail cycles
over eight hours, all targeting the sparsification layer, none
changing the layer assessment.  A monitor that tracks
$(\text{layer}, \text{status})$ pairs across commits and flags
stationarity after $k$ cycles would have triggered the reframing
prompt automatically.

\section{Toward Real-Time Argumentation Structure}

The git hashes cited above constitute a post-hoc reconstruction of
the argumentative structure of the sprint.  Each pivotal commit
records a move: an obstruction identified, a layer switched, an
identity discovered, a closure achieved.  The connections between
moves---``this obstruction motivated that layer switch''---are
implicit in the temporal ordering but not represented explicitly.

This is a degenerate form of the Active Inference Framework
(AIF), in which argument moves are represented as typed states and
policy-relevant transitions (inferential, conflict, and preference edges).
The git log provides the nodes (commits) but not the edges
(argumentative relations).

The gap between post-hoc annotation and real-time capture is
precisely the gap between this chapter and a live AIF graph.
In a real-time system, each proof step, obstruction, and strategy
shift would be recorded as an AIF node at the moment it occurs,
with edges to its premises, targets, and alternatives.  The
``proof patterns'' analysis would then be a graph query, not
a retrospective essay.

Three infrastructure pieces converge on this:

\begin{enumerate}
\item \textbf{Arxana} (futon5) provides the typed-edge graph substrate.
\item \textbf{The peripheral model} (futon3c) provides scoped contexts
  in which moves are recorded as they happen.  A peripheral is not a
  separate process---it is a \emph{constrained execution environment}
  for the same agent, defined by a restricted tool set and scope.
  The agent hops between peripherals (explore, edit, test, deploy,
  reflect), and each hop transfers context while enforcing new
  constraints.  Real-time capture works by interleaving reflect
  hops with action hops: explore $\to$ reflect $\to$ edit $\to$
  reflect $\to$ test $\to$ reflect.  The reflect peripheral permits
  only log-reading---no editing, no deploying---so the agent
  produces argumentative annotation because that is the only
  permitted action.  The constraint is what makes reflection
  generative rather than reflexive.  This addresses the
  metacognitive overhead problem not by delegating to a separate
  scribe, but by repeatedly returning the agent to a mode where
  annotation is all it can do, while the work is still fresh.
\item \textbf{S-expression canonical form} provides the representation
  in which argument structure and mathematical content coexist
  without a separate annotation layer.
\end{enumerate}

To make this concrete, consider Problem~6's layer switch as an AIF
subgraph.  Five nodes suffice:

\begin{description}
\item[$n_1$:] \textsc{Claim}: ``BSS edge sparsification adapts to
  vertex selection.''
\item[$n_2$:] \textsc{Conflict}: ``Edge-to-vertex obstruction is
  structural: multi-rank atoms vs.\ rank-1 atoms.''
  (\texttt{cc11834})
\item[$n_3$:] \textsc{Preference}: ``Switch to the combinatorial
  layer---enumerate alternatives to sparsification.'' (coaching
  intervention)
\item[$n_4$:] \textsc{Claim}: ``Tur\'an $+$ Foster $+$ pigeonhole
  gives an elementary proof.'' (\texttt{73aa112})
\item[$n_5$:] \textsc{Inference}: $n_4$ resolves the problem that
  $n_2$ blocked in $n_1$.
\end{description}

\noindent The typed edges are: $n_2 \xrightarrow{\text{attacks}} n_1$,
$n_3 \xrightarrow{\text{preference}} n_4$ over $n_1$,
$n_5 \xrightarrow{\text{supports}} n_4$.  In S-expression form:

\begin{verbatim}
(aif:graph
 (node n1 :type claim
   :content "BSS adapts to vertex selection")
 (node n2 :type conflict :ref "cc11834"
   :content "edge-to-vertex obstruction is structural")
 (node n3 :type preference
   :content "switch to combinatorial layer")
 (node n4 :type claim :ref "73aa112"
   :content "Turan + Foster + pigeonhole proof")
 (node n5 :type inference
   :content "n4 resolves what n2 blocked in n1")
 (edge n2 n1 :type attacks)
 (edge n3 n4 :type preference :over n1)
 (edge n5 n4 :type supports))
\end{verbatim}

\noindent This is a five-node graph with three typed edges.  Arxana
stores it; the peripheral model would have produced these nodes
incrementally---$n_2$ during a reflect hop after the counterexample
commit, $n_3$ after the coaching intervention, $n_4$ after the
elementary proof landed---each time the agent hopped into reflect
and annotated its recent trace while the reasoning was still fresh.  The S-expression representation
makes the mathematical content (\texttt{:content}) and the
argumentative structure (\texttt{:type}, \texttt{:over}) cohabit
without a separate annotation layer.

With all three pieces, the sprint's argumentative structure would be
captured as it unfolds---not reconstructed from git hashes months
later.  The distance from the present chapter to that system is a
tooling gap, not a conceptual one---and the worked example above
shows that the gap is small.

This chapter is itself a case study for the infrastructure it
proposes.  Most of the ten problems were genuinely ``local''---they
yielded to focused work within a single layer, and the right
approach closed them quickly once found.  That track record creates
a disposition toward local play: attack the problem, find the trick,
close it.  Problem~6 was the exception that required whole-board
awareness, but by the time its hard phase began, nine successful
local closures had trained the expectation that persistence within
one approach would suffice.  The success pattern became the trap.

Problem~6's later phase---after the elementary proof
landed---spent several hours exploring trajectory coupling,
amplification bounds, and spectral spread, all within a single
layer.  The $(\text{layer}, \text{status})$ monitor described in
Section~\ref{sec:learning} would have flagged stationarity and
forced a reflect hop that included external literature search.
When that search was finally performed (ad hoc, by dispatching a
second agent to scan MathOverflow), it returned several alternative
routes---including approaches that build on the existing partial
result but constitute a genuine layer switch.  The reflect hop
worked; it was just five hours late.  In Go, the analogous concept is \emph{whole-board play}: weaker
players get absorbed in local fights while stronger players
recognize when a local position is stalemated and play
\emph{tenuki}---elsewhere, where there are bigger points.  But in
true whole-board play there is no tenuki, because the player never
lost awareness of the whole board.  The same holds here: if reflect
hops are interleaved as the normal operating mode, there is no
TryHarder loop to break out of---the agent maintains whole-board
awareness throughout, and the layer switch happens as part of
ordinary play, not as a rescue.

\subsection{Sidebar: the layer switch that happened while writing this chapter}

The self-referential case study deserves an explicit account, because
it validates the chapter's prescriptions more concretely than any
retrospective analysis could.

\medskip

\textbf{The stuck phase.}  After Problem~6's elementary proof landed
(Tur\'an $+$ Foster $+$ pigeonhole, proved for $K_n$), the remaining
gap was a single quantitative estimate: bounding the $H_t^{-1}$
amplification when the accumulated barrier matrix $M_t \ne 0$.  Two
agents spent several hours exploring this gap from within the same
layer---trajectory coupling, Neumann expansion bounds, spectral
spread analysis, amplification ratios.  Each cycle produced empirical
confirmation (440/440 greedy steps pass, amplification $\le 1.30$,
$\bar{d} \le 0.714$) but no proof.  The $(\text{layer},
\text{status})$ pair was stationary: barrier-greedy amplification,
open.

\textbf{The intervention.}  During the writing of this chapter---while
editing the very paragraphs about metacognitive interrupts and
whole-board play---a second agent was dispatched to scan
152{,}893 MathOverflow questions across six independent strategy tracks
(A--F), each with its own keyword bundle and contextual anchoring.

\textbf{The result.}  Track~E (expansion, conductance, spectral
geometry) dominated with 60 anchored-strong hits, versus 8 for the
next-best track.  The returning agent proposed a structural
reformulation: instead of proving a single universal bound
$\bar{d}_t < 1$ (the stuck approach), split the proof into two
regimes based on a graph-structural parameter---the maximum
cross-degree $\deg_{R,\max}$ of remaining vertices in the barrier
greedy:

\begin{description}
\item[E-regime] ($\deg_{R,\max} \le 2$): the local graph structure
  is sparse enough that the minimum spectral score $m_t$ stays below
  a universal threshold $\theta < 1$.  This is the regime where the
  existing Foster/partial-averages machinery is likely sufficient.
\item[F-regime] ($\deg_{R,\max} \ge 3$): high cross-degree gives
  structural control over the gain-loss decomposition.  A
  deterministic inequality $G_t > P_t$ (gain exceeds penalty) then
  certifies $m_t < 1$ via the proved ratio certificate and AR
  identity.
\end{description}

\noindent The formal reduction---if both regime lemmas hold at every
step, then GPL-H closes with a universal constant---is itself proved.
The open work is now two explicit lemmas tied to a concrete regime
separator, not an amorphous gap.  Empirical validation on graphs up
to $n = 40$ shows $\deg_{R,\max}$ has 0.973 correlation with the
hard transfer rows and separates them perfectly at threshold~3.

\textbf{What this demonstrates.}  The MathOverflow scan is
structurally a reflect hop: a constrained task (read external
literature, do not edit the proof) that returned with annotation
(here are the alternative approaches, ranked by precedent).  It
produced a genuine layer switch---from ``prove a universal
amplification bound'' to ``split by graph-adaptive regime and prove
two targeted lemmas.''  The new approach builds on the existing
partial result (the ratio certificate, the AR identity, the
pigeonhole mechanism) but reframes the open problem in a way that
the previous five hours of local play did not.

The layer switch happened \emph{during the writing of the chapter
that prescribes it}.  The chapter argued that a peripheral model
with interleaved reflect hops would prevent TryHarder loops.  The
sprint then demonstrated exactly that: a reflect hop (dispatched ad
hoc, but functionally identical to a scheduled peripheral
transition) broke a multi-hour stationary phase and produced a
changed approach within thirty minutes.  The prescriptive content
of the chapter and the lived experience of writing it converged on
the same point---not because the chapter caused the intervention,
but because both are responses to the same structural pattern.
The infrastructure this chapter proposes would make such
interventions automatic rather than accidental.

The stationarity detection problem has a direct analogue in the
meta-meta-cellular automata of futon5: when a CA rule produces
static or periodic behavior, the spacetime diagram shows
\emph{stripes}---identical states repeating across timesteps.
The meta-level rule fires when stripes are detected, replacing the
CA rule itself.  The git log of a TryHarder loop is a spacetime
diagram with stripes: the same layer and status repeating across
commits.  The AIF annotation proposed above is what makes the
stripes visible---without typed edges labeling each commit's layer,
the stationarity is present in the data but invisible to the
monitor.  The MMCA provides the detection mechanism; the peripheral
model provides the intervention; the AIF graph provides the
representation that connects them.

\subsection{Postscript: when the averaging argument itself is false}

The sidebar above describes an intervention that reframed
$\bar{d}_t < 1$ from a universal bound to a two-regime argument.
Cycle~7 (several days later) produced a stronger result: the
Barrier Maintenance Invariant ($\bar{d}_t < 1$ at every step) is
\emph{false} for general graphs.  Twelve base-suite configurations
have $\bar{d} \ge 1$ (worst: 1.739).  The pigeonhole argument
that converted $\bar{d} < 1$ to ``$\exists v$ with $\|Y_t(v)\| < 1$''
is valid but unreachable---its hypothesis fails.

And yet the construction still works.  Zero skips in 1,111 runs.
The leverage-ordered greedy processes feasible vertices first,
not because the average is below~1, but because leverage ordering
concentrates small contributions at the front of the queue.  The
gap shifted from an averaging statement (BMI) to a vertex-level
feasibility statement: the \textbf{No-Skip Conjecture} (at each
step, the next leverage-ordered vertex is feasible).

This is a second-order layer switch.  The E+F hybrid from the
sidebar was a layer switch within the averaging framework: split
the average by regime.  The BMI falsification is a layer switch
\emph{away from averaging entirely}: the proof must work at the
vertex level, not the average level.  The sidebar's intervention
was correct in spirit (``the stuck approach is stuck; reframe'')
but still within the same layer.  A monitor that tracked not just
stationarity but \emph{falsification}---detecting that a hypothesis
is not merely unproved but disproved---would have triggered the
larger reframing earlier.

The practical implication for the proposed infrastructure: the
$(\text{layer}, \text{status})$ monitor should distinguish three
status values: \textsc{open} (no evidence either way),
\textsc{stuck} (repeated attempts, no progress), and
\textsc{falsified} (counterexample found).  The response to
\textsc{stuck} is a reflect hop (reframe within the current
approach).  The response to \textsc{falsified} is a layer switch
(abandon the current approach).  The Problem~6 continuation
demonstrates both transitions: five hours of \textsc{stuck}
before the sidebar's reflect hop, then several more cycles before
Cycle~7's \textsc{falsified} forced the larger reframing.
