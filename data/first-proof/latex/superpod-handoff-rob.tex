\documentclass[11pt]{article}
\usepackage[margin=1.0in]{geometry}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{lmodern}
\usepackage{enumitem}
\usepackage{setspace}
\usepackage{hyperref}
\usepackage{microtype}
\usepackage{fancyvrb}

\hypersetup{
  colorlinks=true,
  linkcolor=blue,
  urlcolor=blue,
  pdftitle={Superpod Handoff Runbook},
  pdfauthor={Joseph Corneli}
}

\setlength{\parskip}{0.45em}
\setlength{\parindent}{0pt}
\setlist[itemize]{itemsep=0.2em,topsep=0.2em}
\setstretch{1.08}

\title{\textbf{Superpod Handoff Runbook}\\[2pt]
\large Single-Command Execution}
\author{Joe Corneli}
\date{February 16, 2026}

\begin{document}
\maketitle

\section*{1. Run Location}

Run on the Superpod execution host after cloning \texttt{futon6}.

\begin{Verbatim}[fontsize=\small]
git clone <futon6-repo-url>
cd futon6
\end{Verbatim}

\section*{2. Single Command (required path)}

\begin{Verbatim}[fontsize=\small]
bash scripts/handoff-superpod-all.sh
\end{Verbatim}

What it does (with step-by-step progress and hard fail gates):
\begin{itemize}
\item bootstrap inputs
\item sanity tests
\item smoke run + verification
\item CPU baseline runs + verification
\item required GPU backfill runs + verification
\item packaging
\end{itemize}

\section*{3. Fast Verification Mode}

\begin{Verbatim}[fontsize=\small]
bash scripts/handoff-superpod-all.sh --smoke-only
\end{Verbatim}

\section*{4. Deliverables}

Expected outputs:
\begin{itemize}
\item \texttt{superpod-math-processed.tar.gz}
\item \texttt{superpod-mo-processed.tar.gz}
\item \texttt{superpod-math-processed-gpu.tar.gz}
\item \texttt{superpod-mo-processed-gpu.tar.gz}
\end{itemize}

\section*{5. Return Payload}

Send back:
\begin{itemize}
\item all 4 tarballs above
\item short metric table from CPU and GPU \texttt{manifest.json} files:
\item \texttt{entity\_count}
\item \texttt{stage5\_stats.total\_ner\_hits}
\item \texttt{stage7\_stats.threads\_processed}
\item \texttt{stage7\_stats.total\_nodes}
\item \texttt{stage7\_stats.total\_edges}
\item \texttt{stage7\_stats.n\_categorical}
\item \texttt{stage7\_stats.n\_port\_matches}
\end{itemize}

\section*{6. Notes}

\begin{itemize}
\item \texttt{--skip-bootstrap} and \texttt{--skip-tests} are available for reruns only.
\item \texttt{scripts/handoff-superpod-all.sh} is the source of truth.
\item Companion machine-readable note:
\texttt{data/first-proof/superpod-handoff-rob.lit.md}
\end{itemize}

\newpage
\section*{7. Mission Wiring Diagram (futon5 style)}

\textbf{Intent.} Convert raw public math Q/A corpora into verified typed
wiring artifacts that can be queried by downstream proof work.

\textbf{Legend.} \texttt{M*=control}, \texttt{D*=download/input},
\texttt{P*=process}, \texttt{V*=verify}, \texttt{O*=output}.

\begin{Verbatim}[fontsize=\scriptsize]
Mission Control (single command)
  M0: bash scripts/handoff-superpod-all.sh
      |
      +--> M1 bootstrap: scripts/handoff-superpod-bootstrap.sh
      |       |
      |       +--> D1 math.stackexchange.com (Posts.xml, Comments.xml)
      |       +--> D2 mathoverflow.net       (Posts.xml, Comments.xml)
      |       +--> D3 local refs (nlab-ct-reference.json, terms.tsv)
      |
      +--> M2 tests: pytest smoke + verifier checks
      |
      +--> M3 smoke gate (mini dataset, 4 threads)
      |       |
      |       +--> V1 verify required files + edges_checked > 0
      |
      +--> M4 CPU baseline run (math.SE + MO; stages 1/5/7)
      |       |
      |       +--> P1 parse XML posts/comments
      |       +--> P2 NER + scope extraction
      |       +--> P3 CT-backed thread wiring assembly
      |       +--> V2 manifest sanity + ct-verifier
      |       +--> O1 math-processed/
      |       +--> O2 mo-processed/
      |
      +--> M5 required GPU backfill (math.SE + MO; full stages 1..7)
      |       |
      |       +--> P4 embeddings (GPU)
      |       +--> P5 LLM pattern tagging (GPU)
      |       +--> P6 clustering + reverse morphogenesis
      |       +--> V3 ct-verifier refresh
      |       +--> O3 math-processed-gpu/
      |       +--> O4 mo-processed-gpu/
      |
      +--> M6 packaging
              |
              +--> O5 superpod-math-processed.tar.gz
              +--> O6 superpod-mo-processed.tar.gz
              +--> O7 superpod-math-processed-gpu.tar.gz
              +--> O8 superpod-mo-processed-gpu.tar.gz
\end{Verbatim}

\textbf{Invariants enforced by the orchestrator.}
\begin{itemize}
\item Required artifacts exist: manifest, CT wiring output, and CT verifier output.
\item \texttt{stage7\_stats.ct\_backed=true},
\texttt{stage7\_stats.threads\_processed > 0},
\texttt{edges\_checked > 0}; otherwise fail hard.
\end{itemize}

\newpage
\section*{8. Why this work is interesting and valuable}

This run is not just data collection. It produces a reusable evidence layer for
math reasoning work: each thread becomes a typed wiring object with explicit
nodes, edges, and port matches, not just text.

That is valuable for two reasons. First, retrieval quality improves: we can ask
for threads that match an input/output proof shape, not only threads that share
keywords. Second, verification quality improves: \texttt{ct-verifier} checks
the resulting structure and blocks empty artifacts.

The CPU and GPU outputs are complementary, not interchangeable. CPU gives a
deterministic baseline and immediate wiring products. GPU backfill adds richer
semantic signals (embeddings and LLM-derived structure) over the same corpus.
Keeping both lets us compare quality and track where extra compute changes
results.

At project level, this turns a one-off run into infrastructure. The same
pipeline can be rerun, audited, and diffed over time, so Rob can evaluate
whether later changes improve signal, regress quality, or break invariants.

\end{document}
