\hypertarget{problem-10-rkhs-constrained-tensor-cp-via-preconditioned-conjugate-gradient}{%
\section{Problem 10: RKHS-Constrained Tensor CP via Preconditioned Conjugate Gradient}\label{problem-10-rkhs-constrained-tensor-cp-via-preconditioned-conjugate-gradient}}

\hypertarget{problem-statement}{%
\subsection{Problem Statement}\label{problem-statement}}

Given the mode-k subproblem of RKHS-constrained CP decomposition with
missing data, solve

\[[(Z \times K)^{T} D (Z \times K) + \lambda (I_{r} \times K)] vec(W) = (I_{r} \times K) vec(B),\]

where \(D = S S^{T}\) is the observation projector (q observed entries out of
N = nM), \(B = T\) Z, and n, r \textless\textless{} q \textless\textless{} N.

\hypertarget{assumptions-used-explicit}{%
\subsection{Assumptions Used (explicit)}\label{assumptions-used-explicit}}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \(\lambda > \mNumber{0}\).
\item
  For standard PCG and Cholesky-based preconditioning, use a PD kernel
  \(K_{\tau} = K \mBridgeOperator{+} \tau I_{n}\) with \(\tau > \mNumber{0}\) (or assume K is already PD).
\item
  S is a selection operator, so D is diagonal/projector and sparse by index list.
\end{enumerate}

Then the solved system is

\[\begin{aligned}
A_{\tau} x = b_{\tau}, \\
A_{\tau} = (Z \times K_{\tau})^{T} D (Z \times K_{\tau}) + \lambda (I_{r} \times K_{\tau}), \\
x = vec(W), \\
b_{\tau} = (I_{r} \times K_{\tau}) vec(B).
\end{aligned}\]

Under these assumptions \(A_{\tau}\) is SPD, so PCG applies.

\hypertarget{solution}{%
\subsection{Solution}\label{solution}}

\hypertarget{1-why-naive-direct-methods-fail}{%
\subsubsection{1. Why naive direct methods fail}\label{1-why-naive-direct-methods-fail}}

\(A_{\tau}\) is an (nr) x (nr) system. Dense direct factorization costs
\(O((nr)^\mNumber{3}) = O(n^\mNumber{3} r^\mNumber{3})\).

A naive explicit route also materializes \(\Phi = Z \times K_{\tau} \in R^{N \times nr}\),
which costs \(O(N n r)\) memory/work before factorization. This is the
\(N\)-dependent bottleneck we avoid with matrix-free PCG.

\hypertarget{2-implicit-matrix-vector-product-in-on2-r--q-r}{%
\subsubsection{\texorpdfstring{2. Implicit matrix-vector product in \(O(n^\mNumber{2} r + q r)\)}{2. Implicit matrix-vector product in O(n\^{}\textbackslash mNumber\{2\} r + q r)}}\label{2-implicit-matrix-vector-product-in-on2-r--q-r}}

CG needs only \(y = A_{\tau}\) x, not \(A_{\tau}\) explicitly.

Given x = vec(V), \(V \in R\)\^{}\{n x r\}:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \(U = K_{\tau}\) V. Cost \(O(n^\mNumber{2} r)\).
\item
  Forward sampled action (only observed entries):

  \[(Z \times K_{\tau}) vec(V) = vec(K_{\tau} V Z^{T}).\]

  For each observed coordinate (\(i_{l}, j_{l}),\)

  \[u_{l} = <U[i_{l}, :], Z[j_{l}, :]>.\]

  Total \(O(q r)\).
\item
  Form sparse W\textquotesingle{} in \(R^{n \times M}\) from \(u_{l}.\) Let s = nnz(W\textquotesingle) \textless= q.
\item
  Adjoint sampled action:

  \[(Z^{T} \times K_{\tau}) vec(W') = vec(K_{\tau} W' Z).\]

  Compute W\textquotesingle{} \(Z \in O(s r) \le O(q r)\), then left-multiply by \(K_{\tau} \in O(n^\mNumber{2} r)\).
\item
  Add regularization term \(\lambda\) vec(\(K_{\tau}\) V), cost \(O(n^\mNumber{2} r)\).
\end{enumerate}

Total per matvec:

\[O(n^\mNumber{2} r + q r),\]

with no \(O(N)\) term.

\hypertarget{3-right-hand-side}{%
\subsubsection{3. Right-hand side}\label{3-right-hand-side}}

\(B = T\) Z with T sparse (q nonzeros):

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  T Z: \(O(q r)\)
\item
  \(K_{\tau}\) B: \(O(n^\mNumber{2} r)\)
\end{enumerate}

So \(b_{\tau} = (I_{r} \times K_{\tau})\) vec(B) is formed in \(O(q r + n^\mNumber{2} r)\).

\hypertarget{4-preconditioner-that-matches-the-corrected-algebra}{%
\subsubsection{4. Preconditioner that matches the corrected algebra}\label{4-preconditioner-that-matches-the-corrected-algebra}}

Use \(D = S S^{T}\) and whiten by \(K_{\tau}^{-\mNumber{1}/\mNumber{2}}:\)

\[x = (I_{r} \times K_{\tau}^{-\mNumber{1}/\mNumber{2}}) y.\]

Then

\[\begin{aligned}
Ahat = (I_{r} \times K_{\tau}^{-\mNumber{1}/\mNumber{2}}) A_{\tau} (I_{r} \times K_{\tau}^{-\mNumber{1}/\mNumber{2}}) \\
= (Z \times K_{\tau}^{\mNumber{1}/\mNumber{2}})^{T} D (Z \times K_{\tau}^{\mNumber{1}/\mNumber{2}}) + \lambda I.
\end{aligned}\]

If sampling is roughly uniform, D \textasciitilde{} c I with c = q/N. Then

\[Ahat ~ c (Z^{T} Z \times K_{\tau}) + \lambda I.\]

Choose Kron preconditioner in whitened coordinates:

\[Phat = (c Z^{T} Z + \lambda I_{r}) \times I_{n}.\]

Mapping back gives

\[\begin{aligned}
P = (c Z^{T} Z + \lambda I_{r}) \times K_{\tau} = H \times K_{\tau}, \\
H = c Z^{T} Z + \lambda I_{r}.
\end{aligned}\]

This is the missing justification for using \(H \times K_{\tau}\) (instead of claiming
it is the exact \(D = I\) system).

Khatri-Rao identity still gives efficient Gram formation:

\[Z^{T} Z = Hadamard_{i} (A_{i}^{T} A_{i}),\]

cost \(O(\sum_{i} n_{i} r^\mNumber{2})\).

Preconditioner apply:

\[P^{-\mNumber{1}} = H^{-\mNumber{1}} \times K_{\tau}^{-\mNumber{1}},\]

implemented by solving \(K_{\tau}\) Y \(H^{T}\) = Z\textquotesingle{} after reshape.
Per application cost is \(O(n^\mNumber{2} r + n r^\mNumber{2})\) (often simplified to \(O(n^\mNumber{2} r)\)
when n \textgreater\textgreater{} r).

\hypertarget{5-convergence-tightened}{%
\subsubsection{5. Convergence (tightened)}\label{5-convergence-tightened}}

For SPD \(A_{\tau}\) and SPD P, standard PCG gives

\[||e_{t}||_{A_{\tau}} \le \mNumber{2} ((sqrt(\kappa)-\mNumber{1})/(sqrt(\kappa)+\mNumber{1}))^{t} ||e_\mNumber{0}||_{A_{\tau}},\]

with \(\kappa\) = cond(\(P^{-\mNumber{1}/\mNumber{2}} A_{\tau} P^{-\mNumber{1}/\mNumber{2}}),\) so

\[t = O(sqrt(\kappa) \log(\mNumber{1}/eps)).\]

To claim "fast" convergence, add a spectral-equivalence hypothesis, e.g.

\[(\mNumber{1}-\delta) P \le A_{\tau} \le (\mNumber{1}+\delta) P, \mNumber{0} < \delta < \mNumber{1},\]

which implies

\[\kappa(P^{-\mNumber{1}} A_{\tau}) \le (\mNumber{1}+\delta)/(\mNumber{1}-\delta).\]

Hence t is logarithmic in 1/eps with a modest sqrt(kappa) factor when
\(\delta\) is bounded away from 1. (No unsupported closed-form \(t = O(r sqrt(n/q))\)
claim is needed.)

\textbf{Sufficient conditions for bounded \(\delta\).} The spectral equivalence
(1-delta)P \textless= \(A_{\tau}\) \textless= (1+delta)P holds with \(\delta\) bounded away from 1 when
the sampling pattern satisfies a restricted isometry-type condition:
\textbar\textbar D - (q/N)I\textbar\textbar{} is small relative to \(\lambda\). For uniform random sampling
with \(q \ge C\) n log n (for a universal constant C), matrix concentration
results (Tropp 2011, Theorem 1.6) give \(\delta = O(sqrt(n log n / q))\) with
high probability. Under this regime, \(\kappa = O(\mNumber{1})\) and PCG converges in
\(O(\log(\mNumber{1}/eps))\) iterations.

\hypertarget{6-complexity-summary}{%
\subsubsection{6. Complexity summary}\label{6-complexity-summary}}

Setup per ALS outer step:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \(Cholesky(K_{\tau}): O(n^{\mNumber{3}})\)
\item
  \(\mathbb{Z}^{T}\) Z via Hadamard Grams: \(O(\sum_{i} n_{i} r^\mNumber{2})\)
\item
  Cholesky(H): \(O(r^\mNumber{3})\)
\item
  RHS: \(O(q r + n^\mNumber{2} r)\)
\end{enumerate}

Per PCG iteration:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Matvec: \(O(n^\mNumber{2} r + q r)\)
\item
  Preconditioner apply: \(O(n^\mNumber{2} r + n r^\mNumber{2})\)
\end{enumerate}

Total:

\[\begin{aligned}
O(n^\mNumber{3} + r^\mNumber{3} + \sum_{i} n_{i} r^\mNumber{2} + q r + n^\mNumber{2} r \\
+ t (n^\mNumber{2} r + q r + n r^\mNumber{2})).
\end{aligned}\]

In the common regime \(n \ge r\), this simplifies to

\[O(n^\mNumber{3} + t (n^\mNumber{2} r + q r)),\]

with dependence on q (observed entries) rather than N (all entries).

\textbf{Regime caveat.} When n is large enough that the \(O(n^\mNumber{3})\) Cholesky setup
dominates (i.e., \(n^{\mNumber{3}} > t(n^{\mNumber{2}} r \mBridgeOperator{+} q\) r)), the per-ALS-step cost is effectively
\(O(n^\mNumber{3})\). In this regime, low-rank kernel approximations (e.g., Nystrom
approximation with rank p \textless\textless{} n, reducing the kernel factorization to \(O(n p^\mNumber{2})\))
or iterative inner solves (conjugate gradient on \(K_{\tau} y = z,\) cost \(O(n^\mNumber{2})\)
per inner iteration) can replace the exact Cholesky, reducing the setup to
\(O(n p^\mNumber{2} + t(n p r + q r))\). This is a well-known practical optimization
(see Rudi-Calandriello-Rosasco 2017) and is compatible with the PCG framework
as presented.

\hypertarget{7-algorithm}{%
\subsubsection{7. Algorithm}\label{7-algorithm}}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{SETUP:}
\NormalTok{  K\_tau = K + tau * I\_n                    \# tau \textgreater{} 0 if K is only PSD}
\NormalTok{  L\_K = cholesky(K\_tau)                    \# O(n\^{}3)}
\NormalTok{  G = hadamard\_product(A\_i\^{}T A\_i for i != k)  \# O(sum\_i n\_i r\^{}2)}
\NormalTok{  c = q / N}
\NormalTok{  H = c * G + lambda * I\_r}
\NormalTok{  L\_H = cholesky(H)                        \# O(r\^{}3)}
\NormalTok{  B = sparse\_mttkrp(T, Z)                  \# O(qr)}
\NormalTok{  b = vec(K\_tau @ B)                       \# O(n\^{}2 r)}

\NormalTok{PCG(A\_tau x = b, preconditioner P = H x K\_tau):}
\NormalTok{  x0 = 0}
\NormalTok{  r0 = b}
\NormalTok{  z0 = precond\_solve(L\_K, L\_H, r0)         \# O(n\^{}2 r + n r\^{}2)}
\NormalTok{  p0 = z0}
\NormalTok{  repeat until convergence:}
\NormalTok{    w = matvec\_A\_tau(p)                    \# O(n\^{}2 r + q r)}
\NormalTok{    alpha = (r\^{}T z) / (p\^{}T w)}
\NormalTok{    x = x + alpha * p}
\NormalTok{    r\_new = r {-} alpha * w}
\NormalTok{    if ||r\_new|| \textless{}= eps * ||b||: break}
\NormalTok{    z\_new = precond\_solve(L\_K, L\_H, r\_new)}
\NormalTok{    beta = (r\_new\^{}T z\_new) / (r\^{}T z)}
\NormalTok{    p = z\_new + beta * p}
\NormalTok{    r, z = r\_new, z\_new}
\NormalTok{  W = reshape(x, n, r)}

\NormalTok{matvec\_A\_tau(v):}
\NormalTok{  V = reshape(v, n, r)}
\NormalTok{  U = K\_tau @ V}
\NormalTok{  for each observed (i\_l, j\_l):}
\NormalTok{    u\_l = dot(U[i\_l, :], Z[j\_l, :])}
\NormalTok{  Wprime = sparse(n, M, entries u\_l)}
\NormalTok{  Y = K\_tau @ (Wprime @ Z) + lambda * (K\_tau @ V)}
\NormalTok{  return vec(Y)}

\NormalTok{precond\_solve(L\_K, L\_H, z):}
\NormalTok{  Zp = reshape(z, n, r)}
\NormalTok{  solve K\_tau Y H\^{}T = Zp using triangular solves with L\_K, L\_H}
\NormalTok{  return vec(Y)}
\end{Highlighting}
\end{Shaded}

\hypertarget{key-references-from-futon6-corpus}{%
\subsection{Key References from futon6 corpus}\label{key-references-from-futon6-corpus}}

\begin{itemize}
\tightlist
\item
  PlanetMath: conjugate gradient algorithm; method of conjugate gradients
\item
  PlanetMath: Kronecker product; positive definite matrices
\item
  PlanetMath: properties of tensor product
\item
  physics.SE \#27466: iterative solvers for large systems in physics
\item
  physics.SE \#27556: preconditioning for elliptic PDEs
\end{itemize}
