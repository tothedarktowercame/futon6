\hypertarget{problem-10-rkhs-constrained-tensor-cp-via-preconditioned-conjugate-gradient}{%
\section{Problem 10: RKHS-Constrained Tensor CP via Preconditioned Conjugate Gradient}\label{problem-10-rkhs-constrained-tensor-cp-via-preconditioned-conjugate-gradient}}

\hypertarget{problem-statement}{%
\subsection{Problem Statement}\label{problem-statement}}

Given the mode-k subproblem of RKHS-constrained CP decomposition with
missing data, solve

\[[(Z \times K)^{T} D (Z \times K) + \lambda (I_{r} \times K)] vec(W) = (I_{r} \times K) vec(B),\]

where \(D = S\) \(S^{T}\) is the observation projector (q observed entries out of
N = nM), \(B = T\) Z, and n, r \textless\textless{} q \textless\textless{} N.

\hypertarget{assumptions-used-explicit}{%
\subsection{Assumptions Used (explicit)}\label{assumptions-used-explicit}}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \(\lambda\) \textgreater{} 0.
\item
  For standard PCG and Cholesky-based preconditioning, use a PD kernel
  \(K_{\tau} = K\) + \(\tau\) \(I_{n}\) with \(\tau\) \textgreater{} 0 (or assume K is already PD).
\item
  S is a selection operator, so D is diagonal/projector and sparse by index list.
\end{enumerate}

Then the solved system is

\[\begin{aligned}
A_{\tau} x = b_{\tau}, \\
A_{\tau} = (Z \times K_{\tau})^{T} D (Z \times K_{\tau}) + \lambda (I_{r} \times K_{\tau}), \\
x = vec(W), \\
b_{\tau} = (I_{r} \times K_{\tau}) vec(B).
\end{aligned}\]

Under these assumptions \(A_{\tau}\) is SPD, so PCG applies.

\hypertarget{solution}{%
\subsection{Solution}\label{solution}}

\hypertarget{1-why-naive-direct-methods-fail}{%
\subsubsection{1. Why naive direct methods fail}\label{1-why-naive-direct-methods-fail}}

\(A_{\tau}\) is an (nr) x (nr) system. Dense direct factorization costs
\(O((nr)^3) = O(n^3 r^3)\).

A naive explicit route also materializes \(\Phi = Z \times K_{\tau} \in R^{N \times nr}\),
which costs \(O(N n r)\) memory/work before factorization. This is the
\(N\)-dependent bottleneck we avoid with matrix-free PCG.

\hypertarget{2-implicit-matrix-vector-product-in-on2-r--q-r}{%
\subsubsection{\texorpdfstring{2. Implicit matrix-vector product in \(O(n^2 r + q r)\)}{2. Implicit matrix-vector product in O(n\^{}2 r + q r)}}\label{2-implicit-matrix-vector-product-in-on2-r--q-r}}

CG needs only \(y = A_{\tau}\) x, not \(A_{\tau}\) explicitly.

Given x = vec(V), \(V \in R\)\^{}\{n x r\}:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \(U = K_{\tau}\) V. Cost \(O(n^2 r)\).
\item
  Forward sampled action (only observed entries):

  \[(Z \times K_{\tau}) vec(V) = vec(K_{\tau} V Z^{T}).\]

  For each observed coordinate (\(i_{l}\), \(j_{l}\)),

  \[u_{l} = <U[i_{l}, :], Z[j_{l}, :]>.\]

  Total \(O(q r)\).
\item
  Form sparse W\textquotesingle{} in \(R^{n \times M}\) from \(u_{l}.\) Let s = nnz(W\textquotesingle) \textless= q.
\item
  Adjoint sampled action:

  \[(Z^{T} \times K_{\tau}) vec(W') = vec(K_{\tau} W' Z).\]

  Compute W\textquotesingle{} \(Z \in O(s r) \le O(q r)\), then left-multiply by \(K_{\tau} \in O(n^2 r)\).
\item
  Add regularization term \(\lambda\) vec(\(K_{\tau}\) V), cost \(O(n^2 r)\).
\end{enumerate}

Total per matvec:

\[O(n^2 r + q r),\]

with no \(O(N)\) term.

\hypertarget{3-right-hand-side}{%
\subsubsection{3. Right-hand side}\label{3-right-hand-side}}

\(B = T\) Z with T sparse (q nonzeros):

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  T Z: \(O(q r)\)
\item
  \(K_{\tau}\) B: \(O(n^2 r)\)
\end{enumerate}

So \(b_{\tau}\) = (\(I_{r} \times K_{\tau}\)) vec(B) is formed in \(O(q r + n^2 r)\).

\hypertarget{4-preconditioner-that-matches-the-corrected-algebra}{%
\subsubsection{4. Preconditioner that matches the corrected algebra}\label{4-preconditioner-that-matches-the-corrected-algebra}}

Use \(D = S\) \(S^{T}\) and whiten by \(K_{\tau}^{-1/2}:\)

\[x = (I_{r} \times K_{\tau}^{-1/2}) y.\]

Then

\[\begin{aligned}
Ahat = (I_{r} \times K_{\tau}^{-1/2}) A_{\tau} (I_{r} \times K_{\tau}^{-1/2}) \\
= (Z \times K_{\tau}^{1/2})^{T} D (Z \times K_{\tau}^{1/2}) + \lambda I.
\end{aligned}\]

If sampling is roughly uniform, D \textasciitilde{} c I with c = q/N. Then

\[Ahat ~ c (Z^{T} Z \times K_{\tau}) + \lambda I.\]

Choose Kron preconditioner in whitened coordinates:

\[Phat = (c Z^{T} Z + \lambda I_{r}) \times I_{n}.\]

Mapping back gives

\[\begin{aligned}
P = (c Z^{T} Z + \lambda I_{r}) \times K_{\tau} = H \times K_{\tau}, \\
H = c Z^{T} Z + \lambda I_{r}.
\end{aligned}\]

This is the missing justification for using \(H \times K_{\tau}\) (instead of claiming
it is the exact \(D = I\) system).

Khatri-Rao identity still gives efficient Gram formation:

\[Z^{T} Z = Hadamard_{i} (A_{i}^{T} A_{i}),\]

cost \(O(sum_{i} n_{i} r^2)\).

Preconditioner apply:

\[P^{-1} = H^{-1} \times K_{\tau}^{-1},\]

implemented by solving \(K_{\tau}\) Y \(H^{T}\) = Z\textquotesingle{} after reshape.
Per application cost is \(O(n^2 r + n r^2)\) (often simplified to \(O(n^2 r)\)
when n \textgreater\textgreater{} r).

\hypertarget{5-convergence-tightened}{%
\subsubsection{5. Convergence (tightened)}\label{5-convergence-tightened}}

For SPD \(A_{\tau}\) and SPD P, standard PCG gives

\[||e_{t}||_{A_{\tau}} \le 2 ((sqrt(\kappa)-1)/(sqrt(\kappa)+1))^{t} ||e_0||_{A_{\tau}},\]

with \(\kappa\) = cond(\(P^{-1/2}\) \(A_{\tau}\) \(P^{-1/2}\)), so

\[t = O(sqrt(\kappa) log(1/eps)).\]

To claim "fast" convergence, add a spectral-equivalence hypothesis, e.g.

\[(1-\delta) P \le A_{\tau} \le (1+\delta) P, 0 < \delta < 1,\]

which implies

\[\kappa(P^{-1} A_{\tau}) \le (1+\delta)/(1-\delta).\]

Hence t is logarithmic in 1/eps with a modest sqrt(kappa) factor when
\(\delta\) is bounded away from 1. (No unsupported closed-form \(t = O(r sqrt(n/q))\)
claim is needed.)

\textbf{Sufficient conditions for bounded \(\delta\).} The spectral equivalence
(1-delta)P \textless= \(A_{\tau}\) \textless= (1+delta)P holds with \(\delta\) bounded away from 1 when
the sampling pattern satisfies a restricted isometry-type condition:
\textbar\textbar D - (q/N)I\textbar\textbar{} is small relative to \(\lambda\). For uniform random sampling
with \(q \ge C\) n log n (for a universal constant C), matrix concentration
results (Tropp 2011, Theorem 1.6) give \(\delta = O(sqrt(n log n / q))\) with
high probability. Under this regime, \(\kappa = O(1)\) and PCG converges in
\(O(log(1/eps))\) iterations.

\hypertarget{6-complexity-summary}{%
\subsubsection{6. Complexity summary}\label{6-complexity-summary}}

Setup per ALS outer step:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \(Cholesky(K_{\tau}):\) \(O(n^3)\)
\item
  \(Z^{T}\) Z via Hadamard Grams: \(O(sum_{i} n_{i} r^2)\)
\item
  Cholesky(H): \(O(r^3)\)
\item
  RHS: \(O(q r + n^2 r)\)
\end{enumerate}

Per PCG iteration:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Matvec: \(O(n^2 r + q r)\)
\item
  Preconditioner apply: \(O(n^2 r + n r^2)\)
\end{enumerate}

Total:

\[\begin{aligned}
O(n^3 + r^3 + sum_{i} n_{i} r^2 + q r + n^2 r \\
+ t (n^2 r + q r + n r^2)).
\end{aligned}\]

In the common regime \(n \ge r\), this simplifies to

\[O(n^3 + t (n^2 r + q r)),\]

with dependence on q (observed entries) rather than N (all entries).

\textbf{Regime caveat.} When n is large enough that the \(O(n^3)\) Cholesky setup
dominates (i.e., \(n^3\) \textgreater{} t(\(n^2\) r + q r)), the per-ALS-step cost is effectively
\(O(n^3)\). In this regime, low-rank kernel approximations (e.g., Nystrom
approximation with rank p \textless\textless{} n, reducing the kernel factorization to \(O(n p^2)\))
or iterative inner solves (conjugate gradient on \(K_{\tau}\) \(y = z\), cost \(O(n^2)\)
per inner iteration) can replace the exact Cholesky, reducing the setup to
\(O(n p^2 + t(n p r + q r))\). This is a well-known practical optimization
(see Rudi-Calandriello-Rosasco 2017) and is compatible with the PCG framework
as presented.

\hypertarget{7-algorithm}{%
\subsubsection{7. Algorithm}\label{7-algorithm}}

\[\begin{aligned}
SETUP: \\
K_{\tau} = K + \tau \ast I_{n} \# \tau > 0 if K is only PSD \\
L_{K} = cholesky(K_{\tau}) \# O(n^3) \\
G = hadamard_{product}(A_{i}^{T} A_{i} for i \neq k) \# O(sum_{i} n_{i} r^2) \\
c = q / N \\
H = c \ast G + \lambda \ast I_{r} \\
L_{H} = cholesky(H) \# O(r^3) \\
B = sparse_{mttkrp}(T, Z) \# O(qr) \\
b = vec(K_{\tau} @ B) \# O(n^2 r) \\
PCG(A_{\tau} x = b, preconditioner P = H \times K_{\tau}): \\
x0 = 0 \\
r0 = b \\
z0 = precond_{solve}(L_{K}, L_{H}, r0) \# O(n^2 r + n r^2) \\
p0 = z0 \\
repeat until convergence: \\
w = matvec_{A,\tau}(p) \# O(n^2 r + q r) \\
\alpha = (r^{T} z) / (p^{T} w) \\
x = x + \alpha \ast p \\
r_{new} = r - \alpha \ast w \\
if ||r_{new}|| \le eps * ||b||: break \\
z_{new} = precond_{solve}(L_{K}, L_{H}, r_{new}) \\
\beta = (r_{new}^{T} z_{new}) / (r^{T} z) \\
p = z_{new} + \beta \ast p \\
r, z = r_{new}, z_{new} \\
W = reshape(x, n, r) \\
matvec_{A,\tau}(v): \\
V = reshape(v, n, r) \\
U = K_{\tau} @ V \\
for each observed (i_{l}, j_{l}): \\
u_{l} = dot(U[i_{l}, :], Z[j_{l}, :]) \\
Wprime = sparse(n, M, entries u_{l}) \\
Y = K_{\tau} @ (Wprime @ Z) + \lambda \ast (K_{\tau} @ V) \\
return vec(Y) \\
precond_{solve}(L_{K}, L_{H}, z): \\
Zp = reshape(z, n, r) \\
solve K_{\tau} Y H^{T} = Zp using triangular solves with L_{K}, L_{H} \\
return vec(Y)
\end{aligned}\]

\hypertarget{key-references-from-futon6-corpus}{%
\subsection{Key References from futon6 corpus}\label{key-references-from-futon6-corpus}}

\begin{itemize}
\tightlist
\item
  PlanetMath: conjugate gradient algorithm; method of conjugate gradients
\item
  PlanetMath: Kronecker product; positive definite matrices
\item
  PlanetMath: properties of tensor product
\item
  physics.SE \#27466: iterative solvers for large systems in physics
\item
  physics.SE \#27556: preconditioning for elliptic PDEs
\end{itemize}
