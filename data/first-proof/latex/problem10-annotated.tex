\section{Problem 10: PCG for RKHS-Constrained Tensor CP Decomposition}

\subsection*{Problem statement}
Explain why preconditioned conjugate gradient (PCG) solves the mode-$k$ RKHS CP subproblem without $O(N)$ per-iteration cost, where $N$ is the full ambient sample size.

\subsection*{Answer}
Use implicit Kronecker/Khatri--Rao matrix-vector products and a Kronecker preconditioner built from expected sampling geometry.

\phantomsection\label{sn:p10}
\statusnote{This result is \textbf{conditional}: it is closed under explicit
assumptions ($\lambda>0$, $K_\tau \succ 0$, and sampling/coherence conditions
controlling preconditioned conditioning). Necessity counterexamples are
recorded in the full writeup when these assumptions are dropped.}

\subsection*{Annotated proof sketch}
\begin{enumerate}[leftmargin=1.5em]
\item Never form the full system matrix; compute $Ax$ through structured operations:
  kernel multiplies, sparse sampling/restriction, and Khatri--Rao contractions.
\item Build $b$ by the same structure, again avoiding dense $N$-scale tensors.
\item Precondition with $P=H\otimes K_\tau$ from replacing sparse sampling by its expectation; invert $P$ via two small Cholesky solves.
\item Apply standard PCG convergence bounds in terms of condition number $\kappa(P^{-1/2}AP^{-1/2})$ and matrix concentration for random sampling.
\end{enumerate}

\subsection*{Selected citations}
\begin{itemize}[leftmargin=1.5em]
\item Matrix concentration / Freedman-style bound context: \url{https://arxiv.org/abs/1110.1379}
\item FALKON large-scale kernel method: \url{https://arxiv.org/abs/1705.10958}
\item Kronecker product definition: \url{https://planetmath.org/kroneckerproduct}
\item Conjugate gradient method definition: \url{https://planetmath.org/conjugategradientmethod}
\end{itemize}

\subsection*{Background and prerequisites}
\textbf{Field:} Numerical Linear Algebra / Kernel Methods / Tensor Decomposition.\\
\textbf{What you need:} Conjugate gradient and preconditioning, Kronecker and Khatri--Rao products, reproducing kernel Hilbert spaces (RKHS), and matrix concentration inequalities for random sampling operators.\\
\textbf{Way in:} Trefethen--Bau, \emph{Numerical Linear Algebra} (SIAM, 1997) for iterative methods; Rasmussen--Williams, \emph{Gaussian Processes for Machine Learning} (MIT Press, 2006; freely available online) for RKHS context.


\clearpage
