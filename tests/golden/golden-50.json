{
  "version": "golden-50-v1",
  "generated": "2026-02-10T17:23:19Z",
  "source": "physics.stackexchange",
  "seed": 42,
  "summary": {
    "total": 50,
    "strata": {
      "easy": 17,
      "medium": 17,
      "hard": 16
    },
    "avg_ner_terms": 79.52,
    "avg_patterns": 6.6,
    "avg_scopes": 2.18,
    "pattern_frequency": {
      "check-the-extreme-cases": 34,
      "work-examples-first": 33,
      "encode-as-algebra": 30,
      "unfold-the-definition": 25,
      "construct-an-explicit-witness": 23,
      "monotone-approximation": 20,
      "exploit-symmetry": 20,
      "local-to-global": 20,
      "quotient-by-irrelevance": 14,
      "estimate-by-bounding": 14,
      "transport-across-isomorphism": 14,
      "use-probabilistic-method": 13,
      "find-the-right-abstraction": 12,
      "dualise-the-problem": 11,
      "construct-auxiliary-object": 10,
      "optimise-a-free-parameter": 9,
      "the-diagonal-argument": 6,
      "try-a-simpler-case": 6,
      "argue-by-contradiction": 6,
      "reduce-to-known-result": 3,
      "pass-to-a-subsequence": 2,
      "induction-and-well-ordering": 2,
      "split-into-cases": 2,
      "show-both-inequalities": 1
    },
    "scope_frequency": {
      "where-binding": 35,
      "set-notation": 35,
      "for-any": 17,
      "consider": 14,
      "let-binding": 6,
      "assume": 2
    },
    "entries_with_ner": 50,
    "entries_with_patterns": 50,
    "entries_with_scopes": 33
  },
  "entries": [
    {
      "id": "se-physics-285922",
      "stratum": "easy",
      "title": "Would you feel Centrifugal Force without Friction?",
      "tags": [
        "newtonian-mechanics",
        "reference-frames",
        "friction",
        "free-body-diagram",
        "centrifugal-force"
      ],
      "score": 22,
      "answer_score": 21,
      "question_body": "This question arose from my sci-fi discussions with my friend. It concerns artificial gravity through centrifugal Force. Partly inspired by the Artificial Gravity created on the Hermes ship from the Martian. Imagine there's a hollow torus (donut), in space, without any external forces acting upon it, which is rotating (around the axis through the middle). Will an object which you place inside the 'pipe' of the torus experience the centrifugal force due to rotation and be attracted to the edge of the torus? Or would it only experience the force if it was originally touching one of the walls? And would the results be different if the torus was filled with a gas (air)? And if the inside of the torus was divided into sections (like the ship)?",
      "answer_body": "The answer here is yes . In the reference frame of the spinning torus, the object in the tube receives the expected amount of centrifugal force towards the outer rim ... but, it also receives a coriolis force of even greater magnitude towards the axis of rotation. Let's say that the frame of reference (and the torus) is rotating with angular velocity $\\omega$, and the object of mass $m$ is \"stationary\" (in the stationary frame) at a distance $r$ from the axis of rotation. Imagine viewing your object from the rotating frame. This object is not \"stationary\" -- that's actually whizzing along the tube magically in an apparent perfect circle. From the perspective of the tube frame, this object is actually constantly accelerating inwards with $a = v^2 / r$. (If anything is experencing circular motion, its acceleration is $a = v^2 / r$). $v = \\omega r$, so the object is whizzing around the tube, constantly accelerating inwards with acceleration $a = \\omega^2 r$, somehow. So, to the rotating frame, the object is experiencing some magical force of magnitude: $$F_{net} = m \\omega^2 r$$ Towards the axis of rotation. Where is that coming from? Well, like you mentioned, the object is expected to experience a centrifugal force away from the axis of rotation. The centrifugal force $F_c$ on a body with a given mass in a given rotating frame is: $$|F_c| = m \\omega^2 r$$ Going away from the axis. But! Our object is moving in this rotating frame, and all moving objects in a rotating frame also experience a coriolis force $F_C$: $$F_C = - 2 m (\\omega \\times v)$$ For our object, $\\omega \\times v$ points radially outwards, so $|F_C|$ points radially inwards (because of the negative sign), and remembering that $v = \\omega r$, we have: $$|F_C| = 2 m \\omega^2 r$$ Going inwards . Adding it all together (and considering inwards forces to be positive), we get: $$F_{net} = F_C + F_c$$ $$F_{net} = 2 m \\omega^2 r - m \\omega^2 r$$ $$F_{net} = m \\omega^2 r$$ Which is exactly the force that we should expect to see to explain the apparent motion of our object! So, in summary: YES , the object does experience a centrifugal force away from the center, even though it never touches any walls. BUT , because it is moving with respect to the frame of reference, it also experiences a Coriolis force , towards the center, that's actually even bigger . The two forces act together to create a net force towards the center, which causes circular motion in the reference frame. To observers in the rotating frame, it would appear as if the object was orbiting around the axis of rotation, as if gravitationally attracted to it. EDIT Just to clarify some issues that have come up in the comments. The centrifugal force arises mathematically from the coordinate transformation of moving from a stationary frame to a rotating frame. It doesn't come from any physical interaction. The walls and friction do not provide centrifugal force. Even if it was just a single object in an isolated system, as soon as you move to the rotating frame, it is influenced by a centrifugal force (as long as it is not sitting exactly on the axis of rotation). Centrifugal force is the mathematical result of the coordinate transformation, not a physical force created from physical interactions. What some might be mixing up is the perception of being pulled outwards. But remember, being influenced by a force is very different than perceiving the force as a human being. Astronauts in orbit are moving under the influence of gravity, even though they feel weightless. This is because they're in \"free-fall\" -- they are being pulled by gravity, but nothing is preventing them from falling their free-fall path. As soon as something (like a couch or a chair) resists your freefall path, you perceive being pulled down by gravity. So there's a bit of an irony here -- the only reason you know that gravity is pulling you down is because you feel a force pushing you upwards, provided by something that isn't gravity. The object in the torus (stationary in the stationary frame, flying around in the rotating frame) is in \"free-fall\" in the rotating frame. It's moving under the influence of the centrifugal force (like the astronaut moving under the influence of gravity), but it feels weightless because nothing is resisting its freefall path. So yes, the object is under the influence of the centrifugal force, and experiences it (just like an astronaut is under the influence of gravity, and experiences gravity's pull), but if it was a human, it would not \"perceive\" being pulled outwards (just like an astronaut does not \"perceive\" being pulled by gravity). Not until there is something to impede its freefall motion. To gain an intuition, let's imagine a scenario: The torus (and our reference frame) is spinning. In the rotating frame, the object is whizzing around the torus under the influence of some forces that add up to have it attracted to the center of the torus. Now, imagine that we add a partition suddenly in the tube. The object is whizzing around, and eventually, it hits the partition! The partition is made out of cotton so the object doesn't bounce off. What happens? The partition is stationary in the rotating frame, so now the object is stationary in the rotating frame too. Because the object is now stationary in the rotating frame (it's pinned against the partition), it no longer is influenced by the Coriolis force! And now, the only force on it is the centrifugal force. Now , the object will start being pulled towards the edge of the torus, because the centrifugal force is the only force, so the net force pulls it outwards. It will look like the object is \"sliding down\" the partition to the outer edge. Now, because the object is sliding towards the edge, it once again experiences a Coriolis force! (remember, all moving objects are influenced by the Coriolis force). This Coriolis force is actually directed towards the wall/partition, so the object will actually perceive the Coriolis force pinning it to the wall (because the wall pushes back). The object continues to slide/roll and eventually reaches the outer edge. Now, the object is no longer moving, so there's no Coriolis force. The only force again is the centrifugal force, and now that force is being resisted by the outer edge. So the object will perceive a centrifugal force pulling it out because of the outer edge of the tube pushing back. Now, because there is no Coriolis force, there is no force pushing the object either way (back or forth) up or down the tube. The partition could actually be removed, and the object will naturally stay in place because its only net force is directly radially outwards. Note that friction never comes into play in any of these situations, too :)",
      "question_latex": [],
      "answer_latex": [
        "F_{net} = m \\omega^2 r",
        "|F_c| = m \\omega^2 r",
        "F_C = - 2 m (\\omega \\times v)",
        "|F_C| = 2 m \\omega^2 r",
        "F_{net} = F_C + F_c",
        "F_{net} = 2 m \\omega^2 r - m \\omega^2 r",
        "F_{net} = m \\omega^2 r",
        "\\omega",
        "m",
        "r",
        "a = v^2 / r",
        "v = \\omega r",
        "a = \\omega^2 r",
        "</p>\n\n<p>Towards the axis of rotation.  Where is that coming from?</p>\n\n<p>Well, like you mentioned, the object is expected to experience a centrifugal force <em>away</em> from the axis of rotation.  The centrifugal force",
        "on a body with a given mass in a given rotating frame is:</p>\n\n<p>",
        "</p>\n\n<p>Going <em>away</em> from the axis.  But!  Our object is <em>moving</em> in this rotating frame, and all moving objects in a rotating frame also experience a <em>coriolis force</em>",
        ":</p>\n\n<p>",
        "</p>\n\n<p>For our object,",
        "points radially outwards, so",
        "points radially <em>inwards</em> (because of the negative sign), and remembering that",
        ", we have:</p>\n\n<p>",
        "</p>\n\n<p>Going <em>inwards</em>.  Adding it all together (and considering inwards forces to be positive), we get:</p>\n\n<p>",
        "</p>\n\n<p>"
      ],
      "created": "2016-10-12T21:01:51.737",
      "golden_ner_terms": [
        "acceleration",
        "air",
        "angular velocity",
        "axis",
        "body",
        "center",
        "centrifugal force",
        "circle",
        "circular",
        "coordinate",
        "distance",
        "edge",
        "even",
        "eventually",
        "forces",
        "frame",
        "friction",
        "gas",
        "gravity",
        "interactions",
        "intuition",
        "isolated",
        "mass",
        "mixing",
        "negative",
        "net",
        "object",
        "observers",
        "orbit",
        "outer",
        "partition",
        "path",
        "perception",
        "perfect",
        "place",
        "positive",
        "rotation",
        "scenario",
        "space",
        "stationary",
        "torus",
        "transformation",
        "velocity",
        "way"
      ],
      "golden_ner_count": 44,
      "golden_patterns": [
        {
          "pattern": "find-the-right-abstraction",
          "score": 2.0,
          "hotwords": [
            "perspective"
          ]
        },
        {
          "pattern": "quotient-by-irrelevance",
          "score": 2.0,
          "hotwords": [
            "up to"
          ]
        },
        {
          "pattern": "encode-as-algebra",
          "score": 2.0,
          "hotwords": [
            "ring"
          ]
        }
      ],
      "golden_pattern_names": [
        "find-the-right-abstraction",
        "quotient-by-irrelevance",
        "encode-as-algebra"
      ],
      "golden_scopes": [],
      "golden_scope_count": 0
    },
    {
      "id": "se-physics-383467",
      "stratum": "easy",
      "title": "What does the identity operator look like in Quantum Field Theory?",
      "tags": [
        "quantum-field-theory",
        "operators",
        "path-integral"
      ],
      "score": 23,
      "answer_score": 20,
      "question_body": "In texts on ordinary quantum mechanics the identity operators \\begin{equation}\\begin{aligned} I & = \\int \\operatorname{d}x\\, |x\\rangle\\langle x| \\\\ & = \\int \\operatorname{d}p\\, |p\\rangle\\langle p| \\end{aligned} \\tag1\\end{equation} are frequently used in textbooks, like Shankar's . This allows us to represent position and momentum operators in a concrete way as \\begin{equation}\\begin{aligned} x_S & = \\int \\operatorname{d}x' \\, |x'\\rangle\\langle x'| x' \\ \\mathrm{and}\\\\ p_S & = \\int \\operatorname{d}p' \\, |p'\\rangle\\langle p'| p', \\end{aligned} \\tag2\\end{equation} where the '$S$' subscript is to emphasize these are Schrödinger picture operators. Has a similarly concrete representation been constructed in quantum field theory, or is there some reason it's not possible? I'm imagining something like \\begin{equation}\\begin{aligned} I & = \\int \\left[\\mathcal{D} \\phi(\\mathbf{x}') |\\phi(\\mathbf{x}')\\rangle\\langle\\phi(\\mathbf{x}')|\\right] \\ \\mathrm{and} \\\\ \\phi(\\mathbf{x}) & = \\int \\left[\\mathcal{D} \\phi(\\mathbf{x}') |\\phi(\\mathbf{x}')\\rangle\\langle\\phi(\\mathbf{x}')|\\right] \\, \\phi^{\\mathbf{1}_{\\{\\mathbf{x}=\\mathbf{x}'\\}}}(\\mathbf{x}'), \\end{aligned} \\tag3\\end{equation} where the vectors are inside the path integral metric because they're included in the continuum limit product that defines the path integral, and $\\mathbf{1}_{\\{\\mathbf{x}=\\mathbf{x}'\\}}$ is the indicator function that equals $0$ when $\\mathbf{x}\\neq\\mathbf{x}'$ and $1$ when $\\mathbf{x}=\\mathbf{x}'$. The indicator function is in the exponent of $\\phi(\\mathbf{x}')$ to make sure that the field is a non-identity operator at $\\mathbf{x}$ only.",
      "answer_body": "The equation $(3)$ in the OP is formally 1 correct, and it is in fact one of the main ingredients in the functional integral formulation of quantum field theory. See Ref.1 for the explicit construction. For completeness, we sketch the derivation here. We use a notation much closer to the one in the OP than to that of Ref.1, but with some minors modifications (to allow for a more general result). The setup. Let $\\{\\phi_a,\\pi^a\\}_a$ be a set of phase-space operators, where $a\\in\\mathbb R^{d-1}\\times \\mathbb N^n$ is a DeWitt index (i.e., it contains a continuous part, corresponding to the spatial part of spacetime $\\mathbb R^d$ and a discrete part, corresponding to a certain vector space $\\mathbb N^n$ whose base is spacetime). Note that we are taking these operators to be in the Schrödinger picture. They are assumed conjugate: $$\\tag1 [\\phi_a,\\pi^b]=i\\delta_a^b $$ where $\\delta$ is a Dirac-Kronecker delta. Here, $[\\cdot,\\cdot]$ denotes a commutator (we assume $\\phi,\\pi$ to be Grassmann even; we could consider the general case here by keeping tracks of the signs, but we won't for simplicity). The rest of commutators are assumed to vanish. We take the phase-space operators to be hermitian (or otherwise, we double the dimension and split them into their real and imaginary parts). As $[\\phi_a,\\phi_b]=[\\pi^a,\\pi^b]=0$, we may simultaneously diagonalise them: \\begin{equation} \\begin{aligned} \\phi_a|\\varphi\\rangle&=\\varphi_a|\\varphi\\rangle\\\\ \\pi^a|\\varpi\\rangle&=\\varpi^a|\\varpi\\rangle \\end{aligned}\\tag2 \\end{equation} where $\\varphi_a,\\varpi^b$ are $c$-numbers. After normalising them, if necessary, these eigenvectors are orthonormal: \\begin{equation} \\begin{aligned} \\langle\\varphi|\\varphi'\\rangle&=\\prod_a\\delta(\\varphi_a-\\varphi'_a)\\equiv\\delta(\\varphi-\\varphi')\\\\ \\langle\\varpi|\\varpi'\\rangle&=\\prod_a\\delta(\\varpi^a-\\varpi'^a)\\equiv\\delta(\\varpi-\\varpi') \\end{aligned}\\tag3 \\end{equation} and, as per $[\\phi_a,\\pi^b]=\\delta_a^b$, we also have $$\\tag4 \\langle \\varphi|\\varpi\\rangle=\\prod_a\\frac{1}{\\sqrt{2\\pi}}\\mathrm e^{i\\varphi_a\\varpi^a} $$ As the sets $\\{\\phi_a\\}_a$ and $\\{\\pi^a\\}_a$ are both assumed complete, we also have \\begin{equation} \\begin{aligned} 1&\\equiv \\int\\prod_a|\\varphi\\rangle\\langle\\varphi|\\,\\mathrm d\\varphi_a\\\\ 1&\\equiv \\int\\prod_a|\\varpi\\rangle\\langle\\varpi|\\,\\mathrm d\\varpi^a \\end{aligned}\\tag5 \\end{equation} as OP anticipated. As mentioned in the comments, and for future reference, we note that there is another identity operator that is fundamental in a quantum theory, to wit, the resolution in terms of energy eigenstates: \\begin{equation}\\tag6 1\\equiv \\int|E\\rangle\\langle E|\\,\\mathrm dE \\end{equation} where $\\mathrm dE$ is the counting measure in the case of discrete eigenvalues. Typically, we assume that the Hamiltonian is non-negative and that its ground state energy is zero; furthermore, we assume that there is a non-zero mass gap so that $E=0$ is a regular eigenvalue of $H$ (as opposed to a singular value). It is at this point convenient to switch into the Heisenberg picture, where $\\{\\phi_a,\\pi^a\\}_a$ become time-dependent: \\begin{equation} \\begin{aligned} \\phi_a(t)\\equiv\\mathrm e^{iHt}\\phi_a\\mathrm e^{-iHt}\\\\ \\pi^a(t)\\equiv\\mathrm e^{iHt}\\pi^a\\mathrm e^{-iHt} \\end{aligned}\\tag7 \\end{equation} with eigenstates \\begin{equation} \\begin{aligned} |\\varphi;t\\rangle&=\\mathrm e^{iHt}|\\varphi\\rangle\\\\ |\\varpi;t\\rangle&=\\mathrm e^{iHt}|\\varpi\\rangle \\end{aligned}\\tag8 \\end{equation} such that \\begin{equation} \\begin{aligned} \\phi_a(t)|\\varphi;t\\rangle&=\\varphi_a|\\varphi;t\\rangle\\\\ \\pi^a(t)|\\varpi;t\\rangle&=\\varpi^a|\\varpi;t\\rangle \\end{aligned}\\tag9 \\end{equation} The Heisenberg picture eigenstates satisfy the same completeness and orthonormality relations as the Schrödinger picture eigenstates (inasmuch as the transformation is unitary). The functional integral. Time slicing. From this, and by the usual arguments (time slicing), Ref.1 derives the phase-space functional integral representation of the transition amplitude, to wit, \\begin{equation} \\begin{aligned} &\\langle\\varphi_\\mathrm{in};t_\\mathrm{in}|\\mathrm T\\left(O_1(\\pi(t_1),\\phi(t_1)),\\dots,O_n(\\pi(t_n),\\phi(t_n))\\right)|\\varphi_\\mathrm{out};t_\\mathrm{out}\\rangle\\equiv\\\\ &\\hspace{10pt}\\int_{\\varphi(t_\\mathrm{in})=\\varphi_\\mathrm{in}}^{\\varphi(t_\\mathrm{out})=\\varphi_\\mathrm{out}}\\left(O_1(\\varpi(t_1),\\varphi(t_1)),\\dots,O_n(\\varpi(t_n),\\varphi(t_n))\\right)\\cdot\\\\ &\\hspace{25pt}\\cdot\\exp\\left[i\\int_{t_\\mathrm{in}}^{t_\\mathrm{out}}\\left(\\sum_a\\dot \\varphi_a(\\tau)\\varpi^a(\\tau)-H(\\varphi(\\tau),\\varpi(\\tau))\\right)\\mathrm d\\tau\\right]\\mathrm d\\varphi\\,\\mathrm d\\varpi \\end{aligned}\\tag{10} \\end{equation} where $O_1,\\dots,O_n$ is any set of operators; $\\mathrm T$ denotes the time-ordering symbol; and $\\mathrm d\\varphi,\\mathrm d\\varpi$ denote the measures $$\\tag{11} \\mathrm d\\varphi\\equiv\\prod_{\\tau,a}\\mathrm d\\varphi_a(\\tau),\\qquad \\mathrm d\\varpi\\equiv\\prod_{\\tau,a}\\frac{1}{2\\pi}\\mathrm d\\varpi^a(\\tau) $$ The time-slicing procedure is standard. We only consider the case $O_i=1$ here. We begin by considering the case where $t_\\mathrm{in}$ and $t_\\mathrm{out}$ are infinitesimally close: $$\\tag{12} \\langle\\varphi';\\tau+\\mathrm d\\tau|\\varphi;\\tau\\rangle=\\langle\\varphi';\\tau|\\exp\\left[-iH\\mathrm d\\tau\\right]|\\varphi;\\tau\\rangle $$ where $H=H(\\phi(\\tau),\\pi(\\tau))$ is the Hamiltonian (the generator of time translations, essentially defined by this equation). We take the convention that all the $\\phi$ must always be moved to the left of $\\pi$. In this case, and up to factors of order $\\mathrm O(\\mathrm d\\tau)^2$, we may replace $\\phi$ by its eigenvalue, to wit, $\\varphi$. To deal with $\\pi$, we insert the identity $1$ in the form of the completeness relation: $$\\tag{13} \\langle\\varphi';\\tau+\\mathrm d\\tau|\\varphi;\\tau\\rangle\\overset{(5)}=\\int\\exp\\left[-iH(\\varphi',\\varpi)\\mathrm d\\tau+i\\sum_a(\\varphi'_a-\\varphi_a)\\varpi^a\\right]\\ \\mathrm d\\varpi $$ where each $\\varpi^a$ is integrated over $\\mathbb R$ unrestrictedly. To find the transition amplitude over a finite interval, we just compose an infinite number of infinitesimal transition amplitudes: we break up $t'-t$ into steps $t,\\tau_1,\\tau_2,\\dots,\\tau_N,t'$, with $\\tau_{k+1}-\\tau_k=\\mathrm d\\tau=(t'-t)/(N+1)$. With this, and inserting the identity $1$ in the form of the completeness relation at each $\\tau_k$, we get \\begin{equation} \\begin{aligned} \\langle\\varphi';t'|\\varphi;t\\rangle&\\overset{(5)}=\\int\\langle \\varphi';t'|\\varphi_N;\\tau_N\\rangle\\langle \\varphi_N;\\tau_N|\\varphi_{N-1};\\tau_{N-1}\\rangle\\cdots\\langle \\varphi_1;\\tau_1|\\varphi;t\\rangle\\ \\prod_{k=1}^N\\mathrm d\\varphi_k\\\\ &\\overset{(13)}=\\int\\left[\\prod_{k=1}^N\\prod_a\\mathrm d\\varphi_{k,a}\\right]\\left[\\prod_{k=0}^N\\prod_a\\frac{\\mathrm d\\varpi^a_k}{2\\pi}\\right]\\cdot\\\\ &\\hspace{20pt}\\cdot\\exp\\left[i\\sum_{k=1}^{N+1}\\left(\\sum_a(\\varphi_{k,a}-\\varphi_{k-1,a})\\varpi_{k-1}^a-H(\\varphi_k,\\varpi_{k-1})\\mathrm d\\tau\\right)\\right] \\end{aligned}\\tag{14} \\end{equation} where $\\varphi_0\\equiv\\varphi$ and $\\varphi_{N+1}\\equiv\\varphi'$. By taking the formal limit $N\\to\\infty$, we indeed obtain the claimed formula. The generalisation of the proof to include insertions is straightforward. Vacuum-to-vacuum transition amplitude. Feynman's $\\boldsymbol{+i\\epsilon}$ prescription. In non-relativistic quantum mechanics, the functional integral as written above is the natural object to work with. On the other hand, when dealing with particle physics in the relativistic regime, one usually works with $S$ matrix elements, that is, one considers the transition amplitude, not from eigenstates of $\\phi_a$, but of the creation and annihilation operators. As is well-known from the LSZ theorem, it suffices to consider the vacuum-to-vacuum transition amplitude, \\begin{equation}\\tag{15} \\langle 0|\\mathrm T\\mathrm e^{iJ^a\\phi_a}|0\\rangle \\end{equation} and from which all $S$-matrix elements can be computed. We thus want to obtain the functional integral representation of this transition amplitude. In Ref.1 there is a rather clean derivation of such object which, unfortunately, is only worked out for a scalar boson field. The outcome is that the vacuum-to-vacuum transition amplitude is given by the functional integral over all field configurations, and the correct boundary conditions are enforced by Feynman's $+i\\epsilon$ prescription. The higher spin case is non-trivial because the propagators (and ground-state wave-functionals) are gauge-dependent. Indeed, where to put the $+i\\epsilon$ in an arbitrary gauge theory is a very complicated matter (e.g., in the axial gauge it is far from clear what to do with, say, $k^4$: should it be $(k^2+i\\epsilon)^2$? Should it be $k^4+i\\epsilon$?), and the general case has not been worked out to the best of my knowledge. It is nevertheless possible to argue that, at least in the 't Hooft-Feynman gauge $\\xi=1$, the propagators and ground-state wave-functional are identical to the scalar case (up to a unit matrix in colour space), so that the derivation in the reference holds for a bosonic field of arbitrary spin. The fermionic case requires Grassmann integration, but a similar analysis (in the $\\xi=1$ case) is possible. Gauge invariance then implies the general case. For completeness, we will prove the claim by an alternative method which, although admittedly not nearly as clean, works for a field of arbitrary spin. The trick is to use the completeness relation in terms of energy eigenstates instead of $\\phi_a$ eigenstates. We proceed as follows. We want to calculate $$\\tag{16} \\lim_{t\\to-\\infty}\\langle \\varphi_\\mathrm{in};t|\\overset{(6)}=\\lim_{t\\to-\\infty}\\int\\mathrm e^{-iEt}\\langle \\varphi_\\mathrm{in}|E\\rangle\\langle E|\\,\\mathrm dE $$ If we send $t$ to $-\\infty$ in a slightly imaginary direction, then all excited states acquire a real and negative part in the exponential factor, which vanishes in the large $t$ limit. We are thus left with the ground state only: $$\\tag{17} \\lim_{t\\to-\\infty+i\\epsilon}\\langle \\varphi_\\mathrm{in};t|=\\langle \\varphi_\\mathrm{in}|0\\rangle\\langle 0| $$ Similarly, $$\\tag{18} \\lim_{t\\to+\\infty+i\\epsilon}|\\varphi_\\mathrm{out};t\\rangle=|0\\rangle\\langle 0|\\varphi_\\mathrm{out}\\rangle $$ With this, the matrix element $\\langle\\varphi_\\mathrm{in};-\\infty|O|\\varphi_\\mathrm{out};+\\infty\\rangle$ can be written as \\begin{equation} \\begin{aligned} &\\langle \\varphi_\\mathrm{in}|0\\rangle\\langle 0|O|0\\rangle\\langle 0|\\varphi_\\mathrm{out}\\rangle = \\int_{\\varphi(-\\infty)=\\varphi_\\mathrm{in}}^{\\varphi(+\\infty)=\\varphi_\\mathrm{out}}O\\cdot\\\\ &\\hspace{20pt}\\cdot\\exp\\left[i\\int_{(1+i\\epsilon)\\mathbb R}\\left(\\sum_a\\dot \\varphi_a(\\tau)\\varpi^a(\\tau)-H(\\varphi(\\tau),\\varpi(\\tau))\\right)\\mathrm d\\tau\\right]\\mathrm d\\varphi\\,\\mathrm d\\varpi \\end{aligned}\\tag{19} \\end{equation} Integrating both sides with respect to $\\mathrm d\\varphi_\\mathrm{in}\\,\\mathrm d\\varphi_\\mathrm{out}$, we get the vacuum-to-vacuum transition amplitude in its standard form, where the integral over $\\mathrm d\\varphi$ is unrestricted: \\begin{equation}\\tag{20} \\langle 0|O|0\\rangle =N^{-1} \\int O\\exp\\left[iS(\\varphi,\\varpi)\\right]\\mathrm d\\varphi\\,\\mathrm d\\varpi \\end{equation} where \\begin{equation}\\tag{21} S(\\varphi,\\varpi)\\equiv\\int_{(1-i\\epsilon)\\mathbb R}\\left(\\sum_a\\dot \\varphi_a(\\tau)\\varpi^a(\\tau)-H(\\varphi(\\tau),\\varpi(\\tau))\\right)\\mathrm d\\tau \\end{equation} is the classical action, and where \\begin{equation}\\tag{22} N\\equiv \\int\\langle \\varphi_\\mathrm{in}|0\\rangle\\langle 0|\\varphi_\\mathrm{out}\\rangle\\,\\mathrm d\\varphi_\\mathrm{in}\\,\\mathrm d\\varphi_\\mathrm{out} \\end{equation} is an inconsequential normalisation constant (it is the norm of the ground-state wave-functional). This proves the claim: the vacuum-to-vacuum transition amplitude is given by the standard functional integral, but over all field configurations (unrestricted integral over $\\mathrm d\\varphi$); and the correct boundary conditions are essentially those that result from a Wick rotation $\\tau\\to-i\\tau_\\mathrm{E}$. The configuration space functional integral. The Lagrangian. Finally, it bears mentioning how the configuration space functional integral is obtained. Ref.1 considers the case where the Hamiltonian is a quadratic polynomial in $\\varpi$: \\begin{equation}\\tag{23} H(\\varphi,\\varpi)=\\frac12\\varpi^a A_{ab}(\\varphi)\\varpi^b+B_a(\\varphi)\\varpi^a+C(\\varphi) \\end{equation} In this case, the integral over $\\mathrm d\\varpi$ is gaussian and so its stationary phase approximation is in fact exact. The stationary point $\\varpi^\\star$ is easily computed to be \\begin{equation}\\tag{24} \\dot\\varphi_a=\\frac{\\partial H}{\\partial\\varpi^a}\\bigg|_{\\varpi\\to\\varpi^\\star} \\end{equation} which agrees with the classical canonical relation. Therefore, the Hamiltonian at $\\varpi^\\star=\\varpi^\\star(\\dot\\varphi)$ is nothing but the Lagrangian $L=L(\\varphi,\\dot \\varphi)$, and therefore \\begin{equation}\\tag{25} \\langle 0|O|0\\rangle \\propto \\int O\\exp\\left[iS(\\varphi)\\right]\\mathrm d\\varphi \\end{equation} where \\begin{equation}\\tag{26} S(\\varphi)\\equiv \\int_{(1-i\\epsilon)\\mathbb R}L(\\varphi,\\dot\\varphi) \\end{equation} and where $\\mathrm d\\varphi$ implicitly includes the determinant of the Vilkovisky metric, \\begin{equation}\\tag{27} \\mathrm d\\varphi\\to\\sqrt{\\det(A)}\\,\\mathrm d\\varphi \\end{equation} which is required for covariance in configuration space (or, equivalently, unitarity). If the metric is flat the determinant can be reabsorbed into $N$. If $H$ is not a quadratic polynomial, we may nevertheless use the stationary phase approximation, but the measure will acquire higher order corrections: \\begin{equation}\\tag{28} \\mathrm d\\varphi\\to\\left(\\sqrt{\\det(A)}+\\mathcal O(\\hbar)\\right)\\,\\mathrm d\\varphi \\end{equation} which can be computed order by order in perturbation theory. This proves that the configuration space functional integral can always be made both covariant and unitary by carefully taking care of the integration measure. In any case, there is a subtlety that must be mentioned: the integral over $\\mathrm d\\varpi$ as written above is only valid if $O$ depends on $\\varphi$ only; for otherwise the integral is not gaussian. In other words, we may not use the configuration space functional integral to compute matrix elements of derivatives of $\\phi$. In pragmatical terms, this is easy to understand: the time ordering symbol $\\mathrm T$ does not commute with time-derivatives. The resolution involves the introduction of the so-called covariant time-ordering symbol which is defined such that it commutes with time derivatives (cf. this PSE post ). References. Weinberg, QFT, Vol.I, chapter 9. 1: Formal in the sense that this is not a rigorous statement, inasmuch as the whole functional-integral formalism is not rigorous itself. It seems hard to formalise the sum over all fields, but one may argue that the picture is at least consistent.",
      "question_latex": [
        "S",
        "\\mathbf{1}_{\\{\\mathbf{x}=\\mathbf{x}'\\}}",
        "0",
        "\\mathbf{x}\\neq\\mathbf{x}'",
        "1",
        "\\mathbf{x}=\\mathbf{x}'",
        "\\phi(\\mathbf{x}')",
        "\\mathbf{x}"
      ],
      "answer_latex": [
        "\\tag1\n[\\phi_a,\\pi^b]=i\\delta_a^b",
        "\\tag4\n\\langle \\varphi|\\varpi\\rangle=\\prod_a\\frac{1}{\\sqrt{2\\pi}}\\mathrm e^{i\\varphi_a\\varpi^a}",
        "\\tag{11}\n\\mathrm d\\varphi\\equiv\\prod_{\\tau,a}\\mathrm d\\varphi_a(\\tau),\\qquad \\mathrm d\\varpi\\equiv\\prod_{\\tau,a}\\frac{1}{2\\pi}\\mathrm d\\varpi^a(\\tau)",
        "\\tag{12}\n\\langle\\varphi';\\tau+\\mathrm d\\tau|\\varphi;\\tau\\rangle=\\langle\\varphi';\\tau|\\exp\\left[-iH\\mathrm d\\tau\\right]|\\varphi;\\tau\\rangle",
        "\\tag{13}\n\\langle\\varphi';\\tau+\\mathrm d\\tau|\\varphi;\\tau\\rangle\\overset{(5)}=\\int\\exp\\left[-iH(\\varphi',\\varpi)\\mathrm d\\tau+i\\sum_a(\\varphi'_a-\\varphi_a)\\varpi^a\\right]\\ \\mathrm d\\varpi",
        "\\tag{16}\n\\lim_{t\\to-\\infty}\\langle \\varphi_\\mathrm{in};t|\\overset{(6)}=\\lim_{t\\to-\\infty}\\int\\mathrm e^{-iEt}\\langle \\varphi_\\mathrm{in}|E\\rangle\\langle E|\\,\\mathrm dE",
        "\\tag{17}\n\\lim_{t\\to-\\infty+i\\epsilon}\\langle \\varphi_\\mathrm{in};t|=\\langle \\varphi_\\mathrm{in}|0\\rangle\\langle 0|",
        "\\tag{18}\n\\lim_{t\\to+\\infty+i\\epsilon}|\\varphi_\\mathrm{out};t\\rangle=|0\\rangle\\langle 0|\\varphi_\\mathrm{out}\\rangle",
        "(3)",
        "\\{\\phi_a,\\pi^a\\}_a",
        "a\\in\\mathbb R^{d-1}\\times \\mathbb N^n",
        "\\mathbb R^d",
        "\\mathbb N^n",
        "where",
        "is a Dirac-Kronecker delta. Here,",
        "denotes a commutator (we assume",
        "to be Grassmann even; we could consider the general case here by keeping tracks of the signs, but we won't for simplicity). The rest of commutators are assumed to vanish. We take the phase-space operators to be hermitian (or otherwise, we double the dimension and split them into their real and imaginary parts).</p>\n\n<p>As",
        ", we may simultaneously diagonalise them:\n\\begin{equation}\n\\begin{aligned}\n\\phi_a|\\varphi\\rangle&=\\varphi_a|\\varphi\\rangle\\\\\n\\pi^a|\\varpi\\rangle&=\\varpi^a|\\varpi\\rangle\n\\end{aligned}\\tag2\n\\end{equation}\nwhere",
        "are",
        "-numbers. After normalising them, if necessary, these eigenvectors are orthonormal:\n\\begin{equation}\n\\begin{aligned}\n\\langle\\varphi|\\varphi'\\rangle&=\\prod_a\\delta(\\varphi_a-\\varphi'_a)\\equiv\\delta(\\varphi-\\varphi')\\\\\n\\langle\\varpi|\\varpi'\\rangle&=\\prod_a\\delta(\\varpi^a-\\varpi'^a)\\equiv\\delta(\\varpi-\\varpi')\n\\end{aligned}\\tag3\n\\end{equation}\nand, as per",
        ", we also have",
        "</p>\n\n<p>As the sets",
        "and",
        "are both assumed complete, we also have\n\\begin{equation}\n\\begin{aligned}\n1&\\equiv \\int\\prod_a|\\varphi\\rangle\\langle\\varphi|\\,\\mathrm d\\varphi_a\\\\\n1&\\equiv \\int\\prod_a|\\varpi\\rangle\\langle\\varpi|\\,\\mathrm d\\varpi^a\n\\end{aligned}\\tag5\n\\end{equation}\nas OP anticipated.</p>\n\n<p>As mentioned in the comments, and for future reference, we note that there is another identity operator that is fundamental in a quantum theory, to wit, the resolution in terms of energy eigenstates:\n\\begin{equation}\\tag6\n1\\equiv \\int|E\\rangle\\langle E|\\,\\mathrm dE\n\\end{equation}\nwhere",
        "is the counting measure in the case of discrete eigenvalues. Typically, we assume that the Hamiltonian is non-negative and that its ground state energy is zero; furthermore, we assume that there is a non-zero mass gap so that",
        "is a regular eigenvalue of",
        "(as opposed to a singular value).</p>\n\n<p>It is at this point convenient to switch into the Heisenberg picture, where",
        "become time-dependent:\n\\begin{equation}\n\\begin{aligned}\n\\phi_a(t)\\equiv\\mathrm e^{iHt}\\phi_a\\mathrm e^{-iHt}\\\\\n\\pi^a(t)\\equiv\\mathrm e^{iHt}\\pi^a\\mathrm e^{-iHt}\n\\end{aligned}\\tag7\n\\end{equation}\nwith eigenstates\n\\begin{equation}\n\\begin{aligned}\n|\\varphi;t\\rangle&=\\mathrm e^{iHt}|\\varphi\\rangle\\\\\n|\\varpi;t\\rangle&=\\mathrm e^{iHt}|\\varpi\\rangle\n\\end{aligned}\\tag8\n\\end{equation}\nsuch that\n\\begin{equation}\n\\begin{aligned}\n\\phi_a(t)|\\varphi;t\\rangle&=\\varphi_a|\\varphi;t\\rangle\\\\\n\\pi^a(t)|\\varpi;t\\rangle&=\\varpi^a|\\varpi;t\\rangle\n\\end{aligned}\\tag9\n\\end{equation}</p>\n\n<p>The Heisenberg picture eigenstates satisfy the same completeness and orthonormality relations as the Schrödinger picture eigenstates (inasmuch as the transformation is unitary).</p>\n\n<h2>The functional integral. Time slicing.</h2>\n\n<p>From this, and by the usual arguments (time slicing), Ref.1 derives the phase-space functional integral representation of the transition amplitude, to wit,\n\\begin{equation}\n\\begin{aligned}\n&\\langle\\varphi_\\mathrm{in};t_\\mathrm{in}|\\mathrm T\\left(O_1(\\pi(t_1),\\phi(t_1)),\\dots,O_n(\\pi(t_n),\\phi(t_n))\\right)|\\varphi_\\mathrm{out};t_\\mathrm{out}\\rangle\\equiv\\\\\n&\\hspace{10pt}\\int_{\\varphi(t_\\mathrm{in})=\\varphi_\\mathrm{in}}^{\\varphi(t_\\mathrm{out})=\\varphi_\\mathrm{out}}\\left(O_1(\\varpi(t_1),\\varphi(t_1)),\\dots,O_n(\\varpi(t_n),\\varphi(t_n))\\right)\\cdot\\\\\n&\\hspace{25pt}\\cdot\\exp\\left[i\\int_{t_\\mathrm{in}}^{t_\\mathrm{out}}\\left(\\sum_a\\dot \\varphi_a(\\tau)\\varpi^a(\\tau)-H(\\varphi(\\tau),\\varpi(\\tau))\\right)\\mathrm d\\tau\\right]\\mathrm d\\varphi\\,\\mathrm d\\varpi\n\\end{aligned}\\tag{10}\n\\end{equation}\nwhere",
        "is any set of operators;",
        "denotes the time-ordering symbol; and",
        "denote the measures",
        "</p>\n\n<p>The time-slicing procedure is standard. We only consider the case",
        "here. We begin by considering the case where",
        "are infinitesimally close:",
        "is the Hamiltonian (the generator of time translations, essentially defined by this equation). We take the convention that all the",
        "must always be moved to the left of",
        ". In this case, and up to factors of order",
        ", we may replace",
        "by its eigenvalue, to wit,",
        ". To deal with",
        ", we insert the identity",
        "in the form of the completeness relation:",
        "where each",
        "is integrated over",
        "unrestrictedly.</p>\n\n<p>To find the transition amplitude over a finite interval, we just compose an infinite number of infinitesimal transition amplitudes: we break up",
        "into steps",
        ", with",
        ". With this, and inserting the identity",
        "in the form of the completeness relation at each",
        ", we get\n\\begin{equation}\n\\begin{aligned}\n\\langle\\varphi';t'|\\varphi;t\\rangle&\\overset{(5)}=\\int\\langle \\varphi';t'|\\varphi_N;\\tau_N\\rangle\\langle \\varphi_N;\\tau_N|\\varphi_{N-1};\\tau_{N-1}\\rangle\\cdots\\langle \\varphi_1;\\tau_1|\\varphi;t\\rangle\\ \\prod_{k=1}^N\\mathrm d\\varphi_k\\\\\n&\\overset{(13)}=\\int\\left[\\prod_{k=1}^N\\prod_a\\mathrm d\\varphi_{k,a}\\right]\\left[\\prod_{k=0}^N\\prod_a\\frac{\\mathrm d\\varpi^a_k}{2\\pi}\\right]\\cdot\\\\\n&\\hspace{20pt}\\cdot\\exp\\left[i\\sum_{k=1}^{N+1}\\left(\\sum_a(\\varphi_{k,a}-\\varphi_{k-1,a})\\varpi_{k-1}^a-H(\\varphi_k,\\varpi_{k-1})\\mathrm d\\tau\\right)\\right]\n\\end{aligned}\\tag{14}\n\\end{equation}\nwhere",
        ". By taking the formal limit",
        ", we indeed obtain the claimed formula. The generalisation of the proof to include insertions is straightforward.</p>\n\n<h2>Vacuum-to-vacuum transition amplitude. Feynman's",
        "prescription.</h2>\n\n<p>In non-relativistic quantum mechanics, the functional integral as written above is the natural object to work with. On the other hand, when dealing with particle physics in the relativistic regime, one usually works with",
        "matrix elements, that is, one considers the transition amplitude, not from eigenstates of",
        ", but of the creation and annihilation operators. As is well-known from the LSZ theorem, it suffices to consider the vacuum-to-vacuum transition amplitude,\n\\begin{equation}\\tag{15}\n\\langle 0|\\mathrm T\\mathrm e^{iJ^a\\phi_a}|0\\rangle\n\\end{equation}\nand from which all",
        "-matrix elements can be computed. We thus want to obtain the functional integral representation of this transition amplitude.</p>\n\n<p>In Ref.1 there is a rather clean derivation of such object which, unfortunately, is only worked out for a scalar boson field. The outcome is that the vacuum-to-vacuum transition amplitude is given by the functional integral over all field configurations, and the correct boundary conditions are enforced by Feynman's",
        "prescription. The higher spin case is non-trivial because the propagators (and ground-state wave-functionals) are gauge-dependent. Indeed, where to put the",
        "in an arbitrary gauge theory is a very complicated matter (e.g., in the axial gauge it is far from clear what to do with, say,",
        ": should it be",
        "? Should it be",
        "?), and the general case has not been worked out to the best of my knowledge. It is nevertheless possible to argue that, at least in the 't Hooft-Feynman gauge",
        ", the propagators and ground-state wave-functional are identical to the scalar case (up to a unit matrix in colour space), so that the derivation in the reference holds for a bosonic field of arbitrary spin. The fermionic case requires Grassmann integration, but a similar analysis (in the",
        "case) is possible. Gauge invariance then implies the general case.</p>\n\n<p>For completeness, we will prove the claim by an alternative method which, although admittedly not nearly as clean, works for a field of arbitrary spin. The trick is to use the completeness relation in terms of energy eigenstates instead of",
        "eigenstates.</p>\n\n<p>We proceed as follows. We want to calculate",
        "</p>\n\n<p>If we send",
        "to",
        "in a slightly imaginary direction, then all excited states acquire a real and negative part in the exponential factor, which vanishes in the large",
        "limit. We are thus left with the ground state only:",
        "</p>\n\n<p>Similarly,",
        "</p>\n\n<p>With this, the matrix element",
        "can be written as\n\\begin{equation}\n\\begin{aligned}\n&\\langle \\varphi_\\mathrm{in}|0\\rangle\\langle 0|O|0\\rangle\\langle 0|\\varphi_\\mathrm{out}\\rangle\n=\n\\int_{\\varphi(-\\infty)=\\varphi_\\mathrm{in}}^{\\varphi(+\\infty)=\\varphi_\\mathrm{out}}O\\cdot\\\\\n&\\hspace{20pt}\\cdot\\exp\\left[i\\int_{(1+i\\epsilon)\\mathbb R}\\left(\\sum_a\\dot \\varphi_a(\\tau)\\varpi^a(\\tau)-H(\\varphi(\\tau),\\varpi(\\tau))\\right)\\mathrm d\\tau\\right]\\mathrm d\\varphi\\,\\mathrm d\\varpi\n\\end{aligned}\\tag{19}\n\\end{equation}</p>\n\n<p>Integrating both sides with respect to",
        ", we get the vacuum-to-vacuum transition amplitude in its standard form, where the integral over",
        "is unrestricted:\n\\begin{equation}\\tag{20}\n\\langle 0|O|0\\rangle\n=N^{-1}\n\\int O\\exp\\left[iS(\\varphi,\\varpi)\\right]\\mathrm d\\varphi\\,\\mathrm d\\varpi\n\\end{equation}\nwhere\n\\begin{equation}\\tag{21}\nS(\\varphi,\\varpi)\\equiv\\int_{(1-i\\epsilon)\\mathbb R}\\left(\\sum_a\\dot \\varphi_a(\\tau)\\varpi^a(\\tau)-H(\\varphi(\\tau),\\varpi(\\tau))\\right)\\mathrm d\\tau\n\\end{equation}\nis the classical action, and where \n\\begin{equation}\\tag{22}\nN\\equiv \\int\\langle \\varphi_\\mathrm{in}|0\\rangle\\langle 0|\\varphi_\\mathrm{out}\\rangle\\,\\mathrm d\\varphi_\\mathrm{in}\\,\\mathrm d\\varphi_\\mathrm{out}\n\\end{equation}\nis an inconsequential normalisation constant (it is the norm of the ground-state wave-functional). This proves the claim: the vacuum-to-vacuum transition amplitude is given by the standard functional integral, but over all field configurations (unrestricted integral over",
        "); and the correct boundary conditions are essentially those that result from a Wick rotation",
        ".</p>\n\n<h2>The configuration space functional integral. The Lagrangian.</h2>\n\n<p>Finally, it bears mentioning how the configuration space functional integral is obtained. Ref.1 considers the case where the Hamiltonian is a quadratic polynomial in",
        ":\n\\begin{equation}\\tag{23}\nH(\\varphi,\\varpi)=\\frac12\\varpi^a A_{ab}(\\varphi)\\varpi^b+B_a(\\varphi)\\varpi^a+C(\\varphi)\n\\end{equation}</p>\n\n<p>In this case, the integral over",
        "is gaussian and so its stationary phase approximation is in fact exact. The stationary point",
        "is easily computed to be\n\\begin{equation}\\tag{24}\n\\dot\\varphi_a=\\frac{\\partial H}{\\partial\\varpi^a}\\bigg|_{\\varpi\\to\\varpi^\\star}\n\\end{equation}\nwhich agrees with the classical canonical relation. Therefore, the Hamiltonian at",
        "is nothing but the Lagrangian",
        ", and therefore\n\\begin{equation}\\tag{25}\n\\langle 0|O|0\\rangle\n\\propto\n\\int O\\exp\\left[iS(\\varphi)\\right]\\mathrm d\\varphi\n\\end{equation}\nwhere\n\\begin{equation}\\tag{26}\nS(\\varphi)\\equiv \\int_{(1-i\\epsilon)\\mathbb R}L(\\varphi,\\dot\\varphi)\n\\end{equation}\nand where",
        "implicitly includes the determinant of the Vilkovisky metric,\n\\begin{equation}\\tag{27}\n\\mathrm d\\varphi\\to\\sqrt{\\det(A)}\\,\\mathrm d\\varphi\n\\end{equation}\nwhich is required for covariance in configuration space (or, equivalently, unitarity). If the metric is flat the determinant can be reabsorbed into",
        ".</p>\n\n<p>If",
        "is not a quadratic polynomial, we may nevertheless use the stationary phase approximation, but the measure will acquire higher order corrections:\n\\begin{equation}\\tag{28}\n\\mathrm d\\varphi\\to\\left(\\sqrt{\\det(A)}+\\mathcal O(\\hbar)\\right)\\,\\mathrm d\\varphi\n\\end{equation}\nwhich can be computed order by order in perturbation theory. This proves that the configuration space functional integral can always be made both covariant and unitary by carefully taking care of the integration measure.</p>\n\n<p>In any case, there is a subtlety that must be mentioned: the integral over",
        "as written above is only valid if",
        "depends on",
        "only; for otherwise the integral is not gaussian. In other words, we may not use the configuration space functional integral to compute matrix elements of derivatives of",
        ". In pragmatical terms, this is easy to understand: the time ordering symbol"
      ],
      "created": "2018-01-31T15:02:22.140",
      "golden_ner_terms": [
        "action",
        "analysis",
        "approximation",
        "axial",
        "base",
        "boundary",
        "boundary condition",
        "boundary conditions",
        "calculate",
        "canonical",
        "clear",
        "colour",
        "commutator",
        "complete",
        "configuration",
        "configuration space",
        "conjugate",
        "consistent",
        "constant",
        "contains",
        "continuous",
        "continuum",
        "counting measure",
        "covariance",
        "derivation",
        "determinant",
        "diagonalise",
        "dimension",
        "discrete",
        "eigenvalue",
        "eigenvalues",
        "eigenvectors",
        "elements",
        "energy",
        "equation",
        "even",
        "exponent",
        "exponential",
        "factor",
        "field",
        "field theory",
        "finite",
        "flat",
        "formalism",
        "formula",
        "function",
        "functional",
        "gauge",
        "gauge invariance",
        "gauge theory",
        "gaussian",
        "generator",
        "ground state",
        "hamiltonian",
        "hermitian",
        "higher spin",
        "identity",
        "identity operator",
        "imaginary",
        "imaginary part",
        "implies",
        "index",
        "indicator function",
        "infinite",
        "infinitesimal",
        "integral",
        "integration",
        "interval",
        "lagrangian",
        "limit",
        "mass",
        "matrix",
        "matrix elements",
        "matter",
        "measure",
        "metric",
        "momentum",
        "necessary",
        "negative",
        "norm",
        "number",
        "object",
        "operator",
        "operators",
        "order",
        "ordering",
        "orthonormal",
        "outcome",
        "particle physics",
        "path",
        "path integral",
        "perturbation",
        "perturbation theory",
        "physics",
        "point",
        "polynomial",
        "product",
        "qft",
        "quantum field theory",
        "quantum mechanics",
        "real",
        "regular",
        "relation",
        "representation",
        "rotation",
        "satisfy",
        "scalar",
        "similar",
        "singular",
        "singular value",
        "space",
        "spacetime",
        "state",
        "stationary",
        "stationary point",
        "subscript",
        "sum",
        "theorem",
        "theory",
        "time",
        "transformation",
        "unit",
        "unitarity",
        "unitary",
        "valid",
        "vanish",
        "vanishes",
        "vector",
        "vector space",
        "vectors",
        "way",
        "wick rotation",
        "work",
        "zero"
      ],
      "golden_ner_count": 134,
      "golden_patterns": [
        {
          "pattern": "work-examples-first",
          "score": 4.0,
          "hotwords": [
            "e.g.",
            "consider the case"
          ]
        },
        {
          "pattern": "check-the-extreme-cases",
          "score": 4.0,
          "hotwords": [
            "boundary",
            "zero"
          ]
        },
        {
          "pattern": "construct-an-explicit-witness",
          "score": 4.0,
          "hotwords": [
            "construct",
            "explicit"
          ]
        },
        {
          "pattern": "encode-as-algebra",
          "score": 4.0,
          "hotwords": [
            "polynomial",
            "ring"
          ]
        },
        {
          "pattern": "estimate-by-bounding",
          "score": 4.0,
          "hotwords": [
            "bound",
            "at least"
          ]
        },
        {
          "pattern": "quotient-by-irrelevance",
          "score": 2.0,
          "hotwords": [
            "up to"
          ]
        },
        {
          "pattern": "reduce-to-known-result",
          "score": 2.0,
          "hotwords": [
            "well-known"
          ]
        },
        {
          "pattern": "the-diagonal-argument",
          "score": 2.0,
          "hotwords": [
            "diagonal"
          ]
        },
        {
          "pattern": "transport-across-isomorphism",
          "score": 2.0,
          "hotwords": [
            "equivalent"
          ]
        },
        {
          "pattern": "monotone-approximation",
          "score": 2.0,
          "hotwords": [
            "limit"
          ]
        }
      ],
      "golden_pattern_names": [
        "work-examples-first",
        "check-the-extreme-cases",
        "construct-an-explicit-witness",
        "encode-as-algebra",
        "estimate-by-bounding",
        "quotient-by-irrelevance",
        "reduce-to-known-result",
        "the-diagonal-argument",
        "transport-across-isomorphism",
        "monotone-approximation"
      ],
      "golden_scopes": [
        {
          "type": "let-binding",
          "match": "Let $\\{\\phi_a,\\pi^a\\}_a$ be"
        },
        {
          "type": "for-any",
          "match": "each $\\varpi^a$"
        },
        {
          "type": "for-any",
          "match": "each $\\tau_k$"
        },
        {
          "type": "for-any",
          "match": "all $S$"
        },
        {
          "type": "where-binding",
          "match": "where $a\\in\\mathbb R^{d-1}\\times \\mathbb N^n$ is"
        },
        {
          "type": "where-binding",
          "match": "where $\\delta$ is"
        },
        {
          "type": "where-binding",
          "match": "where $\\mathrm dE$ is"
        },
        {
          "type": "where-binding",
          "match": "where $O_1,\\dots,O_n$ is"
        },
        {
          "type": "where-binding",
          "match": "where $H=H(\\phi(\\tau),\\pi(\\tau))$ is"
        }
      ],
      "golden_scope_count": 9
    },
    {
      "id": "se-physics-313758",
      "stratum": "easy",
      "title": "What's the rigorous definition of phase and phase transition?",
      "tags": [
        "statistical-mechanics",
        "condensed-matter",
        "phase-transition"
      ],
      "score": 51,
      "answer_score": 38,
      "question_body": "I always feel unsure about the definitions of phase and phase transition. First, let's discuss in Laudau's paradigm. For example, some people say that phase is classified by symmetry. Some people say that phase is classified by order parameter and that a phase transition is when there is some discontinuity in free energy. Does this mean that gas and liquid are the same phase? Because in the phase diagram they are connected and they have the same symmetries (translations and rotations). If they are not the same phase, what should we call the state of large pressure and large temperature? Liquid or gas? Does this mean that above the critical point the transition from gas to liquid is not a phase transition, but below the critial point the transition from gas to liquid is a phase transition? If the answers to my first and second question are \"yes\", does this mean that even in the same phase, there can still be a phase transition? This conclusion is so weird! In Landau's paradigm, what's the symmetry breaking and order parameter in the gas-liquid phase transition? It seems that the symmetry is same in gas and liquid. Gas-liquid phase transition must be able to be explained by Landau's paradigm but Landau's paradigm says that there must be symmetry breaking in a phase transition. There is an answer . I admit that from modern point of view phase transition is not necessarily due to symmetry breaking, but I don't think that gas-liquid transition has been beyond Landau's paradigm. Up to now we only talk about the classical phase transition. If we consider the general paradigm, we know that symmetry breaking must imply phase transition, but phase transition don't imply symmetry breaking. For example, in $Z_2$ gauge Ising model, we can prove there is no symmetry breaking and local magnet is always zero. But we can choose Wilson loop as order parameter and find there is confined and deconfined phase. So if given one phase, we first find the symmetry is same in this phase and then check that several other order parameters are also the same in this phase. However how do you prove that there is no weird order parameter that in one part of this phase is zero and in another part of this phase is nonzero? For example, in a solid phase of water which has the same crystal structure, how to prove that any order parameter that you can construct will not be zero in one part of the phase and nonzero in other part?",
      "answer_body": "Does this mean that gas and liquid are the same phases? Because in the phase diagram they are connected and they have the same symmetry(translation and rotation). If they are not the same phase, how to call the state in large pressure and large temperature? Liquid or gas? Yes. From the modern point of view, liquid and gas are in the same phase. Because, as the asker has mentioned, they are continuously connected in the phase diagram through the \"supercritical\" regime. By definition, two states of matter are in the same phase if they can be smoothly deformed to each other without going through phase transitions . Historically, liquid and gas are named as different phases (by mistake) because people thought that \"there must be one different phase at each side of the phase transition\" (as argued in Diracology's answer). But this idea is wrong. We can not declare different phases just by observing phase transitions. Otherwise, for example, in the following phase diagram on the left, we could have declared states A and B to be in different phases, simply because they are separated by phase transitions, as we can first go out of the blue phase and then reenter it. This way of separating phases is clearly stupid. Any reasonable person would agree that A and B should belong to the same phase in this case. Now we just deform the left phase diagram to the right by squeezing the intermediate red phase to a first-order transition line, then why we suddenly get confused about whether A and B are in the same phase or not? Definitely, they should still remain in the same phase! The liquid-gas transition is indeed a situation like this. So a logically consistent definition will have to define liquid and gas as a single phase. Does this mean that above the critical point the transition from gas to liquid is not a phase transition, but below the critical point, the transition from gas to liquid is a phase transition? Yes. If answers to my first and second question are \"yes\", does this mean even in the same phase there still can have phase transition? This conclusion is so weird! Given the example of the above phase diagrams, one will not feel weird about the fact that there can be (first-order) phase transitions inside a single phase. In fact, first-order transitions often appear by merging two second-order transitions together (this can be explained by Landau's theory). So going across a first-order transition is like going out of the phase and back again immediately, which definitely can happen inside a single phase. However, I am not aware of any example that continuous phase transitions can also happen within a single phase. So I conjecture that if a phase transition happens inside a phase, it must be first-order . (The conjecture is falsified by the recent discovery of \"unnecessary\" quantum criticalities in Bi, Senthil 2018 , Jian, Xu 2019 , Verresen, Bibo, Pollmann 2021 . -- Edited 2021) The liquid-gas transition is one example of my conjecture. From the Landau's paradigm, what's the symmetry breaking and order parameter in the gas-liquid phase transition? It seems that the symmetry is same in gas and liquid. Gas-liquid phase transition must be able to be explained by Landau's paradigm but Landau's paradigm says that there must be symmetry breaking in phase transition. There is an answer. I admit that from a modern point of view phase transition is not necessary due to symmetry breaking, but I don't think that gas-liquid transition has been beyond Landau's paradigm. No symmetry breaking is associated with the liquid-gas transition. Landau's paradigm only says that there must be spontaneous symmetry breaking in second-order transitions, but not in first-order transitions. In fact, nothing can be said about first-order transitions, because first-order transitions can happen anywhere in the phase diagram without any reason. The liquid-gas transition is indeed a case like this. Even though the liquid-gas transition is not a symmetry breaking transition, it can still be described within Landau's paradigm phenomenologically (who says that Landau's theory only applies to symmetry breaking transitions?). We can introduce the density $\\rho$ of the fluid as the order parameter. Because no symmetry is acting on this order parameter, so there is no symmetry reason to forbid odd-order terms like $\\rho, \\rho^3, \\cdots$ to appear in Landau's free energy. However the first order term can always be absorbed by redefining the order parameter with a shift $\\rho\\to\\rho+\\rho_0$ , then the Landau free energy takes the general form of $$F= F_0 + a \\rho^2 + b \\rho^3+ c \\rho^4+\\cdots.$$ First-order transition happens by driving the parameter $a$ to $a<9b^2/32c$ . From this example, we can see that (within Landau's paradigm) if a phase transition happens without breaking any symmetry, it must be first-order . Again the liquid-gas transition is one such example. So if given one phase, we firstly find the symmetry is same in this phase and then check several order parameters also same in this phase. However, how do you prove you must not be able to construct some weird order parameters such that in one part of this phase is zero and in another part of this phase is nonzero? For example, in a solid phase of water which has the same crystal structure, how to prove any order parameter that you can construct will not be zero in one part of the phase and nonzero in another part? Indeed, you can never rule out the possibility that some weird order parameter is hiding there to further divide the phase into more phases. That is actually why the solid phase of water is divided into so many different crystal phases. Each crystal structure is associated with a different symmetry-breaking pattern. Sometimes the symmetries are just so complicated that you may miss one or two of them if you are not careful enough. In that case, you will also miss the order parameters associated with the missing symmetry, until you see a specific heat anomaly in the experiment where you did not expect, then you start to realize that oh there is a missing order parameter that actually changes across this transition, and one needs to add some additional symmetry to explain it. This is actually the typical way that physicists work every day, they never figure out the full classification of phases until they see the evidence for new phases and phase transitions. I think this is also the fun part of condensed matter physics: there are always new phases of matter waiting for us to discover .",
      "question_latex": [
        "Z_2"
      ],
      "answer_latex": [
        "F= F_0 + a \\rho^2 + b \\rho^3+ c \\rho^4+\\cdots.",
        "\\rho",
        "\\rho, \\rho^3, \\cdots",
        "\\rho\\to\\rho+\\rho_0",
        "</span></p>\n<p>First-order transition happens by driving the parameter <span class=\"math-container\">",
        "</span> to <span class=\"math-container\">"
      ],
      "created": "2017-02-21T19:13:15.250",
      "golden_ner_terms": [
        "blue",
        "choose",
        "conclusion",
        "condensed matter",
        "conjecture",
        "connected",
        "consistent",
        "construct",
        "continuous",
        "critical point",
        "density",
        "diagram",
        "energy",
        "even",
        "first order",
        "gas",
        "gauge",
        "heat",
        "ising model",
        "line",
        "loop",
        "matter",
        "mean",
        "model",
        "necessary",
        "order",
        "parameter",
        "phase diagram",
        "phase transition",
        "physics",
        "point",
        "pressure",
        "right",
        "rotation",
        "second-order",
        "separated",
        "separating",
        "side",
        "solid",
        "state",
        "states of matter",
        "structure",
        "symmetry",
        "symmetry breaking",
        "temperature",
        "term",
        "theory",
        "water",
        "way",
        "wilson loop",
        "work",
        "zero"
      ],
      "golden_ner_count": 52,
      "golden_patterns": [
        {
          "pattern": "work-examples-first",
          "score": 2.0,
          "hotwords": [
            "example"
          ]
        },
        {
          "pattern": "check-the-extreme-cases",
          "score": 2.0,
          "hotwords": [
            "zero"
          ]
        },
        {
          "pattern": "exploit-symmetry",
          "score": 2.0,
          "hotwords": [
            "symmetry"
          ]
        },
        {
          "pattern": "construct-an-explicit-witness",
          "score": 2.0,
          "hotwords": [
            "construct"
          ]
        },
        {
          "pattern": "local-to-global",
          "score": 2.0,
          "hotwords": [
            "cover"
          ]
        },
        {
          "pattern": "unfold-the-definition",
          "score": 2.0,
          "hotwords": [
            "by definition"
          ]
        },
        {
          "pattern": "construct-auxiliary-object",
          "score": 2.0,
          "hotwords": [
            "introduce"
          ]
        }
      ],
      "golden_pattern_names": [
        "work-examples-first",
        "check-the-extreme-cases",
        "exploit-symmetry",
        "construct-an-explicit-witness",
        "local-to-global",
        "unfold-the-definition",
        "construct-auxiliary-object"
      ],
      "golden_scopes": [],
      "golden_scope_count": 0
    },
    {
      "id": "se-physics-21955",
      "stratum": "easy",
      "title": "Ignorance in statistical mechanics",
      "tags": [
        "statistical-mechanics",
        "entropy",
        "quantum-information",
        "information"
      ],
      "score": 27,
      "answer_score": 25,
      "question_body": "Consider this penny on my desc. It is a particular piece of metal, well described by statistical mechanics, which assigns to it a state, namely the density matrix $\\rho_0=\\frac{1}{Z}e^{-\\beta H}$ (in the simplest model). This is an operator in a space of functions depending on the coordinates of a huge number $N$ of particles. The ignorance interpretation of statistical mechanics , the orthodoxy to which all introductions to statistical mechanics pay lipservice, claims that the density matrix is a description of ignorance, and that the true description should be one in terms of a wave function; any pure state consistent with the density matrix should produce the same macroscopic result. Howewer, it would be very surprising if Nature would change its behavior depending on how much we ignore. Thus the talk about ignorance must have an objective formalizable basis independent of anyones particular ignorant behavior. On the other hand, statistical mechanics always works exclusively with the density matrix (except in the very beginning where it is motivated). Nowhere (except there) one makes any use of the assumption that the density matrix expresses ignorance. Thus it seems to me that the whole concept of ignorance is spurious, a relic of the early days of statistical mechanics. Thus I'd like to invite the defenders of orthodoxy to answer the following questions: (i) Can the claim be checked experimentally that the density matrix (a canonical ensemble, say, which correctly describes a macroscopic system in equilibrium) describes ignorance? - If yes, how, and whose ignorance? - If not, why is this ignorance interpretation assumed though nothing at all depends on it? (ii) In a though experiment, suppose Alice and Bob have different amounts of ignorance about a system. Thus Alice's knowledge amounts to a density matrix $\\rho_A$, whereas Bob's knowledge amounts to a density matrix $\\rho_B$. Given $\\rho_A$ and $\\rho_B$, how can one check in principle whether Bob's description is consistent with that of Alice? (iii) How does one decide whether a pure state $\\psi$ is adequately represented by a statistical mechanics state $\\rho_0$? In terms of (ii), assume that Alice knows the true state of the system (according to the ignorance interpretation of statistical mechanics a pure state $\\psi$, corresponding to $\\rho_A=\\psi\\psi^*$), whereas Bob only knows the statistical mechanics description, $\\rho_B=\\rho_0$. Presumably, there should be a kind of quantitative measure $M(\\rho_A,\\rho_B)\\ge 0$ that vanishes when $\\rho_A=\\rho_B)$ and tells how compatible the two descriptions are. Otherwise, what can it mean that two descriptions are consistent? However, the mathematically natural candidate, the relative entropy (= Kullback-Leibler divergence) $M(\\rho_A,\\rho_B)$, the trace of $\\rho_A\\log\\frac{\\rho_A}{\\rho_B}$, [edit: I corrected a sign mistake pointed out in the discussion below] does not work. Indeed, in the situation (iii), $M(\\rho_A,\\rho_B)$ equals the expectation of $\\beta H+\\log Z$ in the pure state; this is minimal in the ground state of the Hamiltonian. But this would say that the ground state would be most consistent with the density matrix of any temperature, an unacceptable condition. Edit: After reading the paper http://bayes.wustl.edu/etj/articles/gibbs.paradox.pdf by E.T. Jaynes pointed to in the discussion below, I can make more precise the query in (iii): In the terminology of p.5 there, the density matrix $\\rho_0$ represents a macrostate, while each wave function $\\psi$ represents a microstate. The question is then: When may (or may not) a microstate $\\psi$ be regarded as a macrostate $\\rho_0$ without affecting the predictability of the macroscopic observations? In the above case, how do I compute the temperature of the macrostate corresponding to a particular microstate $\\psi$ so that the macroscopic behavior is the same - if it is, and which criterion allows me to decide whether (given $\\psi$) this approximation is reasonable? An example where it is not reasonable to regard $\\psi$ as a canonical ensemble is if $\\psi$ represents a composite system made of two pieces of the penny at different temperature. Clearly no canonical ensemble can describe this situation macroscopically correct. Thus the criterion sought must be able to decide between a state representing such a composite system and the state of a penny of uniform temperature, and in the latter case, must give a recipe how to assign a temperature to $\\psi$, namely the temperature that nature allows me to measure. The temperature of my penny is determined by Nature, hence must be determined by a microstate that claims to be a complete description of the penny. I have never seen a discussion of such an identification criterion, although they are essential if one want to give the idea - underlying the ignorance interpretation - that a completely specified quantum state must be a pure state. Part of the discussion on this is now at: http://chat.stackexchange.com/rooms/2712/discussion-between-arnold-neumaier-and-nathaniel Edit (March 11, 2012): I accepted Nathaniel's answer as satisfying under the given circumstances, though he forgot to mention a fouth possibility that I prefer; namely that the complete knowledge about a quantum system is in fact described by a density matrix, so that microstates are arbitrary density matrces and a macrostate is simply a density matrix of a special form by which an arbitrary microstate (density matrix) can be well approximated when only macroscopic consequences are of interest. These special density matrices have the form $\\rho=e^{-S/k_B}$ with a simple operator $S$ - in the equilibrium case a linear combination of 1, $H$ (and vaiious number operators $N_j$ if conserved), defining the canonical or grand canonical ensemble. This is consistent with all of statistical mechanics, and has the advantage of simplicity and completeness, compared to the ignorance interpretation, which needs the additional qualitative concept of ignorance and with it all sorts of questions that are too imprecise or too difficult to answer.",
      "answer_body": "I wouldn't say the ignorance interpretation is a relic of the early days of statistical mechanics. It was first proposed by Edwin Jaynes in 1957 (see http://bayes.wustl.edu/etj/node1.html , papers 9 and 10, and also number 36 for a more detailed version of the argument) and proved controversial up until fairly recently. (Jaynes argued that the ignorance interpretation was implicit in the work of Gibbs, but Gibbs himself never spelt it out.) Until recently, most authors preferred an interpretation in which (for a classical system at least) the probabilities in statistical mechanics represented the fraction of time the system spends in each state, rather than the probability of it being in a particular state at the present time. This old interpretation makes it impossible to reason about transient behaviour using statistical mechanics, and this is ultimately what makes switching to the ignorance interpretation useful. In response to your numbered points: (i) I'll answer the \"whose ignorance?\" part first. The answer to this is \"an experimenter with access to macroscopic measuring instruments that can measure, for example, pressure and temperature, but cannot determine the precise microscopic state of the system.\" If you knew precisely the underlying wavefunction of the system (together with the complete wavefunction of all the particles in the heat bath if there is one, along with the Hamiltonian for the combined system) then there would be no need to use statistical mechanics at all, because you could simply integrate the Schrödinger equation instead. The ignorance interpretation of statistical mechanics does not claim that Nature changes her behaviour depending on our ignorance; rather, it claims that statistical mechanics is a tool that is only useful in those cases where we have some ignorance about the underlying state or its time evolution. Given this, it doesn't really make sense to ask whether the ignorance interpretation can be confirmed experimentally. (ii) I guess this depends on what you mean by \"consistent with.\" If two people have different knowledge about a system then there's no reason in principle that they should agree on their predictions about its future behaviour. However, I can see one way in which to approach this question. I don't know how to express it in terms of density matrices (quantum mechanics isn't really my thing), so let's switch to a classical system. Alice and Bob both express their knowledge about the system as a probability density function over $x$, the set of possible states of the system (i.e. the vector of positions and velocities of each particle) at some particular time. Now, if there is no value of $x$ for which both Alice and Bob assign a positive probability density then they can be said to be inconsistent, since every state that Alice accepts the system might be in Bob says it is not, and vice versa. If any such value of $x$ does exist then Alice and Bob can both be \"correct\" in their state of knowledge if the system turns out to be in that particular state. I will continue this idea below. (iii) Again I don't really know how to convert this into the density matrix formalism, but in the classical version of statistical mechanics, a macroscopic ensemble assigns a probability (or a probability density) to every possible microscopic state, and this is what you use to determine how heavily represented a particular microstate is in a given ensemble. In the density matrix formalism the pure states are analogous to the microscopic states in the classical one. I guess you have to do something with projection operators to get the probability of a particular pure state out of a density matrix (I did learn it once but it was too long ago), and I'm sure the principles are similar in both formalisms. I agree that the measure you are looking for is $D_\\textrm{KL}(A||B) = \\sum_i p_A(i) \\log \\frac{p_A(i)}{p_B(i)}$. (I guess this is $\\mathrm{tr}(\\rho_A (\\log \\rho_A - \\log \\rho_B))$ in the density matrix case, which looks like what you wrote apart from a change of sign.) In the case where A is a pure state, this just gives $-\\log p_B(i)$, the negative logarithm of the probability that Bob assigns to that particular pure state. In information theory terms, this can be interpreted as the \"surprisal\" of state $i$, i.e. the amount of information that must be supplied to Bob in order to convince him that state $i$ is indeed the correct one. If Bob considers state $i$ to be unlikely then he will be very surprised to discover it is the correct one. If B assigns zero probability to state $i$ then the measure will diverge to infinity, meaning that Bob would take an infinite amount of convincing in order to accept something that he was absolutely certain was false. If A is a mixed state, this will happen as long as A assigns a positive probability to any state to which B assigns zero probability. If A and B are the same then this measure will be 0. Therefore the measure $D_\\textrm{KL}(A||B)$ can be seen as a measure of how \"incompatible\" two states of knowledge are. Since the KL divergence is asymmetric I guess you also have to consider $D_\\textrm{KL}(B||A)$, which is something like the degree of implausibility of B from A's perspective. I'm aware that I've skipped over some things, as there was quite a lot to write and I don't have much time to do it. I'll be happy to expand it if any of it is unclear. Edit (in reply to the edit at the end of the question): The answer to the question \"When may (or may not) a microstate $\\phi$ be regarded as a macrostate $\\rho_0$ without affecting the predictability of the macroscopic observations?\" is \"basically never.\" I will address this is classical mechanics terms because it's easier for me to write in that language. Macrostates are probability distributions over microstates, so the only time a macrostate can behave in the same way as a microstate is if the macrostate happens to be a fully peaked probability distribution (with entropy 0, assigning $p=1$ to one microstate and $p=0$ to the rest), and to remain that way throughout the time evolution. You write in a comment \"if I have a definite penny on my desk with a definite temperature, how can it have several different pure states?\" But (at least in Jaynes' version of the MaxEnt interpretation of statistical mechanics), the temperature is not a property of the microstate but of the macrostate. It is the partial differential of the entropy with respect to the internal energy. Essentially what you're doing is (1) finding the macrostate with the maximum (information) entropy compatible with the internal energy being equal to $U$, then (2) finding the macrostate with the maximum entropy compatible with the internal energy being equal to $U+dU$, then (3) taking the difference and dividing by $dU$. When you're talking about microstates instead of macrostates the entropy is always 0 (precisely because you have no ignorance) and so it makes no sense to do this. Now you might want to say something like \"but if my penny does have a definite pure state that I happen to be ignorant of, then surely it would behave in exactly the same way if I did know that pure state.\" This is true, but if you knew precisely the pure state then you would (in principle) no longer have any need to use temperature in your calculations, because you would (in principle) be able to calculate precisely the fluxes in and out of the penny, and hence you'd be able to give exact answers to the questions that statistical mechanics can only answer statistically. Of course, you would only be able to calculate the penny's future behaviour over very short time scales, because the penny is in contact with your desk, whose precise quantum state you (presumably) do not know. You will therefore have to replace your pure-state-macrostate of the penny with a mixed one pretty rapidly. The fact that this happens is one reason why you can't in general simply replace the mixed state with a single \"most representative\" pure state and use the evolution of that pure state to predict the future evolution of the system. Edit 2: the classical versus quantum cases. (This edit is the result of a long conversation with Arnold Neumaier in chat, linked in the question.) In most of the above I've been talking about the classical case, in which a microstate is something like a big vector containing the positions and velocities of every particle, and a macrostate is simply a probability distribution over a set of possible microstates. Systems are conceived of as having a definite microstate, but the practicalities of macroscopic measurements mean that for all but the simplest systems we cannot know what it is, and hence we model it statistically. In this classical case, Jaynes' arguments are (to my mind) pretty much unassailable: if we lived in a classical world, we would have no practical way to know precisely the position and velocity of every particle in a system like a penny on a desk, and so we would need some kind of calculus to allow us to make predictions about the system's behaviour in spite of our ignorance. When one examines what an optimal such calculus would look like, one arrives precisely at the mathematical framework of statistical mechanics (Boltzmann distributions and all the rest). By considering how one's ignorance about a system can change over time one arrives at results that (it seems to me at least) would be impossible to state, let alone derive, in the traditional frequentist interpretation. The fluctuation theorem is an example of such a result. In a classical world there would be no reason in principle why we couldn't know the precise microstate of a penny (along with that of anything it's in contact with). The only reasons for not knowing it are practical ones. If we could overcome such issues then we could predict the microstate's time-evolution precisely. Such predictions could be made without reference to concepts such as entropy and temperature. In Jaynes' view at least, these are purely macroscopic concepts and don't strictly have meaning on the microscopic level. The temperature of your penny is determined both by Nature and by what you are able to measure about Nature (which depends on the equipment you have available). If you could measure the (classical) microstate in enough detail then you would be able to see which particles had the highest velocities and thus be able to extract work via a Maxwell's demon type of apparatus. Effectively you would be partitioning the penny into two subsystems, one containing the high-energy particles and one containing the lower-energy ones; these two systems would effectively have different temperatures. My feeling is that all of this should carry over on to the quantum level without difficulty, and indeed Jaynes presented much of his work in terms of the density matrix rather than classical probability distributions. However there is a large and (I think it's fair to say) unresolved subtlety involved in the quantum case, which is the question of what really counts as a microstate for a quantum system. One possibility is to say that the microstate of a quantum system is a pure state. This has a certain amount of appeal: pure states evolve deterministically like classical microstates, and the density matrix can be derived by considering probability distributions over pure states. However the problem with this is distinguishability: some information is lost when going from a probability distribution over pure states to a density matrix. For example, there is no experimentally distinguishable difference between the mixed states $\\frac{1}{2}(\\mid \\uparrow \\rangle \\langle \\uparrow \\mid + \\mid \\downarrow \\rangle \\langle \\downarrow \\mid)$ and $\\frac{1}{2}(\\mid \\leftarrow \\rangle \\langle \\leftarrow \\mid + \\mid \\rightarrow \\rangle \\langle \\rightarrow \\mid)$ for a spin-$\\frac{1}{2}$ system. If one considers the microstate of a quantum system to be a pure state then one is committed to saying there is a difference between these two states, it's just that it's impossible to measure. This is a philosophically difficult position to maintain, as it's open to being attacked with Occam's razor. However, this is not the only possibility. Another possibility is to say that even pure quantum states represent our ignorance about some underlying, deeper level of physical reality. If one is willing to sacrifice locality then one can arrive at such a view by interpreting quantum states in terms of a non-local hidden variable theory. Another possibility is to say that the probabilities one obtains from the density matrix do not represent our ignorance about any underlying microstate at all, but instead they represent our ignorance about the results of future measurements we might make on the system. I'm not sure which of these possibilities I prefer. The point is just that on the philosophical level the ignorance interpretation is trickier in the quantum case than in the classical one. But in practical terms it makes very little difference - the results derived from the much clearer classical case can almost always be re-stated in terms of the density matrix with very little modification.",
      "question_latex": [
        "\\rho_0=\\frac{1}{Z}e^{-\\beta H}",
        "N",
        "\\rho_A",
        "\\rho_B",
        "\\psi",
        "\\rho_0",
        "\\rho_A=\\psi\\psi^*",
        "\\rho_B=\\rho_0",
        "M(\\rho_A,\\rho_B)\\ge 0",
        "\\rho_A=\\rho_B)",
        "M(\\rho_A,\\rho_B)",
        "\\rho_A\\log\\frac{\\rho_A}{\\rho_B}",
        "\\beta H+\\log Z",
        "\\rho=e^{-S/k_B}",
        "S",
        "H",
        "N_j"
      ],
      "answer_latex": [
        "x",
        "D_\\textrm{KL}(A||B) = \\sum_i p_A(i) \\log \\frac{p_A(i)}{p_B(i)}",
        "\\mathrm{tr}(\\rho_A (\\log \\rho_A - \\log \\rho_B))",
        "-\\log p_B(i)",
        "i",
        "D_\\textrm{KL}(A||B)",
        "D_\\textrm{KL}(B||A)",
        "\\phi",
        "\\rho_0",
        "p=1",
        "p=0",
        "U",
        "U+dU",
        "dU",
        "\\frac{1}{2}(\\mid \\uparrow \\rangle \\langle \\uparrow \\mid + \\mid \\downarrow \\rangle \\langle \\downarrow \\mid)",
        "\\frac{1}{2}(\\mid \\leftarrow \\rangle \\langle \\leftarrow \\mid + \\mid \\rightarrow \\rangle \\langle \\rightarrow \\mid)",
        "\\frac{1}{2}"
      ],
      "created": "2012-03-06T11:39:11.453",
      "golden_ner_terms": [
        "approximation",
        "argument",
        "assumption",
        "basis",
        "behavior",
        "calculate",
        "calculus",
        "canonical",
        "classical mechanics",
        "classical system",
        "combination",
        "compatible",
        "complete",
        "composite",
        "concept",
        "consistent",
        "coordinates",
        "decide",
        "degree",
        "density",
        "density function",
        "difference",
        "differential",
        "distribution",
        "diverge",
        "divergence",
        "energy",
        "entropy",
        "equation",
        "equilibrium",
        "even",
        "expand",
        "expectation",
        "formalism",
        "fraction",
        "function",
        "ground state",
        "hamiltonian",
        "heat",
        "inconsistent",
        "independent",
        "infinite",
        "infinity",
        "information",
        "integrate",
        "interest",
        "interpretation",
        "involved in",
        "l system",
        "language",
        "level",
        "linear combination",
        "locality",
        "logarithm",
        "matrix",
        "mean",
        "measure",
        "measurements",
        "minimal",
        "model",
        "modification",
        "nature",
        "negative",
        "number",
        "occam's razor",
        "one way",
        "open",
        "operator",
        "operators",
        "order",
        "point",
        "positive",
        "pressure",
        "probability",
        "probability density function",
        "projection",
        "property",
        "pure state",
        "quantum mechanics",
        "quantum state",
        "quantum states",
        "quantum system",
        "query",
        "relative entropy",
        "represents",
        "scales",
        "similar",
        "simple",
        "space",
        "space of functions",
        "state",
        "statistical mechanics",
        "strictly",
        "temperature",
        "theorem",
        "theory",
        "time",
        "time evolution",
        "trace",
        "type",
        "useful",
        "vanishes",
        "variable",
        "vector",
        "velocity",
        "wave function",
        "wavefunction",
        "way",
        "work",
        "zero"
      ],
      "golden_ner_count": 110,
      "golden_patterns": [
        {
          "pattern": "find-the-right-abstraction",
          "score": 4.0,
          "hotwords": [
            "framework",
            "perspective"
          ]
        },
        {
          "pattern": "check-the-extreme-cases",
          "score": 4.0,
          "hotwords": [
            "zero",
            "infinity"
          ]
        },
        {
          "pattern": "local-to-global",
          "score": 4.0,
          "hotwords": [
            "local",
            "cover"
          ]
        },
        {
          "pattern": "try-a-simpler-case",
          "score": 2.0,
          "hotwords": [
            "simplest"
          ]
        },
        {
          "pattern": "work-examples-first",
          "score": 2.0,
          "hotwords": [
            "example"
          ]
        },
        {
          "pattern": "argue-by-contradiction",
          "score": 2.0,
          "hotwords": [
            "impossible"
          ]
        },
        {
          "pattern": "exploit-symmetry",
          "score": 2.0,
          "hotwords": [
            "symmetric"
          ]
        },
        {
          "pattern": "encode-as-algebra",
          "score": 2.0,
          "hotwords": [
            "ring"
          ]
        },
        {
          "pattern": "use-probabilistic-method",
          "score": 2.0,
          "hotwords": [
            "probability"
          ]
        },
        {
          "pattern": "estimate-by-bounding",
          "score": 2.0,
          "hotwords": [
            "at least"
          ]
        }
      ],
      "golden_pattern_names": [
        "find-the-right-abstraction",
        "check-the-extreme-cases",
        "local-to-global",
        "try-a-simpler-case",
        "work-examples-first",
        "argue-by-contradiction",
        "exploit-symmetry",
        "encode-as-algebra",
        "use-probabilistic-method",
        "estimate-by-bounding"
      ],
      "golden_scopes": [],
      "golden_scope_count": 0
    },
    {
      "id": "se-physics-136860",
      "stratum": "easy",
      "title": "Did the Big Bang happen at a point?",
      "tags": [
        "general-relativity",
        "cosmology",
        "big-bang",
        "black-hole-thermodynamics"
      ],
      "score": 397,
      "answer_score": 573,
      "question_body": "TV documentaries invariably show the Big Bang as an exploding ball of fire expanding outwards. Did the Big Bang really explode outwards from a point like this? If not, what did happen?",
      "answer_body": "The simple answer is that no, the Big Bang did not happen at a point. Instead, it happened everywhere in the universe at the same time. Consequences of this include: The universe doesn't have a centre: the Big Bang didn't happen at a point so there is no central point in the universe that it is expanding from. The universe isn't expanding into anything: because the universe isn't expanding like a ball of fire, there is no space outside the universe that it is expanding into. In the next section, I'll sketch out a rough description of how this can be, followed by a more detailed description for the more determined readers. A simplified description of the Big Bang Imagine measuring our current universe by drawing out a grid with a spacing of 1 light year. Although obviously, we can't do this, you can easily imagine putting the Earth at (0, 0), Alpha Centauri at (4.37, 0), and plotting out all the stars on this grid. The key thing is that this grid is infinite $^1$ i.e. there is no point where you can't extend the grid any further. Now wind time back to 7 billion years after the big bang, i.e. about halfway back. Our grid now has a spacing of half a light year, but it's still infinite - there is still no edge to it. The average spacing between objects in the universe has reduced by half and the average density has gone up by a factor of $2^3$ . Now wind back to 0.0000000001 seconds after the big bang. There's no special significance to that number; it's just meant to be extremely small. Our grid now has a very small spacing, but it's still infinite. No matter how close we get to the Big Bang we still have an infinite grid filling all of space. You may have heard pop science programs describing the Big Bang as happening everywhere and this is what they mean. The universe didn't shrink down to a point at the Big Bang, it's just that the spacing between any two randomly selected spacetime points shrank down to zero. So at the Big Bang, we have a very odd situation where the spacing between every point in the universe is zero, but the universe is still infinite. The total size of the universe is then $0 \\times \\infty$ , which is undefined. You probably think this doesn't make sense, and actually, most physicists agree with you. The Big Bang is a singularity , and most of us don't think singularities occur in the real universe. We expect that some quantum gravity effect will become important as we approach the Big Bang. However, at the moment we have no working theory of quantum gravity to explain exactly what happens. $^1$ we assume the universe is infinite - more on this in the next section For determined readers only To find out how the universe evolved in the past, and what will happen to it in the future, we have to solve Einstein's equations of general relativity for the whole universe. The solution we get is an object called the metric tensor that describes spacetime for the universe. But Einstein's equations are partial differential equations, and as a result, have a whole family of solutions. To get the solution corresponding to our universe we need to specify some initial conditions . The question is then what initial conditions to use. Well, if we look at the universe around us we note two things: if we average over large scales the universe looks the same in all directions, that is it is isotropic if we average over large scales the universe is the same everywhere, that is it is homogeneous You might reasonably point out that the universe doesn't look very homogeneous since it has galaxies with a high density randomly scattered around in space with a very low density. However, if we average on scales larger than the size of galaxy superclusters we do get a constant average density. Also, if we look back to the time the cosmic microwave background was emitted (380,000 years after the Big Bang and well before galaxies started to form) we find that the universe is homogeneous to about $1$ part in $10^5$ , which is pretty homogeneous. So as the initial conditions let's specify that the universe is homogeneous and isotropic, and with these assumptions, Einstein's equation has a (relatively!) simple solution. Indeed this solution was found soon after Einstein formulated general relativity and has been independently discovered by several different people. As a result the solution glories in the name Friedmann–Lemaître–Robertson–Walker metric , though you'll usually see this shortened to FLRW metric or sometimes FRW metric (why Lemaître misses out I'm not sure). Recall the grid I described to measure out the universe in the first section of this answer, and how I described the grid shrinking as we went back in time towards the Big Bang? Well the FLRW metric makes this quantitative. If $(x, y, z)$ is some point on our grid then the current distance to that point is just given by Pythagoras' theorem: $$ d^2 = x^2 + y^2 + z^2 $$ What the FLRW metric tells us is that the distance changes with time according to the equation: $$ d^2(t) = a^2(t)(x^2 + y^2 + z^2) $$ where $a(t)$ is a function called the [scale factor]. We get the function for the scale factor when we solve Einstein's equations. Sadly it doesn't have a simple analytical form, but it's been calculated in answers to the previous questions What was the density of the universe when it was only the size of our solar system? and How does the Hubble parameter change with the age of the universe? . The result is: The value of the scale factor is conventionally taken to be unity at the current time, so if we go back in time and the universe shrinks we have $a(t) < 1$ , and conversely in the future as the universe expands we have $a(t) > 1$ . The Big bang happens because if we go back to time to $t = 0$ the scale factor $a(0)$ is zero. This gives us the remarkable result that the distance to any point in the universe $(x, y, z)$ is: $$ d^2(t) = 0(x^2 + y^2 + z^2) = 0 $$ so the distance between every point in the universe is zero. The density of matter (the density of radiation behaves differently but let's gloss over that) is given by: $$ \\rho(t) = \\frac{\\rho_0}{a^3(t)} $$ where $\\rho_0$ is the density at the current time, so the density at time zero is infinitely large. At the time $t = 0$ the FLRW metric becomes singular. No one I know thinks the universe did become singular at the Big Bang. This isn't a modern opinion: the first person I know to have objected publically was Fred Hoyle , and he suggested Steady State Theory to avoid the singularity. These days it's commonly believed that some quantum gravity effect will prevent the geometry from becoming singular, though since we have no working theory of quantum gravity no one knows how this might work. So to conclude: the Big Bang is the zero time limit of the FLRW metric, and it's a time when the spacing between every point in the universe becomes zero and the density goes to infinity. It should be clear that we can't associate the Big Bang with a single spatial point because the distance between all points was zero so the Big Bang happened at all points in space. This is why it's commonly said that the Big Bang happened everywhere. In the discussion above I've several times casually referred to the universe as infinite , but what I really mean is that it can't have an edge. Remember that our going-in assumption is that the universe is homogeneous i.e. it's the same everywhere. If this is true the universe can't have an edge because points at the edge would be different from points away from the edge. A homogenous universe must either be infinite, or it must be closed i.e. have the spatial topology of a 3-sphere. The recent Planck results show the curvature is zero to within experimental error, so if the universe is closed the scale must be far larger than the observable universe.",
      "question_latex": [],
      "answer_latex": [
        "d^2 = x^2 + y^2 + z^2",
        "d^2(t) = a^2(t)(x^2 + y^2 + z^2)",
        "d^2(t) = 0(x^2 + y^2 + z^2) = 0",
        "\\rho(t) = \\frac{\\rho_0}{a^3(t)}",
        "^1",
        "2^3",
        "0 \\times \\infty",
        "1",
        "10^5",
        "(x, y, z)",
        "</span></p>\n<p>What the FLRW metric tells us is that the distance changes with time according to the equation:</p>\n<p><span class=\"math-container\">",
        "</span></p>\n<p>where <span class=\"math-container\">",
        "</span> is a function called the [scale factor]. We get the function for the scale factor when we solve Einstein's equations. Sadly it doesn't have a simple analytical form, but it's been calculated in answers to the previous questions <a href=\"http://physics.stackexchange.com/questions/126553/what-was-the-density-of-the-universe-when-it-was-only-the-size-of-our-solar-syst\">What was the density of the universe when it was only the size of our solar system?</a> and <a href=\"http://physics.stackexchange.com/questions/136056/how-does-the-hubble-parameter-change-with-the-age-of-the-universe\">How does the Hubble parameter change with the age of the universe?</a>. The result is:</p>\n<p><img src=\"https://i.stack.imgur.com/BhiHp.gif\" alt=\"Scale factor\" /></p>\n<p>The value of the scale factor is conventionally taken to be unity at the current time, so if we go back in time and the universe shrinks we have <span class=\"math-container\">",
        "</span>, and conversely in the future as the universe expands we have <span class=\"math-container\">",
        "</span>. The Big bang happens because if we go back to time to <span class=\"math-container\">",
        "</span> the scale factor <span class=\"math-container\">",
        "</span> is zero. This gives us the remarkable result that the distance to <strong>any</strong> point in the universe <span class=\"math-container\">",
        "</span> is:</p>\n<p><span class=\"math-container\">",
        "</span></p>\n<p>so the distance between every point in the universe is zero. The density of matter (the density of radiation behaves differently but let's gloss over that) is given by:</p>\n<p><span class=\"math-container\">",
        "</span> is the density at the current time, so the density at time zero is infinitely large. At the time <span class=\"math-container\">"
      ],
      "created": "2014-09-23T10:03:58.110",
      "golden_ner_terms": [
        "associate",
        "assumption",
        "average",
        "ball",
        "big bang",
        "central",
        "centre",
        "clear",
        "closed",
        "constant",
        "conversely",
        "cosmic microwave background",
        "current",
        "curvature",
        "density",
        "differential",
        "differential equation",
        "differential equations",
        "distance",
        "earth",
        "edge",
        "equation",
        "factor",
        "function",
        "galaxies",
        "general relativity",
        "geometry",
        "gravity",
        "grid",
        "homogeneous",
        "infinite",
        "infinity",
        "initial condition",
        "key",
        "limit",
        "matter",
        "mean",
        "measure",
        "metric",
        "metric tensor",
        "moment",
        "number",
        "object",
        "observable universe",
        "occur in",
        "odd",
        "parameter",
        "partial differential equation",
        "point",
        "pop",
        "quantum gravity",
        "radiation",
        "real",
        "reduced",
        "relativity",
        "scale factor",
        "scales",
        "scattered",
        "section",
        "simple",
        "singular",
        "singularities",
        "size",
        "solar system",
        "solution",
        "space",
        "spacetime",
        "stars",
        "state",
        "steady state",
        "tensor",
        "theorem",
        "theory",
        "time",
        "topology",
        "unity",
        "universe",
        "work",
        "zero"
      ],
      "golden_ner_count": 79,
      "golden_patterns": [
        {
          "pattern": "check-the-extreme-cases",
          "score": 6.0,
          "hotwords": [
            "extreme",
            "zero",
            "infinity"
          ]
        },
        {
          "pattern": "local-to-global",
          "score": 2.0,
          "hotwords": [
            "cover"
          ]
        },
        {
          "pattern": "encode-as-algebra",
          "score": 2.0,
          "hotwords": [
            "ring"
          ]
        },
        {
          "pattern": "use-probabilistic-method",
          "score": 2.0,
          "hotwords": [
            "random"
          ]
        },
        {
          "pattern": "monotone-approximation",
          "score": 2.0,
          "hotwords": [
            "limit"
          ]
        }
      ],
      "golden_pattern_names": [
        "check-the-extreme-cases",
        "local-to-global",
        "encode-as-algebra",
        "use-probabilistic-method",
        "monotone-approximation"
      ],
      "golden_scopes": [
        {
          "type": "where-binding",
          "match": "where $a(t)$ is"
        },
        {
          "type": "where-binding",
          "match": "where $\\rho_0$ is"
        }
      ],
      "golden_scope_count": 2
    },
    {
      "id": "se-physics-633865",
      "stratum": "easy",
      "title": "Is there a \"position operator\" for the \"particle on a ring\" quantum mechanics model?",
      "tags": [
        "quantum-mechanics",
        "operators",
        "wavefunction"
      ],
      "score": 27,
      "answer_score": 40,
      "question_body": "For this quantum mechanical system, is there an operator that corresponds to \"position\" the same way that there are operators corresponding to angular momentum and energy? My first guess for what the operator would be is $$ \\hat \\Phi \\big[ \\psi(\\phi) \\big] = \\phi \\, \\psi(\\phi) $$ which would be analogous to the position operator for the \"particle in a box\" system, but I am getting confused on the following issue: If the particle has a 50% chance of being in one place on the ring and a 50% chance of being in the place that is exactly opposite the first place, what should the \"expectation value\" of the position be? With this operator it seems like the expectation value would depend on where $\\phi = 0$ is chosen to be, which makes me doubt the validity of this operator since its expectation value changes depending on which coordinate system you use. Also if the particle has some probability to be at $\\phi = \\epsilon$ and the rest of the probability to be at $\\phi = 2 \\pi - \\epsilon$ then it seems like the \"expectation value\" should be around $\\langle \\hat \\Phi \\rangle = 0$ but this operator would put it near $\\pi$ .",
      "answer_body": "This question is a great setup for explaining a better way of describing particles in quantum theory, one that bridges the traditional gap between single-particle quantum mechanics and quantum field theory (QFT). I'll start with a little QFT, but don't let that scare you. It's easy, both conceptually and mathematically. In fact, it's easier than the traditional formulation of single-particle quantum-mechanics, both conceptually and mathematically! And it makes the question easy to answer, both conceptually and mathematically. To make things easier, here's a little QFT In QFT, observables are tied to space, not to particles. That's the most important thing to understand about QFT. Instead of assigning a \"position observable\" to each particle (which would be impossible in many models because particles can be created and destroyed), we assign detection observables to regions of space. Let $D(R)$ denote an detection observable associated with region $R$ . In nonrelativistic QFT, the eigenvalues of $D(R)$ are natural numbers ( $0$ , $1$ , $2$ , ...) representing the number of particles found in $R$ when $D(R)$ is measured. If the model has more than one species of particle, then we have different detection observables for each species. By the way, the traditional bit about \"indistinguishable particles\" is really just an obtuse way of saying what I said above: observables are tied to space, not to particles. Very simple. Now, the difference between nonrelativistic QFT and nonrelativistic single-particle quantum mechanics is almost trivial: in the latter, the eigenvalues of $D(R)$ are restricted to $0$ and $1$ . As an example, consider ordinary nonrelativistic single-particle quantum mechanics, in one-dimensional space for simplicity. The traditional formulation uses a position operator $X$ , whose measurement magically returns the particle's spatial coordinate. That formulation is convenient for some purposes, but it also causes trouble: First, it causes conceptual trouble, because this is not how real-world measurements work: there is no measuring device that magically tells us the particle's coordinates no matter where it is in the universe. Real-world measurements are localized — they correspond to observables that are tied to regions of space, not tied to particles. Second, it causes mathematical trouble, because $X$ has a continuous spectrum — it doesn't have any (normalizable) eigenstates, so it can't be perfectly measured even on paper. Fortunately, we don't really need $X$ . We can do better, both conceptually and mathematically, by using a collection of projection operators $D(R)$ instead. For any given spatial region $R$ , the definition of $D(R)$ is simple: If the particle's wavefunction is concentrated entirely within $R$ , then it's an eigenstate of $D(R)$ with eigenvalue $1$ . If the particle's wavefunction is concentrated entirely outside of $R$ , then it's an eigenstate of $D(R)$ with eigenvalue $0$ . In other words, $D(R)$ counts the number of particles in $R$ , and since the model only has one particle, the answer can only be $0$ or $1$ . If the particle's wavefunction is partly inside $R$ and partly outside $R$ , then the result of the measurement will be $1$ with probability $$ p = \\frac{\\langle\\psi|D(R)|\\psi\\rangle}{\\langle\\psi|\\psi\\rangle}, \\tag{1} $$ and the result will be $0$ with the opposite probability $1-p$ . That's nicer conceptually, because it's closer to how we do things in the real world. It's also nicer mathematically, because $D(R)$ is a bounded operator with a discrete spectrum. Note that the observables $D(R)$ for different regions $R$ all commute with each other (this should be obvious from the definition), so we can measure them all simultaneously if we want to — just like we can use an array of detectors in the real world. The traditional position operator How are the operators $D(R)$ related to the usual position operator $X$ ? Simple: $$ X \\approx \\sum_n x_n D(R_n) \\tag{2} $$ where the sum is over a set of regions $R_n$ that partition all of space into non-overlapping cells, and $x_n$ is the coordinate of some point in the $n$ -th cell. In the limit as the size of the cells approaches zero, this approaches the usual position operator $X$ . Conceptually, we're just filling space with an array of little detectors. We know where each one them is located, so if the $n$ -th one detects the particle, then we know where the particle is. To be fair, the traditional position operator (2) has some advantages over the detection observables $D(R)$ . One advantage is that it allows us to use summary-statistics like averages (think of Ehrenfest's theorem) and standard deviations (think of the traditional uncertainty principles). This is just like the usual situation in statistics: the information is in the distribution (the results of measuring lots of $D(R)$ s), but choosing a convenient labeling scheme lets us define things like averages and standard deviations to convey some incomplete but concise information about the distribution. The particle on a circle Now, the answer to the question should be obvious. Does a particle living on a circle have a position operator? Well, it certainly does have the operators $D(R)$ , where now $R$ is any portion of the circle. Those operators are defined just like before. We can also define something analogous to $X$ if we really want to, like this: $$ \\Theta \\approx \\sum_n \\theta_n D(R_n) \\tag{3} $$ or like this: $$ U \\approx \\sum_n e^{i\\theta_n} D(R_n), \\tag{4} $$ where the sum is over a partition of the circle into non-overlapping intervals $R_n$ , and $\\theta_n$ is an angular coordinate within the $n$ -th interval. We can take the limit as the size of the intervals goes to zero, if we want to. The operator (3) is most closely analogous to (2), and it's perfectly well-defined mathematically, but it's unnatural because the angular coordinate must have a discontinuity ( $2\\pi$ jump) somewhere on the circle. That makes the operator (3) less useful for talking about things like averages and standard deviations, but that's not quantum theory's fault. It's just mundane statistics: averages and standard deviations are most useful when using a monotonically increasing coordinate, which we can't do everywhere on a circle if we want the coordinate to be single-valued. The operator (4) is more natural mathematically, because $e^{i\\theta}$ is continuous everywhere around the circle. Beginners might complain that (4) is not hermitian, but observables do not need to be hermitian (see this question ), contrary to what some introductions say. Unitary operators like (4) can also be used as observables, because quantum theory doesn't care about the coefficients of the projection operators, it only cares about the projection operators. The coefficients are useful for talking about averages and standard deviations, but we don't really need to talk about those things. They're convenient, but they're not necessary. Again, this is just like in ordinary statistics. Labels can be convenient, but they're not necessary. So... does a particle on a circle have a position operator? It depends. What properties do you want the position operator to have? It does have the observables that really matter, which are the detection-observables $D(R)$ . And we can use those to construct operators like (3) and (4) that each resemble the position operator (2) in different ways, but of course we can't have an operator that is completely like (2) because a circle is not completely like the real line. So, whether the answer is \"yes\" or \"no\" depends on exactly what you want, but at least the conceptual obstacles should be gone now.",
      "question_latex": [
        "\\hat \\Phi \\big[ \\psi(\\phi) \\big] = \\phi \\, \\psi(\\phi)",
        "</span>\nwhich would be analogous to the position operator for the \"particle in a box\" system, but I am getting confused on the following issue:</p>\n<p>If the particle has a 50% chance of being in one place on the ring and a 50% chance of being in the place that is exactly opposite the first place, what should the \"expectation value\" of the position be? With this operator it seems like the expectation value would depend on where <span class=\"math-container\">",
        "</span> is chosen to be, which makes me doubt the validity of this operator since its expectation value changes depending on which coordinate system you use.</p>\n<p>Also if the particle has some probability to be at <span class=\"math-container\">",
        "</span> and the rest of the probability to be at <span class=\"math-container\">",
        "</span> then it seems like the \"expectation value\" should be around <span class=\"math-container\">",
        "</span> but this operator would put it near <span class=\"math-container\">"
      ],
      "answer_latex": [
        "p = \\frac{\\langle\\psi|D(R)|\\psi\\rangle}{\\langle\\psi|\\psi\\rangle},\n\\tag{1}",
        "X \\approx \\sum_n x_n D(R_n)\n\\tag{2}",
        "\\Theta \\approx \\sum_n \\theta_n D(R_n)\n\\tag{3}",
        "U \\approx \\sum_n e^{i\\theta_n} D(R_n),\n\\tag{4}",
        "D(R)",
        "R",
        "0",
        "1",
        "2",
        "X",
        "</span>\nand the result will be <span class=\"math-container\">",
        "</span> with the opposite probability <span class=\"math-container\">",
        "</span>.</p>\n<p>That's nicer conceptually, because it's closer to how we do things in the real world. It's also nicer mathematically, because <span class=\"math-container\">",
        "</span> is a bounded operator with a discrete spectrum.</p>\n<p>Note that the observables <span class=\"math-container\">",
        "</span> for different regions <span class=\"math-container\">",
        "</span> all commute with each other (this should be obvious from the definition), so we can measure them all simultaneously if we want to — just like we can use an array of detectors in the real world.</p>\n<h2> The traditional position operator </h2>\n<p>How are the operators <span class=\"math-container\">",
        "</span> related to the usual position operator <span class=\"math-container\">",
        "</span>? Simple:\n<span class=\"math-container\">",
        "</span>\nwhere the sum is over a set of regions <span class=\"math-container\">",
        "</span> that partition all of space into non-overlapping cells, and <span class=\"math-container\">",
        "</span> is the coordinate of some point in the <span class=\"math-container\">",
        "</span>-th cell. In the limit as the size of the cells approaches zero, this approaches the usual position operator <span class=\"math-container\">",
        "</span>. Conceptually, we're just filling space with an array of little detectors. We know where each one them is located, so if the <span class=\"math-container\">",
        "</span>-th one detects the particle, then we know where the particle is.</p>\n<p>To be fair, the traditional position operator (2) has some advantages over the detection observables <span class=\"math-container\">",
        "</span>. One advantage is that it allows us to use summary-statistics like averages (think of Ehrenfest's theorem) and standard deviations (think of the traditional uncertainty principles). This is just like the usual situation in statistics: the information is in the distribution (the results of measuring lots of <span class=\"math-container\">",
        "</span>s), but choosing a convenient labeling scheme lets us define things like averages and standard deviations to convey some incomplete but concise information about the distribution.</p>\n<h2> The particle on a circle </h2>\n<p>Now, the answer to the question should be obvious. Does a particle living on a circle have a position operator? Well, it certainly does have the operators <span class=\"math-container\">",
        "</span>, where now <span class=\"math-container\">",
        "</span> is any portion of the circle. Those operators are defined just like before. We can also define something analogous to <span class=\"math-container\">",
        "</span> if we really want to, like this:\n<span class=\"math-container\">",
        "</span>\nor like this:\n<span class=\"math-container\">",
        "</span>\nwhere the sum is over a partition of the circle into non-overlapping intervals <span class=\"math-container\">",
        "</span>, and <span class=\"math-container\">",
        "</span> is an angular coordinate within the <span class=\"math-container\">",
        "</span>-th interval. We can take the limit as the size of the intervals goes to zero, if we want to.</p>\n<p>The operator (3) is most closely analogous to (2), and it's perfectly well-defined mathematically, but it's unnatural because the angular coordinate must have a discontinuity (<span class=\"math-container\">",
        "</span> jump) somewhere on the circle. That makes the operator (3) less useful for talking about things like averages and standard deviations, but that's not quantum theory's fault. It's just mundane statistics: averages and standard deviations are most useful when using a monotonically increasing coordinate, which we can't do everywhere on a circle if we want the coordinate to be single-valued.</p>\n<p>The operator (4) is more natural mathematically, because <span class=\"math-container\">",
        "</span> is continuous everywhere around the circle. Beginners might complain that (4) is not hermitian, but observables do not need to be hermitian (see <a href=\"https://physics.stackexchange.com/q/631665\">this question</a>), contrary to what some introductions say. Unitary operators like (4) can also be used as observables, because quantum theory doesn't care about the coefficients of the projection operators, it only cares about the projection operators. The coefficients are useful for talking about averages and standard deviations, but we don't really need to talk about those things. They're convenient, but they're not necessary. Again, this is just like in ordinary statistics. Labels can be convenient, but they're not necessary.</p>\n<p>So... does a particle on a circle have a position operator? It depends. What properties do you want the position operator to have? It does have the observables that really matter, which are the detection-observables <span class=\"math-container\">"
      ],
      "created": "2021-05-04T00:44:05.283",
      "golden_ner_terms": [
        "angular momentum",
        "bounded",
        "bounded operator",
        "cell",
        "circle",
        "collection",
        "construct",
        "continuous",
        "continuous spectrum",
        "coordinate",
        "coordinates",
        "difference",
        "discrete",
        "distribution",
        "eigenvalue",
        "eigenvalues",
        "energy",
        "even",
        "expectation",
        "expectation value",
        "field",
        "field theory",
        "hermitian",
        "incomplete",
        "increasing",
        "indistinguishable",
        "information",
        "interval",
        "jump",
        "l system",
        "labeling",
        "limit",
        "line",
        "matter",
        "measure",
        "measurements",
        "model",
        "models",
        "momentum",
        "monotonically",
        "monotonically increasing",
        "natural number",
        "near",
        "necessary",
        "number",
        "numbers",
        "observables",
        "obtuse",
        "obvious",
        "operator",
        "operators",
        "opposite",
        "partition",
        "place",
        "point",
        "probability",
        "projection",
        "qft",
        "quantum field theory",
        "quantum mechanics",
        "real",
        "region",
        "ring",
        "scheme",
        "simple",
        "single-valued",
        "size",
        "space",
        "spectrum",
        "standard deviation",
        "statistics",
        "sum",
        "theorem",
        "theory",
        "uncertainty principle",
        "unitary",
        "unitary operator",
        "universe",
        "useful",
        "wavefunction",
        "way",
        "well-defined",
        "work",
        "zero"
      ],
      "golden_ner_count": 84,
      "golden_patterns": [
        {
          "pattern": "estimate-by-bounding",
          "score": 4.0,
          "hotwords": [
            "bound",
            "at least"
          ]
        },
        {
          "pattern": "work-examples-first",
          "score": 2.0,
          "hotwords": [
            "example"
          ]
        },
        {
          "pattern": "argue-by-contradiction",
          "score": 2.0,
          "hotwords": [
            "impossible"
          ]
        },
        {
          "pattern": "check-the-extreme-cases",
          "score": 2.0,
          "hotwords": [
            "zero"
          ]
        },
        {
          "pattern": "construct-an-explicit-witness",
          "score": 2.0,
          "hotwords": [
            "construct"
          ]
        },
        {
          "pattern": "local-to-global",
          "score": 2.0,
          "hotwords": [
            "local"
          ]
        },
        {
          "pattern": "encode-as-algebra",
          "score": 2.0,
          "hotwords": [
            "ring"
          ]
        },
        {
          "pattern": "use-probabilistic-method",
          "score": 2.0,
          "hotwords": [
            "probability"
          ]
        },
        {
          "pattern": "unfold-the-definition",
          "score": 2.0,
          "hotwords": [
            "definition of"
          ]
        },
        {
          "pattern": "monotone-approximation",
          "score": 2.0,
          "hotwords": [
            "limit"
          ]
        }
      ],
      "golden_pattern_names": [
        "estimate-by-bounding",
        "work-examples-first",
        "argue-by-contradiction",
        "check-the-extreme-cases",
        "construct-an-explicit-witness",
        "local-to-global",
        "encode-as-algebra",
        "use-probabilistic-method",
        "unfold-the-definition",
        "monotone-approximation"
      ],
      "golden_scopes": [
        {
          "type": "let-binding",
          "match": "Let $D(R)$ denote"
        }
      ],
      "golden_scope_count": 1
    },
    {
      "id": "se-physics-89659",
      "stratum": "easy",
      "title": "Eigenfunctions of the Runge-Lenz vector",
      "tags": [
        "quantum-mechanics",
        "eigenvalue",
        "laplace-runge-lenz-vector"
      ],
      "score": 35,
      "answer_score": 42,
      "question_body": "The hamiltonian for the hydrogen atom, $$ H = \\frac{\\mathbf{p}^2}{2m} - \\frac{k}{r} $$ is spherically symmetric and it therefore commutes with the angular momentum $\\mathbf{L}$; this causes all its eigenfunctions with equal angular momentum number $l$ but arbitrary magnetic quantum number $m$ to be degenerate in energy. The hydrogen atom also has a further degeneracy, in that given any angular momentum there are usually other $l$s with the same energy. This degeneracy is due to the existence of a second constant of motion, usually called the Laplace-Runge-Lenz vector, $$ \\mathbf{A} = \\frac{1}{2m} ( \\mathbf{p} \\times \\mathbf{L} - \\mathbf{L} \\times \\mathbf{p}) - k \\frac{\\mathbf{r}}{r}, $$ which is the generator of an even bigger symmetry, which is isomorphic for bound states to the group $\\rm{SO}(4)$ of rotations in four dimensions, of the Kepler problem. The Runge-Lenz vector also has a rich geometrical interpretation. For a classical elliptical orbit, it points from the focus to the periapsis and its magnitude is proportional to the orbit's eccentricity. For circular orbits, it vanishes. Image source: Wikipedia The hydrogen atom is usually described in the common eigenbasis of the hamiltonian and the angular momentum, with the well-known and well-loved quantum numbers $|nlm\\rangle$. However, the Runge-Lenz vector $\\mathbf{A}$ is also a constant of the motion. What do its eigenfunctions look like? More concretely, I'm looking for the spatial structure of the common eigenfunctions of $H$ and at least one component of $\\mathbf A$, and possibly also of $A^2$ (which, in analogy with the common eigenfunctions of $H$, $L^2$ and $L_z$, is the most one could expect), and if that's not possible then an explanation of why, and a description of suitable third quantum numbers to complete a CSCO . I would like to know what their corresponding eigenvalues are, and what the uncertainty of the other components is, whether one can assign a classical eccentricity to the orbital, and more generally in the relation to the corresponding classical geometry.",
      "answer_body": "1. Definitions Let's consider the nondimensionalized Hamiltonian $$\\hat H=\\frac{\\hat{p}^2}2-\\frac1r.\\tag1$$ Its standard eigenfunctions diagonalize operators $\\hat H$ , $\\hat L_z$ and $\\hat L^2$ . Laplace-Runge-Lenz operator can be defined as $$\\hat{\\vec A}=\\frac{\\vec r}r-\\frac12\\left(\\hat{\\vec p}\\times\\hat{\\vec L}-\\hat{\\vec L}\\times\\hat{\\vec p}\\right).\\tag2$$ Its $z$ -component $\\hat A_z$ commutes with $\\hat H$ , $\\hat L_z$ , but doesn't commute with $\\hat L^2$ nor with $\\hat A^2$ , although $\\hat A^2$ does commute with $\\hat H$ , $\\hat L_z$ and $\\hat L^2$ . 2. Eigenfunctions Since $\\hat A^2$ commutes with the operators giving quantum numbers $n,l,m$ to the standard hydrogenic eigenstates, these standard eigenstates are also eigenstates of $\\hat A^2$ . So no new functions here. For these, we should look at $\\hat A_z$ . 2.1. Semi-numerical approach to finding exact eigenstates of $\\hat A_z$ Since operators for $H$ and $L_z$ commute with $\\hat A_z$ , any eigenstate of $\\hat A_z$ is a superposition of states with fixed $n$ and $m$ and different $l$ quantum numbers. This lets us find eigenstates of $\\hat A_z$ for given $n,m$ without actually solving the eigenfunction PDE. We can e.g. numerically minimize variance of a sample of values of the expression for the eigenvalue $$A_z=\\frac{\\hat A_z\\sum\\limits_l\\alpha_l\\psi_{n,l,m}}{\\sum\\limits_l\\alpha_l\\psi_{n,l,m}},\\tag3$$ where $\\psi_{n,l,m}$ are the standard simultaneous eigenfunctions of $\\hat H$ , $\\hat L^2$ and $\\hat L_z$ , getting a set of weights $\\alpha_l$ . Then we can use the approximations we got to guess the exact values of these weights (and substitute into $(3)$ , simplifying, to confirm the guess). So, for $n=1$ we have the only eigenfunction, and it's obviously an eigenfunction of $\\hat A_z$ . For $n>1$ we have more basis functions, which lets us actually form more \"interesting\" eigenstates of $\\hat A_z$ . Here're some tables for $n=2,3,4$ (I calculated them using the above mentioned minimization procedure): $$n=2\\\\ \\begin{array}{|c|c|c|} \\hline \\text{Eigenfunction of }\\hat A_z&A_z&m\\\\ \\hline \\psi_{2,1,1} &0 &1 \\\\ \\psi_{2,1,-1} &0 &-1 \\\\ \\frac1{\\sqrt2}(\\psi_{2,0,0}+\\psi_{2,1,0}) &-\\frac12 & 0\\\\ \\frac1{\\sqrt2}(\\psi_{2,0,0}-\\psi_{2,1,0}) &\\frac12 & 0\\\\ \\hline \\end{array}$$ $$n=3\\\\ \\begin{array}{|c|c|c|} \\hline \\text{Eigenfunction of }\\hat A_z&A_z&m\\\\ \\hline \\psi_{3,2,-2} & 0 & -2\\\\ \\psi_{3,2,2} & 0 & 2\\\\ \\frac1{\\sqrt2}(\\psi_{3,1,1}+\\psi_{3,2,1}) & -\\frac13 & 1\\\\ \\frac1{\\sqrt2}(\\psi_{3,1,1}-\\psi_{3,2,1}) & \\frac13 & 1\\\\ \\frac1{\\sqrt2}(\\psi_{3,1,-1}+\\psi_{3,2,-1}) & -\\frac13 & -1\\\\ \\frac1{\\sqrt2}(\\psi_{3,1,-1}-\\psi_{3,2,-1}) & \\frac13 & -1\\\\ \\sqrt{\\frac13}\\psi_{3,0,0}-\\sqrt{\\frac23}\\psi_{3,2,0} & 0 & 0\\\\ \\frac1{\\sqrt3}\\psi_{3,0,0}+\\frac1{\\sqrt2}\\psi_{3,1,0}+\\frac1{\\sqrt6}\\psi_{3,2,0} & -\\frac23 & 0\\\\ \\frac1{\\sqrt3}\\psi_{3,0,0}-\\frac1{\\sqrt2}\\psi_{3,1,0}+\\frac1{\\sqrt6}\\psi_{3,2,0} & \\frac23 & 0\\\\ \\hline \\end{array}$$ $$n=4\\\\ \\begin{array}{|c|c|c|} \\hline \\text{Eigenfunction of }\\hat A_z&A_z&m\\\\ \\hline \\psi_{4,3,3} & 0 & 3\\\\ \\psi_{4,3,-3} & 0 & -3\\\\ \\frac1{\\sqrt2}(\\psi_{4,2,2}+\\psi_{4,3,2}) & -\\frac14 & 2\\\\ \\frac1{\\sqrt2}(\\psi_{4,2,2}-\\psi_{4,3,2}) & \\frac14 & 2\\\\ \\frac1{\\sqrt2}(\\psi_{4,2,-2}+\\psi_{4,3,-2}) & -\\frac14 & -2\\\\ \\frac1{\\sqrt2}(\\psi_{4,2,-2}-\\psi_{4,3,-2}) & \\frac14 & -2\\\\ % \\sqrt{\\frac3{10}}\\psi_{4,1,1}-\\sqrt{\\frac12}\\psi_{4,2,1}+\\sqrt{\\frac15}\\psi_{4,3,1} & \\frac12 & 1\\\\ % \\sqrt{\\frac3{10}}\\psi_{4,1,1}+\\sqrt{\\frac12}\\psi_{4,2,1}+\\sqrt{\\frac15}\\psi_{4,3,1} & -\\frac12 & 1\\\\ % \\sqrt{\\frac3{10}}\\psi_{4,1,-1}-\\sqrt{\\frac12}\\psi_{4,2,-1}+\\sqrt{\\frac15}\\psi_{4,3,-1} & \\frac12 & -1\\\\ % \\sqrt{\\frac3{10}}\\psi_{4,1,-1}+\\sqrt{\\frac12}\\psi_{4,2,-1}+\\sqrt{\\frac15}\\psi_{4,3,-1} & -\\frac12 & -1\\\\ % \\sqrt{\\frac25}\\psi_{4,1,1}-\\sqrt{\\frac35}\\psi_{4,3,1} & 0 & 1\\\\ % \\sqrt{\\frac25}\\psi_{4,1,-1}-\\sqrt{\\frac35}\\psi_{4,3,-1} & 0 & -1\\\\ % \\frac12\\psi_{4,0,0}-\\sqrt{\\frac1{20}}\\psi_{4,1,0}-\\frac12\\psi_{4,2,0}+\\sqrt{\\frac9{20}}\\psi_{4,3,0} & \\frac14 & 0\\\\ % \\frac12\\psi_{4,0,0}+\\sqrt{\\frac1{20}}\\psi_{4,1,0}-\\frac12\\psi_{4,2,0}-\\sqrt{\\frac9{20}}\\psi_{4,3,0} & -\\frac14 & 0\\\\ % \\frac12\\psi_{4,0,0}+\\sqrt{\\frac9{20}}\\psi_{4,1,0}+\\frac12\\psi_{4,2,0}+\\sqrt{\\frac1{20}}\\psi_{4,3,0} & -\\frac34 & 0\\\\ % \\frac12\\psi_{4,0,0}-\\sqrt{\\frac9{20}}\\psi_{4,1,0}+\\frac12\\psi_{4,2,0}-\\sqrt{\\frac1{20}}\\psi_{4,3,0} & \\frac34 & 0\\\\ \\hline \\end{array}$$ The values of $m$ have been added to the table to make it more obvious that they, together with the eigenvalues $A_z$ and $n$ , actually complete the CSCO. We can even plot configurations of the possible states using these quantum numbers, and see a pattern: Looking at these plots, we can guess what the eigenvalues of $\\hat A_z$ will be for the higher $n$ . And then we won't even need the procedure of minimization of variance of the sample of $(3)$ to find the weights $\\alpha_l$ : we can simply solve this equation with a random sample of points in the domain, setting one of the weights to $1$ , and those corresponding to the $m$ we're not interested in, to $0$ (number of points should be chosen to make number of equations equal to number of weights remaining unknown). Now, $k$ th value for $A_z$ appears to follow this formula: $$A_z(n,m,k)=\\frac{|m|-n-1+2k}n,\\tag4$$ where $$k=1,2,\\dots,n-|m|.\\tag5$$ 2.2. Analytical approach to general solution The general solution to eigenproblem of $\\hat A_z$ can be found, if the Schrödinger's equation is expressed in the parabolic coordinates . Then the natural eigenfunctions there will be characterized by a different set of quantum numbers than usual: parabolic ones $n_1$ and $n_2$ , and the usual magnetic quantum number $m$ . The complete expression in terms of confluent hypergeometric function ${}_1F_1$ for the eigenfunctions and its derivation can be found e.g. in Landau and Lifshitz, \"Quantum Mechanics. Non-relativistic theory\" $\\S37$ \"Motion in a Coulomb field (parabolic coordinates)\" . The eigenvalues of $\\hat A_z$ can be expressed in terms of the parabolic and magnetic quantum numbers as $$A_z=\\frac{n_1-n_2}n,\\tag6$$ where $$n=n_1+n_2+|m|+1\\tag7$$ is the principal quantum number, and parabolic quantum numbers can have the values $$n_{1,2}=0,1,...,n-|m|-1.\\tag8$$ In terms of $n$ , $m$ and $n_1$ , $(6)$ can be rewritten as $$A_z=\\frac{|m|-n+2n_1+1}n,\\tag9$$ which is consistent with $(4)$ - $(5)$ . 3. What the eigenfunctions of $\\hat A_z$ look like Eigenfunctions of $\\hat A_z$ with high absolute values of eigenvalues look like bells in shape, with probability density symmetric along the $z$ axis. As $L_z$ is conserved, real and imaginary parts of the eigenfunctions oscillate when we go around the $z$ axis with $m\\ne0$ . Here're plots of some of the eigenfunctions, with 3D density plot on the LHS and cross-sections in $y=0$ plane on the RHS: $n=4,\\,A_z=\\frac34,\\,m=0:$ $n=4,\\,A_z=-\\frac14,\\,m=0:$ $n=4,\\,A_z=-\\frac14,\\,m=2$ , real part: $n=3,\\,A_z=\\frac13,\\,m=1$ , real part: 4. Relation to classical orbits and their eccentricity 4.1. Eigenfunctions of $\\hat A_z$ Unfortunately, the eigenstates of $\\hat A_z$ don't appear to be nicely related to classical eccentric orbits. In classical orbits, $\\vec A$ is always in the plane of rotation. Classical orbital motion corresponds in quantum regime to the case of high expected values of $L_z$ . But $\\hat A_z$ commutes with $\\hat L_z$ , not with e.g. $\\hat L_x$ , so the \"rotating\" (in the sense of $e^{im\\phi}$ ) eigenstates rotate roughly perpendicularly to direction of the orbital eccentricity. We can interpret these as eccentric standing waves in the $\\theta$ direction, which is of course far from classical regime. It's interesting to note that although with the high values of $A_z$ the $z$ component of $\\vec A$ indeed dominates, giving the cross-section of the orbital somewhat elliptic shape, it's much less so for lower values. This is because of uncertainty in $A_x$ and $A_z$ . See e.g. the following eigenfunctions. Ellipses show the supposed classical orbits with semi-major axis $a=n^2$ and LRL vectors $\\vec A=A_z \\vec e_z+s \\sqrt{\\langle A_x^2\\rangle}\\vec e_x$ , where $s=-2,-1,0,1,2$ . $n=20,\\, A_z=\\frac{19}{20},\\, m=0,\\,\\langle A_x^2\\rangle=\\frac{19}{800}\\colon$ $n=20,\\, A_z=\\frac{9}{20},\\, m=0,\\,\\langle A_x^2\\rangle=\\frac{159}{800}\\colon$ $n=20,\\, A_z=\\frac{1}{20},\\, m=0,\\,\\langle A_x^2\\rangle=\\frac{199}{800}\\colon$ 4.2. Eigenfunctions of $\\hat A^2$ We may have better luck if we consider instead eigenstates of $\\hat A^2$ (which, as mentioned above, doesn't commute with $\\hat A_z$ ). These are also eigenstates of $\\hat L^2$ and $\\hat L_z$ , so they are the familiar functions. Eigenvalues of $\\hat A^2$ are $$A_{n,l}^2=1-\\frac{l(l+1)+1}{n^2}.$$ As we can see, consistently with classical intuition, eccentricity $e=\\sqrt{A^2}$ decreases with increasing angular momentum. One might hope to see that at least these states will allow assignment of classical eccentricity. But, despite they do, there's a problem: the eigenstates of $\\hat A^2$ are all almost symmetric with respect to rotations around $z$ axis — modulo the $\\exp(im\\phi)$ oscillations. So we never get anything resembling eccentric ellipses even in eigenstates of $\\hat A^2$ . Instead we get the following cross-sections in $xy$ plane of the real parts of wavefunctions (here $n=21$ , $m=l$ , $l$ changes from $0$ to $20$ with the step of $4$ ): Since these states don't actually have any orientation of the LRL vector (aside from avoiding $z$ direction in case of high $m$ values), a better interpretation and drawing of classical eccentricity for them would be like this: where the ellipses show some of the possible orbits one may get if e.g. one were to form a localized wave packet from similar states with this state being dominant. 5. A note on the method of semi-numerical calculations To make the numerical procedure more understandable, I'll show an example of how one can get an eigenfunction of $\\hat A_z$ and associated eigenvalue with $n=4$ , $m=1$ . The code in this section is in Wolfram Language. I did all the calculations in Mathematica 11.2, but the code is compatible with versions as old as Mathematica 9. First, some definitions for the operators and functions we'll use here. (* Components of momentum operator *) px = -I D[#,x] &; py = -I D[#,y] &; pz = -I D[#,z] &; (* z component of LRL vector operator *) Az = Simplify[ z/Sqrt[x^2+y^2+z^2] # - 1/2 (z px@px@# + px[z px@#] - px[x pz@#] + z py@py@# + py[z py@#] - py[y pz@#] - x pz@px@# - y pz@py@#)] &; (* Hydrogenic wavefunction in spherical coordinates *) ψ[n_,l_,m_,r_,θ_,ϕ_] = Sqrt[(n-l-1)!/(n+l)!] E^(-r/n) (2 r/n)^l 2/n^2 * LaguerreL[n-l-1, 2 l+1, (2 r)/n] * SphericalHarmonicY[l,m,θ,ϕ]; (* The same wavefunction converted to Cartesian coordinates *) Ψ[n_,l_,m_,x_,y_,z_] = ψ[n, l, m, Sqrt[x^2+y^2+z^2], ArcCos[z/Sqrt[x^2+y^2+z^2]], ArcTan[x,y]]; Now the example test function for $n=4$ , $m=1$ . (* Test function with parameters α, β and γ. Restricting arguments to numeric to avoid attempts at symbolic evaluation, which can seriously slow things down. Simplifying it to speedup calculations and reduce roundoff errors. *) test[x_Real,y_Real,z_Real,α_?NumericQ,β_?NumericQ,γ_?NumericQ] = FullSimplify[Az@#/# &[α Ψ[4,1,1,x,y,z] + β Ψ[4,2,1,x,y,z] + γ Ψ[4,3,1,x,y,z]], (x|y|z) ∈ Reals]; And finally minimization of its variance. Note that we don't need to actually calculate an integral as in variational methods: we only need a \"close enough\" approximation of the parameters, the rest can be left to Rationalize . So we use a coarse mesh of points to evaluate the function on. Note also that here we let Mathematica go to complex domain, although the parameters should be real-valued. This lets it avoid singularities in the function by simply going around them, and thus gives much faster convergence. (* Table is generated not on integers to avoid problems like division by zero on evaluation *) With[{ var = Total[Abs[#-Mean@#]^2]& @ Flatten @ Table[test[x,y,z,α,βR + I βI,γR + I γI], {x,-10.123,10,4}, {y,-10.541,10,5}, {z,-10.07,10,5} ] }, {minVal,minim} = NMinimize[{var,Total[#^2] == 1 &[{α,βR,βI,γR,γI}] && α>0.1}, {α,βR,βI,γR,γI}] ] {1.29281973036898*10^-9, {α -> 0.547724712837901, βR -> -0.707106153522197, βI -> 1.84829862406368*10^-7, γR -> 0.447211968838118, γI -> -1.8807768960726*10^-7}} OK, so we see that indeed the imaginary parts are close to zero, so let's guess the form of actual parameters assuming that what we got are square roots of some rationals. (* Tolerance of rationalization is chosen so at to 1) ignore numerical errors of minimization, 2) still give a good enough room to guess the correct number *) Sqrt[Rationalize[#^2, 10^-4]]Sign[#]&[{α,βR,γR} /. minim] {Sqrt[3/10], -(1/Sqrt[2]), 1/Sqrt[5]} This is what we got. Let's check whether this is a correct guess. FullSimplify[Az@#/# &[Sqrt[3/10] Ψ[4,1,1,x,y,z] - Sqrt[1/2] Ψ[4,2,1,x,y,z] + Sqrt[1/5] Ψ[4,3,1,x,y,z]], (x|y|z) ∈ Reals] 1/2 Now we not only confirmed that our test function with the guessed values of parameters is an eigenfunction (since we got constant here), but also obtained the associated eigenvalue, $A_z=1/2$ . This is entry #7 in the table above for $n=4$ . To find another set of parameters we can go the following way. First, we can guess that changing some signs in $\\alpha$ , $\\beta$ and $\\gamma$ might give us some more eigenfunctions. Indeed, it does, so using $+\\sqrt{1/2}$ instead of $-\\sqrt{1/2}$ for $\\beta$ does result in an eigenfunction (entry #8 in the table, with eigenvalue $A_z=-1/2$ ). Another approach at finding other eigenfunctions (useful when there are more parameters, e.g. for $n=4$ , $m=0$ there are $4$ ) is using Orthogonalize to find a basis in the orthogonal subspace of parameters to the one we've already identified. Then we can use that basis to form our new set of parameters for NMinimize to work on. In our example case the situation is trivial, since the whole set of $n=4$ , $m=1$ eigenfunctions consists of 3 elements, and we've already identified two of them, so no need in further NMinimize . So Orthogonalize[{{Sqrt[3/10], -(1/Sqrt[2]), 1/Sqrt[5]}, {Sqrt[3/10], 1/ Sqrt[2], 1/Sqrt[5]}, {1, 1, 1}}] // FullSimplify {{Sqrt[3/10], -(1/Sqrt[2]), 1/Sqrt[5]}, {Sqrt[3/10], 1/Sqrt[2], 1/Sqrt[5]}, {-Sqrt[(2/5)], 0, Sqrt[3/5]}} The third element of the output list is the third eigenfunction (in the $|n,l,m\\rangle$ basis). We can find that associated eigenvalue is $A_z=0$ .",
      "question_latex": [
        "H = \\frac{\\mathbf{p}^2}{2m} - \\frac{k}{r}",
        "\\mathbf{A} = \\frac{1}{2m} ( \\mathbf{p} \\times \\mathbf{L} - \\mathbf{L} \\times \\mathbf{p}) - k \\frac{\\mathbf{r}}{r},",
        "is spherically symmetric and it therefore commutes with the angular momentum",
        "; this causes all its eigenfunctions with equal angular momentum number",
        "but arbitrary magnetic quantum number",
        "to be degenerate in energy. </p>\n\n<p>The hydrogen atom also has a further degeneracy, in that given any angular momentum there are usually other",
        "s with the same energy. This degeneracy is due to the existence of a second constant of motion, usually called the <a href=\"http://en.wikipedia.org/wiki/Laplace%E2%80%93Runge%E2%80%93Lenz_vector\" rel=\"noreferrer\">Laplace-Runge-Lenz</a> vector,",
        "which is the generator of an even bigger symmetry, which is isomorphic for bound states to the group",
        "of rotations in four dimensions, of the Kepler problem.</p>\n\n<p>The Runge-Lenz vector also has a rich geometrical interpretation. For a classical elliptical orbit, it points from the focus to the periapsis and its magnitude is proportional to the orbit's eccentricity. For circular orbits, it vanishes.</p>\n\n<p><img src=\"https://i.stack.imgur.com/9Ejp0.png\" alt=\"enter image description here\"></p>\n\n<p><sup>Image source: <a href=\"http://en.wikipedia.org/wiki/File:Laplace_Runge_Lenz_vector.svg\" rel=\"noreferrer\">Wikipedia</a></sup></p>\n\n<p>The hydrogen atom is usually described in the common eigenbasis of the hamiltonian and the angular momentum, with the well-known and well-loved quantum numbers",
        ". However, the Runge-Lenz vector",
        "is also a constant of the motion.</p>\n\n<h2>What do its eigenfunctions look like?</h2>\n\n<p>More concretely, I'm looking for the spatial structure of the common eigenfunctions of",
        "and at least one component of",
        ", and possibly also of",
        "(which, in analogy with the common eigenfunctions of",
        ",",
        "and"
      ],
      "answer_latex": [
        "\\hat H=\\frac{\\hat{p}^2}2-\\frac1r.\\tag1",
        "\\hat{\\vec A}=\\frac{\\vec r}r-\\frac12\\left(\\hat{\\vec p}\\times\\hat{\\vec L}-\\hat{\\vec L}\\times\\hat{\\vec p}\\right).\\tag2",
        "A_z=\\frac{\\hat A_z\\sum\\limits_l\\alpha_l\\psi_{n,l,m}}{\\sum\\limits_l\\alpha_l\\psi_{n,l,m}},\\tag3",
        "n=2\\\\\n\\begin{array}{|c|c|c|}\n\\hline\n\\text{Eigenfunction of }\\hat A_z&A_z&m\\\\\n\\hline\n\\psi_{2,1,1}   &0  &1  \\\\\n\\psi_{2,1,-1}  &0  &-1 \\\\\n\\frac1{\\sqrt2}(\\psi_{2,0,0}+\\psi_{2,1,0}) &-\\frac12 & 0\\\\\n\\frac1{\\sqrt2}(\\psi_{2,0,0}-\\psi_{2,1,0}) &\\frac12 & 0\\\\\n\\hline\n\\end{array}",
        "n=3\\\\\n\\begin{array}{|c|c|c|}\n\\hline\n\\text{Eigenfunction of }\\hat A_z&A_z&m\\\\\n\\hline\n\\psi_{3,2,-2}   & 0 & -2\\\\\n\\psi_{3,2,2}    & 0 & 2\\\\\n\\frac1{\\sqrt2}(\\psi_{3,1,1}+\\psi_{3,2,1}) & -\\frac13 & 1\\\\\n\\frac1{\\sqrt2}(\\psi_{3,1,1}-\\psi_{3,2,1}) & \\frac13 & 1\\\\\n\\frac1{\\sqrt2}(\\psi_{3,1,-1}+\\psi_{3,2,-1}) & -\\frac13 & -1\\\\\n\\frac1{\\sqrt2}(\\psi_{3,1,-1}-\\psi_{3,2,-1}) & \\frac13 & -1\\\\\n\\sqrt{\\frac13}\\psi_{3,0,0}-\\sqrt{\\frac23}\\psi_{3,2,0} & 0 & 0\\\\\n\\frac1{\\sqrt3}\\psi_{3,0,0}+\\frac1{\\sqrt2}\\psi_{3,1,0}+\\frac1{\\sqrt6}\\psi_{3,2,0} & -\\frac23 & 0\\\\\n\\frac1{\\sqrt3}\\psi_{3,0,0}-\\frac1{\\sqrt2}\\psi_{3,1,0}+\\frac1{\\sqrt6}\\psi_{3,2,0} & \\frac23 & 0\\\\\n\\hline\n\\end{array}",
        "n=4\\\\\n\\begin{array}{|c|c|c|}\n\\hline\n\\text{Eigenfunction of }\\hat A_z&A_z&m\\\\\n\\hline\n\\psi_{4,3,3}  & 0 & 3\\\\\n\\psi_{4,3,-3} & 0 & -3\\\\\n\\frac1{\\sqrt2}(\\psi_{4,2,2}+\\psi_{4,3,2}) & -\\frac14 & 2\\\\\n\\frac1{\\sqrt2}(\\psi_{4,2,2}-\\psi_{4,3,2}) & \\frac14 & 2\\\\\n\\frac1{\\sqrt2}(\\psi_{4,2,-2}+\\psi_{4,3,-2}) & -\\frac14 & -2\\\\\n\\frac1{\\sqrt2}(\\psi_{4,2,-2}-\\psi_{4,3,-2}) & \\frac14 & -2\\\\\n%\n\\sqrt{\\frac3{10}}\\psi_{4,1,1}-\\sqrt{\\frac12}\\psi_{4,2,1}+\\sqrt{\\frac15}\\psi_{4,3,1} & \\frac12 & 1\\\\\n%\n\\sqrt{\\frac3{10}}\\psi_{4,1,1}+\\sqrt{\\frac12}\\psi_{4,2,1}+\\sqrt{\\frac15}\\psi_{4,3,1} & -\\frac12 & 1\\\\\n%\n\\sqrt{\\frac3{10}}\\psi_{4,1,-1}-\\sqrt{\\frac12}\\psi_{4,2,-1}+\\sqrt{\\frac15}\\psi_{4,3,-1} & \\frac12 & -1\\\\\n%\n\\sqrt{\\frac3{10}}\\psi_{4,1,-1}+\\sqrt{\\frac12}\\psi_{4,2,-1}+\\sqrt{\\frac15}\\psi_{4,3,-1} & -\\frac12 & -1\\\\\n%\n\\sqrt{\\frac25}\\psi_{4,1,1}-\\sqrt{\\frac35}\\psi_{4,3,1} & 0 & 1\\\\\n%\n\\sqrt{\\frac25}\\psi_{4,1,-1}-\\sqrt{\\frac35}\\psi_{4,3,-1} & 0 & -1\\\\\n%\n\\frac12\\psi_{4,0,0}-\\sqrt{\\frac1{20}}\\psi_{4,1,0}-\\frac12\\psi_{4,2,0}+\\sqrt{\\frac9{20}}\\psi_{4,3,0} & \\frac14 & 0\\\\\n%\n\\frac12\\psi_{4,0,0}+\\sqrt{\\frac1{20}}\\psi_{4,1,0}-\\frac12\\psi_{4,2,0}-\\sqrt{\\frac9{20}}\\psi_{4,3,0} & -\\frac14 & 0\\\\\n%\n\\frac12\\psi_{4,0,0}+\\sqrt{\\frac9{20}}\\psi_{4,1,0}+\\frac12\\psi_{4,2,0}+\\sqrt{\\frac1{20}}\\psi_{4,3,0} & -\\frac34 & 0\\\\\n%\n\\frac12\\psi_{4,0,0}-\\sqrt{\\frac9{20}}\\psi_{4,1,0}+\\frac12\\psi_{4,2,0}-\\sqrt{\\frac1{20}}\\psi_{4,3,0} & \\frac34 & 0\\\\\n\\hline\n\\end{array}",
        "A_z(n,m,k)=\\frac{|m|-n-1+2k}n,\\tag4",
        "k=1,2,\\dots,n-|m|.\\tag5",
        "A_z=\\frac{n_1-n_2}n,\\tag6",
        "n=n_1+n_2+|m|+1\\tag7",
        "n_{1,2}=0,1,...,n-|m|-1.\\tag8",
        "A_z=\\frac{|m|-n+2n_1+1}n,\\tag9",
        "A_{n,l}^2=1-\\frac{l(l+1)+1}{n^2}.",
        "</span></p>\n\n<p>Its standard eigenfunctions diagonalize operators <span class=\"math-container\">",
        "</span>, <span class=\"math-container\">",
        "</span> and <span class=\"math-container\">",
        "</span>. Laplace-Runge-Lenz operator can be defined as</p>\n\n<p><span class=\"math-container\">",
        "</span></p>\n\n<p>Its <span class=\"math-container\">",
        "</span>-component <span class=\"math-container\">",
        "</span> commutes with <span class=\"math-container\">",
        "</span>, but doesn't commute with <span class=\"math-container\">",
        "</span> nor with <span class=\"math-container\">",
        "</span>, although <span class=\"math-container\">",
        "</span> does commute with <span class=\"math-container\">",
        "</span>.</p>\n\n<h1>2. Eigenfunctions</h1>\n\n<p>Since <span class=\"math-container\">",
        "</span> commutes with the operators giving quantum numbers <span class=\"math-container\">",
        "</span> to the standard hydrogenic eigenstates, these standard eigenstates are also eigenstates of <span class=\"math-container\">",
        "</span>. So no new functions here. For these, we should look at <span class=\"math-container\">",
        "</span>.</p>\n\n<h2>2.1. Semi-numerical approach to finding exact eigenstates of <span class=\"math-container\">",
        "</span></h2>\n\n<p>Since operators for <span class=\"math-container\">",
        "</span> commute with <span class=\"math-container\">",
        "</span>, any eigenstate of <span class=\"math-container\">",
        "</span> is a superposition of states with fixed <span class=\"math-container\">",
        "</span> and different <span class=\"math-container\">",
        "</span> quantum numbers. This lets us find eigenstates of <span class=\"math-container\">",
        "</span> for given <span class=\"math-container\">",
        "</span> without actually solving the eigenfunction PDE. We can e.g. numerically minimize variance of a sample of values of the expression for the eigenvalue</p>\n\n<p><span class=\"math-container\">",
        "</span></p>\n\n<p>where <span class=\"math-container\">",
        "</span> are the standard simultaneous eigenfunctions of <span class=\"math-container\">",
        "</span>, getting a set of weights <span class=\"math-container\">",
        "</span>. Then we can use the approximations we got to guess the exact values of these weights (and substitute into <span class=\"math-container\">",
        "</span>, simplifying, to confirm the guess).</p>\n\n<p>So, for <span class=\"math-container\">",
        "</span> we have the only eigenfunction, and it's obviously an eigenfunction of <span class=\"math-container\">",
        "</span>. For <span class=\"math-container\">",
        "</span> we have more basis functions, which lets us actually form more \"interesting\" eigenstates of <span class=\"math-container\">",
        "</span>. Here're some tables for <span class=\"math-container\">",
        "</span> (I calculated them using the above mentioned minimization procedure):\n<span class=\"math-container\">",
        "</span></p>\n\n<p><span class=\"math-container\">",
        "</span></p>\n\n<p>The values of <span class=\"math-container\">",
        "</span> have been added to the table to make it more obvious that they, together with the eigenvalues <span class=\"math-container\">",
        "</span>, actually complete the CSCO. We can even plot configurations of the possible states using these quantum numbers, and see a pattern:</p>\n\n<p><a href=\"https://i.stack.imgur.com/ijLd1.png\" rel=\"noreferrer\"><img src=\"https://i.stack.imgur.com/ijLd1.png\" alt=\"possible states with different n\"></a></p>\n\n<p>Looking at these plots, we can guess what the eigenvalues of <span class=\"math-container\">",
        "</span> will be for the higher <span class=\"math-container\">",
        "</span>. And then we won't even need the procedure of minimization of variance of the sample of <span class=\"math-container\">",
        "</span> to find the weights <span class=\"math-container\">",
        "</span>: we can simply solve this equation with a random sample of points in the domain, setting one of the weights to <span class=\"math-container\">",
        "</span>, and those corresponding to the <span class=\"math-container\">",
        "</span> we're not interested in, to <span class=\"math-container\">",
        "</span> (number of points should be chosen to make number of equations equal to number of weights remaining unknown).</p>\n\n<p>Now, <span class=\"math-container\">",
        "</span>th value for <span class=\"math-container\">",
        "</span> appears to follow this formula:</p>\n\n<p><span class=\"math-container\">",
        "</span>\nwhere</p>\n\n<p><span class=\"math-container\">",
        "</span></p>\n\n<h2>2.2. Analytical approach to general solution</h2>\n\n<p>The general solution to eigenproblem of <span class=\"math-container\">",
        "</span> can be found, if the Schrödinger's equation is expressed in the <a href=\"https://en.wikipedia.org/wiki/Parabolic_coordinates#Three-dimensional_parabolic_coordinates\" rel=\"noreferrer\">parabolic coordinates</a>. Then the natural eigenfunctions there will be characterized by a different set of quantum numbers than usual: parabolic ones <span class=\"math-container\">",
        "</span>, and the usual magnetic quantum number <span class=\"math-container\">",
        "</span>. The complete expression in terms of confluent hypergeometric function <span class=\"math-container\">",
        "</span> for the eigenfunctions and its derivation can be found e.g. in Landau and Lifshitz, <em>\"Quantum Mechanics. Non-relativistic theory\"</em> <span class=\"math-container\">",
        "</span> <em>\"Motion in a Coulomb field (parabolic coordinates)\"</em>. The eigenvalues of <span class=\"math-container\">",
        "</span> can be expressed in terms of the parabolic and magnetic quantum numbers as</p>\n\n<p><span class=\"math-container\">",
        "</span></p>\n\n<p>where</p>\n\n<p><span class=\"math-container\">",
        "</span></p>\n\n<p>is the principal quantum number, and parabolic quantum numbers can have the values</p>\n\n<p><span class=\"math-container\">",
        "</span></p>\n\n<p>In terms of <span class=\"math-container\">",
        "</span> can be rewritten as</p>\n\n<p><span class=\"math-container\">",
        "</span></p>\n\n<p>which is consistent with <span class=\"math-container\">",
        "</span>-<span class=\"math-container\">",
        "</span>.</p>\n\n<h1>3. What the eigenfunctions of <span class=\"math-container\">",
        "</span> look like</h1>\n\n<p>Eigenfunctions of <span class=\"math-container\">",
        "</span> with high absolute values of eigenvalues look like bells in shape, with probability density symmetric along the <span class=\"math-container\">",
        "</span> axis. As <span class=\"math-container\">",
        "</span> is conserved, real and imaginary parts of the eigenfunctions oscillate when we go around the <span class=\"math-container\">",
        "</span> axis with <span class=\"math-container\">",
        "</span>.</p>\n\n<p>Here're plots of some of the eigenfunctions, with 3D density plot on the LHS and cross-sections in <span class=\"math-container\">",
        "</span> plane on the RHS:</p>\n\n<p><span class=\"math-container\">",
        "</span></p>\n\n<p><a href=\"https://i.stack.imgur.com/4m1rN.gif\" rel=\"noreferrer\"><img src=\"https://i.stack.imgur.com/4m1rN.gif\" alt=\"n=4,A_z=3/4,m=0, 3D plot\"></a><a href=\"https://i.stack.imgur.com/3HhZh.png\" rel=\"noreferrer\"><img src=\"https://i.stack.imgur.com/3HhZh.png\" alt=\"n=4,A_z=3/4,m=0, xz section\"></a></p>\n\n<p><span class=\"math-container\">",
        "</span></p>\n\n<p><a href=\"https://i.stack.imgur.com/EFlCd.gif\" rel=\"noreferrer\"><img src=\"https://i.stack.imgur.com/EFlCd.gif\" alt=\"n=4,A_z=-1/4,m=0, 3D plot\"></a><a href=\"https://i.stack.imgur.com/DLG1X.png\" rel=\"noreferrer\"><img src=\"https://i.stack.imgur.com/DLG1X.png\" alt=\"n=4,A_z=-1/4,m=0, xz section\"></a></p>\n\n<p><span class=\"math-container\">",
        "</span>, real part:</p>\n\n<p><a href=\"https://i.stack.imgur.com/UngMw.gif\" rel=\"noreferrer\"><img src=\"https://i.stack.imgur.com/UngMw.gif\" alt=\"n=4,A_z=-1/4,m=2 real part, 3D plot\"></a><a href=\"https://i.stack.imgur.com/d56Xy.png\" rel=\"noreferrer\"><img src=\"https://i.stack.imgur.com/d56Xy.png\" alt=\"n=4,A_z=-1/4,m=2 real part, xz section\"></a></p>\n\n<p><span class=\"math-container\">",
        "</span>, real part:</p>\n\n<p><a href=\"https://i.stack.imgur.com/iAGGE.gif\" rel=\"noreferrer\"><img src=\"https://i.stack.imgur.com/iAGGE.gif\" alt=\"n=3,A_z=\\frac13,m=1 real part, 3D plot\"></a><a href=\"https://i.stack.imgur.com/V3jLg.png\" rel=\"noreferrer\"><img src=\"https://i.stack.imgur.com/V3jLg.png\" alt=\"n=3,A_z=\\frac13,m=1 real part, xz section\"></a></p>\n\n<h1>4. Relation to classical orbits and their eccentricity</h1>\n\n<h2>4.1. Eigenfunctions of <span class=\"math-container\">",
        "</span></h2>\n\n<p>Unfortunately, the eigenstates of <span class=\"math-container\">",
        "</span> don't appear to be nicely related to classical eccentric orbits. In classical orbits, <span class=\"math-container\">",
        "</span> is always in the plane of rotation. Classical orbital motion corresponds in quantum regime to the case of high expected values of <span class=\"math-container\">",
        "</span>. But <span class=\"math-container\">",
        "</span>, not with e.g. <span class=\"math-container\">",
        "</span>, so the \"rotating\" (in the sense of <span class=\"math-container\">",
        "</span>) eigenstates rotate roughly perpendicularly to direction of the orbital eccentricity. We can interpret these as eccentric standing waves in the <span class=\"math-container\">",
        "</span> direction, which is of course far from classical regime.</p>\n\n<p>It's interesting to note that although with the high values of <span class=\"math-container\">",
        "</span> the <span class=\"math-container\">",
        "</span> component of <span class=\"math-container\">",
        "</span> indeed dominates, giving the cross-section of the orbital somewhat elliptic shape, it's much less so for lower values. This is because of uncertainty in <span class=\"math-container\">",
        "</span>. See e.g. the following eigenfunctions. Ellipses show the supposed classical orbits with semi-major axis <span class=\"math-container\">",
        "</span> and LRL vectors <span class=\"math-container\">",
        "</span>, where <span class=\"math-container\">",
        "</span>.</p>\n\n<p><span class=\"math-container\">",
        "</span></p>\n\n<p><a href=\"https://i.stack.imgur.com/FZXf1.png\" rel=\"noreferrer\"><img src=\"https://i.stack.imgur.com/FZXf1.png\" alt=\"n=20, A_z=19/20, m=0\"></a></p>\n\n<p><span class=\"math-container\">",
        "</span></p>\n\n<p><a href=\"https://i.stack.imgur.com/P9Nyg.png\" rel=\"noreferrer\"><img src=\"https://i.stack.imgur.com/P9Nyg.png\" alt=\"n=20, A_z=9/20, m=0\"></a></p>\n\n<p><span class=\"math-container\">",
        "</span></p>\n\n<p><a href=\"https://i.stack.imgur.com/1Tg4O.png\" rel=\"noreferrer\"><img src=\"https://i.stack.imgur.com/1Tg4O.png\" alt=\"n=20, A_z=1/20, m=0\"></a></p>\n\n<h2>4.2. Eigenfunctions of <span class=\"math-container\">",
        "</span></h2>\n\n<p>We may have better luck if we consider instead eigenstates of <span class=\"math-container\">",
        "</span> (which, as mentioned above, doesn't commute with <span class=\"math-container\">",
        "</span>). These are also eigenstates of <span class=\"math-container\">",
        "</span>, so they are the familiar functions. Eigenvalues of <span class=\"math-container\">",
        "</span> are</p>\n\n<p><span class=\"math-container\">",
        "</span></p>\n\n<p>As we can see, consistently with classical intuition, eccentricity <span class=\"math-container\">",
        "</span> decreases with increasing angular momentum. One might hope to see that at least these states will allow assignment of classical eccentricity. But, despite they do, there's a problem: the eigenstates of <span class=\"math-container\">",
        "</span> are all almost symmetric with respect to rotations around <span class=\"math-container\">",
        "</span> axis — modulo the <span class=\"math-container\">",
        "</span> oscillations. So we never get anything resembling eccentric ellipses even in eigenstates of <span class=\"math-container\">",
        "</span>. Instead we get the following cross-sections in <span class=\"math-container\">",
        "</span> plane of the real parts of wavefunctions (here <span class=\"math-container\">",
        "</span> changes from <span class=\"math-container\">",
        "</span> to <span class=\"math-container\">",
        "</span> with the step of <span class=\"math-container\">",
        "</span>):</p>\n\n<p><a href=\"https://i.stack.imgur.com/u9TUw.png\" rel=\"noreferrer\"><img src=\"https://i.stack.imgur.com/u9TUw.png\" alt=\"xy cross-sections of real parts of wavefunctions for n=21 and different m=l with classical elliptical orbits superimposed on them\"></a></p>\n\n<p>Since these states don't actually have <em>any</em> orientation of the LRL vector (aside from avoiding <span class=\"math-container\">",
        "</span> direction in case of high <span class=\"math-container\">",
        "</span> values), a better interpretation and drawing of classical eccentricity for them would be like this:</p>\n\n<p><a href=\"https://i.stack.imgur.com/zGB75.png\" rel=\"noreferrer\"><img src=\"https://i.stack.imgur.com/zGB75.png\" alt=\"xy cross-section of real part of n=21, l=m=16 with a set of classical ellipses superimposed on it\"></a></p>\n\n<p>where the ellipses show some of the possible orbits one may get if e.g. one were to form a localized wave packet from similar states with this state being dominant.</p>\n\n<h1>5. A note on the method of semi-numerical calculations</h1>\n\n<p>To make the numerical procedure more understandable, I'll show an example of how one can get an eigenfunction of <span class=\"math-container\">",
        "</span> and associated eigenvalue with <span class=\"math-container\">",
        "</span>. The code in this section is in Wolfram Language. I did all the calculations in Mathematica 11.2, but the code is compatible with versions as old as Mathematica 9.</p>\n\n<p>First, some definitions for the operators and functions we'll use here.</p>\n\n<pre><code>(* Components of momentum operator *)\npx = -I D[#,x] &;\npy = -I D[#,y] &;\npz = -I D[#,z] &;\n\n(* z component of LRL vector operator *)\nAz = Simplify[\n    z/Sqrt[x^2+y^2+z^2] # -\n     1/2 (z px@px@# + px[z px@#] - px[x pz@#] + z py@py@# + \n        py[z py@#] - py[y pz@#] - x pz@px@# - y pz@py@#)] &;\n\n(* Hydrogenic wavefunction in spherical coordinates *)\nψ[n_,l_,m_,r_,θ_,ϕ_] = Sqrt[(n-l-1)!/(n+l)!] E^(-r/n) (2 r/n)^l 2/n^2 *\n                          LaguerreL[n-l-1, 2 l+1, (2 r)/n] *\n                          SphericalHarmonicY[l,m,θ,ϕ];\n(* The same wavefunction converted to Cartesian coordinates *)\nΨ[n_,l_,m_,x_,y_,z_] = ψ[n, l, m, Sqrt[x^2+y^2+z^2],\n                         ArcCos[z/Sqrt[x^2+y^2+z^2]],\n                         ArcTan[x,y]];\n</code></pre>\n\n<p>Now the example test function for <span class=\"math-container\">",
        "</span>.</p>\n\n<pre><code>(* Test function with parameters α, β and γ. Restricting arguments to\nnumeric to avoid attempts at symbolic evaluation, which can seriously slow\nthings down. Simplifying it to speedup calculations and reduce roundoff\nerrors. *)\ntest[x_Real,y_Real,z_Real,α_?NumericQ,β_?NumericQ,γ_?NumericQ] =\n        FullSimplify[Az@#/# &[α Ψ[4,1,1,x,y,z] +\n                              β Ψ[4,2,1,x,y,z] +\n                              γ Ψ[4,3,1,x,y,z]],\n                     (x|y|z) ∈ Reals];\n</code></pre>\n\n<p>And finally minimization of its variance. Note that we don't need to actually calculate an integral as in variational methods: we only need a \"close enough\" approximation of the parameters, the rest can be left to <code>Rationalize</code>. So we use a coarse mesh of points to evaluate the function on. Note also that here we let Mathematica go to complex domain, although the parameters should be real-valued. This lets it avoid singularities in the function by simply going around them, and thus gives much faster convergence.</p>\n\n<pre><code>(* Table is generated not on integers to avoid problems like\n   division by zero on evaluation *)\nWith[{\n    var = Total[Abs[#-Mean@#]^2]& @ Flatten @\n             Table[test[x,y,z,α,βR + I βI,γR + I γI],\n                   {x,-10.123,10,4},\n                   {y,-10.541,10,5},\n                   {z,-10.07,10,5}\n                  ]\n },\n{minVal,minim} = NMinimize[{var,Total[#^2] == 1 &[{α,βR,βI,γR,γI}] && α>0.1},\n                           {α,βR,βI,γR,γI}]\n]\n</code></pre>\n\n<blockquote>\n  <p><code>{1.29281973036898*10^-9, {α -> 0.547724712837901, βR -> -0.707106153522197, βI -> 1.84829862406368*10^-7, γR -> 0.447211968838118, γI -> -1.8807768960726*10^-7}}</code></p>\n</blockquote>\n\n<p>OK, so we see that indeed the imaginary parts are close to zero, so let's guess the form of actual parameters assuming that what we got are square roots of some rationals.</p>\n\n<pre><code>(* Tolerance of rationalization is chosen so at to\n    1) ignore numerical errors of minimization,\n    2) still give a good enough room to guess the correct number *)\nSqrt[Rationalize[#^2, 10^-4]]Sign[#]&[{α,βR,γR} /. minim]\n</code></pre>\n\n<blockquote>\n  <p><code>{Sqrt[3/10], -(1/Sqrt[2]), 1/Sqrt[5]}</code></p>\n</blockquote>\n\n<p>This is what we got. Let's check whether this is a correct guess.</p>\n\n<pre><code>FullSimplify[Az@#/# &[Sqrt[3/10] Ψ[4,1,1,x,y,z] - \n                       Sqrt[1/2] Ψ[4,2,1,x,y,z] + \n                       Sqrt[1/5] Ψ[4,3,1,x,y,z]],\n             (x|y|z) ∈ Reals]\n</code></pre>\n\n<blockquote>\n  <p><code>1/2</code></p>\n</blockquote>\n\n<p>Now we not only confirmed that our test function with the guessed values of parameters is an eigenfunction (since we got constant here), but also obtained the associated eigenvalue, <span class=\"math-container\">",
        "</span>. This is entry #7 in the table above for <span class=\"math-container\">",
        "</span>.</p>\n\n<p>To find another set of parameters we can go the following way. First, we can guess that changing some signs in <span class=\"math-container\">",
        "</span> might give us some more eigenfunctions. Indeed, it does, so using <span class=\"math-container\">",
        "</span> instead of <span class=\"math-container\">",
        "</span> for <span class=\"math-container\">",
        "</span> does result in an eigenfunction (entry #8 in the table, with eigenvalue <span class=\"math-container\">",
        "</span>).</p>\n\n<p>Another approach at finding other eigenfunctions (useful when there are more parameters, e.g. for <span class=\"math-container\">",
        "</span> there are <span class=\"math-container\">",
        "</span>) is using <code>Orthogonalize</code> to find a basis in the orthogonal subspace of parameters to the one we've already identified. Then we can use that basis to form our new set of parameters for <code>NMinimize</code> to work on. In our example case the situation is trivial, since the whole set of <span class=\"math-container\">",
        "</span> eigenfunctions consists of 3 elements, and we've already identified two of them, so no need in further <code>NMinimize</code>. So</p>\n\n<pre><code>Orthogonalize[{{Sqrt[3/10], -(1/Sqrt[2]), 1/Sqrt[5]},\n               {Sqrt[3/10], 1/ Sqrt[2], 1/Sqrt[5]},\n               {1, 1, 1}}] // FullSimplify\n</code></pre>\n\n<blockquote>\n  <p><code>{{Sqrt[3/10], -(1/Sqrt[2]), 1/Sqrt[5]}, {Sqrt[3/10], 1/Sqrt[2], 1/Sqrt[5]}, {-Sqrt[(2/5)], 0, Sqrt[3/5]}}</code></p>\n</blockquote>\n\n<p>The third element of the output list is the third eigenfunction (in the <span class=\"math-container\">",
        "</span> basis). We can find that associated eigenvalue is <span class=\"math-container\">"
      ],
      "created": "2013-12-09T21:05:58.687",
      "golden_ner_terms": [
        "absolute value",
        "analogy",
        "angular momentum",
        "approximation",
        "approximations",
        "atom",
        "axis",
        "basis",
        "bound",
        "calculate",
        "cartesian coordinates",
        "circular",
        "code",
        "compatible",
        "complete",
        "complex",
        "component",
        "components",
        "confluent",
        "consistent",
        "constant",
        "coordinates",
        "cross-section",
        "density",
        "derivation",
        "diagonalize",
        "division",
        "division by zero",
        "domain",
        "dominant",
        "dominates",
        "eccentricity",
        "eigenfunction",
        "eigenvalue",
        "eigenvalues",
        "elements",
        "energy",
        "equation",
        "even",
        "expected value",
        "expression",
        "field",
        "fixed",
        "focus",
        "formula",
        "function",
        "general solution",
        "generator",
        "geometry",
        "group",
        "hamiltonian",
        "hydrogen",
        "hypergeometric function",
        "image",
        "imaginary",
        "imaginary part",
        "increasing",
        "integral",
        "interpretation",
        "intuition",
        "isomorphic",
        "language",
        "mathematica",
        "minimization",
        "momentum",
        "nor",
        "number",
        "numbers",
        "obvious",
        "operator",
        "operators",
        "or operator",
        "orbit",
        "orbital motion",
        "orientation",
        "orthogonal",
        "pde",
        "plane",
        "probability",
        "proportional",
        "quantum mechanics",
        "real",
        "real part",
        "relation",
        "rotate",
        "rotation",
        "section",
        "similar",
        "singularities",
        "solution",
        "source",
        "speedup",
        "spherical coordinates",
        "square",
        "square root",
        "state",
        "step",
        "structure",
        "subspace",
        "superposition",
        "symmetric",
        "symmetry",
        "theory",
        "useful",
        "vanishes",
        "variance",
        "variational method",
        "vector",
        "vectors",
        "wavefunction",
        "waves",
        "way",
        "wikipedia",
        "work",
        "zero"
      ],
      "golden_ner_count": 115,
      "golden_patterns": [
        {
          "pattern": "use-probabilistic-method",
          "score": 6.0,
          "hotwords": [
            "probability",
            "random",
            "expected value"
          ]
        },
        {
          "pattern": "work-examples-first",
          "score": 4.0,
          "hotwords": [
            "example",
            "e.g."
          ]
        },
        {
          "pattern": "quotient-by-irrelevance",
          "score": 2.0,
          "hotwords": [
            "modulo"
          ]
        },
        {
          "pattern": "check-the-extreme-cases",
          "score": 2.0,
          "hotwords": [
            "zero"
          ]
        },
        {
          "pattern": "exploit-symmetry",
          "score": 2.0,
          "hotwords": [
            "symmetric"
          ]
        },
        {
          "pattern": "local-to-global",
          "score": 2.0,
          "hotwords": [
            "local"
          ]
        },
        {
          "pattern": "the-diagonal-argument",
          "score": 2.0,
          "hotwords": [
            "diagonal"
          ]
        },
        {
          "pattern": "estimate-by-bounding",
          "score": 2.0,
          "hotwords": [
            "at least"
          ]
        },
        {
          "pattern": "monotone-approximation",
          "score": 2.0,
          "hotwords": [
            "limit"
          ]
        }
      ],
      "golden_pattern_names": [
        "use-probabilistic-method",
        "work-examples-first",
        "quotient-by-irrelevance",
        "check-the-extreme-cases",
        "exploit-symmetry",
        "local-to-global",
        "the-diagonal-argument",
        "estimate-by-bounding",
        "monotone-approximation"
      ],
      "golden_scopes": [],
      "golden_scope_count": 0
    },
    {
      "id": "se-physics-728",
      "stratum": "easy",
      "title": "Will a ball slide down a lumpy hill over the same path it rolls down the hill?",
      "tags": [
        "classical-mechanics"
      ],
      "score": 26,
      "answer_score": 4,
      "question_body": "Suppose I have a lumpy hill. In a first experiment, the hill is frictionless and I let a ball slide down, starting from rest. I watch the path it takes (the time-independent trail it follows). In the next experiment, the hill stays the same shape, but the ball now rolls without slipping down the hill. Assume there is no deformation of the ball or of the hill, no micro-sliding of the contact surface, and no other forms of rolling resistance . Energy is conserved. I release the ball from rest at same point on the hill. I track the path the rolling ball takes. Does the rolling ball follow the same path as the sliding ball, but more slowly, or does it sometimes follow a different path? Note: There may be some \"trick\" answers. For example, if the hill curves significantly on scales similar to the radius of the ball, the ball could get \"wedged in\" somewhere, or two parts of the ball could contact the hill at the same time. Let's assume that geometrically, the shape of the hill is such that it is only possible to have the ball contact the hill at one point, and that the ball always contacts the hill at a single point (i.e. it never flies off).",
      "answer_body": "1) Problem. Assume that the ball at all times touches the hill in precisely one point of contact $P$. Let the ball have radius $R$, have mass $m$, be spherically symmetric (but not necessarily with uniform density), and with moment of inertia $$I~=~(\\alpha-1) mR^2,$$ where $\\alpha\\geq 1$ is a dimensionless constant. The value $I=\\frac{2}{5}mR^2$ ($\\alpha=\\frac{7}{5}$) corresponds to a ball with uniform density. The point is now, as Carl Brannen observes, that a rolling ball with no sliding and zero moment of inertia $I=0$ ($\\alpha=1$) corresponds to a ball sliding without friction and without spinning. From now on, we therefore only have to consider rolling without sliding for various value of $I$. The question can hence be reformulated as follows. Question. Is the (time-independent) path of the rolling ball independent of the moment of inertia $I$ if the ball starts from rest with no initial spinning normal to the surface? We assert the following two theorems. Theorem 1. The answer is in general No. This is basically because the ball can on a generic surface spontaneously acquire a spin component $\\omega_{\\! N}$ perpendicular to the surface. Theorem 2. The answer is Yes, if furthermore the Hessian ${\\bf H}$ in each point is proportional to the $2\\times 2$ unit matrix ${\\bf 1}_{2\\times 2}$, $$ {\\bf H} ~\\propto~ {\\bf 1}_{2\\times 2}. \\qquad (1) $$ The Hessian ${\\bf H}$ is defined in eq. (2) below. We will show that condition (1) implies that that the ball cannot spontaneously acquire a spin component $\\omega_{\\! N}$ perpendicular to the surface. 2) Center-of-mass surface. Let the surface of the hill be described by a smooth function $x^3=h(x^1,x^2)$. Since we assume that the ball with radius $R$ continuous to touch the hill in precisely one point $P$, the center-of-mass (CM) of the ball will lie on a curve $x^3=f(x^1,x^2)$, where the distance, between the two graphs $$\\mathrm{graph}(h)~=~\\{(x^1,x^2,x^3)\\in \\mathbb{R}^3 \\mid x^3=h(x^1,x^2)\\},$$ and $$\\mathrm{graph}(f)~=~\\{(x^1,x^2,x^3)\\in \\mathbb{R}^3 \\mid x^3=f(x^1,x^2)\\},$$ is $R$ everywhere. (Formulated more precisely: The distance from one graph to an arbitrary point $P$ on the other graph is always $R$, independent of the point $P$.) We will next choose the horizontal center-of-mass coordinates $(x^1,x^2)$ as generalized coordinates for the ball. The CM $x^3$-coordinate is then just $f(x^1,x^2)$, which determines the potential energy $$V(x^1,x^2)~=~mgf(x^1,x^2)$$ of the ball. We are not able to describe the orientation of the ball only with the help of the generalized coordinates $(x^1,x^2)$. Since we don't describe the orientation of the ball, we will avoid non-holonomic constraints. Next note that friction and normal force do no work on the ball, and that the total mechanical energy $E=T+V$ of the ball is conserved. 3) Newtonian analysis. A spin normal to the surface is not captured by the configuration space variables $(x^1,x^2,\\dot{x}^1,\\dot{x}^2)$ alone, so the Lagrangian $(x^1,x^2,\\dot{x}^1,\\dot{x}^2)$ formulation is incomplete if there is such spin. Here we will first analyze the general system using Newton's laws without assuming any particular surface or initial conditions. Center-of-mass point CM: $${\\bf r}_{cm}~:=~(x^1,x^2, f(x^1,x^2)) ~=~{\\bf r}_p + R{\\bf n}, \\qquad x^3_{cm}~=~f(x^1,x^2).$$ Contact point $P$: $${\\bf r}_p~:=~{\\bf r}_{cm}- R{\\bf n}, \\qquad \\dot{\\bf r}_{cm}~=~\\dot{\\bf r}_p + R\\dot{\\bf n}, \\qquad \\dot{\\bf r}_{cm} \\perp {\\bf n} \\perp \\dot{\\bf r}_p.$$ Tangent vectors to the surface in the CM point: $$ {\\bf r}_{cm,1}~:=~ (1,0,f_1), \\qquad {\\bf r}_{cm,2}~:=~(0,1,f_2), $$ $$\\dot{\\bf r}_{cm}~=~\\sum_{i=1}^2{\\bf r}_{cm,i}\\dot{x}^i, \\qquad f_i~:=~\\frac{\\partial f}{\\partial x^i}, \\qquad i=1,2.$$ Normal vector to the surface in the CM point: $$ {\\bf N}~:=~{\\bf r}_{cm,1} \\times {\\bf r}_{cm,2}~=~(-f_1,-f_2,1).$$ Length of normal vector: $$N~:=~|{\\bf N}|~=~\\sqrt{1+\\sum_{i=1}^2 f_i f_i}~>~0,$$ Hessian: $$ {\\bf H}~:=~\\left[\\begin{array}{cc} f_{11} &f_{12} \\cr f_{21} &f_{22} \\end{array}\\right], \\qquad f_{ij}~:=~\\frac{\\partial^2 f}{\\partial x^i\\partial x^j}, \\qquad i,j=1,2. \\qquad (2)$$ Time derivative of Normal vector: $$ \\dot{N}_i~=~-\\sum_{j=1}^2 f_{ij} \\dot{x}^j, \\qquad i=1,2, \\qquad \\dot{N}_3~=~0, \\qquad\\dot{N}~=~ \\sum_{i,j=1}^2\\frac{f_i f_{ij} \\dot{x}^j}{N}.$$ Unit normal vector: $${\\bf n}~:=~ \\frac{{\\bf N}}{N}, \\qquad |{\\bf n}|~=~1, \\qquad n_i ~=~- \\frac{f_i}{N}, \\qquad i=1,2,\\qquad n_3 ~=~ \\frac{1}{N}, $$ $$\\dot{\\bf n}~=~ \\frac{\\dot{\\bf N}}{N} -\\frac{\\dot{N}}{N^2} {\\bf N}, \\qquad \\dot{\\bf n} \\perp {\\bf n} . $$ In components: $$\\dot{n}_k~=~ -\\sum_{j=1}^2\\frac{f_{kj} \\dot{x}^j}{N} +\\sum_{i,j=1}^2\\frac{f_i f_{ij} \\dot{x}^j}{N^3} f_k ~\\stackrel{(4)}{=}~ -\\sum_{j=1}^2\\frac{f_{kj} \\dot{x}^j}{N}-\\dot{n}_3 f_k, \\qquad k=1,2,\\qquad (3)$$ $$\\dot{n}_3~=~ -\\sum_{i,j=1}^2\\frac{f_i f_{ij} \\dot{x}^j}{N^3} ~=~\\sum_{k=1}^2\\dot{n}_k f_k. \\qquad (4) $$ Rolling/No slip condition: $$\\dot{\\bf r}_{cm}~=~{\\bf \\omega} \\times R{\\bf n} ~=~{\\bf \\omega}_{\\parallel} \\times R{\\bf n}, \\qquad \\dot{\\bf r}_{cm} \\perp {\\bf n}, \\qquad {\\bf \\omega}_{\\parallel}~=~ \\frac{\\bf n}{R}\\times\\dot{\\bf r}_{cm}, \\qquad (5)$$ $$|\\dot{\\bf r}_{cm}|~=~R|{\\bf \\omega}_{\\parallel}| , \\qquad {\\bf \\omega}~=~{\\bf \\omega}_{\\parallel}+{\\bf \\omega}_{\\perp}, \\qquad |{\\bf \\omega}|^2~=~|{\\bf \\omega}_{\\parallel}|^2+\\omega_{\\! N}^2.$$ Normal component of the angular velocity ${\\bf \\omega}$: $${\\bf \\omega}_{\\perp} ~=~\\omega_{\\! N} {\\bf n}, \\qquad \\omega_{\\! N}~:=~{\\bf \\omega}\\cdot {\\bf n} ~=~ \\frac{\\omega_3-\\sum_{k=1}^2\\omega_k f_k}{N}. \\qquad (6) $$ Rolling/No slip condition (5) in components: $$ \\dot{x}^i~=~\\frac{R}{N} \\sum_{j=1}^2\\epsilon^{ij} ( \\omega_j +\\omega_3 f_j), \\qquad i=1,2, \\qquad \\epsilon^{12}~:=~+1. \\qquad (7) $$ Total angular momentum around origin $0$: $${\\bf J}_0~=~{\\bf L}_0+{\\bf S}, \\qquad {\\bf L}_0 ~=~ {\\bf r}_{cm}\\times m \\dot{\\bf r}_{cm}, \\qquad {\\bf S}~=~I {\\bf \\omega}. $$ Total angular momentum around contact point $P$: $${\\bf J}_p~=~{\\bf L}_p+{\\bf S}, \\qquad {\\bf L}_p ~=~ R{\\bf n}\\times m \\dot{\\bf r}_{cm}, \\qquad {\\bf J}_0-{\\bf J}_p~=~{\\bf L}_0-{\\bf L}_p~=~{\\bf r}_p \\times m\\dot{\\bf r}_{cm} .$$ Newton's 2nd law: $$ m\\ddot{\\bf r}_{cm}~=~{\\bf F}_n + m{\\bf g}, \\qquad {\\bf g}~=~(0,0,-g). \\qquad (8) $$ Angular Newton's 2nd law around origin $0$: $$ \\dot{\\bf J}_0~=~\\tau_0~=~{\\bf r}_p\\times{\\bf F}_n +{\\bf r}_{cm}\\times m{\\bf g} . $$ Angular Newton's 2nd law around contact point $P$: $$ \\dot{\\bf J}_p ~=~\\tau_p~=~R{\\bf n}\\times m{\\bf g} . $$ Angular Newton's 2nd law around CM: $$ I \\dot{\\bf \\omega}~=~\\dot{\\bf S}~=~\\tau_{cm}~=~-R{\\bf n}\\times {\\bf F}_n ~\\stackrel{(8)}{=}~R{\\bf n}\\times m({\\bf g}-\\ddot{\\bf r}_{cm}), \\qquad \\dot{\\bf \\omega} \\perp {\\bf n}. \\qquad (9)$$ Equations (5) and (9) yield the equations of motion for the angular velocity ${\\bf \\omega}$: $$ (mR^2+I) \\dot{\\bf \\omega} ~=~R{\\bf n}\\times m({\\bf g} - {\\bf \\omega} \\times R\\dot{\\bf n}) ~=~R{\\bf n}\\times m {\\bf g} + mR^2\\omega_{\\! N} \\dot{\\bf n} . \\qquad (10) $$ The equations (10) of motion for the angular velocity ${\\bf \\omega}$ in components: $$ \\alpha \\dot{\\omega}_i ~=~\\frac{g}{RN}\\sum_{j=1}^2\\epsilon^{ij}f_j +\\omega_{\\! N} \\dot{n}_i, \\qquad i=1,2, \\qquad \\alpha \\dot{\\omega}_3~=~\\omega_{\\! N} \\dot{n}_3. \\qquad (11) $$ The equations of motion for the normal component $\\omega_{\\! N}$ of the angular velocity: $$\\dot{\\omega}_{\\! N}~\\stackrel{(9)}{=}~{\\bf \\omega}\\cdot \\dot{\\bf n} ~=~ \\sum_{i=1}^2\\omega_i\\dot{n}_i +\\omega_3\\dot{n}_3 ~\\stackrel{(3)+(4)}{=}~-\\sum_{i,j=1}^2\\frac{\\omega_i f_{ij} \\dot{x}^j}{N} +N\\omega_{\\! N} \\dot{n}_3 $$ $$~\\stackrel{(7)}{=}~-\\frac{R}{N^2}\\sum_{i,j,k=1}^2\\omega_i f_{ij}\\epsilon^{jk}(\\omega_k+\\omega_3 f_k) + N\\omega_{\\! N} \\dot{n}_3 .\\qquad (12)$$ Note that the first term in eq. (12) vanish iff condition (1) is satisfied: $${\\bf H} ~\\propto~ {\\bf 1}_{2\\times 2} \\qquad \\Leftrightarrow \\qquad \\forall{\\bf \\omega}\\in\\mathbb{R}^2 : \\sum_{i,j,k=1}^2 \\omega_i f_{ij}\\epsilon^{jk}\\omega_k~=~0. $$ Warning: Condition (1) is not equivalent to saying that every point is umbilical . We assume from now on that condition (1) is satisfied. The main punch line of above Newtonian analysis is the following. The condition (1) and the equations (11) and (12) of motion show that both rates of change of the vertical component $\\dot{\\omega}_3$ and the normal component $\\dot{\\omega}_{\\! N}$ are linear combinations of $\\omega_3$ and the normal component $\\omega_{\\! N}$. Thus since we impose that $\\omega_3$ and $\\omega_{\\! N}$ should vanish at an initial time, they will also vanish at all later times. 4) Kinetic Energy. Since there is no slipping and no spinning normal to the surface at any time, cf. condition (1), we have enough information to fully describe the total kinetic energy $T$ in terms of the configuration space variables $(x^1,x^2,\\dot{x}^1,\\dot{x}^2)$. The total kinetic energy is $$T ~=~ T_{cm}+T_{rot} ~=~ \\alpha T_{cm}, \\qquad \\alpha~:=~1+\\frac{I}{mR^2}, $$ $$T_{cm}~=~\\frac{m}{2}|{\\bf v}|^{2}, \\qquad {\\bf v} ~=~ \\left(\\dot{x}^1,\\dot{x}^2,\\sum_{i=1}^2\\frac{\\partial f}{\\partial x^i}\\dot{x}^i\\right). $$ 5) Maupertuis principle. At this point, let us recall Jacobi's formulation of Maupertuis principle . Ultimately, this approach does not solve the problem, but it does offer a conceptional understanding. The principle states that the ball chooses (among virtual same-energy-paths) a path that extremizes $$ \\int_{P_i}^{P_f} \\sqrt{T} \\ \\mathrm{d}s, $$ for fixed initial and final position, $P_i$ and $P_f$, respectively. Here the metric $$\\mathrm{d}s^2~=~\\sum_{i,j=1}^2 g_{ij}\\mathrm{d}x^i \\mathrm{d}x^j$$ arises from the total kinetic energy $$ T~=~\\frac{1}{2}\\sum_{i,j=1}^2 g_{ij}\\dot{x}^i\\dot{x}^j ~=~ \\frac{1}{2}\\left(\\frac{ds}{dt}\\right)^2, \\qquad g_{ij}~=~ \\alpha m \\left(\\delta_{ij}+ \\frac{\\partial f}{\\partial x^i}\\frac{\\partial f}{\\partial x^j} \\right). $$ Now, an overall constant multiplicative factor $\\alpha$ will clearly not affect the extremization process. Nevertheless, this does not solve the original problem, where we don't want to assume a specific (i.e., fixed) final position $P_f$. So Maupertuis principle does only cover the case, where the end points $P_f$ are assumed to be the same a priori. Then it states that the intermediate paths must also be the same. 6) Lagrange equations. To solve the original question, it turns out to be better to just examine Lagrange equations directly, $$ \\frac{d}{dt}\\left(\\frac{\\partial L}{\\partial \\dot{x}^i}\\right) ~=~ \\frac{\\partial L}{\\partial x^i}, \\qquad L~=~T-V. $$ The total kinetic energy $T$ becomes independent of $\\alpha$ under a scaling of time $t\\to \\sqrt{\\alpha} t$. Also note that the Lagrange equations are invariant under the time scaling $t\\to \\sqrt{\\alpha}t$. Thus the solution will be the same up to a rescaled time parameter and choice of initial conditions. This is also Carl Brannen's point. Interestingly, it is important that the initial velocity is assumed to be zero to achieve the same paths in the two situations, as the following simple example illustrates. Finally, if we compare with the Newtonian analysis, the equation (7) is indeed invariant under a combined scaling $t\\to \\sqrt{\\alpha}t$ and $\\omega_i\\to \\omega_i/\\sqrt{\\alpha}$. However $\\alpha$ is only scaled away from equation (11) if $\\omega_{\\! N}=0$. This is the main reason behind Theorem 1 and 2. 7) Example. Let the hill be an inclined plane with inclination $45^{\\circ}$, and let the $x^1$-axis be in the \"uphill direction\". We take the function $f(x^1,x^2)=x^1$. Let the initial position be $(x^1_i,x^2_i)=(0,0)$, and the initial velocity be $(\\dot{x}^1_i,\\dot{x}^2_i)=(0,v_i)$ along the $x^2$-axis. Then the metric becomes $$ g_{ij} ~=~ \\alpha m \\left[\\begin{array}{cc} 2 & 0 \\\\ 0 & 1 \\end{array} \\right]. $$ The equations of motion are $$ 2\\alpha m \\ddot{x}^1 ~=~ -mg, \\qquad \\alpha m \\ddot{x}^2 ~=~0. $$ The solution is a parabola $$ x^1(t) ~=~ -\\frac{g}{4\\alpha} t^2, \\qquad x^2(t)~=~v_i t. $$ The paths are only the same (i.e., independent of $\\alpha$), if the initial velocity $v_i$ is zero.",
      "question_latex": [],
      "answer_latex": [
        "I~=~(\\alpha-1) mR^2,",
        "{\\bf H} ~\\propto~ {\\bf 1}_{2\\times 2}. \\qquad (1)",
        "\\mathrm{graph}(h)~=~\\{(x^1,x^2,x^3)\\in \\mathbb{R}^3 \\mid x^3=h(x^1,x^2)\\},",
        "\\mathrm{graph}(f)~=~\\{(x^1,x^2,x^3)\\in \\mathbb{R}^3 \\mid x^3=f(x^1,x^2)\\},",
        "V(x^1,x^2)~=~mgf(x^1,x^2)",
        "{\\bf r}_{cm}~:=~(x^1,x^2, f(x^1,x^2)) ~=~{\\bf r}_p + R{\\bf n}, \\qquad \r\nx^3_{cm}~=~f(x^1,x^2).",
        "{\\bf r}_p~:=~{\\bf r}_{cm}- R{\\bf n}, \\qquad \r\n\\dot{\\bf r}_{cm}~=~\\dot{\\bf r}_p + R\\dot{\\bf n}, \\qquad \r\n\\dot{\\bf r}_{cm} \\perp {\\bf n} \\perp \\dot{\\bf r}_p.",
        "{\\bf r}_{cm,1}~:=~ (1,0,f_1), \\qquad {\\bf r}_{cm,2}~:=~(0,1,f_2),",
        "\\dot{\\bf r}_{cm}~=~\\sum_{i=1}^2{\\bf r}_{cm,i}\\dot{x}^i, \\qquad \r\nf_i~:=~\\frac{\\partial f}{\\partial x^i}, \\qquad i=1,2.",
        "{\\bf N}~:=~{\\bf r}_{cm,1} \\times {\\bf r}_{cm,2}~=~(-f_1,-f_2,1).",
        "N~:=~|{\\bf N}|~=~\\sqrt{1+\\sum_{i=1}^2 f_i f_i}~>~0,",
        "{\\bf H}~:=~\\left[\\begin{array}{cc} f_{11} &f_{12} \\cr f_{21} &f_{22} \\end{array}\\right], \\qquad \r\nf_{ij}~:=~\\frac{\\partial^2 f}{\\partial x^i\\partial x^j}, \\qquad i,j=1,2. \\qquad (2)",
        "\\dot{N}_i~=~-\\sum_{j=1}^2 f_{ij} \\dot{x}^j, \\qquad i=1,2, \\qquad  \\dot{N}_3~=~0, \\qquad\\dot{N}~=~ \\sum_{i,j=1}^2\\frac{f_i f_{ij} \\dot{x}^j}{N}.",
        "{\\bf n}~:=~ \\frac{{\\bf N}}{N}, \\qquad |{\\bf n}|~=~1, \\qquad \r\nn_i ~=~-  \\frac{f_i}{N}, \\qquad i=1,2,\\qquad n_3 ~=~   \\frac{1}{N},",
        "\\dot{\\bf n}~=~ \\frac{\\dot{\\bf N}}{N} -\\frac{\\dot{N}}{N^2} {\\bf N}, \\qquad\r\n\\dot{\\bf n} \\perp {\\bf n} .",
        "\\dot{n}_k~=~ -\\sum_{j=1}^2\\frac{f_{kj} \\dot{x}^j}{N}\r\n +\\sum_{i,j=1}^2\\frac{f_i f_{ij} \\dot{x}^j}{N^3} f_k\r\n~\\stackrel{(4)}{=}~ -\\sum_{j=1}^2\\frac{f_{kj} \\dot{x}^j}{N}-\\dot{n}_3 f_k, \\qquad \r\nk=1,2,\\qquad  (3)",
        "\\dot{n}_3~=~  -\\sum_{i,j=1}^2\\frac{f_i f_{ij} \\dot{x}^j}{N^3} \r\n~=~\\sum_{k=1}^2\\dot{n}_k f_k. \\qquad  (4)",
        "\\dot{\\bf r}_{cm}~=~{\\bf \\omega} \\times R{\\bf n}\r\n~=~{\\bf \\omega}_{\\parallel} \\times R{\\bf n}, \\qquad \r\n\\dot{\\bf r}_{cm} \\perp {\\bf n}, \\qquad\r\n{\\bf \\omega}_{\\parallel}~=~  \\frac{\\bf n}{R}\\times\\dot{\\bf r}_{cm}, \\qquad  (5)",
        "|\\dot{\\bf r}_{cm}|~=~R|{\\bf \\omega}_{\\parallel}| ,  \\qquad\r\n{\\bf \\omega}~=~{\\bf \\omega}_{\\parallel}+{\\bf \\omega}_{\\perp}, \\qquad \r\n|{\\bf \\omega}|^2~=~|{\\bf \\omega}_{\\parallel}|^2+\\omega_{\\! N}^2.",
        "{\\bf \\omega}_{\\perp} ~=~\\omega_{\\! N} {\\bf n}, \\qquad \r\n\\omega_{\\! N}~:=~{\\bf \\omega}\\cdot {\\bf n} \r\n~=~ \\frac{\\omega_3-\\sum_{k=1}^2\\omega_k f_k}{N}. \\qquad  (6)",
        "\\dot{x}^i~=~\\frac{R}{N} \\sum_{j=1}^2\\epsilon^{ij} ( \\omega_j  +\\omega_3 f_j), \\qquad\r\n i=1,2, \\qquad  \\epsilon^{12}~:=~+1. \\qquad  (7)",
        "{\\bf J}_0~=~{\\bf L}_0+{\\bf S}, \\qquad {\\bf L}_0\r\n ~=~ {\\bf r}_{cm}\\times m \\dot{\\bf r}_{cm}, \\qquad {\\bf S}~=~I {\\bf \\omega}.",
        "{\\bf J}_p~=~{\\bf L}_p+{\\bf S}, \\qquad \r\n{\\bf L}_p ~=~ R{\\bf n}\\times m \\dot{\\bf r}_{cm},  \\qquad \r\n{\\bf J}_0-{\\bf J}_p~=~{\\bf L}_0-{\\bf L}_p~=~{\\bf r}_p \\times m\\dot{\\bf r}_{cm} .",
        "m\\ddot{\\bf r}_{cm}~=~{\\bf F}_n + m{\\bf g}, \\qquad {\\bf g}~=~(0,0,-g). \\qquad  (8)",
        "\\dot{\\bf J}_0~=~\\tau_0~=~{\\bf r}_p\\times{\\bf F}_n +{\\bf r}_{cm}\\times m{\\bf g} .",
        "\\dot{\\bf J}_p ~=~\\tau_p~=~R{\\bf n}\\times m{\\bf g} .",
        "I \\dot{\\bf \\omega}~=~\\dot{\\bf S}~=~\\tau_{cm}~=~-R{\\bf n}\\times {\\bf F}_n\r\n~\\stackrel{(8)}{=}~R{\\bf n}\\times m({\\bf g}-\\ddot{\\bf r}_{cm}),  \\qquad \r\n\\dot{\\bf \\omega} \\perp {\\bf n}. \\qquad  (9)",
        "(mR^2+I) \\dot{\\bf \\omega}\r\n~=~R{\\bf n}\\times m({\\bf g} - {\\bf \\omega} \\times R\\dot{\\bf n})\r\n~=~R{\\bf n}\\times m {\\bf g} + mR^2\\omega_{\\! N} \\dot{\\bf n}  . \\qquad  (10)",
        "\\alpha \\dot{\\omega}_i\r\n~=~\\frac{g}{RN}\\sum_{j=1}^2\\epsilon^{ij}f_j +\\omega_{\\! N} \\dot{n}_i, \\qquad i=1,2,\r\n\\qquad \\alpha \\dot{\\omega}_3~=~\\omega_{\\! N} \\dot{n}_3. \\qquad  (11)",
        "\\dot{\\omega}_{\\! N}~\\stackrel{(9)}{=}~{\\bf \\omega}\\cdot \\dot{\\bf n} \r\n~=~ \\sum_{i=1}^2\\omega_i\\dot{n}_i +\\omega_3\\dot{n}_3 \r\n~\\stackrel{(3)+(4)}{=}~-\\sum_{i,j=1}^2\\frac{\\omega_i f_{ij} \\dot{x}^j}{N}\r\n+N\\omega_{\\! N} \\dot{n}_3",
        "~\\stackrel{(7)}{=}~-\\frac{R}{N^2}\\sum_{i,j,k=1}^2\\omega_i f_{ij}\\epsilon^{jk}(\\omega_k+\\omega_3 f_k) + N\\omega_{\\! N} \\dot{n}_3 .\\qquad  (12)",
        "{\\bf H} ~\\propto~ {\\bf 1}_{2\\times 2} \\qquad \\Leftrightarrow \\qquad  \\forall{\\bf \\omega}\\in\\mathbb{R}^2 : \\sum_{i,j,k=1}^2 \\omega_i f_{ij}\\epsilon^{jk}\\omega_k~=~0.",
        "T ~=~ T_{cm}+T_{rot} ~=~ \\alpha T_{cm}, \\qquad \\alpha~:=~1+\\frac{I}{mR^2},",
        "T_{cm}~=~\\frac{m}{2}|{\\bf v}|^{2}, \\qquad {\\bf v} ~=~ \\left(\\dot{x}^1,\\dot{x}^2,\\sum_{i=1}^2\\frac{\\partial f}{\\partial x^i}\\dot{x}^i\\right).",
        "\\int_{P_i}^{P_f} \\sqrt{T} \\ \\mathrm{d}s,",
        "\\mathrm{d}s^2~=~\\sum_{i,j=1}^2 g_{ij}\\mathrm{d}x^i \\mathrm{d}x^j",
        "T~=~\\frac{1}{2}\\sum_{i,j=1}^2 g_{ij}\\dot{x}^i\\dot{x}^j ~=~ \\frac{1}{2}\\left(\\frac{ds}{dt}\\right)^2, \\qquad g_{ij}~=~ \\alpha m \\left(\\delta_{ij}+ \\frac{\\partial f}{\\partial x^i}\\frac{\\partial f}{\\partial x^j} \\right).",
        "\\frac{d}{dt}\\left(\\frac{\\partial L}{\\partial \\dot{x}^i}\\right) ~=~ \\frac{\\partial L}{\\partial x^i}, \\qquad L~=~T-V.",
        "g_{ij} ~=~ \\alpha m \\left[\\begin{array}{cc} 2 & 0 \\\\ 0 & 1 \\end{array} \\right].",
        "2\\alpha m  \\ddot{x}^1 ~=~ -mg, \\qquad \\alpha m  \\ddot{x}^2 ~=~0.",
        "x^1(t) ~=~ -\\frac{g}{4\\alpha} t^2, \\qquad  x^2(t)~=~v_i t.",
        "P",
        "R",
        "m",
        "where",
        "is a dimensionless constant. The value",
        "(",
        ") corresponds to a ball with uniform density. The point is now, as Carl Brannen observes, that a rolling ball with no sliding and zero moment of inertia",
        ") corresponds to a ball sliding without friction and without spinning. From now on, we therefore only have to consider rolling without sliding for various value of",
        ". The question can hence be reformulated as follows.</p>\n\n<blockquote>\n  <p><em>Question.</em> Is the (time-independent) path of the rolling ball independent of the moment of inertia",
        "if the ball starts from rest with no initial spinning normal to the surface?</p>\n</blockquote>\n\n<p>We assert the following two theorems.</p>\n\n<blockquote>\n  <p><em>Theorem 1.</em> The answer is in general <em>No.</em> </p>\n</blockquote>\n\n<p>This is basically because the ball can on a generic surface spontaneously acquire a spin component",
        "perpendicular to the surface. </p>\n\n<blockquote>\n  <p><em>Theorem 2.</em> The answer is <em>Yes,</em> if furthermore the Hessian",
        "in each point is proportional to the",
        "unit matrix",
        ",",
        "</p>\n</blockquote>\n\n<p>The Hessian",
        "is defined in eq. (2) below. We will show that condition (1) implies that that the ball cannot spontaneously acquire a spin component",
        "perpendicular to the surface. </p>\n\n<p>2) <em>Center-of-mass surface.</em> Let the surface of the hill be described by a smooth function",
        ". Since we assume that the ball with radius",
        "continuous to touch the hill in precisely one point",
        ", the center-of-mass (CM) of the ball will lie on a curve",
        ", where the distance, between the two graphs</p>\n\n<p>",
        "</p>\n\n<p>and </p>\n\n<p>",
        "</p>\n\n<p>is",
        "everywhere. (Formulated more precisely: The distance from one graph to an arbitrary point",
        "on the other graph is always",
        ", independent of the point",
        ".) We will next choose the horizontal center-of-mass coordinates",
        "as <a href=\"http://en.wikipedia.org/wiki/Generalized_coordinates\" rel=\"nofollow\">generalized coordinates</a> for the ball. The CM",
        "-coordinate is then just",
        ", which determines the potential energy </p>\n\n<p>",
        "</p>\n\n<p>of the ball. We are not able to describe the <em>orientation</em> of the ball only with the help of the generalized coordinates",
        ". Since we don't describe the orientation of the ball, we will avoid non-holonomic constraints. Next note that friction and normal force do no work on the ball, and that the total mechanical energy",
        "of the ball is conserved. </p>\n\n<p>3) <em>Newtonian analysis.</em> A spin normal to the surface is not captured by the configuration space variables",
        "alone, so the Lagrangian",
        "formulation is incomplete if there is such spin. Here we will first analyze the general system using Newton's laws without assuming any particular surface or initial conditions.  </p>\n\n<p>Center-of-mass point CM:",
        "</p>\n\n<p>Contact point",
        ": </p>\n\n<p>",
        "</p>\n\n<p>Tangent vectors to the surface in the CM point:</p>\n\n<p>",
        "</p>\n\n<p>Normal vector to the surface in the CM point: </p>\n\n<p>",
        "</p>\n\n<p>Length of normal vector: </p>\n\n<p>",
        "</p>\n\n<p>Hessian:</p>\n\n<p>",
        "</p>\n\n<p>Time derivative of Normal vector:</p>\n\n<p>",
        "</p>\n\n<p>Unit normal vector: </p>\n\n<p>",
        "</p>\n\n<p>",
        "</p>\n\n<p>In components:</p>\n\n<p>",
        "</p>\n\n<p>Rolling/No slip condition:",
        "</p>\n\n<p>Normal component of the angular velocity",
        ":",
        "</p>\n\n<p>Rolling/No slip condition (5) in components:",
        "</p>\n\n<p>Total angular momentum around origin",
        "</p>\n\n<p>Total angular momentum around contact point",
        "</p>\n\n<p>Newton's 2nd law:",
        "</p>\n\n<p>Angular Newton's 2nd law  around origin",
        "</p>\n\n<p>Angular Newton's 2nd law  around contact point",
        "</p>\n\n<p>Angular Newton's 2nd law  around CM:",
        "</p>\n\n<p>Equations (5) and (9) yield the equations of motion for the angular velocity",
        ":</p>\n\n<p>",
        "</p>\n\n<p>The equations (10) of motion for the angular velocity",
        "in components:</p>\n\n<p>",
        "</p>\n\n<p>The equations of motion for the normal component",
        "of the angular velocity:</p>\n\n<p>",
        "</p>\n\n<p>Note that the first term in eq. (12) vanish iff condition (1) is satisfied:</p>\n\n<p>",
        "</p>\n\n<p>Warning: Condition (1) is <em>not</em> equivalent to saying that every point is <a href=\"http://en.wikipedia.org/wiki/Umbilical_point\" rel=\"nofollow\">umbilical</a>. We assume from now on that condition (1) is satisfied.  The main punch line of above Newtonian analysis is the following.</p>\n\n<blockquote>\n  <p>The condition (1) and the equations (11) and (12) of motion show that both rates of change of the vertical component",
        "and the normal component",
        "are  linear combinations of",
        ". Thus since we impose that",
        "and",
        "should vanish at an initial time, they will also vanish at all later times.</p>\n</blockquote>\n\n<p>4) <em>Kinetic Energy.</em> Since there is no slipping and no spinning normal to the surface at any time, cf. condition (1), we have enough information to fully describe the total kinetic energy",
        "in terms of the configuration space variables",
        ". The total kinetic energy is</p>\n\n<p>",
        "</p>\n\n<p>5) <em>Maupertuis principle.</em> At this point, let us recall Jacobi's formulation of <a href=\"http://en.wikipedia.org/wiki/Maupertuis%27_principle\" rel=\"nofollow\">Maupertuis principle</a>. Ultimately, this approach does not solve the problem, but it does offer a conceptional understanding. The principle states that the ball chooses (among virtual same-energy-paths) a path that extremizes</p>\n\n<p>",
        "</p>\n\n<p>for fixed initial and final position,",
        ", respectively. Here the metric </p>\n\n<p>",
        "</p>\n\n<p>arises from the total kinetic energy</p>\n\n<p>",
        "</p>\n\n<p>Now, an overall constant multiplicative factor",
        "will clearly not affect the extremization process. Nevertheless, this does not solve the original problem, where we don't want to assume a specific (i.e., fixed) final position",
        ". So Maupertuis principle does only cover the case, where the end points",
        "are assumed to be the same a priori. Then it states that the intermediate paths must also be the same. </p>\n\n<p>6) <em>Lagrange equations.</em> To solve the original question, it turns out to be better to just examine Lagrange equations directly,</p>\n\n<p>",
        "</p>\n\n<p>The total kinetic energy",
        "becomes independent of",
        "under a scaling of time",
        ". Also note that the Lagrange equations are invariant under the time scaling",
        ". Thus the solution will be the same up to a rescaled time parameter and choice of initial conditions. This is also Carl Brannen's point. Interestingly, it is important that the initial velocity is assumed to be zero to achieve the same paths in the two situations, as the following simple example illustrates.</p>\n\n<p>Finally, if we compare with the Newtonian analysis, the equation (7) is indeed invariant\nunder a combined scaling",
        ". However",
        "is only scaled away from equation (11) if",
        ". This is the main reason behind Theorem 1 and 2.</p>\n\n<p>7) <em>Example.</em> Let the hill be an inclined plane with inclination",
        ", and let the",
        "-axis be in the \"uphill direction\". We take the function",
        ". Let the initial position be",
        ", and the initial velocity be",
        "along the",
        "-axis. Then the metric becomes</p>\n\n<p>",
        "</p>\n\n<p>The equations of motion are </p>\n\n<p>",
        "</p>\n\n<p>The solution is a parabola</p>\n\n<p>",
        "</p>\n\n<p>The paths are only the same (i.e., independent of",
        "), if the initial velocity"
      ],
      "created": "2010-11-13T19:00:42.380",
      "golden_ner_terms": [
        "a priori",
        "analysis",
        "angular momentum",
        "angular velocity",
        "ball",
        "choose",
        "combinations",
        "component",
        "components",
        "configuration",
        "configuration space",
        "constant",
        "contact point",
        "continuous",
        "coordinates",
        "cover",
        "curve",
        "deformation",
        "density",
        "derivative",
        "distance",
        "end point",
        "end points",
        "energy",
        "equation",
        "equations of motion",
        "equivalent",
        "factor",
        "fixed",
        "friction",
        "function",
        "general system",
        "generic",
        "graph",
        "hessian",
        "iff",
        "implies",
        "inclination",
        "incomplete",
        "independent",
        "inertia",
        "information",
        "initial condition",
        "invariant",
        "l system",
        "lagrange equation",
        "lagrangian",
        "length",
        "lie on",
        "line",
        "linear combination",
        "mass",
        "matrix",
        "metric",
        "moment",
        "moment of inertia",
        "momentum",
        "multiplicative",
        "normal",
        "normal vector",
        "orientation",
        "origin",
        "parabola",
        "parameter",
        "path",
        "perpendicular",
        "plane",
        "point",
        "potential",
        "potential energy",
        "proportional",
        "radius",
        "scales",
        "scaling",
        "similar",
        "simple",
        "smooth",
        "smooth function",
        "solution",
        "space",
        "surface",
        "symmetric",
        "tangent",
        "tangent vector",
        "term",
        "theorem",
        "time",
        "trail",
        "unit",
        "unit normal",
        "vanish",
        "vector",
        "vectors",
        "velocity",
        "work",
        "zero"
      ],
      "golden_ner_count": 96,
      "golden_patterns": [
        {
          "pattern": "work-examples-first",
          "score": 4.0,
          "hotwords": [
            "example",
            "illustrate"
          ]
        },
        {
          "pattern": "exploit-symmetry",
          "score": 4.0,
          "hotwords": [
            "symmetric",
            "invariant"
          ]
        },
        {
          "pattern": "find-the-right-abstraction",
          "score": 2.0,
          "hotwords": [
            "generalize"
          ]
        },
        {
          "pattern": "quotient-by-irrelevance",
          "score": 2.0,
          "hotwords": [
            "up to"
          ]
        },
        {
          "pattern": "check-the-extreme-cases",
          "score": 2.0,
          "hotwords": [
            "zero"
          ]
        },
        {
          "pattern": "local-to-global",
          "score": 2.0,
          "hotwords": [
            "cover"
          ]
        },
        {
          "pattern": "optimise-a-free-parameter",
          "score": 2.0,
          "hotwords": [
            "choose"
          ]
        },
        {
          "pattern": "transport-across-isomorphism",
          "score": 2.0,
          "hotwords": [
            "equivalent"
          ]
        }
      ],
      "golden_pattern_names": [
        "work-examples-first",
        "exploit-symmetry",
        "find-the-right-abstraction",
        "quotient-by-irrelevance",
        "check-the-extreme-cases",
        "local-to-global",
        "optimise-a-free-parameter",
        "transport-across-isomorphism"
      ],
      "golden_scopes": [
        {
          "type": "where-binding",
          "match": "where $\\alpha\\geq 1$ is"
        },
        {
          "type": "set-notation",
          "match": "$\\mathrm{graph}(h)~=~\\{(x^1,x^2,x^3)\\in \\mathbb{R}^3 \\mid x^3=h(x^1,x^2)\\},$"
        },
        {
          "type": "set-notation",
          "match": "$\\mathrm{graph}(f)~=~\\{(x^1,x^2,x^3)\\in \\mathbb{R}^3 \\mid x^3=f(x^1,x^2)\\},$"
        }
      ],
      "golden_scope_count": 3
    },
    {
      "id": "se-physics-133701",
      "stratum": "easy",
      "title": "Propagators, Green’s functions, path integrals and transition amplitudes in quantum mechanics and quantum field theory",
      "tags": [
        "quantum-mechanics",
        "quantum-field-theory",
        "path-integral",
        "propagator"
      ],
      "score": 23,
      "answer_score": 23,
      "question_body": "I’m trying to make a simple conceptual map regarding the things in the title, and I'm finding that I’m a little perplexed about a couple of items. Let me summarize a few things I regard as being true, and then state what I don’t understand. Generally the propagator $K$, or often $D(x-y)$, is a Green’s function of the quantum operator—the Schrödinger operator, the Klein-Gordon operator or the like. In the KG case we would have something like $(\\partial_\\mu\\partial^\\mu + m^2)D\\propto \\delta^4(x)$ The transition amplitude I would tend to think should quantify the probability of a system in a certain state evolving to another state over time, i. e. $\\langle x'',t''|x',t'\\rangle$. The path integral $\\int \\mathcal{D}(\\cdots)\\,e^{iS[\\cdots]}$ should be interchangeable with the transition amplitude, at least according to some of my texts. What I am struggling on is what precisely is meant by “transition amplitude” in some cases. Take for example the propagator for the Klein-Gordon equation, $$D=\\int {d^4p\\over(2\\pi\\hbar)^4}{1\\over E^2}e^{i p\\cdot x/\\hbar}.$$ As far as I can tell from its form, the propagator for the KG equation is not a (Dirac) delta function $\\delta^4 (x)$ or even $\\delta^3(x)$. I don’t actually think that point number 1 applies here. However, I am failing to recognize its relation to “transition amplitude” in this case because I would usually equate usage such as “transition amplitude” with a probability. As the KG propagator is not a normalized distribution, i. e. does not have the form of a delta function, what precisely is it supposed to quantify here? Update: I've since noted that the term ‘propagator’ may be used somewhat differently in different contexts. Specifially, going back to J. J. Sakurai’s Modern Quantum Mechanics, chapter 2.5, he uses $K$ to represent what is called the propagator of the Schrödinger system. He then discusses the equivalent Feynman path integral approach to determine $\\langle x'',t''|x',t'\\rangle$. The use of $D(x-y)$, also referred to as the propagator, in quantum field theory, by contrast, seems to have a different meaning. I now realize $D(x-y)$ is not equivalent to $\\int\\mathcal{D}(\\cdots)\\,e^{iS[\\cdots]}$ but rather something different. So I think that straightens some significant things out in my head. If anyone has anything to add or correct me on, please do.",
      "answer_body": "Quantum mechanics and quantum field theory are different in how they treat their wave equations. The usage of the common term “propagator” could be traced back to the “relativistic wave equation” approach—i. e. people really used to think of the Schrödinger and the KG operators as belonging to the same class of “quantum operators”, but the modern point of view regards these things as being of different nature, so I suggest you do too at first. (Later, you might want to understand the Schrödinger field in non-relativistic QFT by reading chapter III.5 of Zee , and if you’re feeling brave, the origins of modern QFT, described in Weinberg’s first volume , section 1.2.) Accordingly, I will divide my answer into sections on QFT and QM. Quantum mechanics. Assume you know the transition amplitude $\\def\\xi{x_{\\mathrm i}} \\def\\xf{x_{\\mathrm f}} \\def\\ti{t_{\\mathrm i}} \\def\\tf{t_{\\mathrm f}}$ $$K(\\xf,\\tf;\\xi,\\ti) \\equiv \\langle \\xf,\\tf|\\xi,\\ti\\rangle$$ and the wave function $\\psi(x,\\ti) = \\psi_0(x)$ for all $x$ at a certain time $t =\\ti$. Then you also know it at any other time $t=\\tf$: \\begin{multline} \\psi(\\xf,\\tf) \\equiv \\langle \\xf,\\tf|\\psi(\\ti)\\rangle = \\langle \\xf,\\tf|\\left(\\int d^n\\xi\\,|\\xi,\\ti\\rangle\\langle \\xi,\\ti|\\right)|\\psi(\\ti)\\rangle \\\\\\equiv \\int d^n\\xi\\,K(\\xf,\\tf;\\xi,\\ti)\\psi_0(\\xi).\\tag{1} \\end{multline} The first few sections of Feynman and Hibbs or chapter 6 of Srednicki PDF should convince you that $$K(\\xf,\\tf;\\xi,\\ti) = \\int\\limits_{x\\rlap{(\\ti)=\\xi}}^{x\\rlap{(\\tf)=\\xf}} \\mathcal Dx(t)\\,e^{i\\int dt\\,L(x(t),\\dot x(t),t)}.$$ Note well the boundary conditions in the path integral: they will prove to be important in the QFT section. Let us rearrange the arguments, $K(\\xf,\\tf;\\xi,\\ti) = K(\\xf,\\xi;\\tf,\\ti)$. Then you will be able to recognize in (1) an integral representation of the evolution operator $U(\\tf,\\ti)$, $$ \\psi(\\xf,\\tf)\\equiv(U(\\tf,\\ti)\\psi_0)(\\xf) = \\int d^n\\xi\\,K(\\xf,\\xi;\\tf,\\ti)\\psi_0(\\xi).$$ If you think of $\\xi$ and $\\xf$ as indices with continuous number of values, this formula looks very much like matrix multiplication and $K(\\cdot,\\cdot\\,;\\tf,\\ti)$ plays the role of the matrix. This makes sense, because the linear operator $U(\\tf,\\ti)$ should be represented by (something like) a matrix! Mathematicians call that something an (integral) kernel , hence the $K$. But it really is a very big matrix, barring pathologies; speaking of which, convince yourself that the Dirac delta “function” $\\delta^n(\\xf-\\xi)$ is the kernel of the identity transform and that $K(\\xf,\\xi;\\ti,\\ti) = \\delta^n(\\xf-\\xi)$. Armed with the knowledge that Dirac delta is in fact the identity operator, you now see that the definition of the Green’s function (more properly the fundamental solution ) of a linear differential operator $L$, limited to zero space dimensions and no explicit $t$ dependence for simplicity, $$ LG = \\delta(t), $$ is in fact just the definition of an inverse! Given $G$, it’s also obvious how to solve any other inhomogeneous equation: $$ Lu = f(t)\\quad\\Leftarrow\\quad u(t) = \\int ds\\,G(t-s)f(s). $$ But what does it all have to do with the solution to the boundary value problem that is the propagator $K$? Everything, it turns out, according to Duhamel’s principle . The Green’s function $G$ (for the inhomogeneous problem) and the propagator $K$ (for the initial value problem) are in fact the same! A discussion at Math.SE provides some motivation, and Wikipedia has the details on handling equations that are of more than first order in time (e. g. KG not Schrödinger). In any case, the end result is that $K$ above is the inverse of the Schrödinger operator, $$[\\partial_t + iH(x,-i\\partial_x)]K = \\delta(t)\\delta^n(x).$$ Interlude. You might enjoy reading section 2 of Feynman’s classic paper Theory of positrons PDF , Phys. Rev. 76 , 749 (1949), and the beginning of section 2 of the follow-up Space-time approach to quantum electrodynamics PDF , Phys. Rev. 76 , 769 (1949), which provides the link between the QM and QFT approaches by showing how to write a perturbation expansion in $g$ for a Hamiltonian $H = T + gV$ when you can determine the exact evolution under the “kinetic” part $T$ but not the “interaction” part $gV$, $g \\ll 1$. The first-order contribution, for example, ends up looking like $$ K_1(\\xf,\\tf;\\xi,\\ti) = -ig\\int_{\\ti}^{\\tf} dt\\int d^3x\\, K_0(\\xf,\\tf;x,t)V(x,t)K_0(x,t;\\xi,\\ti), $$ which can reasonably be described as “propagating to an arbitrary point $x$, scattering off the potential and propagating to the final point from there”. The second paper has the extension to multi-particle systems. Feynman used this to motivate, for the very first time, his diagrams. The part pertaining to QED itself should be taken with a grain of salt, however, for the reasons stated in the first paragraph. You’d have a lot of fun, for example, explaining why the restriction $\\ti\\le t\\le \\tf$ is not enforced in QFT—Feynman called this the reason for antiparticles . Quantum field theory. Vernacular (as opposed to axiomatic ) quantum field theory starts with a classical field equation. That is what your KG or Dirac or wave equation is: a classical equation derived from a classical action for the field. You can split the equation and the action into a “free” and an “interaction” part; the free (or “kinetic”) part is usually defined as the part you’re able to solve exactly—the linear part of the equation, the quadratic part of the action. The free propagator is then the inverse of that part. It is usually called $D$ for fermionic and $\\Delta$ for bosonic fields, although conventions (and coefficients!) vary. Promote the fields to operators $\\hat\\phi(x)$, using canonical quantization ; after some pain and suffering you’ll find the totally mysterious fact that, in the free theory, $$ \\langle0|\\mathcal T\\hat\\phi(x)\\hat\\phi(y)|0\\rangle = \\theta(x^0-y^0)\\langle0|\\hat\\phi(x)\\hat\\phi(y)|0\\rangle +\\theta(y^0-x^0)\\langle0|\\hat\\phi(y)\\hat\\phi(x)|0\\rangle =\\frac 1i \\Delta(x-y), $$ where $|0\\rangle$ is the ground state, and the first equality serves to define the $\\mathcal T$ symbol, the time ordering . However, in the land of functional integrals, this whole thing is as easy as solving the quadratic equation $ax^2 + bx + c = 0$ by completing the square ; you can find the details in Zee chapter I.2, starting with equation (19). The result is $$ \\frac 1i \\Delta(x-y) = \\langle0|\\mathcal T\\hat\\phi(x)\\hat\\phi(y)|0\\rangle \\equiv \\int\\mathcal D\\phi(x)\\,\\phi(x)\\phi(y)\\,e^{iS[\\phi]} = \\left.\\frac\\delta{i\\,\\delta J(x)}\\frac\\delta{i\\,\\delta J(y)}\\int\\mathcal D\\phi(x)\\,e^{i\\left(S[\\phi] + \\int d^4x\\,J(x)\\phi(x)\\right)}\\right|_{J = 0}, $$ with the equivalence in the middle being nearly the definition of the integral, and the whole thing should look reasonable and not coincidentially reminiscent of statistical physics. Note how the integration is over the four-dimesional field configurations $\\phi(x)$ instead of particle paths $x(t)$: QM is just QFT in one dimension! You have to derive the path integral to understand where the $\\mathcal T$ comes from—however, it makes sense that if the path integral defines correlators, they should come with an ordering prescription: under the integral sign, there are no operators, only numbers, and no ordering. The derivation will also convince you that (remember how I told you to mind the boundary conditions?) $$ \\int\\mathcal Dx(t) \\equiv \\int d^n\\xf\\,d^n\\xi \\langle0|\\xf\\rangle \\langle\\xi|0\\rangle \\int\\limits_{x\\rlap{(\\ti) =\\xi}}^{x\\rlap{(\\tf) =\\xf}}\\mathcal Dx(t) $$ for arbitrary $\\ti$ and $\\tf$ that encompass all the time values you are interested in, in one dimension for simplicity. I recently had to write down the details so you can consult my notes to self PDF if necessary. The final leap is to introduce interactions; I’ll leave that for the AMS notes or Zee chapter I.7, but the idea is again (functionally) differentiating under the (functional) integral: $$ \\int\\mathcal D\\phi(x) e^{i\\left(S[\\phi] + I[\\phi] + \\int d^4x\\,J(x)\\phi(x)\\right)} = e^{iI\\left[\\frac{\\delta}{i\\,\\delta J}\\right]} \\int\\mathcal D\\phi(x) e^{i\\left(S[\\phi] + \\int d^4x\\,J(x)\\phi(x)\\right)} $$ and the result is vertices in Feynman diagrams.",
      "question_latex": [
        "D=\\int {d^4p\\over(2\\pi\\hbar)^4}{1\\over E^2}e^{i p\\cdot x/\\hbar}.",
        "K",
        "D(x-y)",
        "(\\partial_\\mu\\partial^\\mu + m^2)D\\propto \\delta^4(x)",
        "\\langle x'',t''|x',t'\\rangle",
        "\\int \\mathcal{D}(\\cdots)\\,e^{iS[\\cdots]}",
        "</p>\n\n<p>As far as I can tell from its form, the propagator for the KG equation is <em>not</em> a (Dirac) delta function",
        "or even",
        ". I don’t actually think that point number 1 applies here. </p>\n\n<p>However, I am failing to recognize its relation to “transition amplitude” in this case because I would usually equate usage such as “transition amplitude” with a probability.  As the KG propagator is <em>not</em> a normalized distribution, i. e. does not have the form of a delta function, what precisely is it supposed to quantify here?</p>\n\n<p><strong>Update:</strong> I've since noted that the term ‘propagator’ may be used somewhat differently in different contexts.  Specifially, going back to J. J. Sakurai’s Modern Quantum Mechanics, chapter 2.5, he uses",
        "to represent what is called the propagator of the Schrödinger system. He then discusses the equivalent Feynman path integral approach to determine",
        ".\nThe use of",
        ", also referred to as the propagator, in quantum field theory, by contrast, seems to have a different meaning.  I now realize",
        "is not equivalent to"
      ],
      "answer_latex": [
        "K(\\xf,\\tf;\\xi,\\ti) \\equiv \\langle \\xf,\\tf|\\xi,\\ti\\rangle",
        "K(\\xf,\\tf;\\xi,\\ti) = \\int\\limits_{x\\rlap{(\\ti)=\\xi}}^{x\\rlap{(\\tf)=\\xf}} \\mathcal Dx(t)\\,e^{i\\int dt\\,L(x(t),\\dot x(t),t)}.",
        "\\psi(\\xf,\\tf)\\equiv(U(\\tf,\\ti)\\psi_0)(\\xf) = \\int d^n\\xi\\,K(\\xf,\\xi;\\tf,\\ti)\\psi_0(\\xi).",
        "LG = \\delta(t),",
        "Lu = f(t)\\quad\\Leftarrow\\quad u(t) = \\int ds\\,G(t-s)f(s).",
        "[\\partial_t + iH(x,-i\\partial_x)]K = \\delta(t)\\delta^n(x).",
        "K_1(\\xf,\\tf;\\xi,\\ti) = -ig\\int_{\\ti}^{\\tf} dt\\int d^3x\\, K_0(\\xf,\\tf;x,t)V(x,t)K_0(x,t;\\xi,\\ti),",
        "\\langle0|\\mathcal T\\hat\\phi(x)\\hat\\phi(y)|0\\rangle = \\theta(x^0-y^0)\\langle0|\\hat\\phi(x)\\hat\\phi(y)|0\\rangle +\\theta(y^0-x^0)\\langle0|\\hat\\phi(y)\\hat\\phi(x)|0\\rangle =\\frac 1i \\Delta(x-y),",
        "\\frac 1i \\Delta(x-y) = \\langle0|\\mathcal T\\hat\\phi(x)\\hat\\phi(y)|0\\rangle \\equiv \\int\\mathcal D\\phi(x)\\,\\phi(x)\\phi(y)\\,e^{iS[\\phi]} = \\left.\\frac\\delta{i\\,\\delta J(x)}\\frac\\delta{i\\,\\delta J(y)}\\int\\mathcal D\\phi(x)\\,e^{i\\left(S[\\phi] + \\int d^4x\\,J(x)\\phi(x)\\right)}\\right|_{J = 0},",
        "\\int\\mathcal Dx(t) \\equiv \\int d^n\\xf\\,d^n\\xi \\langle0|\\xf\\rangle \\langle\\xi|0\\rangle \\int\\limits_{x\\rlap{(\\ti) =\\xi}}^{x\\rlap{(\\tf) =\\xf}}\\mathcal Dx(t)",
        "\\int\\mathcal D\\phi(x) e^{i\\left(S[\\phi] + I[\\phi] + \\int d^4x\\,J(x)\\phi(x)\\right)}\n= e^{iI\\left[\\frac{\\delta}{i\\,\\delta J}\\right]} \\int\\mathcal D\\phi(x) e^{i\\left(S[\\phi] + \\int d^4x\\,J(x)\\phi(x)\\right)}",
        "\\def\\xi{x_{\\mathrm i}} \\def\\xf{x_{\\mathrm f}} \\def\\ti{t_{\\mathrm i}} \\def\\tf{t_{\\mathrm f}}",
        "and the wave function",
        "for all",
        "at a certain time",
        ".  Then you also know it at any other time",
        ":\n\\begin{multline}\n\\psi(\\xf,\\tf) \\equiv \\langle \\xf,\\tf|\\psi(\\ti)\\rangle = \\langle \\xf,\\tf|\\left(\\int d^n\\xi\\,|\\xi,\\ti\\rangle\\langle \\xi,\\ti|\\right)|\\psi(\\ti)\\rangle \\\\\\equiv \\int d^n\\xi\\,K(\\xf,\\tf;\\xi,\\ti)\\psi_0(\\xi).\\tag{1}\n\\end{multline}\nThe first few sections of <a href=\"http://rads.stackoverflow.com/amzn/click/0486477223\" rel=\"noreferrer\">Feynman and Hibbs</a> or chapter 6 of <a href=\"http://web.physics.ucsb.edu/~mark/ms-qft-DRAFT.pdf\" rel=\"noreferrer\">Srednicki</a><sup>PDF</sup> should convince you that",
        "Note well the boundary conditions in the path integral: they will prove to be important in the QFT section.</p>\n\n<p>Let us rearrange the arguments,",
        ". Then you will be able to recognize in (1) an integral representation of the evolution operator",
        ",",
        "If you think of",
        "and",
        "as indices with continuous number of values, this formula looks very much like <em>matrix multiplication</em> and",
        "plays the role of the matrix. This makes sense, because the <em>linear</em> operator",
        "should be represented by (something like) a matrix! Mathematicians call that something an <a href=\"https://en.wikipedia.org/wiki/Integral_transform\" rel=\"noreferrer\">(integral) kernel</a>, hence the",
        ". But it really is a very big matrix, barring pathologies; speaking of which, convince yourself that the Dirac delta “function”",
        "is the kernel of the <em>identity</em> transform and that",
        ".</p>\n\n<p>Armed with the knowledge that Dirac delta is in fact the identity operator, you now see that the definition of the Green’s function (more properly the <em>fundamental solution</em>) of a linear differential operator",
        ", limited to zero space dimensions and no explicit",
        "dependence for simplicity,",
        "is in fact just the definition of an inverse! Given",
        ", it’s also obvious how to solve <em>any other</em> inhomogeneous equation:",
        "But what does it all have to do with the solution to the boundary value problem that is the propagator",
        "? Everything, it turns out, according to <a href=\"https://en.wikipedia.org/wiki/Duhamel%27s_principle\" rel=\"noreferrer\">Duhamel’s principle</a>. The Green’s function",
        "(for the inhomogeneous problem) and the propagator",
        "(for the initial value problem) are in fact the same! A <a href=\"https://math.stackexchange.com/a/157698/164875\">discussion at Math.SE</a> provides some motivation, and Wikipedia has the details on handling equations that are of more than first order in time (e. g. KG not Schrödinger). In any case, the end result is that",
        "above is the inverse of the Schrödinger operator,",
        "</p>\n\n<p><strong>Interlude.</strong> You might enjoy reading section 2 of Feynman’s classic paper <a href=\"http://authors.library.caltech.edu/3520/1/FEYpr49b.pdf\" rel=\"noreferrer\"><em>Theory of positrons</em></a><sup>PDF</sup>, Phys. Rev. <strong>76</strong>, 749 (1949), and the beginning of section 2 of the follow-up <a href=\"http://hermes.ffn.ub.es/luisnavarro/nuevo_maletin/Feynman_QED_1949.pdf\" rel=\"noreferrer\"><em>Space-time approach to quantum electrodynamics</em></a><sup>PDF</sup>, Phys. Rev. <strong>76</strong>, 769 (1949), which provides the link between the QM and QFT approaches by showing how to write a <em>perturbation expansion</em> in",
        "for a Hamiltonian",
        "when you can determine the exact evolution under the “kinetic” part",
        "but not the “interaction” part",
        ". The first-order contribution, for example, ends up looking like",
        "which can reasonably be described as “propagating to an arbitrary point",
        ", scattering off the potential and propagating to the final point from  there”. The second paper has the extension to multi-particle systems.</p>\n\n<p>Feynman used this to motivate, for the very first time, his diagrams. The part pertaining to QED itself should be taken with a grain of salt, however, for the reasons stated in the first paragraph. You’d have a lot of fun, for example, explaining why the restriction",
        "is <em>not enforced</em> in QFT—Feynman called this the <a href=\"http://rads.stackoverflow.com/amzn/click/0521658624\" rel=\"noreferrer\">reason for antiparticles</a>.</p>\n\n<p><strong>Quantum field theory.</strong> Vernacular (as opposed to <a href=\"https://ncatlab.org/nlab/show/AQFT\" rel=\"noreferrer\">axiomatic</a>) quantum field theory starts with a classical field equation. That is what your KG or Dirac or wave equation is: a <em>classical</em> equation derived from a classical action for the field. You can split the equation and the action into a “free” and an “interaction” part; the free (or “kinetic”) part is usually defined as the part you’re able to solve exactly—the linear part of the equation, the quadratic part of the action. The free propagator is then the inverse of that part. It is usually called",
        "for fermionic and",
        "for bosonic fields, although conventions (and coefficients!) vary.</p>\n\n<p>Promote the fields to operators",
        ", using <a href=\"https://en.wikipedia.org/wiki/Canonical_quantization\" rel=\"noreferrer\">canonical quantization</a>; after some pain and suffering you’ll find the totally mysterious fact that, in the free theory,",
        "where",
        "is the ground state, and the first equality serves to <em>define</em> the",
        "symbol, the <a href=\"https://en.wikipedia.org/wiki/Path-ordering#Time_ordering\" rel=\"noreferrer\">time ordering</a>. However, in the land of functional integrals, this whole thing is as easy as solving the quadratic equation",
        "by <em>completing the square</em>; you can find the details in Zee chapter I.2, starting with equation (19). The result is",
        "with the equivalence in the middle being nearly the <em>definition</em> of the integral, and the whole thing should look reasonable and <a href=\"https://en.wikipedia.org/wiki/Imaginary_time#In_quantum_mechanics\" rel=\"noreferrer\">not coincidentially</a> reminiscent of statistical physics. Note how the integration is over the four-dimesional field configurations",
        "instead of particle paths",
        ": QM is just QFT in one dimension!</p>\n\n<p>You have to derive the path integral to understand where the",
        "comes from—however, it makes sense that if the path integral defines correlators, they should come with an ordering prescription: under the integral sign, there are no operators, only numbers, and no ordering. The derivation will also convince you that (remember how I told you to mind the boundary conditions?)",
        "for arbitrary",
        "that encompass all the time values you are interested in, in one dimension for simplicity. I recently had to write down the details so you can consult <a href=\"http://www.eleves.ens.fr/~shpilkin/tordering.pdf\" rel=\"noreferrer\">my notes to self</a><sup>PDF</sup> if necessary.</p>\n\n<p>The final leap is to introduce interactions; I’ll leave that for <a href=\"http://www.ams.org/samplings/feature-column/fcarc-feynman6\" rel=\"noreferrer\">the AMS notes</a> or Zee chapter I.7, but the idea is again (functionally) differentiating under the (functional) integral:"
      ],
      "created": "2014-09-03T04:13:52.117",
      "golden_ner_terms": [
        "action",
        "boundary",
        "boundary condition",
        "boundary conditions",
        "canonical",
        "canonical quantization",
        "chapter i",
        "chapter ii",
        "class",
        "completing the square",
        "continuous",
        "delta function",
        "derivation",
        "differential",
        "differential operator",
        "dimension",
        "distribution",
        "equality",
        "equate",
        "equation",
        "equivalence",
        "equivalent",
        "even",
        "expansion",
        "extension",
        "feynman diagrams",
        "feynman path integral",
        "field",
        "field theory",
        "first order",
        "formula",
        "function",
        "functional",
        "ground state",
        "hamiltonian",
        "identity",
        "identity operator",
        "indices",
        "inhomogeneous",
        "initial value problem",
        "integral",
        "integral sign",
        "integration",
        "interactions",
        "inverse",
        "kernel",
        "klein-gordon equation",
        "linear operator",
        "link",
        "map",
        "matrix",
        "matrix multiplication",
        "multiplication",
        "nature",
        "necessary",
        "number",
        "numbers",
        "obvious",
        "operator",
        "operators",
        "order",
        "ordering",
        "path",
        "path integral",
        "perturbation",
        "physics",
        "point",
        "potential",
        "probability",
        "propagator",
        "qed",
        "qft",
        "quadratic equation",
        "quantization",
        "quantum electrodynamics",
        "quantum field theory",
        "quantum mechanics",
        "relation",
        "representation",
        "restriction",
        "scattering",
        "section",
        "simple",
        "solution",
        "space",
        "square",
        "state",
        "term",
        "theory",
        "time",
        "transform",
        "vertices",
        "volume",
        "wave equation",
        "wave function",
        "wikipedia",
        "zero"
      ],
      "golden_ner_count": 97,
      "golden_patterns": [
        {
          "pattern": "check-the-extreme-cases",
          "score": 4.0,
          "hotwords": [
            "boundary",
            "zero"
          ]
        },
        {
          "pattern": "work-examples-first",
          "score": 2.0,
          "hotwords": [
            "example"
          ]
        },
        {
          "pattern": "construct-an-explicit-witness",
          "score": 2.0,
          "hotwords": [
            "explicit"
          ]
        },
        {
          "pattern": "encode-as-algebra",
          "score": 2.0,
          "hotwords": [
            "ring"
          ]
        },
        {
          "pattern": "unfold-the-definition",
          "score": 2.0,
          "hotwords": [
            "definition of"
          ]
        },
        {
          "pattern": "construct-auxiliary-object",
          "score": 2.0,
          "hotwords": [
            "introduce"
          ]
        },
        {
          "pattern": "estimate-by-bounding",
          "score": 2.0,
          "hotwords": [
            "bound"
          ]
        },
        {
          "pattern": "monotone-approximation",
          "score": 2.0,
          "hotwords": [
            "limit"
          ]
        }
      ],
      "golden_pattern_names": [
        "check-the-extreme-cases",
        "work-examples-first",
        "construct-an-explicit-witness",
        "encode-as-algebra",
        "unfold-the-definition",
        "construct-auxiliary-object",
        "estimate-by-bounding",
        "monotone-approximation"
      ],
      "golden_scopes": [
        {
          "type": "for-any",
          "match": "for all $x$"
        },
        {
          "type": "where-binding",
          "match": "where $|0\\rangle$ is"
        }
      ],
      "golden_scope_count": 2
    },
    {
      "id": "se-physics-516511",
      "stratum": "easy",
      "title": "Was X17 predicted before it was observed?",
      "tags": [
        "forces",
        "interactions",
        "bosons",
        "carrier-particles",
        "x17"
      ],
      "score": 41,
      "answer_score": 69,
      "question_body": "Articles, with very little detail, have made their rounds about an X17 boson (16.7 MeV) being observed in tests of decaying beryllium-8 and perhaps once in a test with helium. Most of the undiscovered particles that are searched for in CERN or other colliders or dark underground quiet dark pools filled with water were predicted prior to being searched for. (WIMPs, the Higgs boson, neutrinos, axions - all predicted first), after which, there was a long period of trying to find them, in some cases, still ongoing. And, likewise, sometimes objects are observed before they're predicted ( dark matter and dark energy comes to mind, probably more). I know that a 5th force has been theorized, so in a sense, a 5th boson has long been proposed, but the articles I've read imply only that beryllium decay products were studied and X17 has been observed as a bump in the chart of output data, but no indication of what they were looking for when studying beryllium decay. I realize the discovery isn't official yet, but some articles suggest pretty good certainty that the observation is legitimate. No mention of what they were looking for suggests to me that this was a surprise discovery, not a study where they expected to find something new, but I wanted to confirm, as the articles I've seen are all very short. If there was something specific they were looking for when studying beryllium decay, then identifying that would be appreciated. A paragraph or two in layman's terms is also appreciated.",
      "answer_body": "Is the X17 predicted, or discovered? It depends upon what the meaning of the word \"is\", is. Prediction vs. Discovery In particle physics, phrases like \"the Higgs boson\" or \"dark matter\" actually stand in for mechanisms or effects, which could be realized in many different concrete models. And even when you specify a model, it's still ambiguous because you can vary how \"fundamental\" your model is. The Higgs boson itself is a good example of this. Before the LHC, we knew for sure that particles in the Standard Model had mass, that this was difficult to accommodate the standard way (by adding so-called \"mass terms\" to the Lagrangian), and that a very simple and elegant way to do it was the Higgs mechanism . By calling it a mechanism, we mean that it required only general ingredients, such as spontaneous symmetry breaking and new bosons, but that it wasn't specified how the symmetries were broken or what new bosons were added. What we call \"the\" Higgs boson is just the simplest set of ingredients you could supply to get the Higgs mechanism to function. There were many alternatives, such as the two-Higgs-doublet model , which would give you five Higgs bosons instead. And even once you fix \"the\" Higgs boson, you could go deeper. In the resulting model, the mass of the Higgs boson is put in by hand, and basic consistency requirements only fix it within a rather wide range. You could consider more complex models that predict more specific values of the mass, or even explain where the Higgs boson itself comes from, e.g. if it falls out of an elegant grand unified model or is itself made of other particles, like the proton is made of quarks. There were so many of these theories that just about any Higgs mass in the reasonable range could be accommodated by one or ten. So I would say it's fairly clear that a Higgs boson was predicted before discovery... but it's completely up for debate whether a Higgs boson of mass $125$ GeV was predicted or discovered! Fifth forces Like \"dark matter\" or \"the Higgs boson\", \"the fifth force\" is another one of those vague phrases that really stands for thousands of distinct models -- basically any model that consists of the Standard Model and any new gauge bosons. Fifth forces are interesting because they are extremely simple extensions of the Standard Model (e.g. you could just take one of the existing forces and exactly copy it), and many more complex/fundamental models give you them automatically. Another appealing feature is that many types of fifth forces give rise to striking experimental signatures, which we can look for with great sensitivity. So in that sense, a fifth force has been predicted for a long time. However, because of the avalanche of possible distinct fifth force models, experimentalists usually don't try to test specific models. Instead, they parametrize the effects of the fifth force in terms of a few quantities (the mass of the gauge boson, the gauge coupling, the coupling to electrons, the mixing with the photon, etc.). Then they run tests that try to capture as much of this parameter space as possible. Many such tests have been performed in the past and are ongoing now. The experimentalists behind the X17 boson were looking for a specific type of fifth force, called a \" dark photon \". These are part of a relatively new mechanism to produce dark matter (DM), using an entire \"dark sector\" where the DM only affects normal matter through the dark photon. This mechanism has been gaining in popularity because it results in lighter DM, and the traditional heavy DM has been pretty conclusively tested by WIMP searches. However, \"dark photons\" are still not a specific model, but rather stand for a general idea that has a range of corresponding concrete models, and within each one the mass of the dark photon can lie in a wide range. So they were inspired by these theoretical ideas to look for MeV-scale bosons, but not directed to any specific mass; they just were trying to capture as big a slice of parameter space as possible. They indeed found evidence for a new MeV-scale boson, but it behaved in a rather strange way. In a later theory paper , it was explained that the boson could not have been a dark photon, as a dark photon with that mass and coupling would have already been discovered in earlier experiments. Dumping the dark photon interpretation, the theorists showed that the observations could be accommodated, without conflict with earlier experiments, if the new boson had the unusual property of being \"protophobic\", i.e. not coupling to protons. The experimentalists then did a follow-up work confirming their results, leading to the recent media frenzy, and that's where we are now. I hope it's apparent that your question does not have a yes or no answer! Fifth forces have been thought about for a long time, but they're a very general thing. The experimentalists were motivated to look by a recent, particular kind of fifth force (dark photons) -- but what they saw wasn't compatible with it. Science often progresses this way. Addendum I realize the discovery isn't official yet but some articles suggest pretty good certainty that the observation is legit. This needs to be said: I don't think any practicing particle physicist would give this observation more than a 5% chance of holding up. At any moment, there are about 50 distinct experimental anomalies, each of which could revolutionize physics if true. Historically, the vast majority don't pan out. That's because physics is subtle, experimental physics is even more subtle, and these measurements are often pushing the limits of what our equipment can do. (There is the further problem that the lab claiming the X17 has a history of reporting similar anomalies -- if I recall correctly, they already pulled this exact same thing twice in the past, with different boson masses, and never explained why those observations went away.) It just so happens that you've heard more about this anomaly because it blew up in the media. This happens naturally: the more prominent a story gets, the more each journalist wants to write their own take on it, and so you automatically get a winner-take-all situation where X17 gets more attention than the next ten most recent anomalies combined. I wouldn't recommend worrying about the X17. Science will go on, the experiments will be thoroughly scrutinized and replicated, and in ten years' time we'll know for sure if it's real or not. If it is, you'll hear about it everywhere. If you hear nothing, assume it disappeared.",
      "question_latex": [],
      "answer_latex": [
        "125"
      ],
      "created": "2019-11-28T04:26:58.597",
      "golden_ner_terms": [
        "ambiguous",
        "basic",
        "bosons",
        "chart",
        "clear",
        "compatible",
        "complex",
        "dark energy",
        "dark matter",
        "data",
        "electrons",
        "energy",
        "entire",
        "even",
        "experimental physics",
        "fix",
        "forces",
        "function",
        "gauge",
        "higgs",
        "interpretation",
        "lagrangian",
        "mass",
        "matter",
        "mean",
        "measurements",
        "mixing",
        "model",
        "models",
        "moment",
        "neutrinos",
        "normal",
        "observation",
        "parameter",
        "parameter space",
        "particle physics",
        "period",
        "photons",
        "physics",
        "property",
        "protons",
        "quarks",
        "range",
        "real",
        "sector",
        "similar",
        "simple",
        "simple extension",
        "space",
        "standard model",
        "symmetry",
        "symmetry breaking",
        "theory",
        "thousands",
        "time",
        "type",
        "water",
        "way",
        "wimps",
        "word",
        "work"
      ],
      "golden_ner_count": 61,
      "golden_patterns": [
        {
          "pattern": "work-examples-first",
          "score": 4.0,
          "hotwords": [
            "example",
            "e.g."
          ]
        },
        {
          "pattern": "try-a-simpler-case",
          "score": 2.0,
          "hotwords": [
            "simplest"
          ]
        },
        {
          "pattern": "check-the-extreme-cases",
          "score": 2.0,
          "hotwords": [
            "extreme"
          ]
        },
        {
          "pattern": "exploit-symmetry",
          "score": 2.0,
          "hotwords": [
            "symmetry"
          ]
        },
        {
          "pattern": "local-to-global",
          "score": 2.0,
          "hotwords": [
            "cover"
          ]
        },
        {
          "pattern": "monotone-approximation",
          "score": 2.0,
          "hotwords": [
            "limit"
          ]
        }
      ],
      "golden_pattern_names": [
        "work-examples-first",
        "try-a-simpler-case",
        "check-the-extreme-cases",
        "exploit-symmetry",
        "local-to-global",
        "monotone-approximation"
      ],
      "golden_scopes": [],
      "golden_scope_count": 0
    },
    {
      "id": "se-physics-178417",
      "stratum": "easy",
      "title": "Why can't I do this to get infinite energy?",
      "tags": [
        "general-relativity",
        "gravity",
        "energy-conservation",
        "mass-energy",
        "perpetual-motion"
      ],
      "score": 54,
      "answer_score": 90,
      "question_body": "I know that I cannot do this because of conservation of energy, so I am looking for an answer as to why this will not work. So by my understanding of Einstein's whole famous $E=mc^2$ thing it is possible to turn matter into energy, and energy into matter. So what if you were to create a space station way up high and run a wire down to Earth, then pump some energy up it. The space station would then turn that energy into matter and let it drop down to Earth. People down at Earth would harvest the kinetic energy somehow, then turn the matter back into energy, pump it back up, then repeat the process. The result would be the energy they started with plus the kinetic energy. Where is the flaw in this reasoning? (I'm guessing it might take energy for the electrons to fight gravity up to the space station).",
      "answer_body": "[5/3 - Extended the answer, made some corrections, and responded to John Duffield's comment] This is actually the paradox that led Einstein to General Relativity. Consider a special case: An electron and positron are at the Earth's surface. Bring them together and they annihilate, creating gamma rays (which is very energetic light). The gamma rays travel up to the Space Station, where they are converted back to a positron and electron. These are dropped back to Earth where the kinetic energy is harvested. Einstein found a way out of the paradox: Gravity must affect energy just like matter. Light must lose energy if it climbs against gravity. Likewise, it must gain energy if it drops from the Space Station to Earth. Minor point - the space station has a large horizontal velocity. Let us pretend that it is stationary at the top of a very tall tower. Let us also ignore the rotation of the Earth. This leads to another of Einstein's thought experiments. Unlike other forces, gravity attracts all particles and therefore must affect all energy. Einstein could not think of an experiment that could distinguish between a freely falling elevator in a uniform gravitational field and an elevator floating in space where there is no gravity. (There are some caveats. The elevator must be small or the effects of a non-uniform field will be noticeable. Looking outside the elevator is cheating. See this for more details.) He concluded the laws of physics in a freely falling elevator are the same as those in an inertial frame of reference without gravity. Furthermore, the laws in a rocket in empty space accelerating at 1 g are the same as the laws in a uniform 1 g gravitational field. On Earth the ground pushes you upward with enough force to accelerate you at 1 g. This is completely equivalent to a rocket. This is called the equivalence principle. In a rocket, you can use an inertial frame of reference. You take into account for your acceleration, and then you see that particles with no forces move in straight lines, particles at rest remain at rest. You can also work in a frame of reference where the rocket is at rest. To do this, you say that a force toward the tail of the rocket acts on everything in the universe. This kind of force is an accounting trick you must use if you want to work in a non-inertial frame of reference. It is called a pseudo force. The equivalence principal is a statement that gravity is a pseudo force, and that a freely falling frame of reference is inertial. Unlike a rocket, the pseudo force of gravity is not always uniform. We can use the Equivalence Principal to calculate the effect of gravity on light. Suppose you are in a rocket at rest a distance L from a space station far from Earth. You start accelerating toward it at 1 g just as a beam of light leaves the station toward you. You would see a blue Doppler shift, a higher frequency. At low speeds, the Doppler shift is related to velocity as below. $\\nu = \\nu_0 \\left[1 + \\dfrac{v}{c}\\right]$ Neglecting the distance you travel, light takes time $L/c$ to reach you. During this time you would accelerate to velocity $v= gL/c$ . So $\\nu = \\nu_0 \\left[1 + \\dfrac{gL}{c^2}\\right]$ Since gravity and the acceleration of a rocket are completely equivalent, you see the same Doppler shift on Earth. (See this for more info.) This seems very odd when you think about it. The laws of physics are the same when you don't look out of the elevator or rocket. When you do look out, you see an obvious difference. The acceleration of the rocket gives the universe a velocity, the velocity causes time dilation and length contraction, and these generate the Doppler sift. On Earth, the upward acceleration keeps you from falling. The universe stays still. If the frequency of light increases, then its period decreases. If you measure a shorter period on Earth than the space station finds, it is because your clock is slower. The acceleration of gravity causes the same time dilation as the acceleration of the rocket. $t = \\dfrac{t_0} {\\left[1 + \\dfrac{gL}{c^2}\\right]}$ This is valid for points separated by a small L. To compare time intervals on Earth to time intervals not influenced by gravity far from Earth, we would have to use $g = GM/r^2$ , take into account length changes, and integrate. The result of a more exact calculation is $t = \\dfrac{t_0}{\\sqrt{1 - \\dfrac{2GM}{rc^2}}}$ From quantum mechanics, the energy of a photon is related to its wavelength. $E = h \\nu = \\dfrac{h c}{\\lambda}$ . So light that climbs against gravity is red shifted and has a drop in energy. I need to be careful about statements like \"Light loses energy as it climbs against gravity.\" Light doesn't slow down as it climbs. Light travels at the speed of light. You also cannot follow a photon and watch its energy change. There is no frame of reference that follows a photon. You get singularities if you try to look at the universe from the point of view of a photon. All you can do is measure the frequency or energy of a photon as it leaves and as it arrives. You can measure a frequency shift or energy difference between two points. If you aim a laser upwards, many photons in identical states climb against gravity. You can measure the frequency at various altitudes with a diffraction grating or measure the recoil when a particle is struck. If the diffraction grating or particle is at rest with respect to earth, you will find that there is a frequency and energy drop as the beam climbs. If the diffraction grating or particle is fired from a cannon on earth with enough speed to coast up to the space station and stop there, you would find no frequency or energy drop. The values of the entire beam would be the as same measured at the space station. In both cases, the energy is not some \"stuff\" inside the photon that is lost as the photon rises. If is a result of an interaction between the photon and the diffraction grating or particle. In the stationary with respect to Earth case, the rate of the clock of the diffraction grating or particle changes with altitude, and so do the measured values. In the coasting case, the diffraction grating or particle are at rest in an inertial frame of reference. Their clocks run at the same rate as the space station's.",
      "question_latex": [
        "E=mc^2"
      ],
      "answer_latex": [
        "\\nu = \\nu_0 \\left[1 + \\dfrac{v}{c}\\right]",
        "L/c",
        "v= gL/c",
        "\\nu = \\nu_0 \\left[1 + \\dfrac{gL}{c^2}\\right]",
        "t = \\dfrac{t_0} {\\left[1 + \\dfrac{gL}{c^2}\\right]}",
        "g = GM/r^2",
        "t = \\dfrac{t_0}{\\sqrt{1 - \\dfrac{2GM}{rc^2}}}",
        "E = h \\nu = \\dfrac{h c}{\\lambda}"
      ],
      "created": "2015-04-25T23:37:25.427",
      "golden_ner_terms": [
        "acceleration",
        "acts on",
        "altitude",
        "blue",
        "calculate",
        "contraction",
        "difference",
        "diffraction",
        "dilation",
        "distance",
        "earth",
        "electrons",
        "energy",
        "entire",
        "equivalence",
        "equivalence principle",
        "equivalent",
        "field",
        "forces",
        "frame",
        "frequency",
        "gamma rays",
        "general relativity",
        "generate",
        "gravity",
        "integrate",
        "laser",
        "laws of physics",
        "length",
        "length contraction",
        "matter",
        "measure",
        "minor",
        "obvious",
        "odd",
        "paradox",
        "period",
        "photons",
        "physics",
        "plus",
        "point",
        "quantum mechanics",
        "relativity",
        "rotation",
        "separated",
        "sift",
        "singularities",
        "small o",
        "space",
        "speed",
        "speed of light",
        "stationary",
        "straight",
        "surface",
        "thought experiment",
        "time",
        "time dilation",
        "top",
        "universe",
        "valid",
        "velocity",
        "wavelength",
        "way",
        "work"
      ],
      "golden_ner_count": 64,
      "golden_patterns": [
        {
          "pattern": "quotient-by-irrelevance",
          "score": 2.0,
          "hotwords": [
            "up to"
          ]
        },
        {
          "pattern": "encode-as-algebra",
          "score": 2.0,
          "hotwords": [
            "ring"
          ]
        },
        {
          "pattern": "transport-across-isomorphism",
          "score": 2.0,
          "hotwords": [
            "equivalent"
          ]
        }
      ],
      "golden_pattern_names": [
        "quotient-by-irrelevance",
        "encode-as-algebra",
        "transport-across-isomorphism"
      ],
      "golden_scopes": [
        {
          "type": "consider",
          "match": "Consider a special case: An electron and positron are at the Earth's su"
        }
      ],
      "golden_scope_count": 1
    },
    {
      "id": "se-physics-707721",
      "stratum": "easy",
      "title": "Am I in a superposition?",
      "tags": [
        "quantum-mechanics",
        "quantum-interpretations",
        "superposition",
        "quantum-measurements"
      ],
      "score": 32,
      "answer_score": 36,
      "question_body": "Someone looks at me. Now, they know my position and my momentum, with some uncertainty. Therefore, they haven't measured either my position nor my momentum, since neither is known perfectly. They measured some other observable $O$ , and found me in some eigenstate of $O$ : $|\\psi\\rangle$ . $|\\psi\\rangle$ is such that, if I express it in position basis, I get a (very narrow) superposition of eigenstates, centered at some location. The same with momentum basis. This means that right now, I'm in a superposition of position eigenstates. This means that right now, I don't have a well-defined position. Is this correct? (If it is, it means I have no such thing as a defined position. I'm nowhere?! I also don't have a momentum. In fact, unless my momentum becomes completely undefined, I can't ever be anywhere...)",
      "answer_body": "This is a deceptively simple question, and I would like to answer it in the context of Condensed Matter Theory. We will start with the question: why do objects in our everyday life appear to be localized? The Model: Let's consider a simple model for a quantum crystal. The Hamiltonian for a collection of atoms held together by harmonic forces between neighbors is given by: $$\\hat{H}=\\sum_{\\vec{x},\\vec{\\delta}} \\frac{\\hat{P}^2(\\vec{x})}{2m} + \\frac{1}{2}m\\omega_0 (\\hat{X}(\\vec{x})-\\hat{X}(\\vec{x}+\\hat{\\delta}))^2.$$ The position of the atoms $\\vec{x}$ may be one, two, or three-dimensional, and the connection between interacting atoms $\\vec{\\delta}$ may cover nearest-neighbours, next-nearest-neighbours, or any other interatomic distance. We are thus not just considering an oversimplified model for a hypothetical piece of material, but also the family of Hamiltonians which in principle describe all solids. The crucial point to notice, is that this Hamiltonian commutes with the operator of total (or centre-of-mass) momentum: $$\\hat{P}_{tot}=\\sum_{\\vec{x}}\\hat{P}(\\vec{x})$$ $$\\implies [\\hat{H},\\hat{P}_{tot}]=0.$$ The vanishing of this commutator is a consequence of the homogeneity of space: Any translationally invariant Hamiltonian commutes with the total momentum operator! That aside, if two quantum mechanical operators commute, this implies that it is possible to find a set of states which are simultaneous eigenstates of both operators. The fact that the Hamiltonian for a crystal commutes with the total momentum operator thus implies that all eigenstates of the crystal are total momentum eigenstates. Because of Heisenberg’s uncertainty principle, states in which the value of the total momentum can be known with certainty correspond to states in which the centre-of-mass position is entirely unpredictable. In other words, the total momentum eigenstates, and hence the eigenstates of the crystal Hamiltonian, are all completely spread out over all of space! This is not the end of the story however. We can write the Hamiltonian in reciprocal space, and separate it into two independent parts: $$\\hat{H}=\\hat{H}_{coll} + \\sum_{\\vec{k} \\ne 0} \\hat{H}_{int}(\\vec{k}).$$ The part of the Hamiltonian for non-zero values of $\\vec{k}$ describes the internal dynamics of the crystal, in terms of all of its phonon excitations and their interactions. The part at $\\vec{k} = 0$ on the other hand, describes the collective dynamics of the crystal as a whole. It can be straightforwardly shown that in the case of the crystal, the collective part of the Hamiltonian is given by: $$\\hat{H}_{coll}=\\frac{\\hat{P}^2_{tot}}{2mN}.$$ From now on, we will only consider the properties of the collective part of the Hamiltonian, and completely ignore the internal, phonon-related part (this is possible because the two parts of the Hamiltonian commute; and at extremely low temperatures, the collective part of the Hamiltonian is the only part that matters). As expected, the eigenstates of the collective Hamiltonian are the eigenstates $|P \\rangle$ of the total momentum operator, and the corresponding energies are given by the kinetic energy $E_P=\\frac{P^2}{2mN}$ of the crystal as a whole. The ground state of the crystal is the state with total momentum $P = 0$ . Its wavefunction is thus completely and evenly spread out over all of space, with equal amplitude and even equal phase at every possible positions. Additionally, the energy separating the ground state from the collective excitations with non-zero total momentum, is inversely proportional to the total mass $mN$ of the whole crystal. This means that as we consider a larger and larger crystal, it becomes easier and easier to make excitations of the crystal. In fact, in the limit $N\\rightarrow \\infty$ of an infinitely large crystal, it would cost no energy at all to make collective excitations, and all states in the collective part of the spectrum become degenerate with the ground state. In that limit, a wave packet of total momentum states with a well-defined centre-of-mass position would have the same energy expectation value as the zero-momentum state. Of course, real crystals are not infinitely large, and superpositions of momentum states do cost energy to create. But the pieces of matter we are interested in (like you) contain a very large, albeit finite, number of atoms. How difficult would it then be to make a superposition of crystal eigenstates which has a well-defined centre-of-mass position? In terms of the collective Hamiltonian, this amounts to adding a perturbation which tends to localize the crystal: $$\\hat{H}'_{coll}=\\frac{\\hat{P}^2}{2mN}+V\\hat{X}^2_{com}.$$ Here V is the strength of a potential which tends to localize the centre of mass position $\\hat{X}_{com}$ at the origin of our coordinate system. Since $\\hat{P}_{tot}$ and $\\hat{X}_{com}$ obey canonical commutation relations, the perturbed Hamiltonian $\\hat{H}'_{coll}$ is that of a harmonic oscillator, with the well known energies and eigenstates: $$\\hat{H}'_{coll}=\\hbar \\omega (\\hat{n}+1/2),$$ $$\\langle x|n=0 \\rangle = (\\frac{2mNV}{\\pi^2 \\hbar^2})^{\\frac{1}{8}} exp(-\\sqrt{\\frac{mNV}{2\\hbar^2}}x^2).$$ The ground state in the presence of a perturbation corresponds to a Gaussian wave packet of width $\\sigma = \\frac{\\hbar}{\\sqrt{2mNV}}$ . The effect of spontaneous symmetry breaking emerges when we consider two non-commuting limits: $$\\lim_{N\\rightarrow \\infty} \\lim_{V \\rightarrow 0} \\langle x|n=0 \\rangle = constant,$$ $$\\lim_{V \\rightarrow 0} \\lim_{N\\rightarrow \\infty} \\langle x|n=0 \\rangle = \\delta(x).$$ Conclusion: For a system of any finite size, no matter how large, the perturbation can be made small enough for the ground state wavefunction to be essentially spread out over the entire universe. But for an infinitely large system, any perturbation, no matter how weak , is enough to completely localize the wavefunction into a single position. In the thermodynamic limit , the localization happens even in the presence of only an infinitesimal potential, which in effect means that the wavefunction can spontaneously localize and break the translational symmetry of the unperturbed Hamiltonian. But even if the material is not infinitely large, the important message of the last equation is that even if N is not yet truly infinite, the approach towards the thermodynamic limit is singular . This implies in particular that as you consider larger and larger pieces of matter, a weaker and weaker perturbation suffices to make its ground state a localized wave packet. Therefore, in a realistic piece of material, even entirely immeasurable perturbations are easily strong enough to completely localize the matter into a single position. Finally, it's important to note that if we find a crystal (or a person like you) in a localized state, that thus implies not only that the crystal picks out a preferred location in space, but also that it exists in a state that is not the ground state of its Hamiltonian. In fact, it is not even an eigenstate of the Hamiltonian. Spontaneous symmetry breaking explains both how an infinitesimal perturbation can suffice to pick a location and break the symmetry, and how that same infinitesimal perturbation suffices to make the localized state the ground state of the perturbed system. Therefore, to finally answer your question: You are indeed localized into a single position . Source: Lectures notes from a course on Condensed Matter Theory by Jasper van Wezel from University of Amsterdam. Update: The comment by OP caught me off guard: When some perturbation causes \"me\" to become localized in a single position, does this mean \"I\" am completely spread out in total momentum space? Or is my total momentum also localized to a single value? I didn't realize at the time, but my answer only explained why objects appear to have a well-defined position - it did not provide an answer as to why they also appear to have a well-defined momentum. Therefore, I Fourier transformed the ground state wave function $\\psi_0(x)=\\langle x|n=0 \\rangle$ , to momentum space $\\tilde{\\psi_0}(p)$ and applied the two limits. The results were: $$\\lim_{N\\rightarrow \\infty} \\lim_{V \\rightarrow 0} \\tilde{\\psi_0}(p)=\\delta(p)$$ $$\\lim_{V \\rightarrow 0} \\lim_{N\\rightarrow \\infty} \\tilde{\\psi_0}=constant.$$ First thing to note is that this is the exact opposite of the results we got for the wavefunction in position space. Secondly, as before, we are interested in the equation where we first take the $N\\rightarrow \\infty$ limit, and then the $V \\rightarrow 0$ limit (this corresponds to having a very large object and then applying an infinitesimal perturbation). Thus, the answer is that the wavefunction is momentum space is completely spread out , i.e. it's a constant. This is not what I expected, even though it makes total sense due to Heisenberg's uncertainty principle. Triggered by this result, I consulted the professor himself. This is basically what he said (paraphrased to make more sense in our context): The result at which we arrived is indeed, a very good point. The example of the quantum crystal is unfortunately a not well-behaved mathematical object, unlike antiferromagnets and Bose-Einstein condensates which are much more mathematically clean, and subtleties do not come up. The crystal was given merely as an example to illustrate that everyday objects can display strange collective quantum behavior. Some problems with the quantum crystal are that its order parameter is hard to define, the required symmetry-breaking field $V$ for the crystal is not obvious. In reality, the symmetry-broken ground state is a Gaussian wavefunction whose width does not depend on $N$ , does not get completely localized in the thermodynamic limit, and does not have undefined momentum. This is because in order to have an extensive energy for the symmetry-broken Hamiltonian, the symmetry-breaking field V needs to scale with the inverse system size, i.e. $V$ should be replaced by $V/N$ in all equations . This raises the question: Does that mean that translational symmetry was not broken after all? The answer is simply no, and this is because we've been looking at it the wrong way all along. Rather than considering the wavefunction itself, and saying whether or not it is absolutely localized, we should consider how localized it is, compared to some natural measure for localization. Since we're talking about a single crystal without any other objects around, the only sensible measure is to ask whether or not the crystal is localized to within its own size . In other words, whether fluctuations in position, $\\sqrt{<x^2>}$ , are small compared to the linear size of the object, $L$ . You can then see that, by taking the right order of limits, the fluctuations are always large in absolute size (for small $V$ ) but that $\\sqrt{<x^2>} / L$ disappears in the thermodynamic limit . So an object (or a person) is localized in real space in units of its own size (which are the only relevant units!). Similarly, for the fluctuations in momentum, we can show that although they almost disappear in absolute size ( $\\sim V$ ), they are still precisely large enough to allow the object not to fluctuate compared to its own size. (Hopefully) Final Conclusion: An everyday object is still a wave packet in the thermodynamic limit, with uncertainty in both position and momentum. But the fluctuations in position and momentum relative to any natural scale for measuring them, do disappear, and the object thus appears localized in both position and momentum on human scales . My apologies if my original answer was somewhat misleading! All credit goes to Jasper van Wezel .",
      "question_latex": [
        "O",
        "|\\psi\\rangle"
      ],
      "answer_latex": [
        "\\hat{H}=\\sum_{\\vec{x},\\vec{\\delta}} \\frac{\\hat{P}^2(\\vec{x})}{2m} + \\frac{1}{2}m\\omega_0 (\\hat{X}(\\vec{x})-\\hat{X}(\\vec{x}+\\hat{\\delta}))^2.",
        "\\hat{P}_{tot}=\\sum_{\\vec{x}}\\hat{P}(\\vec{x})",
        "\\implies [\\hat{H},\\hat{P}_{tot}]=0.",
        "\\hat{H}=\\hat{H}_{coll} + \\sum_{\\vec{k} \\ne 0} \\hat{H}_{int}(\\vec{k}).",
        "\\hat{H}_{coll}=\\frac{\\hat{P}^2_{tot}}{2mN}.",
        "\\hat{H}'_{coll}=\\frac{\\hat{P}^2}{2mN}+V\\hat{X}^2_{com}.",
        "\\hat{H}'_{coll}=\\hbar \\omega (\\hat{n}+1/2),",
        "\\langle x|n=0 \\rangle = (\\frac{2mNV}{\\pi^2 \\hbar^2})^{\\frac{1}{8}} exp(-\\sqrt{\\frac{mNV}{2\\hbar^2}}x^2).",
        "\\lim_{N\\rightarrow \\infty} \\lim_{V \\rightarrow 0} \\langle x|n=0 \\rangle = constant,",
        "\\lim_{V \\rightarrow 0} \\lim_{N\\rightarrow \\infty} \\langle x|n=0 \\rangle = \\delta(x).",
        "\\lim_{N\\rightarrow \\infty} \\lim_{V \\rightarrow 0} \\tilde{\\psi_0}(p)=\\delta(p)",
        "\\lim_{V \\rightarrow 0} \\lim_{N\\rightarrow \\infty} \\tilde{\\psi_0}=constant.",
        "</span></p>\n<p>The position of the atoms <span class=\"math-container\">",
        "</span> may be one, two, or three-dimensional, and the connection between interacting atoms <span class=\"math-container\">",
        "</span> may cover nearest-neighbours, next-nearest-neighbours, or any other interatomic distance. We are thus not just considering an oversimplified model for a hypothetical piece of material, but also the family of Hamiltonians which in principle describe all solids.</p>\n<p>The crucial point to notice, is that this Hamiltonian commutes with the operator of total (or centre-of-mass) momentum:</p>\n<p><span class=\"math-container\">",
        "</span></p>\n<p><span class=\"math-container\">",
        "</span></p>\n<p>The vanishing of this commutator is a consequence of the homogeneity of space: <em>Any</em> <a href=\"https://en.wikipedia.org/wiki/Translational_symmetry\" rel=\"noreferrer\">translationally invariant</a> Hamiltonian commutes with the total momentum operator!</p>\n<p>That aside, if two quantum mechanical operators commute, this implies that it is possible to find a set of states which are simultaneous eigenstates of both operators. The fact that the Hamiltonian for a crystal commutes with the total momentum operator thus implies that all eigenstates of the crystal are total momentum eigenstates. Because of Heisenberg’s uncertainty principle, states in which the value of the total momentum can be known with certainty correspond to states in which the centre-of-mass position is entirely unpredictable. In other words, the total momentum eigenstates, and hence the eigenstates of the crystal Hamiltonian, are all completely spread out over all of space!</p>\n<p>This is not the end of the story however.  We can write the Hamiltonian in reciprocal space, and separate it into two independent parts:</p>\n<p><span class=\"math-container\">",
        "</span></p>\n<p>The part of the Hamiltonian for non-zero values of <span class=\"math-container\">",
        "</span> describes the internal dynamics of the crystal, in terms of all of its phonon excitations and their interactions. The part at <span class=\"math-container\">",
        "</span> on the other hand, describes the collective dynamics of the crystal as a whole. It can be straightforwardly shown that in the case of the crystal, the collective part of the Hamiltonian is given by:</p>\n<p><span class=\"math-container\">",
        "</span></p>\n<p>From now on, we will only consider the properties of the collective part of the Hamiltonian, and completely ignore the internal, phonon-related part (this is possible because the two parts of the Hamiltonian commute; and at extremely low temperatures, the collective part of the Hamiltonian is the only part that matters).</p>\n<p>As expected, the eigenstates of the collective Hamiltonian are the eigenstates <span class=\"math-container\">",
        "</span> of the total momentum operator, and the corresponding energies are given by the kinetic energy <span class=\"math-container\">",
        "</span> of the crystal as a whole. The ground state of the crystal is the state with total momentum <span class=\"math-container\">",
        "</span>. Its wavefunction is thus completely and evenly spread out over all of space, with equal amplitude and even equal phase at every possible positions. Additionally, the energy separating the ground state from the collective excitations with non-zero total momentum, is inversely proportional to the total mass <span class=\"math-container\">",
        "</span> of the whole crystal. This means that as we consider a larger and larger crystal, it becomes easier and easier to make excitations of the crystal. In fact, in the limit <span class=\"math-container\">",
        "</span> of an infinitely large crystal, it would cost no energy at all to make collective excitations, and all states in the collective part of the spectrum become degenerate with the ground state. In that limit, a wave packet of total momentum states with a well-defined centre-of-mass position would have the same energy expectation value as the zero-momentum state.</p>\n<p>Of course, real crystals are not infinitely large, and superpositions of momentum states do cost energy to create. But the pieces of matter we are interested in (like you) contain a very large, albeit finite, number of atoms. How difficult would it then be to make a superposition of crystal eigenstates which has a well-defined centre-of-mass position? In terms of the collective Hamiltonian, this amounts to adding a perturbation which tends to localize the crystal:</p>\n<p><span class=\"math-container\">",
        "</span></p>\n<p>Here V is the strength of a potential which tends to localize the centre of mass position <span class=\"math-container\">",
        "</span> at the origin of our coordinate system.  Since <span class=\"math-container\">",
        "</span> and <span class=\"math-container\">",
        "</span> obey canonical commutation relations, the perturbed Hamiltonian <span class=\"math-container\">",
        "</span> is that of a harmonic oscillator, with the well known energies and eigenstates:</p>\n<p><span class=\"math-container\">",
        "</span>\n<span class=\"math-container\">",
        "</span></p>\n<p>The ground state in the presence of a perturbation corresponds to a Gaussian wave packet of width <span class=\"math-container\">",
        "</span>. The effect of <a href=\"https://en.wikipedia.org/wiki/Spontaneous_symmetry_breaking\" rel=\"noreferrer\">spontaneous symmetry breaking</a> emerges when we consider two non-commuting limits:</p>\n<p><span class=\"math-container\">",
        "</span></p>\n<h2>Conclusion:</h2>\n<p>For a system of any finite size, no matter how large, the perturbation can be made small enough for the ground state wavefunction to be essentially spread out over the entire universe. But for an infinitely large system, any perturbation, <em>no matter how weak</em>, is enough to completely localize the wavefunction into a single position. In the <a href=\"https://en.wikipedia.org/wiki/Thermodynamic_limit\" rel=\"noreferrer\">thermodynamic limit</a>, the localization happens even in the presence of only an infinitesimal potential, which in effect means that the wavefunction can spontaneously localize and break the translational symmetry of the unperturbed Hamiltonian.</p>\n<p>But even if the material is not infinitely large, the important message of the last equation is that even if N is not yet truly infinite, the approach towards the <a href=\"https://en.wikipedia.org/wiki/Thermodynamic_limit\" rel=\"noreferrer\">thermodynamic limit</a> is <a href=\"https://physicstoday.scitation.org/doi/10.1063/1.1485555\" rel=\"noreferrer\">singular</a>.  This implies in particular that as you consider larger and larger pieces of matter, a weaker and weaker perturbation suffices to make its ground state a localized wave packet.  Therefore, in a realistic piece of material, even entirely immeasurable perturbations are easily strong enough to completely localize the matter into a single position.</p>\n<p>Finally, it's important to note that if we find a crystal (or a person like you) in a localized state, that thus implies not only that the crystal picks out a preferred location in space, but also that it exists in a state that is not the ground state of its Hamiltonian. In fact, it is not even an eigenstate of the Hamiltonian. <a href=\"https://en.wikipedia.org/wiki/Spontaneous_symmetry_breaking\" rel=\"noreferrer\">Spontaneous symmetry breaking</a> explains both how an infinitesimal perturbation can suffice to pick a location and break the symmetry, and how that same infinitesimal perturbation suffices to make the localized state the ground state of the perturbed system.</p>\n<p><em>Therefore, to finally answer your question:</em></p>\n<p><strong>You are indeed localized into a single position</strong>.</p>\n<p><strong>Source:</strong> Lectures notes from a course on Condensed Matter Theory by <a href=\"https://www.jvanwezel.com/\" rel=\"noreferrer\">Jasper van Wezel</a> from University of Amsterdam.</p>\n<hr />\n<h2>Update:</h2>\n<p>The comment by OP caught me off guard:</p>\n<blockquote>\n<p>When some perturbation causes \"me\" to become localized in a single position, does this mean \"I\" am completely spread out in total momentum space? Or is my total momentum also localized to a single value?</p>\n</blockquote>\n<p>I didn't realize at the time, but my answer only explained why objects appear to have a well-defined position - it did not provide an answer as to why they also appear to have a well-defined momentum.</p>\n<p>Therefore, I Fourier transformed the ground state wave function <span class=\"math-container\">",
        "</span>, to momentum space <span class=\"math-container\">",
        "</span> and applied the two limits.  The results were:</p>\n<p><span class=\"math-container\">",
        "</span></p>\n<p>First thing to note is that this is the exact opposite of the results we got for the wavefunction in position space.  Secondly, as before, we are interested in the equation where we first take the <span class=\"math-container\">",
        "</span> limit, and <em>then</em> the <span class=\"math-container\">",
        "</span> limit (this corresponds to having a very large object and <em>then</em> applying an infinitesimal perturbation).  Thus, the answer is that <strong>the wavefunction is momentum space is completely spread out</strong>, i.e. it's a constant.  This is not what I expected, even though it makes total sense due to Heisenberg's uncertainty principle.</p>\n<p><a href=\"https://www.merriam-webster.com/dictionary/triggered\" rel=\"noreferrer\">Triggered</a> by this result, I consulted the professor himself.</p>\n<p><strong>This is basically what he said (paraphrased to make more sense in our context):</strong></p>\n<p>The result at which we arrived is indeed, a very good point.  The example of the quantum crystal is unfortunately a not well-behaved mathematical object, unlike <a href=\"https://en.wikipedia.org/wiki/Antiferromagnetism\" rel=\"noreferrer\">antiferromagnets</a> and <a href=\"https://en.wikipedia.org/wiki/Bose%E2%80%93Einstein_condensate\" rel=\"noreferrer\">Bose-Einstein condensates</a> which are much more mathematically clean, and subtleties do not come up.  The crystal was given merely as an example to illustrate that everyday objects can display strange collective quantum behavior.</p>\n<p>Some problems with the quantum crystal are that</p>\n<ul>\n<li>its <a href=\"https://en.wikipedia.org/wiki/Phase_transition#Order_parameters\" rel=\"noreferrer\">order parameter</a> is hard to define,</li>\n<li>the required symmetry-breaking field <span class=\"math-container\">",
        "</span> for the crystal is not obvious.</li>\n</ul>\n<p>In reality, the symmetry-broken ground state is a Gaussian wavefunction whose width does not depend on <span class=\"math-container\">",
        "</span>, does not get completely localized in the thermodynamic limit, and does not have undefined momentum.  This is because in order to have an extensive energy for the symmetry-broken Hamiltonian, the symmetry-breaking field V needs to scale with the inverse system size, i.e. <span class=\"math-container\">",
        "</span> should be replaced by <span class=\"math-container\">",
        "</span> in <em>all equations</em>.</p>\n<p><strong>This raises the question:</strong>\n<em>Does that mean that translational symmetry was not broken after all?</em></p>\n<p>The answer is simply no, and this is because we've been looking at it the wrong way all along.  Rather than considering the wavefunction itself, and saying whether or not it is absolutely localized, we should consider <em>how</em> localized it is, compared to some natural measure for localization. Since we're talking about a single crystal without any other objects around, the only sensible measure is to ask whether or not <em>the crystal is localized to within its own size</em>. In other words, whether fluctuations in position, <span class=\"math-container\">",
        "</span>, are small compared to the linear size of the object, <span class=\"math-container\">",
        "</span>.  You can then see that, by taking the right order of limits, the fluctuations are always large in absolute size (for small <span class=\"math-container\">",
        "</span>) but that <span class=\"math-container\">",
        "</span> disappears in the <a href=\"https://en.wikipedia.org/wiki/Thermodynamic_limit\" rel=\"noreferrer\">thermodynamic limit</a>.</p>\n<p><em>So an object (or a person) is localized in real space in units of its own size (which are the only relevant units!).</em></p>\n<p>Similarly, for the fluctuations in momentum, we can show that although they almost disappear in absolute size (<span class=\"math-container\">"
      ],
      "created": "2022-05-09T12:26:42.473",
      "golden_ner_terms": [
        "atoms",
        "basis",
        "behavior",
        "canonical",
        "centre",
        "centre of mass",
        "collection",
        "collective excitations",
        "commutator",
        "conclusion",
        "condensed matter",
        "connection",
        "consequence",
        "constant",
        "context",
        "coordinate",
        "cover",
        "crystals",
        "distance",
        "energy",
        "entire",
        "equation",
        "even",
        "expectation",
        "expectation value",
        "field",
        "finite",
        "forces",
        "fourier transform",
        "function",
        "gaussian",
        "ground state",
        "hamiltonian",
        "harmonic",
        "harmonic oscillator",
        "implies",
        "independent",
        "infinite",
        "infinitesimal",
        "interactions",
        "invariant",
        "inverse",
        "inverse system",
        "inversely proportional",
        "limit",
        "localization",
        "mass",
        "matter",
        "mean",
        "measure",
        "model",
        "momentum",
        "nor",
        "number",
        "object",
        "obvious",
        "operator",
        "operators",
        "opposite",
        "order",
        "origin",
        "parameter",
        "perturbation",
        "point",
        "potential",
        "proportional",
        "real",
        "reciprocal",
        "right",
        "scales",
        "separating",
        "simple",
        "singular",
        "size",
        "source",
        "space",
        "spectrum",
        "state",
        "strength",
        "strong",
        "superposition",
        "symmetry",
        "symmetry breaking",
        "theory",
        "time",
        "uncertainty principle",
        "universe",
        "wave function",
        "wavefunction",
        "way",
        "weaker",
        "well-defined",
        "width"
      ],
      "golden_ner_count": 93,
      "golden_patterns": [
        {
          "pattern": "check-the-extreme-cases",
          "score": 6.0,
          "hotwords": [
            "extreme",
            "zero",
            "degenerate"
          ]
        },
        {
          "pattern": "work-examples-first",
          "score": 4.0,
          "hotwords": [
            "example",
            "illustrate"
          ]
        },
        {
          "pattern": "exploit-symmetry",
          "score": 4.0,
          "hotwords": [
            "symmetry",
            "invariant"
          ]
        },
        {
          "pattern": "local-to-global",
          "score": 4.0,
          "hotwords": [
            "local",
            "cover"
          ]
        },
        {
          "pattern": "try-a-simpler-case",
          "score": 2.0,
          "hotwords": [
            "degenerate"
          ]
        },
        {
          "pattern": "encode-as-algebra",
          "score": 2.0,
          "hotwords": [
            "ring"
          ]
        },
        {
          "pattern": "unfold-the-definition",
          "score": 2.0,
          "hotwords": [
            "means that"
          ]
        },
        {
          "pattern": "optimise-a-free-parameter",
          "score": 2.0,
          "hotwords": [
            "pick"
          ]
        },
        {
          "pattern": "monotone-approximation",
          "score": 2.0,
          "hotwords": [
            "limit"
          ]
        }
      ],
      "golden_pattern_names": [
        "check-the-extreme-cases",
        "work-examples-first",
        "exploit-symmetry",
        "local-to-global",
        "try-a-simpler-case",
        "encode-as-algebra",
        "unfold-the-definition",
        "optimise-a-free-parameter",
        "monotone-approximation"
      ],
      "golden_scopes": [],
      "golden_scope_count": 0
    },
    {
      "id": "se-physics-305602",
      "stratum": "easy",
      "title": "Conservation of information and determinism?",
      "tags": [
        "conservation-laws",
        "information",
        "determinism"
      ],
      "score": 23,
      "answer_score": 2,
      "question_body": "I'm having a hard time wrapping my head around the conservation of information principle as formulated by Susskind and others. From the most upvoted answers to this post , it seems that the principle of conservation of information is a consequence of the reversibility (unitarity) of physical processes. Reversibility implies determinism: Reversibility means that we have a one to one correspondence between a past state and a future state, and so given complete knowledge of the current state of the universe, we should be able to predict all future states of the universe ( Laplace's famous demon ). But hasn't this type of determinism been completely refuted by Quantum Mechanics, the uncertainty principle and the probabilistic outcome of measurement? Isn't the whole point of Quantum Mechanics that this type of determinism no longer holds? Moreover, David Wolpert proved that even in a classical, non-chaotic universe, the presence of devices that perform observation and prediction makes Laplace style determinism impossible. Doesn't Wolpert's result contradict the conservation of information as well? So to summarize my question: How is the conservation of information compatible with the established non-determinism of the universe?",
      "answer_body": "The short answer to this question is that the Schrödinger equation is deterministic and time reversible up to the point of a measurement. Determinism says that given an initial state of a system and the laws of physics you can calculate what the state of the system will be after any arbitrary amount of time (including a positive or negative amount of time). Classically, the deterministic laws of motion are given by Newton's force laws, the Euler-Lagrange equation, and the Hamiltonian. In quantum mechanics, the law that governs the time evolution of a system is the Schrödinger equation. It shows that quantum states are time reversible up until the point of a measurement, at which point the wave function collapses and it is no longer possible to apply a unitary that will tell you what the state was before, deterministically. However, it should be noted that many-world interpreters who don’t believe that measurements are indeterministic don’t agree with this statement, they think that even measurements are deterministic in the grand scheme of quantum mechanics. To quote Scott Aronson: Reversibility has been a central concept in physics since Galileo and Newton. Quantum mechanics says that the only exception to reversibility is when you take a measurement, and the Many-Worlders say not even that is an exception. The reason that people are loose with the phrasing “information is always conserved” is because the “up until a measurement” is taken for granted as background knowledge. In general, the first things you learn about in a quantum mechanics class or textbook is what a superposition is, the Heisenberg uncertainty principle and then the Schrödinger equation. For an explanation of the Schrödinger equation from Wolfram: The Schrödinger equation is the fundamental equation of physics for describing quantum mechanical behavior. It is also often called the Schrödinger wave equation, and is a partial differential equation that describes how the wavefunction of a physical system evolves over time. The Schrödinger equation explains how quantum states develop from one state to another. This evolution is completely deterministic and it is time reversible. Remember that a quantum state is described by a wave function $|\\psi\\rangle$ , which is a collection of probability amplitudes. The Schrödinger equation states that any given wave function $|\\psi_{t_0}\\rangle$ at moment $t_0$ will evolve to become $|\\psi_{t_1}\\rangle$ at time $t_1$ unless a measurement is made before $t_1$ . This is a completely deterministic process and it is time reversible. Given $|\\psi_{t_1}\\rangle$ we can use the equation to calculate what $|\\psi_{t_0}\\rangle$ is equal to. If the electron is in a superposition then the wave function will be so: $|\\psi\\rangle = \\alpha|0\\rangle + \\beta|1\\rangle$ where $\\alpha$ and $\\beta$ are equal to $\\frac{1}{\\sqrt{2}}$ . The state of an electron that is spin up is $|\\psi\\rangle = 1|1\\rangle$ . Clearly, a quantum state that is in a superposition of some observables is a valid ontological object. It behaves in a way completely different than an object that is collapsed into only one of the possibilities via a measurement. The problem of measurements, what they are and what constitutes one, is central to the interpretations of quantum mechanics. The most common view is that a measurement is made when the wave function collapses into one of its eigenstates. The Schrödinger equation provides a deterministic description of a state up to the point of a measurement. Information, as defined by Susskind here, is always conserved up to the point of a measurement. This is because the Schrödinger equation describes the evolution of a quantum state deterministically up until a measurement. The black hole information paradox can be succinctly stated as this: Quantum states evolve unitarily governed by the Schrödinger equation. However, when a particle passes through the event horizon of a black hole and is later radiated out via Hawking radiation it is no longer in a pure quantum state (meaning a measurement was made). A measurement could not have been made because the equivalency principle of general relativity assures us that there is nothing special going on at the event horizon. How can all of this be true? This paradox would not be a paradox if the laws of quantum mechanics didn't give a unitary, deterministic, evolution for quantum states up to a measurement. The reason being, if measurements are the only time unitarity breaks down and the equivalency principle tells us a measurement cannot be happening at the horizon of a black hole, how can unitarity break down and cause the Hawking radiation to be thermal and therefore uncorrelated with the in-falling information? Scott Aaronson gave a talk about quantum information theory and its application to this paradox as well as quantum public key cryptography. In it he explains The Second Law says that entropy never decreases, and thus the whole universe is undergoing a mixing process (even though the microscopic laws are reversible). [After having described how black holes seem to destroy infomration in contradiction to the second law] This means that, when bits of information are thrown into a black hole, the bits seem to disappear from the universe, thus violating the Second Law. So let’s come back to Alice. What does she see? Suppose she knows the complete quantum state $|\\psi\\rangle$ (we’ll assume for simplicity that it’s pure) of all the infalling matter. Then, after collapse to a black hole and Hawking evaporation, what’s come out is thermal radiation in a mixed state $\\rho$ . This is a problem. We’d like to think of the laws of physics as just applying one huge unitary transformation to the quantum state of the world. But there’s no unitary U that can be applied to a pure state $|\\psi\\rangle$ to get a mixed state $\\rho$ . Hawking proposed that black holes were simply a case where unitarity broke down, and pure states evolved into mixed states. That is, he again thought that black holes were exceptions to the laws that hold everywhere else. The information paradox was considered to be solved via Susskind's proposal of black hole complementarity and the holographic principle. Later AMPS showed that the solution is not as simple as it was stated and further work needs to be done. Currently the field of physics is engaged in an amazingly beautiful collection of ideas and solutions being proposed to solve the black hole information paradox as well as the AMPS paradox. At the heart of all of these proposals, however, is the belief that information is, conserved up to the point of a measurement.",
      "question_latex": [],
      "answer_latex": [
        "|\\psi\\rangle",
        "|\\psi_{t_0}\\rangle",
        "t_0",
        "|\\psi_{t_1}\\rangle",
        "t_1",
        "|\\psi\\rangle = \\alpha|0\\rangle + \\beta|1\\rangle",
        "\\alpha",
        "\\beta",
        "\\frac{1}{\\sqrt{2}}",
        "|\\psi\\rangle = 1|1\\rangle",
        "\\rho"
      ],
      "created": "2017-01-17T07:12:22.017",
      "golden_ner_terms": [
        "behavior",
        "black holes",
        "calculate",
        "central",
        "class",
        "collection",
        "compatible",
        "complete",
        "concept",
        "consequence",
        "contradiction",
        "cryptography",
        "current",
        "determinism",
        "deterministic",
        "differential",
        "differential equation",
        "entropy",
        "equation",
        "evaporation",
        "even",
        "event",
        "event horizon",
        "field",
        "function",
        "general relativity",
        "hamiltonian",
        "hawking radiation",
        "heisenberg uncertainty principle",
        "holographic principle",
        "implies",
        "information",
        "key",
        "l system",
        "laws of physics",
        "matter",
        "measurements",
        "mixing",
        "moment",
        "negative",
        "object",
        "observables",
        "observation",
        "one to one correspondence",
        "outcome",
        "paradox",
        "partial differential equation",
        "passes through",
        "physics",
        "point",
        "positive",
        "probability",
        "public key cryptography",
        "pure state",
        "quantum information",
        "quantum mechanics",
        "quantum state",
        "quantum states",
        "radiation",
        "relativity",
        "reversibility",
        "scheme",
        "simple",
        "solution",
        "state",
        "superposition",
        "theory",
        "thermal radiation",
        "time",
        "time evolution",
        "transformation",
        "type",
        "uncertainty principle",
        "unitarity",
        "unitary",
        "unitary transformation",
        "universe",
        "valid",
        "wave equation",
        "wave function",
        "wavefunction",
        "way",
        "work"
      ],
      "golden_ner_count": 83,
      "golden_patterns": [
        {
          "pattern": "argue-by-contradiction",
          "score": 2.0,
          "hotwords": [
            "contradiction"
          ]
        },
        {
          "pattern": "quotient-by-irrelevance",
          "score": 2.0,
          "hotwords": [
            "up to"
          ]
        },
        {
          "pattern": "dualise-the-problem",
          "score": 2.0,
          "hotwords": [
            "complement"
          ]
        },
        {
          "pattern": "use-probabilistic-method",
          "score": 2.0,
          "hotwords": [
            "probability"
          ]
        },
        {
          "pattern": "unfold-the-definition",
          "score": 2.0,
          "hotwords": [
            "means that"
          ]
        }
      ],
      "golden_pattern_names": [
        "argue-by-contradiction",
        "quotient-by-irrelevance",
        "dualise-the-problem",
        "use-probabilistic-method",
        "unfold-the-definition"
      ],
      "golden_scopes": [],
      "golden_scope_count": 0
    },
    {
      "id": "se-physics-299286",
      "stratum": "easy",
      "title": "Why does the expectation value of an operator $A$ take the form $\\langle A\\,\\rangle=\\int{\\psi^* (x) A(x) \\psi (x) dx}$ in QM?",
      "tags": [
        "quantum-mechanics",
        "operators",
        "wavefunction",
        "probability",
        "complex-numbers"
      ],
      "score": 20,
      "answer_score": 18,
      "question_body": "The following is a quote from an answer I was given to this previous question of mine : The definition of the expectation value of an operator $A$ is $$\\begin{equation} \\langle A\\,\\rangle=\\int{\\psi^* (x) A(x) \\psi (x) dx} \\end{equation}\\tag{1}$$ (because it represents \"the value of the variable\" $A(x)$ times \"the probability of being in that configuration\" $P(x)=\\psi^* (x) \\psi (x)$) and for the particular case of the expectation value of the position operator $$\\begin{equation} \\langle x\\rangle=\\int{x \\psi^* (x) \\psi (x) dx} \\end{equation}\\tag{2}$$ Ever since I asked that question I have been using equation $(1)$ without fully understanding why for a arbitrary operator $A$ its expectation value takes the form \\begin{equation} \\int{\\psi^* (x) A(x) \\psi (x) dx} \\end{equation} I understand that the probability is given by $$\\begin{equation} \\int{\\psi^* (x) \\psi (x) dx} =\\int |\\psi(x)|^2dx\\end{equation}$$ So equation $(2)$ makes sense to me as it is simply $$\\langle x \\rangle=\\int x|\\psi(x)|^2dx$$ which is \"the value of the variable times the probability of being in that configuration\" as mentioned in the quote. But for equation $(1)$ the arbitrary operator $A$ is $\\color{red}{\\text{in-between}}$ the $\\psi^*(x)$ and $\\psi(x)$. So unless I can rewrite $(1)$ in the form of \"value times probability\": $$\\begin{equation} \\langle A\\,\\rangle=\\int{A(x)\\psi^* (x) \\psi (x) dx}=\\int A|\\psi(x)|^2dx \\end{equation}\\tag{3}$$ I fail to see how equation $(1)$ gives the expectation value. But I already know that equation $(3)$ is wrong since in equation $(1)$ the operator $A$ is acting on $\\psi(x)$, so it doesn't make any sense to move the operator to the front of the integrand just to make it look like equation $(2)$. Can anyone please explain to me why equation $(1)$ is justified as the expectation value even though the operator is in the middle?",
      "answer_body": "There is a slight, but important aspect you are missing here. The expectation value of the observable $A$ is defined as $$\\langle A\\rangle_\\psi=\\int\\psi^*(x)A\\psi(x)dx$$ where as the probability of being in the configuration $\\psi(x)$ is $$P=\\int\\psi^*(x)\\psi(x)dx$$ But I already know that equation $(3)$ is wrong since in equation $(1)$ the operator $A$ is acting on $\\psi(x)$, so it doesn't make any sense to move the operator to the front of the integrand just to make it look like equation $(2)$. Yes of course. You are right. Now, we see the part you are missing. In quantum mechanics, we define the operators representing observables as Hermitian and an operator has got certain eigen functions. If $\\psi(x)$ is such an eigen function of the operator $A$, then you will have the eigen value equation $$A\\psi(x)=a\\psi(x)$$ where $a$ is the corresponding eigen value which is a real number. In such a case, $$\\langle A\\rangle_\\psi=\\int\\psi^*(x)a\\psi(x)dx=a\\int\\psi^*(x)\\psi(x)dx=aP$$ where $P$ as defined above is the probability that the system can be found in the state $\\psi(x)$. Hence we can say that the expectation value of an operator w.r.t a particular state is the eigen value of that state times the probability of being in that state. That's the difference between an expectation value and the eigen value. Unless the wavefunction is normalized ($P=1$), we will not get the eigen value of the operator as it's expectation value. Now, the wavefunction $\\psi(x)$ need not be always an eigen function of $A$. In such cases, we expand our wavefunction as a superposition of the eigen functions of the operator $A$ in Dirac's bra-ket notation: $$\\vert \\psi\\rangle=\\int d\\zeta '\\vert\\zeta'\\rangle\\langle\\zeta'\\vert\\psi\\rangle $$ where {$\\zeta_j$} forms a complete set of eigen functions of $A$ and $\\displaystyle{\\int d\\zeta '\\vert\\zeta'\\rangle\\langle\\zeta\\vert}$ is the identity operator $1$ and $\\vert\\zeta'\\rangle\\langle\\zeta'\\vert$ is the projection operator $\\Lambda_{\\zeta'}$. The operations all happen in the appropriate Hilbert space spanned by the complete inner products of the eigen kets and eigen bras of the operator. Before we proceed further, let's have a short brief on Dirac's formalism: Short brief on Dirac's bra-ket notation : The ket, like the wavefunction represent a particular state of the system, but it's not actually the wavefunction of the system. It is represented as $\\vert\\psi\\rangle$. The wave function of the system can be derived from the ket, and the ket representing a state, called the state ket, is a vector in the vector space spanned by the eigen kets of the operator $A$, just as like we speak the eigen functions of the operator $A$. Now, for the wavefunction, we have a corresponding complex wave function. Similarly, the complex dual of a state ket is called a state bra and is represented by $\\langle\\psi\\vert$. So, expectation value of some operator of the quantum mechanical system is what we want to measure. The first thing we consider is that we represent the general state ket (which is of course undefined) as a linear superposition of the eigen kets of the operator (which are known, once you solve the eigen value equation). It's like writing a vector as a linear combination of the independent coordinates. However, a vector space is a different thing. But the concept is the same. So, a general state ket $\\vert\\alpha\\rangle$ can be expanded in terms of the complete eigen vectors of the operator $A$ as: $$\\vert\\alpha\\rangle=\\sum_{a'}c_{a'}\\vert a'\\rangle=c_{a'}\\vert a'\\rangle+c_{a''}\\vert a''\\rangle+c_{a'''}\\vert a'''\\rangle+...$$ where the kets $\\vert a'\\rangle, \\vert a''\\rangle,\\vert a'''\\rangle...$ are the eigen kets of $A$ and are complete. The set {$a'$} are the corresponding eigen values. The expansion coefficients $c_{a'},c_{a''},...$ are the probability amplitudes of the corresponding eigen kets. This can be understood in the coming paragraphs where we define the inner product of a ket and a bra. We represent the state of the system in question as a linear combination of the eigen kets of the observable, whose expectation value is to be measured. This vector is represented as a ket and is defined in a complex vector space called the ket space. So, the ket space is spanned by the eigen kets of the operator. This means the eigen kets of the operator forms the basis vectors of our vector space. Since there is a one-to-one correspondence between a ket and the corresponding bra, we can define a space spanned by eigen bras and is called a bra space. If we take the inner product of the state ket and the state bra, defined respectively in the ket space and the bra space, we will get a complete inner product space called the Hilbert space. All the quantum \"mechanics\" happen in the Hilbert space. Why do we need an inner product space? Well, the ket and bra are complex vectors and they are useless, unless we can extract some information from them. To obtain that, we take the inner product of the ket and bra. The inner product is taken between a bra and a ket. The inner product between the state ket $\\vert\\alpha\\rangle$ and the state bra $\\langle\\beta\\vert$ is denoted as $\\langle\\beta\\vert\\alpha\\rangle$. It gives the probability amplitude that the system, found initially in the state $\\vert\\alpha\\rangle$ to be found in the state $\\vert\\beta\\rangle$, whose square of the modulus gives the probability of the same. The inner product is a real number. This probability is the fundamental thing that accompanies all the rest of the operations, which you will see in the coming discussions. The probability is a real number and must be positive. So the inner product explained above should be positive. Now lets look back where we defined $c_{a'}$ as the probability amplitude of the state defined by the ket $\\vert\\alpha\\rangle$ to be found in the state $\\vert a'\\rangle$, which is an eigen state of the operator $A$. For that, we take the inner product of $\\vert\\alpha\\rangle$ with the eigen bra $\\langle a'\\vert$, we get $$\\langle a'\\vert\\alpha\\rangle=\\sum_{a'}c_{a'}\\langle a'\\vert a'\\rangle=c_{a'}$$ where we have used an important relation called the orthonormality condition of two kets. If two kets $\\vert a'\\rangle$ and $\\vert a''\\rangle$ are orthogonal (independent) and normalized (so that the inner product of the ket with it's own bra gives $1$), then the orthonormality condition states that $$\\langle a'\\vert a''\\rangle=\\delta_{a',a''}$$ which is $1$ if the two kets are the same and $0$ when they are not. So, we demand the eigen kets of the operators to be orthonormal so that they satisfy the above orthonormality condition. So, we have got $c_{a'}$ as the probability amplitude of the eigen ket $\\vert a'\\rangle$. Hence the square of its modulus give us the probability that the system is found to be in the eigen state $\\vert a'\\rangle$: $$\\vert c_{a'}\\vert^2=\\vert\\langle a'\\vert\\alpha\\rangle\\vert^2$$ Now, we see that $$\\sum_{a'} \\vert c_{a'}\\vert^2=\\sum_{a'}\\vert\\langle a'\\vert\\alpha\\rangle\\vert^2=1$$ a requirement by the probability conservation theorem. Now, what happens if we take the inner product of a general ket and the corresponding bra? That answer will give us the probability to find the system to be in that state. If the state kets are normalized, then this probability will be one. Now, while taking the inner product of a state ket with a state bra, we are combining the two spaces- the ket and bra spaces- somehow to get a complete inner product space called Hilbert space. All the information about the state is hidden in this Hilbert space. So we ask the state ket to reveal some information, for example the energy. We do this by operating the state ket buy the energy operator. Then we will get the value of energy, which is present in the Hilbert space. So, the operations on state ket happens in the Hilbert space. Now, let's see the operation of the operators on the state kets. Its similar to the operation of the operators on a wavefunction. The operator $A$ acting on the general ket $\\vert\\alpha\\rangle$ is given by $$A\\vert\\alpha\\rangle=A\\sum_{a'}c_{a'}\\vert a'\\rangle=A\\sum_{a'}\\left(\\langle a'\\vert\\alpha\\rangle\\right)\\vert a'\\rangle=A\\sum_{a'}\\vert a'\\rangle\\langle a'\\vert\\alpha\\rangle$$ When we compare both sides that the effect of $\\displaystyle{\\sum_{a'}\\vert a'\\rangle\\langle a'\\vert}$ is just like operating by the identity operator $1$. Hence $\\displaystyle{\\sum_{a'}\\vert a'\\rangle\\langle a'\\vert}=1$ is regarded as the identity opertor. Now, what does the outer product $\\Lambda_{a'}=\\vert a'\\rangle\\langle a'\\vert$ gives us? Even though the inner product is a scalar, the outer product is an operator. To see this, let it act on the ket $\\vert\\alpha\\rangle$ $$\\Lambda_{a'}\\vert\\alpha\\rangle=\\vert a'\\rangle\\langle a'\\vert\\vert\\alpha\\rangle=\\vert a'\\rangle\\left(\\langle a'\\vert\\vert\\alpha\\rangle\\right)=c_{a'}\\vert a'\\rangle.$$ The ket $\\vert\\alpha\\rangle$ is a combination of the all possible eigen kets. When we operate this ket with $\\Lambda_{a'}$, the operator selects the portion of the ket $\\vert\\alpha\\rangle$ parallel to $\\vert a'\\rangle$. Hence it is known as the projection operator. Comparing the identity operator and the projection operator, we find that $$\\sum_{a'} \\Lambda_{A'}=1$$ Okay, now we are almost equipped with the tools for the further discussion. We have only considered above discrete spectrum cases only. The above facts holds for continuous spectrum. All we have to do is just replace the summation by an integral and the Kronecker delta symbol by the Dirac delta function. Note: This is not a complete description about Dirac's notation. There are a lot of things to see. However I've limitations here. You can found more illuminating discussions on Dirac's notation in Modern Quantum Mechanics by J. J. Sakurai. Now, we continue. The expectation value is defined as $$\\langle A\\rangle_\\psi=\\langle\\psi\\vert A\\vert\\psi\\rangle$$ Substituting the above expansion of $\\vert\\psi\\rangle$ in the equation, we get $$ \\begin{align} \\langle A\\rangle_\\psi&=\\iint d\\zeta'd\\zeta''\\langle\\psi\\vert\\zeta'\\rangle\\langle\\zeta'\\vert A \\vert\\zeta''\\rangle\\langle\\zeta''\\vert\\psi\\rangle\\\\ &= \\iint d\\zeta'd\\zeta''\\langle\\psi\\vert\\zeta'\\rangle\\zeta' \\delta\\left(\\zeta''-\\zeta'\\right)\\langle\\zeta''\\vert\\psi\\rangle\\\\ &=\\int d\\zeta' \\zeta' \\langle\\psi\\vert\\zeta'\\rangle\\langle\\zeta'\\vert\\psi\\rangle \\end{align} $$ Now, $\\langle\\zeta'\\vert\\psi\\rangle$ is defined as an inner product of two kets. It gives the probability that the system is transferred from state $\\vert\\psi\\rangle$ to the state $\\vert\\zeta'\\rangle$ and is the transition probability. If I represent $\\langle\\zeta'\\vert\\psi\\rangle=c_{\\zeta'}$, which in general is a complex number and is the transition amplitude, then $\\langle\\psi\\vert\\zeta'\\rangle=\\langle\\zeta'\\vert\\psi\\rangle^*=c^*_{\\zeta'}$. Hence $$\\langle A\\rangle_\\psi=\\int d\\zeta ' \\zeta' \\vert c_{\\zeta'}\\vert^2$$ which means the expectation value ofthe operator $A$ is the eigen ket of $A$ times the probability of the system to be found in that particular eigen state of $A$.",
      "question_latex": [
        "\\begin{equation}\n\\langle  A\\,\\rangle=\\int{\\psi^* (x) A(x) \\psi (x) dx}\n\\end{equation}\\tag{1}",
        "\\begin{equation}\n\\langle x\\rangle=\\int{x \\psi^* (x) \\psi (x) dx}\n\\end{equation}\\tag{2}",
        "\\begin{equation}\n\\int{\\psi^* (x) \\psi (x) dx}\n=\\int |\\psi(x)|^2dx\\end{equation}",
        "\\langle x \\rangle=\\int x|\\psi(x)|^2dx",
        "\\begin{equation}\n\\langle  A\\,\\rangle=\\int{A(x)\\psi^* (x)  \\psi (x) dx}=\\int A|\\psi(x)|^2dx\n\\end{equation}\\tag{3}",
        "A",
        "(because it represents \"the value of the variable\"",
        "times \"the probability of being in that configuration\"",
        ")\n  and for the particular case of the expectation value of the position operator",
        "</p>\n</blockquote>\n\n<p>Ever since I asked that question I have been using equation",
        "without fully understanding why for a arbitrary operator",
        "its expectation value takes the form \\begin{equation}\n\\int{\\psi^* (x) A(x) \\psi (x) dx}\n\\end{equation}  </p>\n\n<p>I understand that the probability is given by",
        "</p>\n\n<p>So equation",
        "makes sense to me as it is simply",
        "which is \"the value of the variable times the probability of being in that configuration\" as mentioned in the quote.</p>\n\n<p>But for equation",
        "the arbitrary operator",
        "is",
        "the",
        "and",
        ". </p>\n\n<p>So <em>unless</em> I can rewrite",
        "in the form of \"value times probability\":",
        "I fail to see how equation",
        "gives the expectation value. </p>\n\n<p>But I already know that equation",
        "is wrong since in equation",
        "the operator",
        "is <em>acting on</em>",
        ", so it doesn't make any sense to move the operator to the front of the integrand just to make it <em>look</em> like equation",
        ".</p>\n\n<p>Can anyone please explain to me why equation"
      ],
      "answer_latex": [
        "\\langle A\\rangle_\\psi=\\int\\psi^*(x)A\\psi(x)dx",
        "P=\\int\\psi^*(x)\\psi(x)dx",
        "A\\psi(x)=a\\psi(x)",
        "\\langle A\\rangle_\\psi=\\int\\psi^*(x)a\\psi(x)dx=a\\int\\psi^*(x)\\psi(x)dx=aP",
        "\\vert \\psi\\rangle=\\int d\\zeta '\\vert\\zeta'\\rangle\\langle\\zeta'\\vert\\psi\\rangle",
        "\\vert\\alpha\\rangle=\\sum_{a'}c_{a'}\\vert a'\\rangle=c_{a'}\\vert a'\\rangle+c_{a''}\\vert a''\\rangle+c_{a'''}\\vert a'''\\rangle+...",
        "\\langle a'\\vert\\alpha\\rangle=\\sum_{a'}c_{a'}\\langle a'\\vert a'\\rangle=c_{a'}",
        "\\langle a'\\vert a''\\rangle=\\delta_{a',a''}",
        "\\vert c_{a'}\\vert^2=\\vert\\langle a'\\vert\\alpha\\rangle\\vert^2",
        "\\sum_{a'} \\vert c_{a'}\\vert^2=\\sum_{a'}\\vert\\langle a'\\vert\\alpha\\rangle\\vert^2=1",
        "A\\vert\\alpha\\rangle=A\\sum_{a'}c_{a'}\\vert a'\\rangle=A\\sum_{a'}\\left(\\langle a'\\vert\\alpha\\rangle\\right)\\vert a'\\rangle=A\\sum_{a'}\\vert a'\\rangle\\langle a'\\vert\\alpha\\rangle",
        "\\Lambda_{a'}\\vert\\alpha\\rangle=\\vert a'\\rangle\\langle a'\\vert\\vert\\alpha\\rangle=\\vert a'\\rangle\\left(\\langle a'\\vert\\vert\\alpha\\rangle\\right)=c_{a'}\\vert a'\\rangle.",
        "\\sum_{a'} \\Lambda_{A'}=1",
        "\\langle A\\rangle_\\psi=\\langle\\psi\\vert A\\vert\\psi\\rangle",
        "\\begin{align}\n \\langle A\\rangle_\\psi&=\\iint d\\zeta'd\\zeta''\\langle\\psi\\vert\\zeta'\\rangle\\langle\\zeta'\\vert A \\vert\\zeta''\\rangle\\langle\\zeta''\\vert\\psi\\rangle\\\\ \n&= \\iint d\\zeta'd\\zeta''\\langle\\psi\\vert\\zeta'\\rangle\\zeta' \\delta\\left(\\zeta''-\\zeta'\\right)\\langle\\zeta''\\vert\\psi\\rangle\\\\\n&=\\int d\\zeta' \\zeta' \\langle\\psi\\vert\\zeta'\\rangle\\langle\\zeta'\\vert\\psi\\rangle\n\\end{align}",
        "\\langle A\\rangle_\\psi=\\int d\\zeta ' \\zeta' \\vert c_{\\zeta'}\\vert^2",
        "A",
        "</p>\n\n<p>where as the probability of being in the configuration",
        "is  </p>\n\n<p>",
        "</p>\n\n<blockquote>\n  <p>But I already know that equation",
        "is wrong since in equation",
        "the operator",
        "is acting on",
        ", so it doesn't make any sense to move the operator to the front of the integrand just to make it look like equation",
        ".  </p>\n</blockquote>\n\n<p>Yes of course. You are right. Now, we see the part you are missing. In quantum mechanics, we define the operators representing observables as Hermitian and an operator has got certain eigen functions.  </p>\n\n<p>If",
        "is such an eigen function of the operator",
        ", then you will have the eigen value equation  </p>\n\n<p>",
        "</p>\n\n<p>where",
        "is the corresponding eigen value which is a real number. In such a case,  </p>\n\n<p>",
        "as defined above is the probability that the system can be found in the state",
        ". Hence we can say that the expectation value of an operator w.r.t a particular state is the eigen value of that state times the probability of being in that state. That's the difference between an expectation value and the eigen value.      </p>\n\n<p>Unless the wavefunction is normalized (",
        "), we will not get the eigen value of the operator as it's expectation value.  </p>\n\n<p>Now, the wavefunction",
        "need not be always an eigen function of",
        ". In such cases, we expand our wavefunction as a superposition of the eigen functions of the operator",
        "in Dirac's bra-ket notation:  </p>\n\n<p>",
        "</p>\n\n<p>where {",
        "} forms a complete set of eigen functions of",
        "and",
        "is the identity operator",
        "is the projection operator",
        ". The operations all happen in the appropriate Hilbert space spanned by the complete inner products of the eigen kets and eigen bras of the operator.   </p>\n\n<p>Before we proceed further, let's have a short brief on Dirac's formalism:</p>\n\n<blockquote>\n  <p><strong>Short brief on Dirac's bra-ket notation</strong>: The ket, like the wavefunction represent a particular state of the system, but it's not actually the wavefunction of the system. It is represented as",
        ". The wave function of the system can be derived from the ket, and the ket representing a state, called the state ket, is a vector in the vector space spanned by the eigen kets of the operator",
        ", just as like we speak the eigen functions of the operator",
        ". Now, for the wavefunction, we have a corresponding complex wave function. Similarly, the complex dual of a state ket is called a state bra and is represented by",
        ".<br>\n  So, expectation value of some operator of the quantum mechanical system is what we want to measure. The first thing we consider is that we represent the general state ket (which is of course undefined) as a linear superposition of the eigen kets of the operator (which are known, once you solve the eigen value equation). It's like writing a vector as a linear combination of the independent coordinates. However, a vector space is a different thing. But the concept is the same. So, a general state ket",
        "can be expanded in terms of the complete eigen vectors of the operator",
        "as:",
        "where the kets",
        "are the eigen kets of",
        "and are complete. The set {",
        "} are the corresponding eigen values. The expansion coefficients",
        "are the probability amplitudes of the corresponding eigen kets. This can be understood in the coming paragraphs where we define the inner product of a ket and a bra.<br>\n  We represent the state of the system in question as a linear combination of the eigen kets of the observable, whose expectation value is to be measured. This vector is represented as a ket and is defined in a complex vector space called the ket space. So, the ket space is spanned by the eigen kets of the operator. This means the eigen kets of the operator forms the basis vectors of our vector space. Since there is a one-to-one correspondence between a ket and the corresponding bra, we can define a space spanned by eigen bras and is called a bra space. If we take the inner product of the state ket and the state bra, defined respectively in the ket space and the bra space, we will get a complete inner product space called the Hilbert space. All the quantum \"mechanics\" happen in the Hilbert space.<br>\n  Why do we need an inner product space? Well, the ket and bra are complex vectors and they are useless, unless we can extract some information from them. To obtain that, we take the inner product of the ket and bra. The inner product is taken between a bra and a ket. The inner product between the state ket",
        "and the state bra",
        "is denoted as",
        ". It gives the probability amplitude that the system, found initially in the state",
        "to be found in the state",
        ", whose square of the modulus gives the probability of the same. The inner product is a real number. This probability is the fundamental thing that accompanies all the rest of the operations, which you will see in the coming discussions. The probability is a real number and must be positive. So the inner product explained above should be positive.<br>\n  Now lets look back where we defined",
        "as the probability amplitude of the state defined by the ket",
        ", which is an eigen state of the operator",
        ". For that, we take the inner product of",
        "with the eigen bra",
        ", we get<br>",
        "where we have used an important relation called the orthonormality condition of two kets. If two kets",
        "are orthogonal (independent) and normalized (so that the inner product of the ket with it's own bra gives",
        "), then the orthonormality condition states that",
        "which is",
        "if the two kets are the same and",
        "when they are not. So, we demand the eigen kets of the operators to be orthonormal so that they satisfy the above orthonormality condition.  So, we have got",
        "as the probability amplitude of the eigen ket",
        ". Hence the square of its modulus give us the probability that the system is found to be in the eigen state",
        ":",
        "Now, we see that",
        "a requirement by the probability conservation theorem.<br>\n  Now, what happens if we take the inner product of a general ket and the corresponding bra? That answer will give us the probability to find the system to be in that state. If the state kets are normalized, then this probability will be one.<br>\n  Now, while taking the inner product of a state ket with a state bra, we are combining the two spaces- the ket and bra spaces- somehow to get a complete inner product space called Hilbert space. All the information about the state is hidden in this Hilbert space. So we ask the state ket to reveal some information, for example the energy. We do this by operating the state ket buy the energy operator. Then we will get the value of energy, which is present in the Hilbert space. So, the operations on state ket happens in the Hilbert space.<br>\n  Now, let's see the operation of the operators on the state kets. Its similar to the operation of the operators on a wavefunction. The operator",
        "acting on the general ket",
        "is given by<br>",
        "<br>\n  When we compare both sides that the effect of",
        "is just like operating by the identity operator",
        ". Hence",
        "is regarded as the identity opertor. Now, what does the outer product",
        "gives us? Even though the inner product is a scalar, the outer product is an operator. To see this, let it act on the ket",
        "<br>",
        "<br>\n  The ket",
        "is a combination of the all possible eigen kets. When we operate this ket with",
        ", the operator selects the portion of the ket",
        "parallel to",
        ". Hence it is known as the projection operator. Comparing the identity operator and the projection operator, we find that<br>",
        "<br>\n  Okay, now we are almost equipped with the tools for the further discussion. We have only considered above discrete spectrum cases only. The above facts holds for continuous spectrum. All we have to do is just replace the summation by an integral and the Kronecker delta symbol by the Dirac delta function.<br>\n  <strong>Note:</strong> This is not a complete description about Dirac's notation. There are a lot of things to see. However I've limitations here. You can found more illuminating discussions on Dirac's notation in Modern Quantum Mechanics by J. J. Sakurai.  </p>\n</blockquote>\n\n<p>Now, we continue. The expectation value is defined as  </p>\n\n<p>",
        "</p>\n\n<p>Substituting the above expansion of",
        "in the equation, we get  </p>\n\n<p>",
        "</p>\n\n<p>Now,",
        "is defined as an inner product of two kets. It gives the probability that the system is transferred from state",
        "to the state",
        "and is the transition probability. If I represent",
        ", which in general is a complex number and is the transition amplitude, then",
        ". Hence  </p>\n\n<p>",
        "</p>\n\n<p>which means the expectation value ofthe operator",
        "is the eigen ket of",
        "times the probability of the system to be found in that particular eigen state of"
      ],
      "created": "2016-12-17T11:07:43.773",
      "golden_ner_terms": [
        "act on",
        "basis",
        "combination",
        "complete",
        "complex",
        "complex number",
        "concept",
        "configuration",
        "continuous",
        "continuous spectrum",
        "coordinates",
        "delta function",
        "difference",
        "dirac delta function",
        "discrete",
        "eigen",
        "energy",
        "equation",
        "even",
        "expand",
        "expanded",
        "expansion",
        "expectation",
        "expectation value",
        "formalism",
        "function",
        "hermitian",
        "hilbert space",
        "identity",
        "identity operator",
        "independent",
        "information",
        "inner",
        "inner product",
        "inner product space",
        "integral",
        "integrand",
        "kronecker delta",
        "l system",
        "linear combination",
        "measure",
        "modulus",
        "number",
        "observables",
        "one-to-one",
        "one-to-one correspondence",
        "operation",
        "operator",
        "operators",
        "orthogonal",
        "orthonormal",
        "outer",
        "outer product",
        "parallel",
        "positive",
        "probability",
        "product",
        "projection",
        "quantum mechanics",
        "real",
        "real number",
        "relation",
        "represents",
        "right",
        "satisfy",
        "scalar",
        "similar",
        "space",
        "spanned by",
        "spectrum",
        "square",
        "state",
        "summation",
        "superposition",
        "the dirac delta function",
        "theorem",
        "transition probability",
        "variable",
        "vector",
        "vector space",
        "vectors",
        "wave function",
        "wavefunction"
      ],
      "golden_ner_count": 83,
      "golden_patterns": [
        {
          "pattern": "work-examples-first",
          "score": 2.0,
          "hotwords": [
            "example"
          ]
        },
        {
          "pattern": "dualise-the-problem",
          "score": 2.0,
          "hotwords": [
            "dual"
          ]
        },
        {
          "pattern": "reduce-to-known-result",
          "score": 2.0,
          "hotwords": [
            "it is known"
          ]
        },
        {
          "pattern": "encode-as-algebra",
          "score": 2.0,
          "hotwords": [
            "ring"
          ]
        },
        {
          "pattern": "use-probabilistic-method",
          "score": 2.0,
          "hotwords": [
            "probability"
          ]
        },
        {
          "pattern": "monotone-approximation",
          "score": 2.0,
          "hotwords": [
            "limit"
          ]
        }
      ],
      "golden_pattern_names": [
        "work-examples-first",
        "dualise-the-problem",
        "reduce-to-known-result",
        "encode-as-algebra",
        "use-probabilistic-method",
        "monotone-approximation"
      ],
      "golden_scopes": [
        {
          "type": "where-binding",
          "match": "where $a$ is"
        }
      ],
      "golden_scope_count": 1
    },
    {
      "id": "se-physics-135098",
      "stratum": "easy",
      "title": "Confusion about duality transformation in 1+1D Ising model in a transverse field",
      "tags": [
        "condensed-matter",
        "topology",
        "ising-model",
        "duality",
        "spin-chains"
      ],
      "score": 27,
      "answer_score": 20,
      "question_body": "In 1+1D Ising model with a transverse field defined by the Hamiltonian \\begin{equation} H(J,h)=-J\\sum_i\\sigma^z_i\\sigma_{i+1}^z-h\\sum_i\\sigma_i^x \\end{equation} There is a duality transformation which defines new Pauli operators $\\mu^x_i$ and $\\mu^z_i$ in a dual lattice \\begin{equation} \\mu_i^z=\\prod_{j\\leq i}\\sigma^x_j \\qquad \\mu_i^x=\\sigma^z_{i+1}\\sigma^z_{i} \\end{equation} then these $\\mu_i^x$ and $\\mu_i^z$ satisfy the same commutation and anti-commutation relations of $\\sigma^x_i$ and $\\sigma^z_i$, and the original Hamiltonian can be written in terms of $\\mu_i^x$ and $\\mu_i^z$ as \\begin{equation} H(J,h)=-J\\sum_i\\mu_i^x-h\\sum_i\\mu_i^z\\mu_{i+1}^z \\end{equation} At this stage, many textbooks will tell us since $\\sigma$'s and $\\mu$'s have the same algebra relations, the right hand side of the last equation is nothing but $H(h,J)$. My confusions are: Does that the operators having the same algebra really imply that $H(J,h)$ and $H(h,J)$ have the same spectrum? We know for a given algebra we can have different representations and these different representations may give different results. For example, the angular momentum algebra is always the same, but we can have different eigenvalues of spin operators. This is related to the first confusion. Instead of looking at the algebra of the new operators, we can also look at how the states transform under this duality transformation. In the eigenbasis of $\\mu_i^x$, if I really consider it as a simple Pauli matrix, the state $|\\rightarrow\\rangle$ corresponds to two states in the original picture, i.e. $|\\uparrow\\uparrow\\rangle$ and $|\\downarrow\\downarrow\\rangle$. The same for state $|\\leftarrow\\rangle$. In the $\\mu_i^z$ basis, the correspondence is more complicated. A state corresponds to many states in the original picture, and the number of the corresponding states depend on the position of this state. Therefore, this duality transformation is not unitary, which makes me doubt whether $H(J,h)$ and $H(h,J)$ should have the same spectrum. Further, what other implication may this observation lead to? For example, doing one duality transformation is a many-to-one correspondence, then doing it back should still be a many-to-one correspondence, then can we recover the original spectrum? Another observation is that in the above $\\mu_i^z$ involves a string of operators on the left side, we can equally define it in terms of a string of operators on the right side, so it seems there is an unobservable string. What implication can this observation lead to? Is this unobservable string related to the unobservable strings in Levin-Wen model?",
      "answer_body": "This is a very good question. The same operator algebra does not imply that $H(J,h)$ and $H(h,J)$ have the same spectrum. As has been mentioned in Dominic's answer, even the ground state degeneracy is different under the interchange of $J$ and $h$ ($J\\gg h$: symmetry-broken two-fold degeneracy, and $J\\ll h$ unique ground state), therefore it is impossible to establish a one-to-one mapping between the eigenstates of $H(J,h)$ and $H(h,J)$. One must keep in mind that the duality transformation only preserves local dynamics but the global (topological) properties will be lost . This statement becomes sharper in higher dimensions. Like in 2D, the $\\mathbb{Z}_2$ lattice gauge theory is dual to the quantum Ising model, however the topological order of the gauge theory (most prominently the topology-dependent ground state degeneracy) is completely lost in the dual Ising model, even though there is a beautiful correspondence between their local excitations (e.g. charge and vison). Nevertheless, duality is still very useful if we only focus on the local excitations. Many important problems like low-energy dynamics, phase transitions and criticality are only related to local excitations, then the duality transformation can help us a lot in understanding these things. To appreciate the duality in the 1D transverse field Ising model, it is better to just look locally and try to figure out the local correspondence of the bulk excitations, without caring too much about the global properties such as boundary conditions, infinite strings, ground state degeneracy etc. The idea of duality is actually simple: one can describe a 1D Ising chain either by the spin variables living on each site, or by the kink variables living on each link. A kink on the spin chain is a link across which the Ising spins are opposite. Then every link $l$ can only have two possible states: $$\\tau_l^z=\\left\\{\\begin{array}{cc}+1 & \\text{unkinked,}\\\\ -1 & \\text{kinked,}\\end{array}\\right.$$ If we have specified all the kink configuration $\\tau_l^z$ on each link $l$, we can actually determine the spin configuration $\\sigma_i^z$ on each site $i$, with only one additional piece of knowledge about the left-most spin $\\sigma_0^z$. The trick is just to accumulate the kink configurations from left all the way to the right, $$\\sigma_i^z=\\sigma_0^z\\prod_{0<l<i}\\tau_l^z.$$ So the spin configuration is uniquely determined by the kink configuration (up to the left-most spin). If we count the number of states in the Hilbert space, the Hilbert space dimension will be $2^{N_\\text{site}}$ in the spin language and $2^{N_\\text{link}}$ in the kink language, where $N_\\text{site}$ and $N_\\text{link}$ are the number of sites and links respectively, which are equal (apart from the left-most site) on the 1D lattice, so the Hilbert space dimension is actually the same in both languages. In this sense, we can say that the correspondence between the spin and the kink descriptions is almost one-to-one (especially in the thermodynamic limit), even though there might be some complication arising from the left-most boundary (which however is to be ignored in the duality transform, as duality only cares local properties). Now we can rephrase the original transverse field Ising model $$H=-J\\sum_{i}\\sigma_i^z\\sigma_{i+1}^z-h\\sum_{i}\\sigma_i^x,$$ in the kink language. It is not hard to see that the coupling between the Ising spins is just the chemical potential for the kink $$-J\\sum_{i}\\sigma_i^z\\sigma_{i+1}^z=-J\\sum_l\\tau_l^z,$$ which basically follows from the physical meaning of the kink variable $\\tau_l^z$. Admittedly this equality may run into a little trouble on the boundary, where some sites or links might be missing, but they do not affect the local properties deep in the bulk, so we just ignore. The translation of transverse field term is more involved. In the spin language $\\sigma_i^x$ operator just flips the spin on site $i$, which would correspond to simultaneous creation or annihilation two kinks on the links adjacent to that site, or moving an existed kink across the site. In either case, flipping a spin would correspond to simultaneously changing the kink variables $\\tau^z$ on adjacent links, which can be carried out by $\\tau^x_l\\tau^x_{l+1}$, s.t. $$-h\\sum_i\\sigma_i^x=-h\\sum_{l}\\tau^x_l\\tau^x_{l+1}.$$ Obviously the relation between $\\tau^x$ and $\\tau^z$ is exactly the same as our familiar $2\\times 2$ Pauli matrices $\\sigma^x$ and $\\sigma^z$, s.t. $\\tau^x|\\tau^z=+1\\rangle=|\\tau^z=-1\\rangle$ and $\\tau^x|\\tau^z=-1\\rangle=|\\tau^z=+1\\rangle$, and the algebraic relations like $\\{\\tau^x,\\tau^z\\}=0$ are just consequences that follow. As the OP have pointed out, the algebraic relations can not guarantee the representation to be fundamental, it is actually the above physical picture that guarantees the representation of the kink operators. Putting together the above results, we arrive at the Hamiltonian in terms of the kink operators $\\tau_l^x$ and $\\tau_l^z$ $$H=-h\\sum_{l}\\tau^x_l\\tau^x_{l+1}-J\\sum_l\\tau_l^z.$$ One might have been wondering for a while that why I kept using the symbol $\\tau$ instead of $\\mu$ in the original post. Now it is clear that $\\tau$ is still one step away from $\\mu$ by a basis transformation on each link that redefines $\\tau^x=\\mu^z$ and $\\tau^z=\\mu^x$ (relabeling $x\\leftrightarrow z$). Such a unitary transform will not change any physics, but just to bring back the standard form of the transverse field Ising model to accomplish the duality transform, $$H=-h\\sum_{l}\\mu^z_l\\mu^z_{l+1}-J\\sum_l\\mu_l^x.$$ So the duality between $J$ and $h$ is now manifest, but what is the physical meaning of $\\mu_l^z$ indeed? To answer this question, one should first understand that the relation between $\\tau^z$ and $\\tau^x$ is just like that between the coordinate and the momentum. They are related by a $\\mathbb{Z}_2$ version of the Fourier transformation. If we treat the two states of $|\\tau^z=\\pm 1\\rangle$ as two position eigenstates in a two-site system, then the $\\tau^x$ eigenstates $|\\tau^z=+1\\rangle\\pm|\\tau^z=-1\\rangle$ are nothing but the momentum eigenstates with the momentum = $0$ and $\\pi$ respectively. In this sense, we can say $\\mu_l^z\\equiv\\tau_l^x$ is the conjugate momentum of the kink variable $\\tau_l^z$ on each link. In fact, this concept is so important that people invent a name for $\\mu^z$, i.e. the vison variable, s.t. $$\\mu_l^z=\\left\\{\\begin{array}{cc}+1 & \\text{vison off,}\\\\ -1 & \\text{vison on.}\\end{array}\\right.$$ By saying that the vison is the conjugate momentum of the kink, we mean that if there is a vison sitting on a link then the kinked and the unkinked configurations across that link will be differed by a minus sign in the wave function. Unlike the kink which is just another way to encode the spin configuration, the vison does not have a correspondent spin configuration. In fact, the vison configuration is encoded in the relative sign between different spin configurations in the wave function. It represents the inter-relation among the spin configurations other than any particular spin configuration itself, or in other words, the quantum entanglement in the spin wave function. Mathematically this can be seen from the fact that the vison operator $\\mu_l^z$ is non-local in terms of the spin operator $$\\mu_l^z=\\prod_{i<l}\\sigma_i^x=\\tau_l^x,$$ which flips all the spins to its left to create (or annihilate) a kink. Now let us discuss more about this infinite string of $\\sigma^x$ stretching all the way to the left. A first question is that can we \"gauge\" this string to right? This can be done by applying the operator $S\\equiv\\prod_i\\sigma_i^x$, as $$\\mu_l^z\\to S\\mu_l^z= \\prod_{i}\\sigma_i^x\\prod_{i<l}\\sigma_i^x = \\prod_{i>l}\\sigma_i^x.$$ One can see the operator $S$ simply flips all the spins in the system, meaning that it actually implement the global Ising symmetry transformation to the spins $\\sigma_i^z\\to-\\sigma_i^z$. Because $S$ is a symmetry of the Hamiltonian (as $[S,H]=0$), the eigenstates of $H$ are spitted into the even ($S=+1$) and the odd ($S=-1$) sectors. In the even sector, we can gauge the string to the right; while in the odd sector, gauging the string to the right will induce a $\\mathbb{Z}_2$ gauge transformation of the vison $\\mu_l^z\\to-\\mu_l^z$. So combined with the gauge transformation of the vison, the vison string can be made invisible indeed. Then what is the significance of the this vison string? Recall that $\\mu_l^z=\\tau_l^x$ is also the creation/annihilation operator of the the kink. So applying a string of $\\sigma^x$ operators will actually create two kinks at both ends of the string $$\\tau_{l_1}^x\\tau_{l_2}^x=\\prod_{l_1<i<l_2}\\sigma_i^x.$$ The kink is a local excitation of the system (in the $J>h$ phase), so it can be viewed as a particle. From this perspective, we can see emergent particles at the ends of the string, which is exactly one of the central theme of Levin-Wen model and string-net condensation. In fact, there is a very interesting relation between the 1D transverse field Ising model and the 2D Levin-Wen model (with $\\mathbb{Z}_2$ topological order), that the former can be considered as a synthetic dislocation of the latter by anyon condensation, which was described in a paper ( arXiv:1208.4109 ) that I wrote with my friend Chao-Ming and Prof. Wen. We basically showed that the 1D transverse field Ising model can emerge in the 2D $\\mathbb{Z}_2$ topological ordered system as some kind of line defect, where some particular type of the strings between the anyons in 2D will naturally degrade to the vison strings along the emergent 1D Ising chain. So in this sense, the invisible string in the Ising model is really the same invisible string in the string-net condensate (but just confined to the 1D system).",
      "question_latex": [
        "\\mu^x_i",
        "\\mu^z_i",
        "\\mu_i^x",
        "\\mu_i^z",
        "\\sigma^x_i",
        "\\sigma^z_i",
        "\\sigma",
        "\\mu",
        "H(h,J)",
        "H(J,h)",
        "|\\rightarrow\\rangle",
        "|\\uparrow\\uparrow\\rangle",
        "|\\downarrow\\downarrow\\rangle",
        "|\\leftarrow\\rangle"
      ],
      "answer_latex": [
        "\\tau_l^z=\\left\\{\\begin{array}{cc}+1 & \\text{unkinked,}\\\\ -1 & \\text{kinked,}\\end{array}\\right.",
        "\\sigma_i^z=\\sigma_0^z\\prod_{0<l<i}\\tau_l^z.",
        "H=-J\\sum_{i}\\sigma_i^z\\sigma_{i+1}^z-h\\sum_{i}\\sigma_i^x,",
        "-J\\sum_{i}\\sigma_i^z\\sigma_{i+1}^z=-J\\sum_l\\tau_l^z,",
        "-h\\sum_i\\sigma_i^x=-h\\sum_{l}\\tau^x_l\\tau^x_{l+1}.",
        "H=-h\\sum_{l}\\tau^x_l\\tau^x_{l+1}-J\\sum_l\\tau_l^z.",
        "H=-h\\sum_{l}\\mu^z_l\\mu^z_{l+1}-J\\sum_l\\mu_l^x.",
        "\\mu_l^z=\\left\\{\\begin{array}{cc}+1 & \\text{vison off,}\\\\ -1 & \\text{vison on.}\\end{array}\\right.",
        "\\mu_l^z=\\prod_{i<l}\\sigma_i^x=\\tau_l^x,",
        "\\mu_l^z\\to S\\mu_l^z= \\prod_{i}\\sigma_i^x\\prod_{i<l}\\sigma_i^x = \\prod_{i>l}\\sigma_i^x.",
        "\\tau_{l_1}^x\\tau_{l_2}^x=\\prod_{l_1<i<l_2}\\sigma_i^x.",
        "H(J,h)",
        "H(h,J)",
        "J",
        "h",
        "J\\gg h",
        "J\\ll h",
        "\\mathbb{Z}_2",
        "l",
        "<img src=\"https://i.stack.imgur.com/wMc9Z.png\" alt=\"enter image description here\"></p>\n\n<p>If we have specified all the kink configuration",
        "on each link",
        ", we can actually determine the spin configuration",
        "on each site",
        ", with only one additional piece of knowledge about the left-most spin",
        ". The trick is just to accumulate the kink configurations from left all the way to the right,",
        "So the spin configuration is uniquely determined by the kink configuration (up to the left-most spin). If we count the number of states in the Hilbert space, the Hilbert space dimension will be",
        "in the spin language and",
        "in the kink language, where",
        "and",
        "are the number of sites and links respectively, which are equal (apart from the left-most site) on the 1D lattice, so the Hilbert space dimension is actually the same in both languages. In this sense, we can say that the correspondence between the spin and the kink descriptions is almost one-to-one (especially in the thermodynamic limit), even though there might be some complication arising from the left-most boundary (which however is to be ignored in the duality transform, as duality only cares local properties).</p>\n\n<p>Now we can rephrase the original transverse field Ising model",
        "in the kink language. It is not hard to see that the coupling between the Ising spins is just the chemical potential for the kink",
        "which basically follows from the physical meaning of the kink variable",
        ". Admittedly this equality may run into a little trouble on the boundary, where some sites or links might be missing, but they do not affect the local properties deep in the bulk, so we just ignore. The translation of transverse field term is more involved. In the spin language",
        "operator just flips the spin on site",
        ", which would correspond to simultaneous creation or annihilation two kinks on the links adjacent to that site, or moving an existed kink across the site.</p>\n\n<p><img src=\"https://i.stack.imgur.com/1r0Ss.png\" alt=\"flipping the middle spin\"></p>\n\n<p>In either case, flipping a spin would correspond to simultaneously changing the kink variables",
        "on adjacent links, which can be carried out by",
        ", s.t.",
        "Obviously the relation between",
        "is exactly the same as our familiar",
        "Pauli matrices",
        ", and the algebraic relations like",
        "are just consequences that follow. As the OP have pointed out, the algebraic relations can not guarantee the representation to be fundamental, it is actually the above physical picture that guarantees the representation of the kink operators.</p>\n\n<p>Putting together the above results, we arrive at the Hamiltonian in terms of the kink operators",
        "One might have been wondering for a while that why I kept using the symbol",
        "instead of",
        "in the original post. Now it is clear that",
        "is still one step away from",
        "by a basis transformation on each link that redefines",
        "(relabeling",
        "). Such a unitary transform will not change any physics, but just to bring back the standard form of the transverse field Ising model to accomplish the duality transform,",
        "So the duality between",
        "is now manifest, but what is the physical meaning of",
        "indeed? To answer this question, one should first understand that the relation between",
        "is just like that between the coordinate and the momentum. They are related by a",
        "version of the Fourier transformation. If we treat the two states of",
        "as two position eigenstates in a two-site system, then the",
        "eigenstates",
        "are nothing but the momentum eigenstates with the momentum =",
        "respectively. In this sense, we can say",
        "is the conjugate momentum of the kink variable",
        "on each link. In fact, this concept is so important that people invent a name for",
        ", i.e. the <strong>vison</strong> variable, s.t.",
        "By saying that the vison is the conjugate momentum of the kink, we mean that if there is a vison sitting on a link then the kinked and the unkinked configurations across that link will be differed by a minus sign in the wave function. Unlike the kink which is just another way to encode the spin configuration, the vison does not have a correspondent spin configuration. In fact, the vison configuration is encoded in the relative sign between different spin configurations in the wave function. It represents the inter-relation among the spin configurations other than any particular spin configuration itself, or in other words, the quantum entanglement in the spin wave function.</p>\n\n<p>Mathematically this can be seen from the fact that the vison operator",
        "is non-local in terms of the spin operator",
        "which flips all the spins to its left to create (or annihilate) a kink. Now let us discuss more about this infinite string of",
        "stretching all the way to the left. A first question is that can we \"gauge\" this string to right? This can be done by applying the operator",
        ", as",
        "One can see the operator",
        "simply flips all the spins in the system, meaning that it actually implement the global Ising symmetry transformation to the spins",
        ". Because",
        "is a symmetry of the Hamiltonian (as",
        "), the eigenstates of",
        "are spitted into the even (",
        ") and the odd (",
        ") sectors. In the even sector, we can gauge the string to the right; while in the odd sector, gauging the string to the right will induce a",
        "gauge transformation of the vison",
        ". So combined with the gauge transformation of the vison, the vison string can be made invisible indeed. Then what is the significance of the this vison string? Recall that",
        "is also the creation/annihilation operator of the the kink. So applying a string of",
        "operators will actually create two kinks at both ends of the string",
        "The kink is a local excitation of the system (in the",
        "phase), so it can be viewed as a particle. From this perspective, we can see emergent particles at the ends of the string, which is exactly one of the central theme of Levin-Wen model and string-net condensation. </p>\n\n<p>In fact, there is a very interesting relation between the 1D transverse field Ising model and the 2D Levin-Wen model (with",
        "topological order), that the former can be considered as a synthetic dislocation of the latter by anyon condensation, which was described in a paper (<a href=\"http://arxiv.org/abs/1208.4109\">arXiv:1208.4109</a>) that I wrote with my friend Chao-Ming and Prof. Wen. We basically showed that the 1D transverse field Ising model can emerge in the 2D"
      ],
      "created": "2014-09-12T00:53:00.710",
      "golden_ner_terms": [
        "adjacent",
        "algebra",
        "algebraic",
        "angular momentum",
        "anyons",
        "basis",
        "boundary",
        "boundary condition",
        "boundary conditions",
        "central",
        "chain",
        "charge",
        "chemical potential",
        "clear",
        "concept",
        "condensation",
        "configuration",
        "conjugate",
        "coordinate",
        "defect",
        "dimension",
        "duality",
        "eigenvalues",
        "equality",
        "equation",
        "even",
        "field",
        "focus",
        "fourier transform",
        "function",
        "gauge",
        "gauge theory",
        "ground state",
        "hamiltonian",
        "hilbert space",
        "implication",
        "induce",
        "infinite",
        "ising model",
        "language",
        "lattice",
        "lattice gauge theory",
        "limit",
        "line",
        "link",
        "mapping",
        "matrix",
        "mean",
        "model",
        "momentum",
        "number",
        "observation",
        "odd",
        "one-to-one",
        "operator",
        "operators",
        "opposite",
        "order",
        "pauli matrices",
        "phase transition",
        "physics",
        "potential",
        "quantum entanglement",
        "relation",
        "representation",
        "represents",
        "right",
        "right hand side",
        "satisfy",
        "sector",
        "side",
        "simple",
        "site",
        "space",
        "spectrum",
        "state",
        "step",
        "string",
        "symmetry",
        "term",
        "theory",
        "topological order",
        "transform",
        "transformation",
        "translation",
        "transverse",
        "type",
        "unitary",
        "useful",
        "variable",
        "wave function",
        "way"
      ],
      "golden_ner_count": 92,
      "golden_patterns": [
        {
          "pattern": "encode-as-algebra",
          "score": 8.0,
          "hotwords": [
            "algebra",
            "algebraic",
            "ring",
            "encode"
          ]
        },
        {
          "pattern": "local-to-global",
          "score": 6.0,
          "hotwords": [
            "local",
            "global",
            "locally"
          ]
        },
        {
          "pattern": "work-examples-first",
          "score": 2.0,
          "hotwords": [
            "e.g."
          ]
        },
        {
          "pattern": "argue-by-contradiction",
          "score": 2.0,
          "hotwords": [
            "impossible"
          ]
        },
        {
          "pattern": "find-the-right-abstraction",
          "score": 2.0,
          "hotwords": [
            "perspective"
          ]
        },
        {
          "pattern": "quotient-by-irrelevance",
          "score": 2.0,
          "hotwords": [
            "up to"
          ]
        },
        {
          "pattern": "check-the-extreme-cases",
          "score": 2.0,
          "hotwords": [
            "boundary"
          ]
        },
        {
          "pattern": "exploit-symmetry",
          "score": 2.0,
          "hotwords": [
            "symmetry"
          ]
        },
        {
          "pattern": "dualise-the-problem",
          "score": 2.0,
          "hotwords": [
            "dual"
          ]
        },
        {
          "pattern": "unfold-the-definition",
          "score": 2.0,
          "hotwords": [
            "recall that"
          ]
        },
        {
          "pattern": "estimate-by-bounding",
          "score": 2.0,
          "hotwords": [
            "bound"
          ]
        },
        {
          "pattern": "monotone-approximation",
          "score": 2.0,
          "hotwords": [
            "limit"
          ]
        }
      ],
      "golden_pattern_names": [
        "encode-as-algebra",
        "local-to-global",
        "work-examples-first",
        "argue-by-contradiction",
        "find-the-right-abstraction",
        "quotient-by-irrelevance",
        "check-the-extreme-cases",
        "exploit-symmetry",
        "dualise-the-problem",
        "unfold-the-definition",
        "estimate-by-bounding",
        "monotone-approximation"
      ],
      "golden_scopes": [],
      "golden_scope_count": 0
    },
    {
      "id": "se-physics-678127",
      "stratum": "easy",
      "title": "Doesn't Veritasium's Recent Video About Circuits Violate The Speed Of Light?",
      "tags": [
        "special-relativity",
        "electromagnetic-radiation",
        "electric-circuits",
        "causality",
        "faster-than-light"
      ],
      "score": 24,
      "answer_score": 16,
      "question_body": "A recent Veritasium video discusses the following circuit: ,_____|L|_____, _ | | | 1m \\_____|B|_/ __/ - |-------------| 1 lightyear L = Light bulb B = Battery His claim is that when the switch is closed it takes $\\sim1/c$ seconds for the light bulb to turn on (since the light is turned on by the magnetic field whose origin is only $1$ m away from the light bulb). My question is, doesn't that violate the transfer of information at the speed of light? If you close the circuit, the light bulb would turn on, but if you move the switch so it's an equal distance from the light source and battery, then presumably you would be able to transfer information (switch is opened vs. closed) to points L/B at faster than the speed of light. What am I not understanding?",
      "answer_body": "This is a transmission line problem. Transmission-line problems can be counterintuitive for people who are used to thinking about the moving charges in circuit but are used to ignoring the fields. ( One example here ). Let’s imagine that the long cable to nowhere is not pair of parallel wires, but instead that the \"outgoing\" and \"return\" wires are wrapped around an iron core, like a transformer . In the approximation that the freshly-connected battery is only pushing current along the outgoing wire, this high- inductance makes it a little more obvious that the current cannot instantaneously start moving down the entire outgoing wire. The basic principle of an inductor is that the magnetic field has some intrinsic \"inertia.\" If the magnetic field through some loop isn't constant, an electromotive force develops around the boundary of the loop; charges which are free to move under the influence of this e.m.f. will generate a current which tries to prevent the magnetic field from changing. If you treat the outgoing wire, wrapped around an iron core, as a single solenoid, each coil in the solenoid will be fighting the changing magnetic fields as the current changes in the neighboring coils. But if the wires for the \"outgoing\" and \"returning\" currents are wrapped around the same magnetizable core, there's a much easier way for the entire system to fight the changes in the magnetic field: the outgoing current can induce a current in the return wire. (Of course I already gave away the game by talking about transformers .) Note that the direction of the return current, in this transformer-transmission-line model, depends on whether the two coils are wound with the same handedness or not. But the current in the outgoing wire will unquestionably induce some nonzero current in the return wire. Now let's unwind our two wires and return to the parallel-cable setup from the video. Unwinding the wires from the imaginary transformer, and removing the imaginary magnetizable core, dramatically reduces the mutual inductance between them —— but that inductance is still nonzero . Pick any two points along your lightyear-long cable, and the gap between the two wires makes an Amperian loop: A B ,_____|L|__________________⋮________⋮___________, _ | ⋮ ⋮ | | 1m \\_____|B|_/ _______________⋮________⋮___________/ - ⋮ ⋮ C D If the battery changes the current along the segment $CD$ from zero to not-zero, the magnetic flux changes in the loop $\\mathit{ACDB}$ , and the electromotive force associated with this changing flux will induce an opposing current in the segment $BA$ . This is happening in both arms, and the induced current flows through the lamp, which illuminates. Now the amount of current that flows through the lamp, and the time it takes to reach a steady state, will depend in a complicated way on the transmission-line characteristics of the cable: its inductance, capacitance, series resistance, and shunt conductance (all traditionally measured per unit length). The point of the linked video is to remind you (or teach you) to think about these circuit properties in terms of their fields, rather than only in terms of the motions of charges. but if you move the switch so it's an equal distance from the light source and battery? I'm not 100% sure I understand your proposal here. But suppose there is a second switch, which is also initially open, located many light-seconds down the transmission line: ,_____|L|_____________________________________, _ | | | 1m \\_____|B|_/ _________________________/ _______/ - S1 S2 If we close $S_1$ and leave $S_2$ open, the signal will propagate down the cable as before until it reaches $S_2$ . The open switch $S_2$ will effectively function as a series capacitance, which will change the impedance of that section of cable. At an impedance mismatch in a transmission line, part of the signal is transmitted and part of it is reflected. However, the signal that \" $S_1$ has been opened\" has already been propagating down the cable, with current in both wires, and the lamp has already illuminated. Likewise if we close $S_2$ but leave $S_1$ open, that signal will propagate to the left down the cable from the location of $S_2$ . In this geometry, the lamp will be illuminated by the part of the signal transmitted through the impedance mismatch at $S_1$ . Electromagnetism is local. Think about the fields. For what it’s worth, I did nearly this exact experiment (mostly by accident) back when I was a postdoctoral researcher. The context was sending pulses with a width of only a few nanoseconds down a cable with a propagation time of a few microseconds, to an LED next to a photomultiplier tube, then collecting a few-photon response from the detector a few microseconds later. My function generator output was connected by a “tee” to an oscilloscope; the cable to the scope was short, but still longer than the nanosecond rise/fall time of the brief pulse. So my setup was something like ,_________|f.g.|_____________________________, |scope| |LED| \\_________|ground|___________________________/ One of the things I did as I was building this setup was to send long (millisecond) pulses, so that microsecond-scale transients had time to dissipate and the system entered a steady state. That’s roughly analogous to the Veritasium scenario of connecting the battery and waiting a year for the current to get to the end of the lightyear-long cable. If a millisecond-long pulse has a nanosecond rise (or fall) time, that leading (or trailing) edge and its reflections behave in the same way as the leading (or trailing) edge of the few-nanosecond pulse. I might reproduce the thought experiment here by making the following changes: Introduce the battery in series with the center conductors of the coaxial cables, using a box like this one . A ground-isolated bipolar d.c. power supply might be easier to connect, but its complexity would distract a skeptical observer. In another box (or perhaps even the same box), put a small series resistance in series with the sheath conductors of the coaxial cables. Connect an oscilloscope (in its high-input-impedance mode) in parallel with this resistor. Ta-da, a fast ammeter. In a third box, perhaps with three inputs, an isolated transistor switch driven by the fast function generator between “conducting” and “insulating.” Complete the circuit by putting a zero-resistance terminator at the ends of the long cables. If I were actually doing the experiment, I’d also try swallowing the echos using an impedance-matched terminator, as well as leaving the cable ends open to demonstrate the current flows in both conductors until the reflection arrives from the open end. The prediction in the video, as clarified by this question, is that if the resistor in series with the sheath conductor is near the switch, the current in that resistor will change when the state of the switch changes, in addition to when echos of the state change arrive from the ends of the long cables. Also, the timing of that current is essentially the same whether the switch interrupts the center conductor or the sheath conductor of the cable. If I still worked at a place where I had all of these fast electronics lying around, I could probably put together a demo in an afternoon or two. (There would be some tedious issues related to the practice of fast electronics sharing a common ground.) Unfortunately I disassembled this setup a decade ago.",
      "question_latex": [
        "\\sim1/c",
        "1"
      ],
      "answer_latex": [
        "CD",
        "\\mathit{ACDB}",
        "BA",
        "S_1",
        "S_2"
      ],
      "created": "2021-11-19T20:48:56.320",
      "golden_ner_terms": [
        "addition",
        "approximation",
        "basic",
        "boundary",
        "capacitance",
        "center",
        "circuit",
        "closed",
        "complete",
        "conductor",
        "conductors",
        "connected",
        "constant",
        "context",
        "core",
        "current",
        "distance",
        "edge",
        "electromagnetism",
        "electronics",
        "entire",
        "even",
        "field",
        "flux",
        "function",
        "game",
        "generate",
        "generator",
        "geometry",
        "imaginary",
        "induce",
        "induced",
        "inductance",
        "inertia",
        "information",
        "isolated",
        "length",
        "line",
        "loop",
        "magnetic fields",
        "mode",
        "model",
        "near",
        "obvious",
        "open",
        "origin",
        "parallel",
        "place",
        "point",
        "power",
        "reflection",
        "scenario",
        "scope",
        "section",
        "segment",
        "series",
        "source",
        "speed",
        "speed of light",
        "state",
        "steady state",
        "thought experiment",
        "time",
        "unit",
        "way",
        "width",
        "zero"
      ],
      "golden_ner_count": 67,
      "golden_patterns": [
        {
          "pattern": "check-the-extreme-cases",
          "score": 4.0,
          "hotwords": [
            "boundary",
            "zero"
          ]
        },
        {
          "pattern": "work-examples-first",
          "score": 2.0,
          "hotwords": [
            "example"
          ]
        },
        {
          "pattern": "construct-an-explicit-witness",
          "score": 2.0,
          "hotwords": [
            "build"
          ]
        },
        {
          "pattern": "local-to-global",
          "score": 2.0,
          "hotwords": [
            "local"
          ]
        },
        {
          "pattern": "encode-as-algebra",
          "score": 2.0,
          "hotwords": [
            "ring"
          ]
        },
        {
          "pattern": "construct-auxiliary-object",
          "score": 2.0,
          "hotwords": [
            "introduce"
          ]
        },
        {
          "pattern": "estimate-by-bounding",
          "score": 2.0,
          "hotwords": [
            "bound"
          ]
        },
        {
          "pattern": "optimise-a-free-parameter",
          "score": 2.0,
          "hotwords": [
            "pick"
          ]
        }
      ],
      "golden_pattern_names": [
        "check-the-extreme-cases",
        "work-examples-first",
        "construct-an-explicit-witness",
        "local-to-global",
        "encode-as-algebra",
        "construct-auxiliary-object",
        "estimate-by-bounding",
        "optimise-a-free-parameter"
      ],
      "golden_scopes": [],
      "golden_scope_count": 0
    },
    {
      "id": "se-physics-492711",
      "stratum": "easy",
      "title": "What's the physical meaning of the statement that \"photons don't have positions\"?",
      "tags": [
        "quantum-mechanics",
        "electromagnetism",
        "quantum-field-theory"
      ],
      "score": 71,
      "answer_score": 58,
      "question_body": "It's been mentioned elsewhere on this site that one cannot define a position operator for the one-photon sector of the quantized electromagnetic field, if one requires the position operator have certain formal properties. This is a theorem that holds only for massless particles of helicity $|\\lambda| \\geq 1$ , in particular it does not apply to massless scalars. A lot of people, particularly mathematical physicists or older quantum field theory textbooks, seem to interpret this to mean that we should never speak of the position of anything in relativistic quantum field theory. But it still seems possible to say something about where a photon is. For example, if I have an ideal cavity and excite the lowest mode with one photon, I know that the photon is in that cavity. Furthermore, I can localize the photon arbitrarily well using smaller and smaller cavities. When an optics experiment is done using a laser beam, it is perfectly meaningful to talk about photons being in the beam. We can also speak of a photon being emitted by an atom, in which case it is obviously localized near the atom when the emission occurs. Furthermore, in the usual analysis of the double slit experiment one has, at least implicitly, a wavefunction for the photon, which successfully recovers the high school result. When one talks about scattering experiments, such as in photon-photon scattering, one has to talk about localized wavepackets in order to describe a real beam. Furthermore, unlike the massive case, where the Compton wavelength provides a characteristic length, there is no characteristic length for photons, suggesting that beams can be made arbitrarily narrow in principle: the complaint that you would start causing pair production below the Compton wavelength doesn't apply. In other words, while the theorem is airtight, it doesn't seem to impose any practical limitations on things we would actually like to do experimentally. But you can find very strange-sounding descriptions of what this theorem is telling us online. For example, on PhysicsForums you can read many obviously wrong statements (e.g. here and here and here ) such as: The photon has no rest frame. Computing an expectation of position for such an object is nonsense. One good reason is that photons are massless and move at the speed of light and have no rest frame! Then also they are bosons, so you can't tell which are which. These are wrong because they also apply to massless scalars, for which there does exist a (Newton-Wigner) position operator. It also just doesn't make sense -- if you can't measure the position of something if you're not in its rest frame, then how can I catch a ball? In relativistic quantum (field) theory there is no concept of single photons. You cannot define \"position\" for an electromagnetic field or of photons, which are certain states of this field (namely single-photon Fock states). Nobody thinking about classical electromagnetic waves would ever come to the idea to ask, what the position of a field might be. This is wrong because the one-particle sector of a quantum field theory is perfectly well-defined, and it is perfectly valid to define operators acting on it alone. It can be shown that in the context of relativistic quantum theory the position operator leads to violations of causality. This is rather vague because quantum field theory is causal, so it's unclear how \"the position operator\" overturns that. It could just be that PhysicsForums is an exceptionally low-quality site, but I think the real problem is that interpreting this theorem is actually quite tricky. What nontrivial physical consequences does the nonexistence of a formal photon position operator have?",
      "answer_body": "We could spend forever playing whac-a-mole with all of the confusing/confused statements that continue popping up on this subject, on PhysicsForums and elsewhere. Instead of doing that, I'll offer a general perspective that, for me at least, has been refreshingly clarifying. I'll start by reviewing a general no-go result, which applies to all relativistic QFTs, not just to photons. Then I'll explain how the analogous question for electrons would be answered, and finally I'll extend the answer to photons. The reason for doing this in that order will probably be clear in hindsight. A general no-go result First, here's a review of the fundamental no-go result for relativistic QFT in flat spacetime: In QFT, observables are associated with regions of spacetime (or just space, in the Schrödinger picture). This association is part of the definition of any given QFT. In relativistic QFT, the Reeh-Schlieder theorem implies that an observable localized in a bounded region of spacetime cannot annihilate the vacuum state. Intuitively, this is because the vacuum state is entangled with respect to location. Particles are defined relative to the vacuum state. By definition, the vacuum state has zero particles, so the Reeh-Schlieder theorem implies that an observable representing the number of particles in a given bounded region of spacetime cannot exist: if an observable is localized in a bounded region of spacetime, then it can't always register zero particles in the vacuum state. That's the no-go result, and it's very general. It's not restricted to massless particles or to particles of helicity $\\geq 1$ . For example, it also applies to electrons. The no-go result says that we can't satisfy both requirements: in relativistic QFT, we can't have a detector that is both perfectly reliable, localized in a strictly bounded region. But here's the important question: how close can we get to satisfying both of these requirements? Warm-up: electrons First consider the QFT of non-interacting electrons, with Lagrangian $L\\sim \\overline\\psi(i\\gamma\\partial+m)\\psi$ . The question is about photons, and I'll get to that, but let's start with electrons because then we can use the electron mass $m$ to define a length scale $\\hbar/mc$ to which other quantities can be compared. To construct observables that count electrons, we can use the creation/annihilation operators. We know from QFT $101$ how to construct creation/annihilation operators from the Dirac field operators $\\psi(x)$ , and we know that this relationship is non-local (and non-localizable) because of the function $\\omega(\\vec p) = (\\vec p^2+m^2)^{1/2}$ in the integrand, as promised by Reeh-Schlieder. However, for electrons with sufficiently low momentum, this function might as well be $\\omega\\approx m$ . If we replace $\\omega\\to m$ in the integrand, then the relationship between the creation/annihilation operators becomes local. Making this replacement changes the model from relativistic to non-relativistic, so the Reeh-Schlieder theorem no longer applies. That's why we can have electron-counting observables that satisfy both of the above requirements in the non-relativistic approximation. Said another way: Observables associated with mutually spacelike regions are required to commute with each other (the microcausality requirement). The length scale $\\hbar/mc$ is the scale over which commutators of our quasi-local detector-observables fall off with increasing spacelike separation. Since the non-zero tails of those commutators fall off exponentially with characteristic length $\\hbar/mc$ , we won't notice them in experiments that have low energy/low resolution compared to $\\hbar/mc$ . Instead of compromising strict localization, we can compromise strict reliability instead: we can construct observables that are localized in a strictly bounded region and that almost annihilate the vacuum state. Such an observable represents a detector that is slightly noisy. The noise is again negligible for low-resolution detectors — that is, for detector-observables whose localization region is much larger than the scale $\\hbar/mc$ . This is why non-relativistic few-particle quantum mechanics works — for electrons. Photons Now consider the QFT of the electromagnetic field by itself, which I'll call QEM. All of the observables in this model can be expressed in terms of the electric and magnetic field operators, and again we know from QFT $101$ how to construct creation/annihilation operators that define what \"photon\" means in this model: they are the positive/negative frequency parts of the field operators. This relationship is manifestly non-local. We can see this from the explicit expression, but we can also anticipate it more generally: the definition of positive/negative frequency involves the infinite past/future, and thanks to the time-slice principle , this implies access to arbitrarily large spacelike regions. In QEM, there is no characteristic scale analogous to $\\hbar/mc$ , because $m=0$ . The ideas used above for electrons still work, except that the deviations from localization and/or reliability don't fall off exponentially with any characteristic scale. They fall of like a power of the distance instead. As far as this question is concerned, that's really the only difference between the electron case and the photon case. That's enough of a difference to prevent us from constructing a model for photons that is analogous to non-relativistic quantum mechanics for electrons, but it's not enough of a difference to prevent photon-detection observables from being both localized and reliable for most practical purposes. The larger we allow its localization region to be, the more reliable (less noisy) a photon detector can be. Our definition of how-good-is-good-enough needs to be based on something else besides QEM itself, because QEM doesn't have any characteristic length-scale of its own. That's not an obstacle to having relatively well-localized photon-observables in practice, because there's more to the real world than QEM. Position operators What is a position operator? Nothing that I said above refers to such a thing. Instead, everything I said above was expressed in terms of observables that represent particle detectors (or counters). I did that because the starting point was relativistic QFT, and QFT is expressed in terms of observables that are localized in bounded regions. Actually, non-relativistic QM can also be expressed that way. Start with the traditional formulation in terms of the position operator $X$ . (I'll consider only one dimension for simplicity.) This single operator $X$ is really just a convenient way of packaging-and-labeling a bunch of mutually-commuting projection operators, namely the operators $P(R)$ that project a wavefunction $\\Psi(x)$ onto the part with $x\\in R$ , cutting off the parts with $x\\notin R$ . In fancy language, the commutative von Neumann algebra generated by $X$ is the same as the commutative von Neumann algebra generated by all of the $P(R)$ s, so aside from how things are labeled with \"eigenvalues,\" they both represent the same observable as far as Born's rule is concerned. If we look at how non-relativistic QM is derived from its relativistic roots, we see that the $P(R)$ s are localized within the region $R$ by QFT's definition of \"localized\" — at least insofar as the non-relativistic approximation is valid. In this sense, non-relativistic single-particle QM is, like QFT, expressed in terms of observables associated with bounded regions of space. The traditional formulation of single-particle QM obscures this. Here's the point: when we talk about a position operator for an electron in a non-relativistic model, we're implicitly talking about the projection operators $P(R)$ , which are associated with bounded regions of space. The position operator $X$ is a neat way of packaging all of those projection operators and labeling them with a convenient spatial coordinate, so that we can use concise statistics like means and standard deviations, but you can't have $X$ without also having the projection operators $P(R)$ , because the existence of the former implies the existence of the latter (through the spectral theorem or, through the von-Neumann-algebra fanciness that I mentioned above). So... can a photon have a position operator? If by position operator we mean something like the projection operators $P(R)$ , which are both (1) localized in a strictly bounded region and (2) strictly reliable as \"detectors\" of things in that region, then the answer is no. A photon can't have a position operator for the same reason that a photon can't have a non-relativistic approximation: for a photon, there is no characteristic length scale analogous to $\\hbar/mc$ to which the size of a localization region can be compared, without referring to something other than the electromagnetic field itself. What we can do is use the usual photon creation/annihilation operators to construct photon-detecting/counting observables that are not strictly localized in any bounded region but whose \"tails\" are negligible compared to anything else that we care about (outside of QEM), if the quasi-localization region is large enough. What is a physical consequence? What is a physical consequence of the non-existence of a strict position operator? Real localized detectors are necessarily noisy. The more localized they are, the noisier they must be. Reeh-Schlieder guarantees this, both for electrons and for photons, the main difference being that for electrons, the effect decreases exponentially as the size of the localization region is increased. For photons, it decreases only like a power of the size.",
      "question_latex": [
        "|\\lambda| \\geq 1"
      ],
      "answer_latex": [
        "\\geq 1",
        "L\\sim \\overline\\psi(i\\gamma\\partial+m)\\psi",
        "m",
        "\\hbar/mc",
        "101",
        "\\psi(x)",
        "\\omega(\\vec p) = (\\vec p^2+m^2)^{1/2}",
        "\\omega\\approx m",
        "\\omega\\to m",
        "m=0",
        "X",
        "P(R)",
        "\\Psi(x)",
        "x\\in R",
        "x\\notin R",
        "R"
      ],
      "created": "2019-07-20T17:05:04.107",
      "golden_ner_terms": [
        "algebra",
        "analysis",
        "approximation",
        "atom",
        "ball",
        "bosons",
        "bounded",
        "causality",
        "characteristic",
        "clear",
        "commutative",
        "concept",
        "consequence",
        "construct",
        "context",
        "coordinate",
        "difference",
        "dimension",
        "distance",
        "double slit experiment",
        "eigenvalues",
        "electrons",
        "entangled",
        "expectation",
        "expression",
        "field",
        "field theory",
        "flat",
        "frame",
        "frequency",
        "function",
        "generated by",
        "helicity",
        "ideal",
        "implies",
        "increasing",
        "infinite",
        "integrand",
        "labeling",
        "lagrangian",
        "language",
        "laser",
        "length",
        "localization",
        "mass",
        "mean",
        "measure",
        "mode",
        "model",
        "momentum",
        "near",
        "noise",
        "number",
        "object",
        "observables",
        "onto",
        "operator",
        "operators",
        "optics",
        "order",
        "pair production",
        "particle detectors",
        "photons",
        "point",
        "power",
        "production",
        "project",
        "projection",
        "qft",
        "quantum field theory",
        "quantum mechanics",
        "real",
        "region",
        "represents",
        "satisfy",
        "scattering",
        "sector",
        "separation",
        "site",
        "size",
        "space",
        "spacetime",
        "spectral theorem",
        "speed",
        "speed of light",
        "standard deviation",
        "state",
        "statistics",
        "strict",
        "strictly",
        "theorem",
        "theory",
        "vacuum",
        "valid",
        "von neumann algebra",
        "wavefunction",
        "wavelength",
        "waves",
        "way",
        "well-defined",
        "work",
        "zero"
      ],
      "golden_ner_count": 102,
      "golden_patterns": [
        {
          "pattern": "construct-an-explicit-witness",
          "score": 4.0,
          "hotwords": [
            "construct",
            "explicit"
          ]
        },
        {
          "pattern": "encode-as-algebra",
          "score": 4.0,
          "hotwords": [
            "algebra",
            "ring"
          ]
        },
        {
          "pattern": "unfold-the-definition",
          "score": 4.0,
          "hotwords": [
            "by definition",
            "definition of"
          ]
        },
        {
          "pattern": "estimate-by-bounding",
          "score": 4.0,
          "hotwords": [
            "bound",
            "at least"
          ]
        },
        {
          "pattern": "work-examples-first",
          "score": 2.0,
          "hotwords": [
            "example"
          ]
        },
        {
          "pattern": "find-the-right-abstraction",
          "score": 2.0,
          "hotwords": [
            "perspective"
          ]
        },
        {
          "pattern": "check-the-extreme-cases",
          "score": 2.0,
          "hotwords": [
            "zero"
          ]
        },
        {
          "pattern": "local-to-global",
          "score": 2.0,
          "hotwords": [
            "local"
          ]
        }
      ],
      "golden_pattern_names": [
        "construct-an-explicit-witness",
        "encode-as-algebra",
        "unfold-the-definition",
        "estimate-by-bounding",
        "work-examples-first",
        "find-the-right-abstraction",
        "check-the-extreme-cases",
        "local-to-global"
      ],
      "golden_scopes": [
        {
          "type": "set-notation",
          "match": "$x\\in R$"
        }
      ],
      "golden_scope_count": 1
    },
    {
      "id": "se-physics-673818",
      "stratum": "medium",
      "title": "Change of Basis in quantum mechanics using Bra-Ket notation",
      "tags": [
        "quantum-mechanics",
        "hilbert-space",
        "operators",
        "wavefunction",
        "schroedinger-equation"
      ],
      "score": 5,
      "answer_score": 3,
      "question_body": "Suppose I have a function $u=g(x)$ , and I want to create a basis $|u\\rangle$ out of this. If I know the form of some wavefunction $\\psi$ in a known basis, say $|x\\rangle$ , how can I find the form of the wavefunction in this new basis ? For example, suppose $u=x^2$ . We have $\\psi(x)=\\langle x|\\psi\\rangle=\\sin(x)$ , for example. Basic substitution tells us that in the basis $u=x^2$ , this wavefunction should have the form $\\phi(u) = \\langle u|\\psi\\rangle = \\sin(\\sqrt{u})$ instead. Remember, in this problem, my wave function didn't evolve or change. It is the same, just represented in two different basis. In basic substitution, we have $u=g(x)$ , and $\\psi(x)=\\psi(g^{-1}(u))=\\phi(u)$ How do I realize this using Dirac Notation? For example, $\\langle u|\\psi\\rangle = \\int dx \\langle u|x\\rangle\\langle x|\\psi\\rangle = \\int dx\\langle u|x\\rangle\\sin(x)$ . However, I don't see how shall I get the $\\phi(u)=\\sin(\\sqrt{u})$ form from here. The transformation between the position and momentum basis is rather simple as it is related by a Fourier transform. However, it is difficult to visualize this simple transformation from a basis $x$ to a basis $g(x)$ , using the Bra Ket notation. Any help would be highly appreciated.",
      "answer_body": "This is a long post. Beware! But I think it answers the OP's concerns. Later on, I'll try to post a tldr. Preliminaries: promoting $u$ to an operator I don't think of the transformation $u=g(x)$ as actually a change of basis, for the following reason. One way to interpret the relationship $u=g(x)$ is as a relationship between the eigenvalues of two operators. That is, there is an operatorial relationship, given by $$ \\hat{u} = g(\\hat{x})\\,, $$ and the eigenbasis of $\\hat{u}$ is exactly the same as the eigenbasis for $\\hat{x}$ , because $$ \\hat{u}\\lvert x\\rangle=g(\\hat{x})\\lvert x\\rangle=g(x)\\lvert x\\rangle = u\\lvert x\\rangle\\,. $$ In this case, the transformation from one basis to another is not really a transformation at all, it is really a re-labeling (which is basically the same as a re-ordering of the basis, which is a trivial kind of basis change). For clarity, let's consider the operator $\\hat{u}=\\hat{x}^3$ . Then, we have $$ \\hat{u}\\lvert x\\rangle = \\hat{x}^3\\lvert x\\rangle = x^3\\lvert x\\rangle = u\\lvert x\\rangle\\,. $$ The basis is singly degenerate because the function $x^3$ is injective, and so we can label these vectors with the eigenvalues of as \\begin{align} \\lvert u\\rangle = \\lvert x^3\\rangle\\,, \\end{align} where $u=x^3$ , uniquely. Alternatively, we can write this as \\begin{align} \\lvert x\\rangle &= \\lvert \\sqrt[3]{u}\\rangle\\,. \\end{align} The ambiguity comes in because we are dealing with a continuous basis, so let's consider two separate cases. $\\lvert x\\rangle$ is a discrete basis For the moment, let's pretend that the basis is actually discrete, and let $x$ take on integer values between $-\\infty$ and $\\infty$ . In that case, we can expand an arbitrary state $\\lvert \\psi\\rangle$ in this basis. Starting with the $x$ basis, we have \\begin{align} \\lvert \\psi \\rangle &= \\sum_{x=-\\infty}^{\\infty} \\lvert x\\rangle\\langle x| \\psi\\rangle = \\sum_{x=-\\infty}^{\\infty} \\lvert \\sqrt[3]{u}\\rangle\\langle \\sqrt[3]{u}| \\psi\\rangle = \\sum_{u} \\lvert \\sqrt[3]{u}\\rangle\\langle \\sqrt[3]{u}| \\psi\\rangle\\,. \\end{align} Note that in these sums, $x$ takes on integer values, whereas $u$ takes on only cubed -integer values. Despite the re-labeling, nothing has really changed, because $\\langle \\sqrt[3]{u}| \\psi\\rangle = \\langle x|\\psi\\rangle$ . The expansion coefficients haven't changed; instead, we've just rearranged them. We could also choose to re-label the state $\\langle \\sqrt[3]{u}| \\psi\\rangle\\to\\langle u| \\psi\\rangle$ , so that we are actually labeling the state with the eigenvalue of the operator $\\hat{u}$ , but to make the connection to the continuous case, I'll leave it as is. $\\lvert x\\rangle$ is a continuous basis Now, let's do the infinite case. Starting with the $x$ basis, we make the substitution $u=x^3$ , yielding \\begin{align} \\lvert x \\rangle &= \\int_{-\\infty}^{\\infty}dx\\, \\lvert x\\rangle\\langle x| \\psi\\rangle = \\int_{-\\infty}^{\\infty}\\left(\\frac{1}{3u^{2/3}}\\,du\\right) \\lvert \\sqrt[3]{u}\\rangle\\langle \\sqrt[3]{u}| \\psi\\rangle\\,. \\end{align} Comparing to the above, we can see that $\\langle \\sqrt[3]{u}| \\psi\\rangle$ and $\\langle x| \\psi\\rangle$ are equal to each other as numbers (ignore the interpretation of them as functions of $x$ or $u$ for the moment). We've done nothing but re-parameterize these expressions, so they are still equal. We can see that the big difference is in the integration measure $dx\\to\\frac{1}{3u^{2/3}}\\,du$ . Part of the question is about where is the \"proper\" place for the factor of $\\frac{1}{3u^{2/3}}$ . There are a few ways to go about interpreting what we have. The first is to recognize that we have again just written the identity operator in a different way, i.e., $$ \\hat{I} = \\int_{-\\infty}^{\\infty}\\left(\\frac{1}{3u^{2/3}}\\,du\\right) \\lvert \\sqrt[3]{u}\\rangle\\langle \\sqrt[3]{u}|\\,, $$ since it clearly hits $\\lvert \\psi\\rangle$ and makes $\\lvert \\psi\\rangle$ . We don't have to worry about the meaning of wave functions at all. Instead, we just recognize that this is another way to resolve the identity, and so it's just another way to re-write our state. Thus, $\\frac{1}{3u^{2/3}}$ stays with the integration measure. The problem with this is that we also want to interpret what we have in a physical context, and we know that the interpretations here all about probabilities. Thus, we look at the second way to make sense of this. A brief important digression on the discrete case In the discrete case, there's no problem of interpretation, because there's no change in the \"integration measure\" in the discrete case, and so $$ |\\langle \\sqrt[3]{u}| \\psi\\rangle|^2= |\\langle x|\\psi\\rangle|^2 $$ is just the probability that if we get the result $u=x^3$ upon measuring the observable $\\hat{u}$ , which is the same probability as getting $x$ if we measure the operator $\\hat{x}$ . A second brief important digression on terminology The symbol $\\psi$ is reserved for the name of the state represented by the normalized ket $\\vert\\psi\\rangle$ . Every time I write a ket, it will mean a normalized ket in the abstract Hilbert space. This ensures that things remain as consistent as they can between the discrete and continuous cases. It also means that wherever $\\frac{1}{3}\\sqrt[3]{u}$ shows up, it will never show up attached to a lone ket. Probability interpretation of the transformed continuous distribution This is essentially answered in the linked answer by Cosmos Zachos, but I want to approach it in a way that keeps the $\\frac{1}{3u^{2/3}}$ away from the kets. I don't think it belongs there, because kets \"should\" be normalized (but that might be a matter of taste). In any case, let's suppose our measurement of $\\hat{u}$ corresponds to getting a value of $u$ in some interval $[a,b]$ . How should we represent that in our current language? The idea is to construct the projection operator as a \"piece\" of the identify operator, i.e., define $$ \\hat{P}_{u,[a,b]} = \\int_{a}^{b}\\left(\\frac{1}{3u^{2/3}}\\,du\\right) \\lvert \\sqrt[3]{u}\\rangle\\langle \\sqrt[3]{u}\\lvert\\,, $$ in which case the probability of getting a result in $[a,b]$ given that the state is $\\lvert\\psi\\rangle$ is \\begin{align} \\langle \\psi \\lvert \\hat{P}_{u,[a,b]} \\lvert \\psi\\rangle &= \\int_{a}^{b}\\left(\\frac{1}{3u^{2/3}}\\,du\\right) \\langle \\psi\\lvert \\sqrt[3]{u}\\rangle\\langle \\sqrt[3]{u}\\lvert \\psi\\rangle\\\\ &= \\int_{a}^{b}\\left(\\frac{1}{3u^{2/3}}\\,du\\right) \\left\\lvert\\langle \\sqrt[3]{u}\\lvert \\psi\\rangle\\right\\rvert^2\\,. \\end{align} Now here's the crux of the matter! Do we keep $\\frac{1}{3u^{2/3}}$ with the integration measure, or do we attach it to the \"wave function\". Well, if we want a probability density function, we are forced to say that this probability density function is defined as \\begin{align} p(u) = \\frac{1}{3u^{2/3}}\\left\\lvert\\langle \\sqrt[3]{u}\\lvert \\psi\\rangle\\right\\rvert^2\\,. \\end{align} This means that if want to define a wave function associated with the $u$ variable, we are forced to define it in the following way, as $$ \\tilde{\\psi}(u) = \\frac{1}{\\sqrt{3}\\sqrt[3]{u}}\\langle \\sqrt[3]{u}\\lvert \\psi\\rangle =\\frac{1}{\\sqrt{3}\\sqrt[3]{u}}\\psi\\left(\\sqrt[3]{u}\\right)\\, $$ where $$ \\psi(x) = \\langle x \\lvert \\psi\\rangle\\,. $$ Crucially for our understanding, here, we know that, as numbers , $ \\psi(x)$ , $\\langle x \\lvert \\psi\\rangle$ , $\\langle \\sqrt[3]{u} \\lvert \\psi\\rangle$ , and $\\psi\\left(\\sqrt[3]{u}\\right)$ are all equal, exactly because $u=x^3$ . In addition, I am using the symbol $\\psi$ for both the wave functions $\\psi(x)$ and $\\tilde{\\psi}(u)$ , because they represent the same state $\\lvert\\psi\\rangle$ . However, from the identification above, they are clearly not the same function , in the sense that they have different functional forms, which is why one is called $\\psi$ and the other $\\tilde{\\psi}$ . (A similar thing is done for the position-space and momentum-space wave functions, which I also often denote as $\\psi(x)$ and $\\tilde{\\psi}(p)$ .) Alternatively, we can decide to keep the $\\frac{1}{3u^{2/3}}$ with the integration measure, and just say that $\\tilde{\\psi}(u) = \\psi(\\sqrt[3](u))$ is our wave function. However, the square of the wave function is no longer a probability density function, because we have a different integration measure. But that's fine: we just remember that we have to add in a factor of $\\frac{1}{3u^{2/3}}$ when calculating probabilities (and, inner products). Final note What we've done here has some precedent. When we write a hydrogenic wave function $\\psi_{nlm}(r,\\theta,\\phi)$ , the square of this wave function is not a probability density function for $r$ , $\\theta$ , and $\\phi$ ! That's because there is an integration measure $r^2dr\\sin\\theta d\\theta d\\phi$ , and so the actual probability density function is $$ p(r,\\theta,\\phi) = r^2\\sin\\theta |\\psi_{nlm}(r,\\theta,\\phi)|^2\\,. $$ In this case, everyone is fine with the way things are. We leave the extra functions attached to the integration measure, and the square of the wave function is no longer a probability density function, but that's fine! We know how to calculate things, and so we're good.",
      "question_latex": [
        "u=g(x)",
        "|u\\rangle",
        "\\psi",
        "|x\\rangle",
        "u=x^2",
        "\\psi(x)=\\langle x|\\psi\\rangle=\\sin(x)",
        "\\phi(u) = \\langle u|\\psi\\rangle = \\sin(\\sqrt{u})",
        "\\psi(x)=\\psi(g^{-1}(u))=\\phi(u)",
        "\\langle u|\\psi\\rangle = \\int dx \\langle u|x\\rangle\\langle x|\\psi\\rangle = \\int dx\\langle u|x\\rangle\\sin(x)",
        "\\phi(u)=\\sin(\\sqrt{u})",
        "x",
        "g(x)"
      ],
      "answer_latex": [
        "\\hat{u} = g(\\hat{x})\\,,",
        "\\hat{u}\\lvert x\\rangle=g(\\hat{x})\\lvert x\\rangle=g(x)\\lvert x\\rangle = u\\lvert x\\rangle\\,.",
        "\\hat{u}\\lvert x\\rangle = \\hat{x}^3\\lvert x\\rangle\n= x^3\\lvert x\\rangle  = u\\lvert x\\rangle\\,.",
        "\\hat{I} =\n\\int_{-\\infty}^{\\infty}\\left(\\frac{1}{3u^{2/3}}\\,du\\right)\n\\lvert \\sqrt[3]{u}\\rangle\\langle \\sqrt[3]{u}|\\,,",
        "|\\langle \\sqrt[3]{u}| \\psi\\rangle|^2= |\\langle x|\\psi\\rangle|^2",
        "\\hat{P}_{u,[a,b]} =\n\\int_{a}^{b}\\left(\\frac{1}{3u^{2/3}}\\,du\\right)\n\\lvert \\sqrt[3]{u}\\rangle\\langle \\sqrt[3]{u}\\lvert\\,,",
        "\\tilde{\\psi}(u) = \\frac{1}{\\sqrt{3}\\sqrt[3]{u}}\\langle \\sqrt[3]{u}\\lvert \\psi\\rangle\n=\\frac{1}{\\sqrt{3}\\sqrt[3]{u}}\\psi\\left(\\sqrt[3]{u}\\right)\\,",
        "\\psi(x) = \\langle x \\lvert \\psi\\rangle\\,.",
        "p(r,\\theta,\\phi) = r^2\\sin\\theta |\\psi_{nlm}(r,\\theta,\\phi)|^2\\,.",
        "u",
        "u=g(x)",
        "</span>\nand the eigenbasis of <span class=\"math-container\">",
        "</span> is exactly the same as the eigenbasis for <span class=\"math-container\">",
        "</span>, because\n<span class=\"math-container\">",
        "</span>\nIn this case, the transformation from one basis to another is not really a transformation at all, it is really a re-labeling (which is basically the same as a <em>re-ordering</em> of the basis, which is a trivial kind of basis change). For clarity, let's consider the operator <span class=\"math-container\">",
        "</span>. Then, we have\n<span class=\"math-container\">",
        "</span>\nThe basis is singly degenerate because the function <span class=\"math-container\">",
        "</span> is injective, and so we can label these vectors with the eigenvalues of as\n<span class=\"math-container\">\\begin{align}\n\\lvert u\\rangle = \\lvert x^3\\rangle\\,,\n\\end{align}</span>\nwhere <span class=\"math-container\">",
        "</span>, uniquely. Alternatively, we can write this as\n<span class=\"math-container\">\\begin{align}\n\\lvert x\\rangle &= \\lvert \\sqrt[3]{u}\\rangle\\,.\n\\end{align}</span></p>\n<p>The ambiguity comes in because we are dealing with a continuous basis, so let's consider two separate cases.</p>\n<p><strong><span class=\"math-container\">",
        "</span> is a discrete basis</strong></p>\n<p>For the moment, let's pretend that the basis is actually discrete, and let <span class=\"math-container\">",
        "</span> take on integer values between <span class=\"math-container\">",
        "</span> and <span class=\"math-container\">",
        "</span>. In that case, we can expand an arbitrary state <span class=\"math-container\">",
        "</span> in this basis. Starting with the <span class=\"math-container\">",
        "</span> basis, we have\n<span class=\"math-container\">\\begin{align}\n\\lvert \\psi \\rangle &= \\sum_{x=-\\infty}^{\\infty}\n\\lvert x\\rangle\\langle x| \\psi\\rangle\n=\n\\sum_{x=-\\infty}^{\\infty}\n\\lvert \\sqrt[3]{u}\\rangle\\langle \\sqrt[3]{u}| \\psi\\rangle\n=\n\\sum_{u}\n\\lvert \\sqrt[3]{u}\\rangle\\langle \\sqrt[3]{u}| \\psi\\rangle\\,.\n\\end{align}</span>\nNote that in these sums, <span class=\"math-container\">",
        "</span> takes on integer values, whereas <span class=\"math-container\">",
        "</span> takes on only <em>cubed</em>-integer values. Despite the re-labeling, nothing has really changed, because <span class=\"math-container\">",
        "</span>.  The expansion coefficients haven't changed; instead, we've just rearranged them. We could also choose to re-label the state <span class=\"math-container\">",
        "</span>, so that we are actually labeling the state with the eigenvalue of the operator <span class=\"math-container\">",
        "</span>, but to make the connection to the continuous case, I'll leave it as is.</p>\n<p><strong><span class=\"math-container\">",
        "</span> is a continuous basis</strong></p>\n<p>Now, let's do the infinite case. Starting with the <span class=\"math-container\">",
        "</span> basis, we make the substitution <span class=\"math-container\">",
        "</span>, yielding\n<span class=\"math-container\">\\begin{align}\n\\lvert x \\rangle &= \\int_{-\\infty}^{\\infty}dx\\,\n\\lvert x\\rangle\\langle x| \\psi\\rangle\n=\n\\int_{-\\infty}^{\\infty}\\left(\\frac{1}{3u^{2/3}}\\,du\\right)\n\\lvert \\sqrt[3]{u}\\rangle\\langle \\sqrt[3]{u}| \\psi\\rangle\\,.\n\\end{align}</span>\nComparing to the above, we can see that <span class=\"math-container\">",
        "</span> are equal to each other <em>as numbers</em> (ignore the interpretation of them as functions of <span class=\"math-container\">",
        "</span> or <span class=\"math-container\">",
        "</span> for the moment). We've done nothing but re-parameterize these expressions, so they are still equal.</p>\n<p>We can see that the big difference is in <em>the integration measure</em> <span class=\"math-container\">",
        "</span>. Part of the question is about where is the \"proper\" place for the factor of <span class=\"math-container\">",
        "</span>. There are a few ways to go about  interpreting what we have. The first is to recognize that we have again just written the identity operator in a different way, i.e.,\n<span class=\"math-container\">",
        "</span>\nsince it clearly hits <span class=\"math-container\">",
        "</span> and makes <span class=\"math-container\">",
        "</span>. We don't have to worry about the meaning of wave functions at all. Instead, we just recognize that this is another way to resolve the identity, and so it's just another way to re-write our state. Thus, <span class=\"math-container\">",
        "</span> stays with the integration measure.  The problem with this is that we also want to <em>interpret</em> what we have in a physical context, and we know that the interpretations here all about probabilities.  Thus, we look at the second way to make sense of this.</p>\n<p><strong>A  brief important digression on the discrete case</strong></p>\n<p>In the discrete case, there's no problem of interpretation, because there's no change in the \"integration measure\" in the discrete case, and so\n<span class=\"math-container\">",
        "</span>\nis just the probability that if we get the result <span class=\"math-container\">",
        "</span> upon measuring the observable <span class=\"math-container\">",
        "</span>, which is the same probability as getting <span class=\"math-container\">",
        "</span> if we measure the operator <span class=\"math-container\">",
        "</span>.</p>\n<p><strong>A  second brief important digression on terminology</strong></p>\n<p>The symbol <span class=\"math-container\">",
        "</span> is reserved for the <em>name</em> of the state represented by the normalized ket <span class=\"math-container\">",
        "</span>. Every time I write a ket, it will mean a normalized ket in the abstract Hilbert space.  This ensures that things remain as consistent as they can between the discrete and continuous cases. It also means that wherever <span class=\"math-container\">",
        "</span> shows up, it will <em>never</em> show up attached to a lone ket.</p>\n<p><strong>Probability interpretation of the transformed continuous distribution</strong></p>\n<p>This is essentially answered in the linked answer by Cosmos Zachos, but I want to approach it in a way that keeps the <span class=\"math-container\">",
        "</span> away from the kets. I don't think it belongs there, because kets \"should\" be normalized (but that might be a matter of taste). In any case, let's suppose our measurement of <span class=\"math-container\">",
        "</span> corresponds to getting a value of <span class=\"math-container\">",
        "</span> in some interval <span class=\"math-container\">",
        "</span>. How should we represent that in our current language? The idea is to construct the projection operator as a \"piece\" of the identify operator, i.e., define\n<span class=\"math-container\">",
        "</span>\nin which case the probability of getting a result in <span class=\"math-container\">",
        "</span> given that the state is <span class=\"math-container\">",
        "</span> is\n<span class=\"math-container\">\\begin{align}\n\\langle \\psi \\lvert \\hat{P}_{u,[a,b]} \\lvert \\psi\\rangle &=\n\\int_{a}^{b}\\left(\\frac{1}{3u^{2/3}}\\,du\\right)\n\\langle \\psi\\lvert \\sqrt[3]{u}\\rangle\\langle \\sqrt[3]{u}\\lvert \\psi\\rangle\\\\\n&=\n\\int_{a}^{b}\\left(\\frac{1}{3u^{2/3}}\\,du\\right)\n\\left\\lvert\\langle \\sqrt[3]{u}\\lvert \\psi\\rangle\\right\\rvert^2\\,.\n\\end{align}</span>\nNow here's the crux of the matter! Do we keep <span class=\"math-container\">",
        "</span> with the integration measure, or do we attach it to the \"wave function\".  Well, if we want a probability density function, we are forced to say that this probability density function is defined as\n<span class=\"math-container\">\\begin{align}\np(u) = \\frac{1}{3u^{2/3}}\\left\\lvert\\langle \\sqrt[3]{u}\\lvert \\psi\\rangle\\right\\rvert^2\\,.\n\\end{align}</span>\nThis means that if want to define a <em>wave function</em> associated with the <span class=\"math-container\">",
        "</span> variable, we are <em>forced</em> to define it in the following way, as\n<span class=\"math-container\">",
        "</span>\nwhere\n<span class=\"math-container\">",
        "</span>\nCrucially for our understanding, here, we know that, <em>as numbers</em>,\n<span class=\"math-container\">",
        "</span>, <span class=\"math-container\">",
        "</span>, and <span class=\"math-container\">",
        "</span> are all equal, exactly because <span class=\"math-container\">",
        "</span>. In addition, I am using the symbol <span class=\"math-container\">",
        "</span> for both the wave functions <span class=\"math-container\">",
        "</span>, because they represent the same state <span class=\"math-container\">",
        "</span>. However, from the identification above, they are clearly not the same <em>function</em>, in the sense that they have different functional forms, which is why one is called <span class=\"math-container\">",
        "</span> and the other <span class=\"math-container\">",
        "</span>. (A similar thing is done for the position-space and momentum-space wave functions, which I also often denote as <span class=\"math-container\">",
        "</span>.)</p>\n<p>Alternatively, we can decide to keep the <span class=\"math-container\">",
        "</span> with the integration measure, and just say that <span class=\"math-container\">",
        "</span> is our wave function. However, the square of the wave function is no longer a probability density function, because we have a different integration measure. But that's fine: we just remember that we have to add in a factor of <span class=\"math-container\">",
        "</span> when calculating probabilities (and, inner products).</p>\n<p><strong>Final note</strong></p>\n<p>What we've done here has some precedent. When we write a hydrogenic wave function <span class=\"math-container\">",
        "</span>, the square of this wave function is <em>not a probability density function</em> for <span class=\"math-container\">",
        "</span>! That's because there is an integration measure <span class=\"math-container\">",
        "</span>, and so the actual probability density function is\n<span class=\"math-container\">"
      ],
      "created": "2021-10-26T22:33:46.207",
      "golden_ner_terms": [
        "addition",
        "basic",
        "basis",
        "basis change",
        "calculate",
        "change of basis",
        "choose",
        "connection",
        "consistent",
        "construct",
        "context",
        "continuous",
        "current",
        "decide",
        "density",
        "density function",
        "difference",
        "discrete",
        "distribution",
        "eigenvalue",
        "eigenvalues",
        "expand",
        "expansion",
        "factor",
        "fourier transform",
        "function",
        "functional",
        "hilbert space",
        "identity",
        "identity operator",
        "infinite",
        "injective",
        "inner",
        "inner product",
        "integer",
        "integration",
        "interpretation",
        "interval",
        "label",
        "labeling",
        "language",
        "matter",
        "mean",
        "measure",
        "moment",
        "momentum",
        "numbers",
        "one way",
        "operator",
        "operators",
        "place",
        "probability",
        "probability density function",
        "projection",
        "similar",
        "simple",
        "space",
        "square",
        "state",
        "substitution",
        "time",
        "transform",
        "transformation",
        "variable",
        "vectors",
        "wave function",
        "wavefunction",
        "way"
      ],
      "golden_ner_count": 68,
      "golden_patterns": [
        {
          "pattern": "try-a-simpler-case",
          "score": 2.0,
          "hotwords": [
            "degenerate"
          ]
        },
        {
          "pattern": "check-the-extreme-cases",
          "score": 2.0,
          "hotwords": [
            "degenerate"
          ]
        },
        {
          "pattern": "construct-an-explicit-witness",
          "score": 2.0,
          "hotwords": [
            "construct"
          ]
        },
        {
          "pattern": "encode-as-algebra",
          "score": 2.0,
          "hotwords": [
            "ring"
          ]
        },
        {
          "pattern": "use-probabilistic-method",
          "score": 2.0,
          "hotwords": [
            "probability"
          ]
        },
        {
          "pattern": "unfold-the-definition",
          "score": 2.0,
          "hotwords": [
            "means that"
          ]
        },
        {
          "pattern": "optimise-a-free-parameter",
          "score": 2.0,
          "hotwords": [
            "choose"
          ]
        },
        {
          "pattern": "transport-across-isomorphism",
          "score": 2.0,
          "hotwords": [
            "identify"
          ]
        }
      ],
      "golden_pattern_names": [
        "try-a-simpler-case",
        "check-the-extreme-cases",
        "construct-an-explicit-witness",
        "encode-as-algebra",
        "use-probabilistic-method",
        "unfold-the-definition",
        "optimise-a-free-parameter",
        "transport-across-isomorphism"
      ],
      "golden_scopes": [],
      "golden_scope_count": 0
    },
    {
      "id": "se-physics-461696",
      "stratum": "medium",
      "title": "What is the 't Hooft determinant?",
      "tags": [
        "quantum-field-theory",
        "mass",
        "symmetry-breaking",
        "instantons",
        "functional-determinants"
      ],
      "score": 7,
      "answer_score": 2,
      "question_body": "The 't Hooft vertex/determinant is somehow generated by instantons and is responsible for the generation of mass gap in pseudo-Goldstone bosons, such as an axion. For example, the complex Peccei-Quinn scalar couples to a fermion as $\\phi\\bar\\psi_L\\psi_R + h.c.$ , which somehow develops a 't Hooft determinant when instantons come in the picture. This then generates a linear term in the Higgs potential, explicitly breaking the $U(1)_{PQ}$ chiral symmetry, ultimately giving a mass to the axion. As of this writing, there is nothing on Wikipedia on 't Hooft determinants. I also have been unsuccessful in finding a pedagogical introduction to such technology. I have the following questions. What is a 't Hooft determinant? What is the role of instantons in this discussion? How does one compute modifications to the Higgs potential from a 't Hooft determinant such as the one shown in the diagram above?",
      "answer_body": "I am amazed that this question has not received an answer in the 4 years since it was asked. I'll give the broad picture of what the 't Hooft determinant is, and its relation to the axion story a la Peccei-Quinn, providing original and pedagogical references. 't Hooft Determinant in QCD We begin with the standard Euclidean Yang-Mills Lagrangian coupled to $N_f$ fermions. $$S=\\int d^4 x\\, \\left[\\frac{1}{2g^2}F^2 - \\sum_{f}^{N_f} \\psi^{\\dagger}_f \\left(\\gamma^\\mu D_\\mu + m_f\\right) \\psi_f + \\mathcal{L}_{\\textrm{g.f.}} + \\mathcal{L}_{\\textrm{ghost}}\\right] \\tag{1}$$ Here $\\mathcal{L}_{\\textrm{g.f.}}$ is the gauge-fixing term, and $\\mathcal{L}_{\\textrm{ghost}}$ is the Faddeev-Popov ghost Lagrangian. The corresponding path integral is: $$Z = \\int \\mathcal{D}\\psi^{\\dagger} \\mathcal{D}\\psi \\mathcal{D}A \\mathcal{D} \\bar c \\mathcal{D} c \\, e^{-S[A,\\bar\\psi,\\psi, \\bar c, c]} \\tag{2}$$ The basic idea of the 't Hooft determinant is to compute the path integral by first integrating over the gauge-field $A$ and ghost-fields $\\bar c, c$ , assuming the dominant contributions come from instanton configurations. This leaves us with an effective action for the fermions $S'[\\psi^{\\dagger}, \\psi]$ . $$Z = C \\int \\mathcal{D} \\psi^{\\dagger} \\mathcal{D}\\psi \\, e^{-S'[\\bar \\psi, \\psi]} \\tag{3}$$ The new effective action will of course involve non-trivial fermionic interactions. The hallmark nontrivial interaction that arises from integrating over the moduli space of a single-instanton configuration (using Gaussian approximation to handle leading quantum fluctuations, i.e. the semiclassical method) is the so-called 't Hooft determinant interaction [ 2 ]. This is a $2N_f$ -fermion interaction term which can be written in the form of a determinant [ 3 ]. This determinant interaction is commonly denoted by $Y^{(\\pm)}[\\psi^{\\dagger},\\psi]$ , where $+$ corresponds to the instanton and $-$ for the anti-instanton. Notice that it is an action-term, i.e. a functional in the fermion fields $\\psi^{\\dagger},\\psi$ . It is most conveniently expressed in momentum space: $$Y^{(\\pm)}\\sim \\int d^4x\\,\\,\\, \\left.\\det\\right._f J^{(\\pm)} \\tag{4a}$$ $$J_{f_1 f_2} = \\int \\frac{d^4 k d^4 l}{(2\\pi)^8} e^{i(k-l)\\cdot x} \\sqrt{M(k)M(l)}\\,\\psi^{\\dagger}_{f_1}\\frac{1\\pm \\gamma_5}{2}\\psi_{f_2} \\tag{4b}$$ Here, $f_{1,2}$ are flavor indices, and $\\left.\\det\\right._f$ means determinant over flavor indices only. $M(k)$ is a specific function related to the Fourier transform of the fermionic zero mode. In fact for $N_f = 1$ , we get a 2-fermion interaction which for a balanced collection of instantons and anti-instantons leads to a momentum-dependent mass $M(k)$ ! (hence the notation) A possibly more illustrative way to view this interaction is as follows - for an instanton, the determinant interaction looks like the following. $$\\mathcal{L}_{\\textrm{det}} \\sim \\det \\begin{vmatrix}\\bar\\psi_1 P_L\\psi_1 & \\bar\\psi_1 P_L\\psi_2 & \\cdots \\\\ \\bar\\psi_2 P_L\\psi_1 & \\bar\\psi_2 P_L\\psi_2 & \\cdots \\\\ \\vdots & \\vdots & \\ddots \\end{vmatrix} \\tag{5}$$ where $\\psi^{\\dagger}_1 P_L \\psi_2 = \\psi_{1,R}\\psi_{2,L}$ . The anti-instanton configuration involves only right-handed spinor sources. Notice that the sum of the interactions due to an averaged instanton and anti-instanton produces an interaction term which preserves $SU(N_f)_L \\times SU(N_f)_R$ and $U(1)_V$ flavor symmetries, but violates $U(1)_A$ flavor symmetry. Keep in mind that, to calculate quantities in real QCD, we need not only effective interactions in a single instanton or anti-instanton, but rather in an entire ensemble of them. This is a the next logical step in the story, but is long and complicated. More details can be found in the second reference [ 3 ]. I will just mention that, a first-order approximation of the real QCD vacuum is a balanced ensemble of instantons and anti-instantons with instanton-interactions ignored, with fixed density $N/V\\approx 1\\,\\,\\textrm{fm}^{-4}$ and radius $\\rho\\approx 1/3\\,\\,\\textrm{fm}$ . In this case, the effective fermionic action will really just be the sum of $Y^{(+)}$ and $Y^{(-)}$ (with the kinetic terms of course), which perfectly provides a $U(1)_A$ -violating term. Relation to Axion in Peccei-Quinn Theory To recap the entire point of the axion as per the original PQ paper [ 1 ], in normal QCD we have the strong-CP problem which is that the $\\theta$ term is somehow identically zero. $$\\mathcal{L}_\\theta = i\\theta Q_5 \\tag{6a} $$ $$Q_5 = \\int d^4x\\,\\left(\\partial^\\mu j^5_{\\mu}\\right) = \\frac{g^2}{16\\pi^2} \\int d^4x\\, F_{\\mu\\nu} \\tilde F^{\\mu\\nu} \\tag{6b}$$ where $j^5_\\mu(x)$ is the axial current, i.e. the Noether current arising from axial rotations. Recall that performing an axial-rotation of the fermion fields by $\\psi\\rightarrow \\exp \\left(i\\eta \\gamma^5\\right)\\psi$ induces a change in the effective action via $$S\\rightarrow S - 2 i \\eta Q_5 \\tag{7}$$ This shifts the $\\theta$ parameter via $$\\theta\\rightarrow \\theta - 2\\eta \\tag{8}$$ If there are fermionic mass terms, this axial rotation will rotate the complex phase of the masses. The strong-CP problem is thus having simultaneously real fermion masses and $\\theta = 0$ . In their original paper, Peccei and Quinn pointed out that we can find a natural resolution to the strong-CP problem by introducing a complex-scalar Higgs-type field that couples to the fermions via Yukawa interactions, which is charged under global $U(1)_A$ rotations. $$\\mathcal{L}_{\\phi \\psi \\psi} = i\\bar \\psi \\left[ G \\phi \\frac{1+\\gamma_5}{2} + G^* \\phi^* \\frac{1-\\gamma_5}{2}\\right] \\psi \\tag{9a}$$ $$\\mathcal{L}_\\phi = -\\frac{1}{2}\\left|\\partial \\phi\\right|^2 - \\mu^2 |\\phi |^2 - h |\\phi |^4 \\tag{9b}$$ where $\\mu^2 < 0$ , and $G = |G|e^{i\\gamma}$ is a complex number. I have expressed the general Yukawa interaction (9a) in terms of left and right-handed components, following the notation of the original PQ paper. Let us assume $\\phi$ acquires a VEV $\\langle \\phi \\rangle = \\lambda e^{i\\beta}$ . It is clear that the fermions will acquire a mass proportional to $\\lambda$ which will largely be fixed by the classical quartic potential in $\\mathcal{L}_\\phi$ . Of course there may be quantum corrections to this potential, but we will assume those are small. But what about the complex phase $\\beta$ ? Notice that if we could somehow have $\\beta = - \\gamma - \\theta$ , a simple axial rotation $\\psi\\rightarrow \\exp (i\\theta\\gamma_5 / 2 ) \\psi$ would solve the strong-CP problem, i.e. $\\theta$ will be cancelled as per (8), and the fermions will only have a real mass term as per (9). Spoiler alert: The complex phase $\\beta$ is actually fixed to this magical value by instantons. This, I believe, is what you were looking for. Allow me to briefly walk you through the original PQ paper [ 1 ]. I will reference equations in that paper by angle-brackets. Eq. <7> shows the full Lagrangian, with fermion, gauge, and axion fields. The next logical equation is eq. <10>, which is achieved by integrating out our femion and gauge fields $\\bar\\psi$ , $\\psi$ , and $A$ . Integrating the gauge fields $A$ is exactly the procedure outlined in the previous section of this answer, which led to the 't Hooft determinantal interaction for $\\psi$ . Integrating out the fermion fields is a monstrous task which I have not touched upon. The authors did not evaluate either of these integrals explicitly, but merely gave a schematic result <10> in terms of to-be-calculated functions $c_m^n(x,y)$ . All that remains is an effective path integral for the complex-scalar field $\\phi$ and $\\phi^*$ (which has been reparameterized in terms of $\\rho$ and $\\sigma$ as per <9>). The schematic evaluation they did is all that is necessary to see that VEV-phase $\\beta$ will ultimately acquire the magical value $\\beta = -\\gamma -\\theta$ . A precise evaluation would be necessary to calculate quantum corrections to the VEV-magnitude $\\lambda$ . This is the rough idea for how the 't Hooft interactions would affect the VEV of $\\phi$ and, in turn, affect the axion mass. For now I will not dive into the calculation of the quantum-corrected axion mass, because I am not familiar with it at all, but I will point out some references which talk about it. The first (partial) computation of the quantum-corrections to the axion mass was done by Michał Spaliński in 1988 [ 4 ]. A more complete calculation was done recently by Giovanni Grilli di Cortona et al in 2016, see [ 5 ] and the references+discussions therein. References [1] Computation of the quantum effects due to a four-dimensional pseudoparticle ; G. 't Hooft; Phys. Rev. D 18, 2199 (1978) [2] Chiral Symmetry Breaking by Instantons ; D. Diakonov; arXiv:hep-ph/9602375 [3] \"CP Conservation in the Presence of Pseudoparticles\"; R. D. Peccei and Helen R. Quinn; Phys. Rev. Lett. 38, 1440 (1977) [4] \"Chiral Corrections to the axion mass\"; M. Spaliński; Zeitschrift für Physik C Particles and Fields volume 41, pages 87–90 (1988) [5] \"The QCD axion, precisely\"; G. Grilli di Cortona et al ; Journal of High Energy Physics volume 2016, Article number: 34 (2016) ; Arxiv Preprint: 1511.02867",
      "question_latex": [
        "\\phi\\bar\\psi_L\\psi_R + h.c.",
        "U(1)_{PQ}"
      ],
      "answer_latex": [
        "S=\\int d^4 x\\, \\left[\\frac{1}{2g^2}F^2 - \\sum_{f}^{N_f} \\psi^{\\dagger}_f \\left(\\gamma^\\mu D_\\mu + m_f\\right) \\psi_f + \\mathcal{L}_{\\textrm{g.f.}} + \\mathcal{L}_{\\textrm{ghost}}\\right] \\tag{1}",
        "Z = \\int \\mathcal{D}\\psi^{\\dagger} \\mathcal{D}\\psi \\mathcal{D}A \\mathcal{D} \\bar c \\mathcal{D} c \\, e^{-S[A,\\bar\\psi,\\psi, \\bar c, c]}  \\tag{2}",
        "Z = C \\int \\mathcal{D} \\psi^{\\dagger} \\mathcal{D}\\psi \\, e^{-S'[\\bar \\psi, \\psi]} \\tag{3}",
        "Y^{(\\pm)}\\sim \\int d^4x\\,\\,\\, \\left.\\det\\right._f J^{(\\pm)}  \\tag{4a}",
        "J_{f_1 f_2} = \\int \\frac{d^4 k d^4 l}{(2\\pi)^8} e^{i(k-l)\\cdot x} \\sqrt{M(k)M(l)}\\,\\psi^{\\dagger}_{f_1}\\frac{1\\pm \\gamma_5}{2}\\psi_{f_2} \\tag{4b}",
        "\\mathcal{L}_{\\textrm{det}} \\sim \\det \\begin{vmatrix}\\bar\\psi_1 P_L\\psi_1 & \\bar\\psi_1 P_L\\psi_2 & \\cdots \\\\\n\\bar\\psi_2 P_L\\psi_1 & \\bar\\psi_2 P_L\\psi_2 & \\cdots \\\\\n\\vdots & \\vdots & \\ddots\n\\end{vmatrix} \\tag{5}",
        "\\mathcal{L}_\\theta = i\\theta Q_5 \\tag{6a}",
        "Q_5 = \\int d^4x\\,\\left(\\partial^\\mu j^5_{\\mu}\\right) = \\frac{g^2}{16\\pi^2} \\int d^4x\\, F_{\\mu\\nu} \\tilde F^{\\mu\\nu} \\tag{6b}",
        "S\\rightarrow S - 2 i \\eta Q_5 \\tag{7}",
        "\\theta\\rightarrow \\theta - 2\\eta \\tag{8}",
        "\\mathcal{L}_{\\phi \\psi \\psi} = i\\bar \\psi \\left[ G \\phi \\frac{1+\\gamma_5}{2} + G^* \\phi^* \\frac{1-\\gamma_5}{2}\\right] \\psi  \\tag{9a}",
        "\\mathcal{L}_\\phi = -\\frac{1}{2}\\left|\\partial \\phi\\right|^2 - \\mu^2 |\\phi |^2 - h |\\phi |^4  \\tag{9b}",
        "N_f",
        "</span></p>\n<p>Here <span class=\"math-container\">",
        "</span> is the gauge-fixing term, and <span class=\"math-container\">",
        "</span> is the Faddeev-Popov ghost Lagrangian. The corresponding path integral is:</p>\n<p><span class=\"math-container\">",
        "</span></p>\n<p>The basic idea of the 't Hooft determinant is to compute the path integral by first integrating over the gauge-field <span class=\"math-container\">",
        "</span> and ghost-fields <span class=\"math-container\">",
        "</span>, assuming the dominant contributions come from instanton configurations. This leaves us with an effective action for the fermions <span class=\"math-container\">",
        "</span>.</p>\n<p><span class=\"math-container\">",
        "</span></p>\n<p>The new effective action will of course involve non-trivial fermionic interactions. The hallmark nontrivial interaction that arises from integrating over the moduli space of a single-instanton configuration (using Gaussian approximation to handle leading quantum fluctuations, i.e. the semiclassical method) is the so-called <em>'t Hooft determinant interaction</em> [<a href=\"https://journals.aps.org/prd/abstract/10.1103/PhysRevD.14.3432\" rel=\"nofollow noreferrer\">2</a>]. This is a <span class=\"math-container\">",
        "</span>-fermion interaction term which can be written in the form of a determinant [<a href=\"https://arxiv.org/abs/hep-ph/9602375\" rel=\"nofollow noreferrer\">3</a>]. This determinant interaction is commonly denoted by <span class=\"math-container\">",
        "</span>, where <span class=\"math-container\">",
        "</span> corresponds to the instanton and <span class=\"math-container\">",
        "</span> for the anti-instanton. Notice that it is an action-term, i.e. a functional in the fermion fields <span class=\"math-container\">",
        "</span>. It is most conveniently expressed in momentum space:</p>\n<p><span class=\"math-container\">",
        "</span></p>\n<p><span class=\"math-container\">",
        "</span></p>\n<p>Here, <span class=\"math-container\">",
        "</span> are flavor indices, and <span class=\"math-container\">",
        "</span> means determinant over flavor indices only. <span class=\"math-container\">",
        "</span> is a specific function related to the Fourier transform of the fermionic zero mode. In fact for <span class=\"math-container\">",
        "</span>, we get a 2-fermion interaction which for a balanced collection of instantons and anti-instantons leads to a momentum-dependent mass <span class=\"math-container\">",
        "</span>! (hence the notation)</p>\n<p>A possibly more illustrative way to view this interaction is as follows - for an instanton, the determinant interaction looks like the following.</p>\n<p><span class=\"math-container\">",
        "</span></p>\n<p>where <span class=\"math-container\">",
        "</span>. The anti-instanton configuration involves only right-handed spinor sources. <strong>Notice</strong> that the sum of the interactions due to an averaged instanton and anti-instanton produces an interaction term which preserves <span class=\"math-container\">",
        "</span> and <span class=\"math-container\">",
        "</span> flavor symmetries, but violates <span class=\"math-container\">",
        "</span> flavor symmetry.</p>\n<p>Keep in mind that, to calculate quantities in real QCD, we need not only effective interactions in a single instanton or anti-instanton, but rather in an entire ensemble of them. This is a the next logical step in the story, but is long and complicated. More details can be found in the second reference [<a href=\"https://arxiv.org/abs/hep-ph/9602375\" rel=\"nofollow noreferrer\">3</a>]. I will just mention that, a first-order approximation of the real QCD vacuum is a balanced ensemble of instantons and anti-instantons with instanton-interactions ignored, with fixed density <span class=\"math-container\">",
        "</span> and radius <span class=\"math-container\">",
        "</span>. In this case, the effective fermionic action will really just be the sum of <span class=\"math-container\">",
        "</span> (with the kinetic terms of course), which perfectly provides a <span class=\"math-container\">",
        "</span>-violating term.</p>\n<hr />\n<h2>Relation to Axion in Peccei-Quinn Theory</h2>\n<p>To recap the entire point of the axion as per the original PQ paper [<a href=\"https://journals.aps.org/prl/abstract/10.1103/PhysRevLett.38.1440\" rel=\"nofollow noreferrer\">1</a>], in normal QCD we have the strong-CP problem which is that the <span class=\"math-container\">",
        "</span> term is somehow identically zero.</p>\n<p><span class=\"math-container\">",
        "</span> is the axial current, i.e. the Noether current arising from axial rotations. Recall that performing an axial-rotation of the fermion fields by <span class=\"math-container\">",
        "</span> induces a change in the effective action via</p>\n<p><span class=\"math-container\">",
        "</span></p>\n<p>This shifts the <span class=\"math-container\">",
        "</span> parameter via</p>\n<p><span class=\"math-container\">",
        "</span></p>\n<p>If there are fermionic mass terms, this axial rotation will rotate the complex phase of the masses. The strong-CP problem is thus having <strong>simultaneously</strong> real fermion masses <strong>and</strong> <span class=\"math-container\">",
        "</span>.</p>\n<p>In their original paper, Peccei and Quinn pointed out that we can find a natural resolution to the strong-CP problem by introducing a complex-scalar Higgs-type field that couples to the fermions via Yukawa interactions, which is charged under global <span class=\"math-container\">",
        "</span> rotations.</p>\n<p><span class=\"math-container\">",
        "</span>, and <span class=\"math-container\">",
        "</span> is a complex number. I have expressed the general Yukawa interaction (9a) in terms of left and right-handed components, following the notation of the original PQ paper. Let us assume <span class=\"math-container\">",
        "</span> acquires a VEV <span class=\"math-container\">",
        "</span>. It is clear that the fermions will acquire a mass proportional to <span class=\"math-container\">",
        "</span> which will largely be fixed by the classical quartic potential in <span class=\"math-container\">",
        "</span>. Of course there may be quantum corrections to this potential, but we will assume those are small.</p>\n<p>But what about the complex phase <span class=\"math-container\">",
        "</span>? Notice that if we could somehow have <span class=\"math-container\">",
        "</span>, a simple axial rotation <span class=\"math-container\">",
        "</span> would solve the strong-CP problem, i.e. <span class=\"math-container\">",
        "</span> will be cancelled as per (8), and the fermions will only have a real mass term as per (9). <strong>Spoiler alert: The complex phase <span class=\"math-container\">",
        "</span> is actually fixed to this magical value by instantons.</strong> This, I believe, is what you were looking for.</p>\n<p>Allow me to briefly walk you through the original PQ paper [<a href=\"https://journals.aps.org/prl/abstract/10.1103/PhysRevLett.38.1440\" rel=\"nofollow noreferrer\">1</a>]. I will reference equations in that paper by angle-brackets. Eq. <7> shows the full Lagrangian, with fermion, gauge, and axion fields. The next logical equation is eq. <10>, which is achieved by integrating out our femion and gauge fields <span class=\"math-container\">",
        "</span>, <span class=\"math-container\">",
        "</span>. Integrating the gauge fields <span class=\"math-container\">",
        "</span> is exactly the procedure outlined in the previous section of this answer, which led to the 't Hooft determinantal interaction for <span class=\"math-container\">",
        "</span>. Integrating out the fermion fields is a monstrous task which I have not touched upon. The authors did not evaluate either of these integrals explicitly, but merely gave a schematic result <10> in terms of to-be-calculated functions <span class=\"math-container\">",
        "</span>. All that remains is an effective path integral for the complex-scalar field <span class=\"math-container\">",
        "</span> (which has been reparameterized in terms of <span class=\"math-container\">",
        "</span> as per <9>).</p>\n<p>The schematic evaluation they did is all that is necessary to see that VEV-phase <span class=\"math-container\">",
        "</span> will ultimately acquire the magical value <span class=\"math-container\">",
        "</span>. <strong>A precise evaluation would be necessary to calculate quantum corrections to the VEV-magnitude <span class=\"math-container\">",
        "</span>.</strong> This is the rough idea for how the 't Hooft interactions would affect the VEV of <span class=\"math-container\">"
      ],
      "created": "2019-02-19T13:28:51.480",
      "golden_ner_terms": [
        "action",
        "approximation",
        "axial",
        "axion",
        "balanced",
        "basic",
        "bosons",
        "calculate",
        "clear",
        "collection",
        "complete",
        "complex",
        "complex number",
        "components",
        "configuration",
        "current",
        "density",
        "determinant",
        "diagram",
        "dominant",
        "effective",
        "energy",
        "entire",
        "equation",
        "euclidean",
        "fermions",
        "field",
        "fixed",
        "fourier transform",
        "function",
        "functional",
        "gauge",
        "gaussian",
        "generated by",
        "generates",
        "higgs",
        "indices",
        "instantons",
        "integral",
        "interactions",
        "lagrangian",
        "mass",
        "mode",
        "moduli",
        "momentum",
        "necessary",
        "normal",
        "number",
        "parameter",
        "path",
        "path integral",
        "physics",
        "point",
        "potential",
        "proportional",
        "qcd",
        "radius",
        "real",
        "relation",
        "right-handed",
        "rotate",
        "rotation",
        "scalar",
        "section",
        "semiclassical",
        "simple",
        "space",
        "spinor",
        "step",
        "sum",
        "symmetry",
        "symmetry breaking",
        "technology",
        "term",
        "theory",
        "transform",
        "vacuum",
        "volume",
        "walk",
        "way",
        "wikipedia",
        "zero"
      ],
      "golden_ner_count": 82,
      "golden_patterns": [
        {
          "pattern": "check-the-extreme-cases",
          "score": 2.0,
          "hotwords": [
            "zero"
          ]
        },
        {
          "pattern": "exploit-symmetry",
          "score": 2.0,
          "hotwords": [
            "symmetry"
          ]
        },
        {
          "pattern": "construct-an-explicit-witness",
          "score": 2.0,
          "hotwords": [
            "explicit"
          ]
        },
        {
          "pattern": "local-to-global",
          "score": 2.0,
          "hotwords": [
            "global"
          ]
        },
        {
          "pattern": "unfold-the-definition",
          "score": 2.0,
          "hotwords": [
            "recall that"
          ]
        }
      ],
      "golden_pattern_names": [
        "check-the-extreme-cases",
        "exploit-symmetry",
        "construct-an-explicit-witness",
        "local-to-global",
        "unfold-the-definition"
      ],
      "golden_scopes": [
        {
          "type": "where-binding",
          "match": "where $j^5_\\mu(x)$ is"
        }
      ],
      "golden_scope_count": 1
    },
    {
      "id": "se-physics-404093",
      "stratum": "medium",
      "title": "Examples of density operators $\\rho=\\sum\\limits_n p_n|\\phi_n\\rangle\\langle\\phi_n|$ in which the states $\\{|\\phi_n\\rangle\\}$ are not orthogonal",
      "tags": [
        "quantum-mechanics",
        "statistical-mechanics",
        "quantum-information",
        "density-operator",
        "quantum-statistics"
      ],
      "score": 10,
      "answer_score": 18,
      "question_body": "The set of quantum states $\\{|\\phi_n\\rangle\\}$ in the definition of the density operator $$\\rho=\\sum\\limits_n p_n|\\phi_n\\rangle\\langle\\phi_n|$$ need not be orthonormal, and need not form a basis. But unfortunately, in the examples that I have seen so far, the states $\\{|\\phi_n\\rangle\\}$ were both orthonormal and forms a basis. Example 1 In the Stern-Gerlach (SG) set-up, the state of the silver atoms coming out of the oven and before passing through the magnetic field, is imperfectly known because $S_z$ remained unmeasured. Therefore, on the ignorance ground, such an ensemble will be represented by $$\\rho=\\frac{1}{2}(|{\\uparrow}\\rangle\\langle{\\uparrow}|+|{\\downarrow}\\rangle \\langle{\\downarrow}|).\\tag{1}$$ Note that, in this case, the states $|{\\uparrow}\\rangle$ and $|{\\downarrow}\\rangle$ are orthonormal and forms the $S_z$-basis. Example 2 Consider an unpolarized light moving in the z-direction so that its polarization must be in the $xy$-plane. Since we do not know the state vector, it is described by the density operator $$\\rho=\\frac{1}{2}(|x\\rangle\\langle x|+|y\\rangle\\langle y|)\\tag{2}$$ where $|x\\rangle$ and $|y\\rangle$ describe plane polarized states along the $x$ and $y$-axes respectively. Question Can someone suggest an example of a mixed ensemble where the states $\\{|\\phi_n\\rangle\\}$ need not be orthonormal and need not form a basis? I'm not looking for the trivial example where the desity operator describes a pure state.",
      "answer_body": "This thread has seen a ton of incorrect statements coming from a number of sides, so it's probably a good idea to set the record straight in a bit more detail, and to provide some more examples of how expressions of this form come up in practice. So, let's go through a brief rundown of some pertinent points. The definition of a density matrix is just an operator $\\rho:\\mathcal H \\to \\mathcal H$ that is self-adjoint and positive semidefinite (and trace class if $\\dim(\\mathcal H)=\\infty$), and whose trace satisfies $$\\mathrm{Tr}(\\rho)=1.$$ More importantly, this is all that's required by the definition. Any operator that satisfies those conditions can legitimately be called a density matrix, period. Because of that, all operators that can be expressed in the OP's form , $$ \\rho = \\sum_n p_n |\\phi_n \\rangle\\langle \\phi_n|, \\tag{$*$}$$ are valid density matrices so long as the component projectors are normalized to $\\langle \\phi_n|\\phi_n \\rangle =1$ and the weigths add up to $\\sum_n p_n = 1$. Those two requirements are the only actual requirements. None of the conditions for density-matrix-ness ($\\rho^\\dagger=\\rho$, $\\rho\\geq 0$, and $\\mathrm{Tr}(\\rho)=1$) are impacted if the $|\\phi_n\\rangle$ are not pairwise orthogonal, or if their number exceeds the state space's dimension. That means that it's perfectly fine to take non-orthogonal states in a representation of the form $(*)$. Explicit examples with non-orthogonal projectors are trivial to construct. Norbert Shuch's answer contains one example, but if you go looking for them you can build them instantly by just taking any collection of unit-normalized vectors weighted by unit-normalized weights $p_n$. To provide one such example explicitly, consider the two-level space $\\mathcal H = \\mathbb C^2$, and a sequence of $N$ vectors lying equispaced along the equator of its Bloch sphere, giving $$ \\rho = \\sum_{n=0}^{N-1} p_n |\\varphi_n\\rangle\\langle \\varphi_n| \\quad \\text{for} \\quad |\\varphi_n\\rangle = \\frac{1}{\\sqrt{2}} \\bigg( |0\\rangle + e^{i 2\\pi n/N} |1\\rangle\\bigg). \\tag{$\\star$} $$ Here the weights can be arbitrary so long as $\\sum_{n=0}^{N-1} p_n=1$; one obvious choice is $p_n = 1/N$ which gives the maximally-mixed state $\\rho = \\frac12 \\mathbb I$, but there's plenty of other possible choices. Representations of the form $(*)$ are not unique. Suppose, say, that you have some density matrix $\\rho$ that you've managed to represent as a sum of normalized projectors in two different ways, say, $$ \\rho = \\sum_n p_n |\\phi_n \\rangle\\langle \\phi_n| = \\sum_m q_m |\\chi_m \\rangle\\langle \\chi_m|, \\tag{$**$}$$ where $\\sum_n p_n = 1 = \\sum_m q_m$ and $\\langle \\phi_n|\\phi_n \\rangle =1=\\langle \\chi_m|\\chi_m \\rangle$. Then there are some loose requirements on the two sets of vectors , starting with the fact that $\\mathrm{span}\\{|\\phi_n\\rangle\\}$ needs to match $\\mathrm{span}\\{|\\chi_m\\rangle\\}$, but in general, the layout of the $|\\phi_n\\rangle$ and the $|\\chi_m\\rangle$ within that span can be very different . This is evident in the example $(\\star)$ above with equal weights, where $\\rho$ is independent of the number $N$ of vectors in your collection, and it can also be represented as $\\rho = \\tfrac12 \\left[ |0\\rangle\\langle 0| + |1\\rangle\\langle 1| \\right]$. Representations of the form $(*)$ are interpretations, and little more. There is some physical content in the statement $$ \\rho = \\sum_n p_n |\\phi_n \\rangle\\langle \\phi_n|, \\tag{$*$}$$ namely, that you can produce the system state $\\rho$ by producing the pure states $|\\phi_n\\rangle$ with probabilities $p_n$ and then forgetting which pure state you actually produced. However, the operative word there is \"can\": the fact that that procedure will produce $\\rho$ does not say, at all, that it is the only possible procedure that will produce that state. Representations do not imply that the vectors involved are eigenvectors of the resultant density matrix. That's true if the projectors are pairwise orthogonal, but that's not a requirement at all, so it is perfectly possible to construct $\\rho$ as a sum of projectors that have nothing to do with the sum's eigenprojectors. It's probably helpful to illustrate this with an explicit example, for clarity. Consider a two-level system that's prepared in a superposition of the form $$ |\\theta_\\pm\\rangle = \\cos(\\theta/2)|0\\rangle \\pm \\sin(\\theta/2)|1\\rangle,$$ i.e. an angle $\\theta$ down from the north pole of the Bloch sphere, except that each time we flip a fair coin to see which sign of $\\theta$ (i.e. which direction on the prime meridian) we take. Then the density matrix reads \\begin{align} \\rho & = \\frac12 \\bigg( |\\theta_+\\rangle\\langle\\theta_+| +|\\theta_-\\rangle\\langle\\theta_-| \\bigg) \\\\ & = \\frac12 \\bigg( \\big(\\cos(\\theta/2)|0\\rangle + \\sin(\\theta/2)|1\\rangle \\big) \\big(\\cos(\\theta/2)\\langle 0| + \\sin(\\theta/2)\\langle 1| \\big) \\\\ & \\qquad + \\big(\\cos(\\theta/2)|0\\rangle - \\sin(\\theta/2)|1\\rangle \\big) \\big(\\cos(\\theta/2)\\langle 0| - \\sin(\\theta/2)\\langle 1| \\big) \\bigg) %\\\\ & = \\frac12 \\bigg( %\\big(\\cos^2(\\theta/2)|0\\rangle\\langle 0| + \\sin(\\theta/2)\\cos(\\theta/2)|1\\rangle %\\langle 0| + \\sin(\\theta/2)\\cos(\\theta/2)|0\\rangle \\langle 1| + %\\sin^2(\\theta/2)|1\\rangle\\langle 1| \\big) %\\\\ & \\qquad + %\\big(\\cos^2(\\theta/2)|0\\rangle\\langle 0| - \\sin(\\theta/2)\\cos(\\theta/2)|1\\rangle %\\langle 0| - \\sin(\\theta/2)\\cos(\\theta/2)|0\\rangle \\langle 1| + %\\sin^2(\\theta/2)|1\\rangle\\langle 1| \\big) % \\bigg) \\\\ & = \\cos^2(\\theta/2)|0\\rangle\\langle 0| + \\sin^2(\\theta/2)|1\\rangle\\langle 1| \\end{align} because the off-diagonal terms cancel out. In this second representation, we do have orthogonal projectors, so here $|0\\rangle$ and $|1\\rangle$ are indeed the unique eigenvectors of $\\rho$ (unless $\\theta=\\pi/2$ and $\\rho$ is maximally mixed). But that doesn't stop our initial representation, $\\rho = \\frac12 \\left( |\\theta_+\\rangle\\langle\\theta_+| +|\\theta_-\\rangle\\langle\\theta_-| \\right)$, with its non-orthogonal, non-eigenvector components, from also being true. If a state is built up using non-orthogonal projectors, then it also has a separate representation in terms of orthogonal projectors , and that's perfectly fine. Representations of the form $(*)$ are a dime a dozen if you know where to look. So, you found one that's not the canonical one: great! there's millions where that one came from. Representations of the form $(*)$ really are a dime a dozen. If you want to build one yourself, say, for a two-level system, there's a few points that are particularly relevant to the recipe: The Pauli matrices are a basis for all valid density matrices, i.e. if $\\rho=\\rho^\\dagger$ is traceless, then it can be represented as $$ \\rho = \\tfrac12 \\mathbb I + \\vec p \\cdot \\vec \\sigma,$$ where $\\vec p = (p_x,p_y,p_z)\\in \\mathbb R^3$ and $\\vec \\sigma =(\\sigma_x, \\sigma_y, \\sigma_z)$ are the Pauli matrices. (Further, that relationship can be inverted via $\\vec p = \\mathrm{Tr}(\\rho\\vec\\sigma)$.) The positivity condition $\\rho\\geq 0$ translates into the condition $||\\vec p||\\leq 1$, i.e. $\\vec p$ lives inside the unit ball or its boundary $-$ generally known as the Bloch ball and the Bloch sphere in this context. If $|\\vec p|=1$, i.e. $\\vec p$ is on the Bloch sphere boundary, then $\\rho = |\\psi\\rangle\\langle\\psi|$ is a pure state, and if you write $|\\psi\\rangle = \\cos(\\theta/2) |0\\rangle + e^{i\\varphi}\\sin(\\theta/2)|1\\rangle$ (which you always can) then $\\theta\\in [0,\\pi]$ and $\\varphi\\in[0,2\\pi)$ are the polar and azimuthal spherical coordinates for $$ \\vec p = (\\sin(\\theta)\\cos(\\varphi), \\sin(\\theta)\\sin(\\varphi), \\cos(\\theta).$$ The relationship between $\\vec p$ and $\\rho$ is linear and bijective. If $\\rho_1$ and $\\rho_2$ are valid density matrices, then any convex combination $$ \\rho = q_1 \\rho_1 + q_2 \\rho_2$$ of the two, with weights adding to $q_1+q_2=1$, is also a valid density matrix. Because the relationship between density matrices and Bloch-ball vectors is linear, any convex combination of density matrices translates directly into a convex combination of the corresponding Bloch-ball vectors. Thus, if $ \\rho_1 = \\tfrac12 \\mathbb I + \\vec p_1 \\cdot \\vec \\sigma,$ $ \\rho_2 = \\tfrac12 \\mathbb I + \\vec p_2 \\cdot \\vec \\sigma,$ and $ \\rho = q_1 \\rho_1 + q_2 \\rho_2$, then $ \\vec p= q_1 \\vec p_1 + q_2 \\vec p_2$ lies on the line that goes from $\\vec p_1$ to $\\vec p_2$, a fraction $q_1=1-q_2$ of the way in that direction. So, what does this mean for density-matrix representations? If you have a target density matrix $\\rho$ that you want to represent, simply take its Bloch-ball vector $\\vec p = \\mathrm {Tr}(\\rho\\vec\\sigma)$, and then pick $N$ points $\\vec p_n$ on the Bloch sphere itself (the boundary) and weights $q_n$ (normalized to $\\sum_n q_n=1$) such that their average $\\sum_n q_n \\vec p_n=\\vec p$ gives you your chosen point. That will then naturally give you a representation of your density matrix as a weighted sum of $N$ pure-state projectors, and you can read off the computational-basis components directly from the spherical coordinates of your chosen extremal points.",
      "question_latex": [
        "\\rho=\\sum\\limits_n p_n|\\phi_n\\rangle\\langle\\phi_n|",
        "\\rho=\\frac{1}{2}(|{\\uparrow}\\rangle\\langle{\\uparrow}|+|{\\downarrow}\\rangle \\langle{\\downarrow}|).\\tag{1}",
        "\\rho=\\frac{1}{2}(|x\\rangle\\langle x|+|y\\rangle\\langle y|)\\tag{2}",
        "\\{|\\phi_n\\rangle\\}",
        "need not be orthonormal, and need not form a basis. But unfortunately, in the examples that I have seen so far, the states",
        "were both orthonormal and forms a basis. </p>\n\n<p><strong>Example 1</strong> In the Stern-Gerlach (SG) set-up, the state of the silver atoms coming out of the oven and before passing through the magnetic field, is imperfectly known because",
        "remained unmeasured. Therefore, on the ignorance ground, such an ensemble will be represented by",
        "Note that, in this case, the states",
        "and",
        "are orthonormal and forms the",
        "-basis.</p>\n\n<p><strong>Example 2</strong> Consider an unpolarized light moving in the z-direction so that its polarization must be in the",
        "-plane. Since we do not know the state vector, it is described by the density operator",
        "where",
        "describe plane polarized states along the",
        "-axes respectively.  </p>\n\n<hr>\n\n<p><strong>Question</strong> <em>Can someone suggest an example of a mixed ensemble where the states"
      ],
      "answer_latex": [
        "\\mathrm{Tr}(\\rho)=1.",
        "\\rho = \\sum_n p_n |\\phi_n \\rangle\\langle \\phi_n|, \\tag{$*$}",
        "\\rho = \\sum_{n=0}^{N-1} p_n |\\varphi_n\\rangle\\langle \\varphi_n|\n   \\quad \\text{for} \\quad\n   |\\varphi_n\\rangle = \\frac{1}{\\sqrt{2}} \\bigg( |0\\rangle + e^{i 2\\pi n/N} |1\\rangle\\bigg).\n   \\tag{$\\star$}",
        "\\rho = \\sum_n p_n |\\phi_n \\rangle\\langle \\phi_n| = \\sum_m q_m |\\chi_m \\rangle\\langle \\chi_m|, \\tag{$**$}",
        "\\rho = \\sum_n p_n |\\phi_n \\rangle\\langle \\phi_n|, \\tag{$*$}",
        "|\\theta_\\pm\\rangle = \\cos(\\theta/2)|0\\rangle \\pm \\sin(\\theta/2)|1\\rangle,",
        "\\rho = \\tfrac12 \\mathbb I + \\vec p \\cdot \\vec \\sigma,",
        "\\vec p = (\\sin(\\theta)\\cos(\\varphi), \\sin(\\theta)\\sin(\\varphi), \\cos(\\theta).",
        "\\rho = q_1 \\rho_1 + q_2 \\rho_2",
        "\\rho:\\mathcal H \\to \\mathcal H",
        "\\dim(\\mathcal H)=\\infty",
        "More importantly, this is <strong>all</strong> that's required by the definition. Any operator that satisfies those conditions can legitimately be called a density matrix, period.</p></li>\n<li><p>Because of that, <strong>all operators that can be expressed in the OP's form</strong>,",
        "\\rho = \\sum_n p_n |\\phi_n \\rangle\\langle \\phi_n|, \\tag{",
        "}",
        "<strong>are valid density matrices</strong> so long as the component projectors are normalized to",
        "and the weigths add up to",
        ". </p></li>\n<li><p><strong>Those two requirements are the <em>only</em> actual requirements.</strong> None of the conditions for density-matrix-ness (",
        ",",
        ", and",
        ") are impacted if the",
        "are not pairwise orthogonal, or if their number exceeds the state space's dimension. That means that it's perfectly fine to take non-orthogonal states in a representation of the form",
        ".</p></li>\n<li><p><strong>Explicit examples with non-orthogonal projectors are trivial to construct.</strong> Norbert Shuch's answer contains one example, but if you go looking for them you can build them instantly by just taking <em>any</em> collection of unit-normalized vectors weighted by unit-normalized weights",
        ". </p>\n\n<p>To provide one such example explicitly, consider the two-level space",
        ", and a sequence of",
        "vectors lying equispaced along the equator of its Bloch sphere, giving",
        "\\rho = \\sum_{n=0}^{N-1} p_n |\\varphi_n\\rangle\\langle \\varphi_n|\n   \\quad \\text{for} \\quad\n   |\\varphi_n\\rangle = \\frac{1}{\\sqrt{2}} \\bigg( |0\\rangle + e^{i 2\\pi n/N} |1\\rangle\\bigg).\n   \\tag{",
        "Here the weights can be arbitrary so long as",
        "; one obvious choice is",
        "which gives the maximally-mixed state",
        ", but there's plenty of other possible choices.</p></li>\n<li><p><strong>Representations of the form",
        "are not unique.</strong> Suppose, say, that you have some density matrix",
        "that you've managed to represent as a sum of normalized projectors in two different ways, say,",
        "\\rho = \\sum_n p_n |\\phi_n \\rangle\\langle \\phi_n| = \\sum_m q_m |\\chi_m \\rangle\\langle \\chi_m|, \\tag{",
        "where",
        "and",
        ". Then <a href=\"https://physics.stackexchange.com/questions/156777/proving-the-unitary-relation-of-ensemble-decompositions\">there are some loose requirements on the two sets of vectors</a>, starting with the fact that",
        "needs to match",
        ", but <strong>in general, the layout of the",
        "and the",
        "within that span can be very different</strong>. This is evident in the example",
        "above with equal weights, where",
        "is independent of the number",
        "of vectors in your collection, and it can also be represented as",
        ".</p></li>\n<li><p><strong>Representations of the form",
        "are interpretations, and little more.</strong> There <em>is</em> some physical content in the statement",
        "namely, that you can produce the system state",
        "by producing the pure states",
        "with probabilities",
        "and then forgetting which pure state you actually produced. However, the operative word there is \"can\": the fact that that procedure will produce",
        "does not say, at all, that it is the <em>only</em> possible procedure that will produce that state.</p></li>\n<li><p><strong>Representations do not imply that the vectors involved are eigenvectors of the resultant density matrix.</strong> That's true <em>if</em> the projectors are pairwise orthogonal, but that's not a requirement at all, so it is perfectly possible to construct",
        "as a sum of projectors that have nothing to do with the sum's eigenprojectors.</p>\n\n<p>It's probably helpful to illustrate this with an explicit example, for clarity. Consider a two-level system that's prepared in a superposition of the form",
        "i.e. an angle",
        "down from the north pole of the Bloch sphere, except that each time we flip a fair coin to see which sign of",
        "(i.e. which direction on the prime meridian) we take. Then the density matrix reads\n\\begin{align}\n\\rho \n& = \\frac12 \\bigg( |\\theta_+\\rangle\\langle\\theta_+| +|\\theta_-\\rangle\\langle\\theta_-| \\bigg)\n\\\\ & = \\frac12 \\bigg( \n\\big(\\cos(\\theta/2)|0\\rangle + \\sin(\\theta/2)|1\\rangle \\big) \\big(\\cos(\\theta/2)\\langle 0| + \\sin(\\theta/2)\\langle 1| \\big)\n\\\\ & \\qquad +\n\\big(\\cos(\\theta/2)|0\\rangle - \\sin(\\theta/2)|1\\rangle \\big) \\big(\\cos(\\theta/2)\\langle 0| - \\sin(\\theta/2)\\langle 1| \\big)\n \\bigg)\n%\\\\ & = \\frac12 \\bigg( \n%\\big(\\cos^2(\\theta/2)|0\\rangle\\langle 0|  + \\sin(\\theta/2)\\cos(\\theta/2)|1\\rangle %\\langle 0| + \\sin(\\theta/2)\\cos(\\theta/2)|0\\rangle \\langle 1| + %\\sin^2(\\theta/2)|1\\rangle\\langle 1| \\big)\n%\\\\ & \\qquad +\n%\\big(\\cos^2(\\theta/2)|0\\rangle\\langle 0|  - \\sin(\\theta/2)\\cos(\\theta/2)|1\\rangle %\\langle 0| - \\sin(\\theta/2)\\cos(\\theta/2)|0\\rangle \\langle 1| + %\\sin^2(\\theta/2)|1\\rangle\\langle 1| \\big)\n% \\bigg)\n\\\\ & = \n\\cos^2(\\theta/2)|0\\rangle\\langle 0| + \\sin^2(\\theta/2)|1\\rangle\\langle 1| \n\\end{align}\nbecause the off-diagonal terms cancel out. In this second representation, we <em>do</em> have orthogonal projectors, so here",
        "are indeed the unique eigenvectors of",
        "(unless",
        "is maximally mixed). But that doesn't stop our initial representation,",
        ", with its non-orthogonal, non-eigenvector components, from also being true.</p></li>\n<li><p><strong>If a state is built up using non-orthogonal projectors, then it <em>also</em> has a separate representation in terms of orthogonal projectors</strong>, and that's perfectly fine. Representations of the form",
        "are a dime a dozen if you know where to look. So, you found one that's not the canonical one: great! there's millions where that one came from.</p></li>\n<li><p><strong>Representations of the form",
        "really <em>are</em> a dime a dozen.</strong> If you want to build one yourself, say, for a two-level system, there's a few points that are particularly relevant to the recipe:</p>\n\n<ul>\n<li>The Pauli matrices are a basis for all valid density matrices, i.e. if",
        "is traceless, then it can be represented as",
        "are the Pauli matrices. (Further, that relationship can be inverted via",
        ".)</li>\n<li>The positivity condition",
        "translates into the condition",
        ", i.e.",
        "lives inside the unit ball or its boundary",
        "generally known as the Bloch ball and the Bloch sphere in this context.</li>\n<li>If",
        "is on the Bloch sphere boundary, then",
        "is a pure state, and if you write",
        "(which you always can) then",
        "are the polar and azimuthal spherical coordinates for",
        "</li>\n<li>The relationship between",
        "is linear and bijective.</li>\n<li>If",
        "are valid density matrices, then any <a href=\"https://en.wikipedia.org/wiki/Convex_combination\" rel=\"noreferrer\">convex combination</a>",
        "of the two, with weights adding to",
        ", is also a valid density matrix.</li>\n<li>Because the relationship between density matrices and Bloch-ball vectors is linear, any convex combination of density matrices translates directly into a convex combination of the corresponding Bloch-ball vectors. Thus, if",
        ", then",
        "lies on the line that goes from",
        "to",
        ", a fraction",
        "of the way in that direction.</li>\n</ul>\n\n<p>So, what does this mean for density-matrix representations? If you have a target density matrix",
        "that you want to represent, simply take its Bloch-ball vector",
        ", and then pick",
        "points",
        "on the Bloch sphere itself (the boundary) and weights",
        "(normalized to",
        ") such that their average",
        "gives you your chosen point. That will then naturally give you a representation of your density matrix as a weighted sum of"
      ],
      "created": "2018-05-05T14:50:15.443",
      "golden_ner_terms": [
        "angle",
        "atoms",
        "average",
        "ball",
        "basis",
        "bijective",
        "bloch sphere",
        "boundary",
        "canonical",
        "class",
        "class i",
        "collection",
        "combination",
        "component",
        "components",
        "construct",
        "contains",
        "context",
        "convex",
        "convex combination",
        "coordinates",
        "density",
        "density operator",
        "dimension",
        "eigenvectors",
        "extremal",
        "field",
        "fraction",
        "independent",
        "l system",
        "lies on",
        "line",
        "matrix",
        "matrix representation",
        "mean",
        "meridian",
        "north pole",
        "number",
        "obvious",
        "operator",
        "operators",
        "orthogonal",
        "orthonormal",
        "passing through",
        "pauli matrices",
        "period",
        "plane",
        "point",
        "polar",
        "polarization",
        "pole",
        "positive",
        "positive semidefinite",
        "prime",
        "pure state",
        "quantum state",
        "quantum states",
        "representation",
        "resultant",
        "self-adjoint",
        "sequence",
        "space",
        "span",
        "sphere",
        "spherical coordinates",
        "state",
        "state space",
        "straight",
        "sum",
        "superposition",
        "system state",
        "time",
        "trace",
        "unit",
        "unit ball",
        "valid",
        "vector",
        "vector component",
        "vectors",
        "way",
        "word"
      ],
      "golden_ner_count": 81,
      "golden_patterns": [
        {
          "pattern": "construct-an-explicit-witness",
          "score": 6.0,
          "hotwords": [
            "construct",
            "explicit",
            "build"
          ]
        },
        {
          "pattern": "work-examples-first",
          "score": 4.0,
          "hotwords": [
            "example",
            "illustrate"
          ]
        },
        {
          "pattern": "unfold-the-definition",
          "score": 4.0,
          "hotwords": [
            "definition of",
            "means that"
          ]
        },
        {
          "pattern": "quotient-by-irrelevance",
          "score": 2.0,
          "hotwords": [
            "up to"
          ]
        },
        {
          "pattern": "check-the-extreme-cases",
          "score": 2.0,
          "hotwords": [
            "boundary"
          ]
        },
        {
          "pattern": "dualise-the-problem",
          "score": 2.0,
          "hotwords": [
            "adjoint"
          ]
        },
        {
          "pattern": "the-diagonal-argument",
          "score": 2.0,
          "hotwords": [
            "diagonal"
          ]
        },
        {
          "pattern": "estimate-by-bounding",
          "score": 2.0,
          "hotwords": [
            "bound"
          ]
        },
        {
          "pattern": "optimise-a-free-parameter",
          "score": 2.0,
          "hotwords": [
            "pick"
          ]
        }
      ],
      "golden_pattern_names": [
        "construct-an-explicit-witness",
        "work-examples-first",
        "unfold-the-definition",
        "quotient-by-irrelevance",
        "check-the-extreme-cases",
        "dualise-the-problem",
        "the-diagonal-argument",
        "estimate-by-bounding",
        "optimise-a-free-parameter"
      ],
      "golden_scopes": [
        {
          "type": "consider",
          "match": "Consider a two-level system that's prepared in a superposition of the f"
        },
        {
          "type": "where-binding",
          "match": "where $\\rho$ is"
        },
        {
          "type": "set-notation",
          "match": "$\\vec p = (p_x,p_y,p_z)\\in \\mathbb R^3$"
        },
        {
          "type": "set-notation",
          "match": "$\\theta\\in [0,\\pi]$"
        }
      ],
      "golden_scope_count": 4
    },
    {
      "id": "se-physics-420080",
      "stratum": "medium",
      "title": "What is the symmetry of the pion triplet ($\\pi^{-}, \\pi^{0}, \\pi^{+}$)?",
      "tags": [
        "particle-physics",
        "symmetry",
        "standard-model",
        "isospin-symmetry",
        "pions"
      ],
      "score": 8,
      "answer_score": 11,
      "question_body": "Under the entry \"Isospin\" in Wikipedia, it states: The pions are assigned to the triplet (the spin-1, $\\mathbf{3}$ , or adjoint representation) of $SU(2)$ Why is the symmetry not $SU(3)$ since there are three particles? And in what circumstance do we have an $SU(3)$ symmetry?",
      "answer_body": "$\\newcommand{\\BK}[3]{\\left|{#1},{#2}\\right\\rangle_{#3}} \\newcommand{\\BKB}[3]{\\mathbf{\\left|{#1},{#2}\\right\\rangle_{\\boldsymbol{#3}}}} \\newcommand{\\FR}[2]{{\\textstyle \\frac{#1}{#2}}} \\newcommand{\\BoldExp}[2]{{#1}^{\\boldsymbol{#2}}} \\newcommand{\\CMRR}[2] { \\begin{bmatrix} #1 \\\\ #2 \\end{bmatrix} } \\newcommand{\\MM}[4] { \\begin{bmatrix} #1 & #2\\\\ #3 & #4 \\end{bmatrix} } \\newcommand{\\MMM}[9] { \\begin{bmatrix} #1 & #2 & #3 \\\\ #4 & #5 & #6 \\\\ #7 & #8 & #9 \\\\ \\end{bmatrix} } \\newcommand{\\CMRRRR}[4] { \\begin{bmatrix} #1 \\\\ #2 \\\\ #3 \\\\ #4 \\end{bmatrix} } \\newcommand{\\CMRRR}[3] { \\begin{bmatrix} #1 \\\\ #2 \\\\ #3 \\end{bmatrix} } \\newcommand{\\RMCC}[2] { \\begin{bmatrix} #1 & #2 \\end{bmatrix} } \\newcommand{\\RMCCC}[3] { \\begin{bmatrix} #1 & #2 & #3 \\end{bmatrix} } \\newcommand{\\RMCCCC}[4] { \\begin{bmatrix} #1 & #2 & #3 & #4 \\end{bmatrix} } \\newcommand{\\OSS}[1] {\\overset{\\boldsymbol{\\sim}}{#1}} \\newcommand{\\BoldSub}[2]{{#1}_{\\boldsymbol{#2}}} \\newcommand{\\OSB}[1] {\\overset{\\boldsymbol{-\\!\\!\\!\\!\\!-}}{#1}}$ These pions are mesons, composite particles of a quark $\\boldsymbol{\\lbrace}\\boldsymbol{u},\\boldsymbol{d}\\boldsymbol{\\rbrace}$ and an antiquark $\\boldsymbol{\\lbrace}\\OSB{\\boldsymbol{u}},\\overline{\\boldsymbol{d}}\\boldsymbol{\\rbrace}$ : \\begin{equation} \\begin{array}{cccccccc} &\\boldsymbol{\\lbrace}\\boldsymbol{u},\\boldsymbol{d}\\boldsymbol{\\rbrace} \\!\\!\\!\\!\\!&\\boldsymbol{\\otimes}& \\!\\!\\!\\!\\boldsymbol{\\lbrace}\\OSB{\\boldsymbol{u}},\\overline{\\boldsymbol{d}}\\boldsymbol{\\rbrace} & \\!\\!\\boldsymbol{=}\\!\\! & \\boldsymbol{\\lbrace}\\boldsymbol{\\omega}\\boldsymbol{\\rbrace}& \\!\\!\\!\\!\\boldsymbol{\\oplus}\\!\\!&\\boldsymbol{\\lbrace}\\BoldExp{\\boldsymbol{\\pi}}{-},\\BoldExp{\\boldsymbol{\\pi}}{0},\\BoldExp{\\boldsymbol{\\pi}}{+}\\boldsymbol{\\rbrace} & \\\\ & \\boldsymbol{2}\\!\\!\\!\\!\\! & \\boldsymbol{\\otimes} & \\!\\!\\!\\!\\OSB{\\boldsymbol{2}} & \\!\\!\\boldsymbol{=}\\!\\!&\\boldsymbol{1}&\\!\\!\\!\\!\\boldsymbol{\\oplus}\\!\\!&\\boldsymbol{3}& \\end{array} \\tag{01}\\label{eq01} \\end{equation} \\begin{align} &\\left\\{ \\boldsymbol{\\omega} = \\sqrt{\\tfrac{1}{2}}\\left(\\boldsymbol{u}\\OSB{\\boldsymbol{u}}+\\boldsymbol{d}\\overline{\\boldsymbol{d}} \\right)\\hphantom{=\\,}\\right\\} \\quad \\,\\text{the singlet }\\boldsymbol{1} \\tag{02.1}\\label{eq02.1}\\\\ &\\left. \\begin{cases} \\BoldExp{\\boldsymbol{\\pi}}{-} =\\boldsymbol{d}\\OSB{\\boldsymbol{u}} \\\\ \\BoldExp{\\boldsymbol{\\pi}}{0} =\\sqrt{\\tfrac{1}{2}}\\left(\\boldsymbol{u}\\OSB{\\boldsymbol{u}}-\\boldsymbol{d}\\overline{\\boldsymbol{d}} \\right)\\\\ \\BoldExp{\\boldsymbol{\\pi}}{+} =\\boldsymbol{u}\\overline{\\boldsymbol{d}} \\end{cases}\\right\\}\\quad \\text{the triplet }\\boldsymbol{3} \\tag{02.2}\\label{eq02.2} \\end{align} The subspaces $\\;\\boldsymbol{1},\\boldsymbol{3}\\;$ are invariant under the isospin group $\\;SU(2)$ . EDIT responds to a comment by the OP owner : This explanation is fine. But I still have a puzzlement. While the three pions ( $\\pi^{-}, \\pi^{0}, \\pi^{+}$ ) have an $SU(2)$ symmetry, why do the three quarks ( $u,d,s$ ) have an $SU(3)$ [not $SU(2)$ ] symmetry? More generally, given three similar particles, how do we know whether they have an $SU(2)$ symmetry or an $SU(3)$ symmetry? We must not confuse the number $\\;n\\;$ of the symmetry group $\\;SU(n)\\;$ with the number $\\;m\\;$ of the resulting $\\;m-$ plets (singlets,doublets,triplets,...nonets, etc). In the following three examples the number $\\;n\\;$ of the symmetry group $\\;SU(n)\\;$ is the number of the $\\;n\\;$ independent $\\;n-$ dimensional systems we put together to build a composite system. $\\color{blue}{\\textbf{Example A :}}$ If we put together a particle $\\;\\alpha\\;$ of spin angular momentum $\\;j_{\\alpha}=\\frac12\\;$ with a particle $\\;\\beta\\;$ of spin angular momentum $\\;j_{\\beta}=\\frac12\\;$ then the resulting multiplets is a singlet of angular momentum $\\;j_{1}=0\\;$ and a triplet of angular momentum $\\;j_{2}=1\\;$ \\begin{equation} \\boldsymbol{2}\\boldsymbol{\\otimes}\\boldsymbol{2}=\\boldsymbol{1}\\boldsymbol{\\oplus}\\boldsymbol{3} \\tag{ed-01}\\label{eqed-01} \\end{equation} Now let apply the following $\\;SU(2)\\;$ transformations to the systems $\\;\\alpha,\\beta\\;$ (particles) respectively \\begin{align} ^{\\bf 2}U_{\\bf \\alpha} & = \\MM{\\hphantom{\\boldsymbol{-}}g_{\\bf \\alpha}}{h_{\\bf \\alpha}}{\\vphantom{h^{\\boldsymbol{*}}_{\\bf \\beta}}\\boldsymbol{-}h^{\\boldsymbol{*}}_{\\bf \\alpha}}{g^{\\boldsymbol{*}}_{\\bf \\alpha}}_{\\bf a} \\,,\\quad g_{\\bf \\alpha}g^{\\boldsymbol{*}}_{\\bf \\alpha}\\boldsymbol{+}h_{\\bf \\alpha}h^{\\boldsymbol{*}}_{\\bf \\alpha}=1 \\tag{ed-02a}\\label{eqed-02a}\\\\ ^{\\bf 2}U_{\\bf \\beta} & = \\MM{\\hphantom{\\boldsymbol{-}}g_{\\bf \\beta}}{h_{\\bf \\beta}}{\\boldsymbol{-}h^{\\boldsymbol{*}}_{\\bf \\beta}}{g^{\\boldsymbol{*}}_{\\bf \\beta}}_{\\bf b} \\,,\\quad g_{\\bf \\beta}g^{\\boldsymbol{*}}_{\\bf \\beta}\\boldsymbol{+}h_{\\bf \\beta}h^{\\boldsymbol{*}}_{\\bf \\beta}=1 \\tag{ed-02b}\\label{eqed-02b} \\end{align} In the composite system this is a $\\;SU(4)\\;$ transformation, the product of the two ones above \\begin{equation} ^{\\bf 4}U_{ f} = \\left(^{\\bf 2}U_{\\bf \\alpha}\\right)\\boldsymbol{\\otimes}\\left(^{\\bf 2}U_{\\bf \\beta}\\right) = \\MM{\\hphantom{\\boldsymbol{-}}g_{\\bf \\alpha}}{h_{\\bf \\alpha}}{\\vphantom{h^{\\boldsymbol{*}}_{\\bf \\beta}}\\boldsymbol{-}h^{\\boldsymbol{*}}_{\\bf \\alpha}}{g^{\\boldsymbol{*}}_{\\bf \\alpha}}_{\\bf a}\\!\\!\\! \\boldsymbol{\\otimes} \\MM{\\hphantom{\\boldsymbol{-}}g_{\\bf \\beta}}{h_{\\bf \\beta}}{\\boldsymbol{-}h^{\\boldsymbol{*}}_{\\bf \\beta}}{g^{\\boldsymbol{*}}_{\\bf \\beta}}_{\\bf b}\\!\\!\\! = \\begin{bmatrix} \\hphantom{\\boldsymbol{-}}g_{\\bf \\alpha}g_{\\bf \\beta} & \\hphantom{\\boldsymbol{-}}g_{\\bf \\alpha}h_{\\bf \\beta} & \\hphantom{\\boldsymbol{-}}h_{\\bf \\alpha}g_{\\bf \\beta} & h_{\\bf \\alpha}h_{\\bf \\beta} \\\\ \\boldsymbol{-}g_{\\bf \\alpha}h^{\\boldsymbol{*}}_{\\bf \\beta} & \\hphantom{\\boldsymbol{-}}g_{\\bf \\alpha}g^{\\boldsymbol{*}}_{\\bf \\beta} & \\boldsymbol{-}h_{\\bf \\alpha}h^{\\boldsymbol{*}}_{\\bf \\beta} & h_{\\bf \\alpha}g^{\\boldsymbol{*}}_{\\bf \\beta} \\\\ \\boldsymbol{-}h^{\\boldsymbol{*}}_{\\bf \\alpha}g_{\\bf \\beta} & \\boldsymbol{-}h^{\\boldsymbol{*}}_{\\bf \\alpha}h_{\\bf \\beta} & \\hphantom{\\boldsymbol{-}}g^{\\boldsymbol{*}}_{\\bf \\alpha}g_{\\bf \\beta} & g^{\\boldsymbol{*}}_{\\bf \\alpha}h_{\\bf \\beta} \\\\ \\hphantom{\\boldsymbol{-}}h^{\\boldsymbol{*}}_{\\bf \\alpha}h^{\\boldsymbol{*}}_{\\bf \\beta} & \\boldsymbol{-}h^{\\boldsymbol{*}}_{\\bf \\alpha}g^{\\boldsymbol{*}}_{\\bf \\beta} & \\boldsymbol{-}g^{\\boldsymbol{*}}_{\\bf \\alpha}h^{\\boldsymbol{*}}_{\\bf \\beta} & g^{\\boldsymbol{*}}_{\\bf \\alpha}g^{\\boldsymbol{*}}_{\\bf \\beta} \\end{bmatrix}_{\\bf e} \\tag{ed-03}\\label{eqed-03} \\end{equation} But the $\\;SU(2)\\;$ transformations in \\eqref{eqed-02a},\\eqref{eqed-02b} represent rotations in the real space $\\;\\mathbb{R}^{3}\\;$ wherein both particles live, so they must be identical (we would not rotate one system differently from the other) \\begin{equation} ^{\\bf 2}U_{\\bf \\alpha} =\\,^{\\bf 2}U_{\\bf \\beta}=\\, ^{\\bf 2}U = \\MM{\\:\\:g}{h}{\\boldsymbol{-}h^{\\boldsymbol{*}}}{\\:\\:g^{\\boldsymbol{*}}} \\,,\\quad gg^{\\boldsymbol{*}}\\boldsymbol{+}hh^{\\boldsymbol{*}}=1 \\tag{ed-04}\\label{eqed-04} \\end{equation} so that \\eqref{eqed-03} yields \\begin{equation} ^{\\bf 4}U_{ f} = \\left(^{\\bf 2}U_{\\bf \\alpha}\\right)\\boldsymbol{\\otimes}\\left(^{\\bf 2}U_{\\bf \\beta}\\right) =\\left(^{\\bf 2}U\\right)^{\\boldsymbol{\\otimes}2} = \\begin{bmatrix} \\:g^{2} & \\:\\:gh & \\:hg & \\!\\!\\!h^{2} \\\\ \\boldsymbol{-}gh^{\\boldsymbol{*}} & \\hphantom{\\boldsymbol{-}}gg^{\\boldsymbol{*}} & \\boldsymbol{-}hh^{\\boldsymbol{*}} & hg^{\\boldsymbol{*}}\\\\ \\boldsymbol{-}h^{\\boldsymbol{*}}g & \\,\\boldsymbol{-}h^{\\boldsymbol{*}}h & \\hphantom{\\boldsymbol{-}}g^{\\boldsymbol{*}}g & g^{\\boldsymbol{*}}h \\\\ \\hphantom{\\boldsymbol{-}}h^{\\boldsymbol{*}2} & \\:\\:\\boldsymbol{-}h^{\\boldsymbol{*}}g^{\\boldsymbol{*}} & \\:\\:\\boldsymbol{-}g^{\\boldsymbol{*}}h^{\\boldsymbol{*}} & g^{\\boldsymbol{*}2} \\end{bmatrix}_{\\bf e} \\tag{ed-05}\\label{eqed-05} \\end{equation} This matrix expressed in the basis of the irreducible direct sum \\eqref{eqed-01} is \\begin{equation} ^{\\bf 4}\\OSS{U}_{ f}= \\begin{bmatrix} \\begin{array}{c|ccc} \\:\\: 1 \\:\\: &\\rule [0ex]{20pt}{0.0ex}&\\rule [-2.5ex]{0pt}{6.0ex} \\rule [0ex]{16pt}{0ex}& \\rule [0ex]{16pt}{0ex}\\\\ \\hline \\rule [-3ex]{0pt}{6ex}&g^{2}& \\sqrt{2} g h & h^{2} \\\\ \\rule [-3ex]{0pt}{6ex}& -\\sqrt{2} g h^{\\boldsymbol{*}} & \\left(g g^{\\boldsymbol{*}}-h h^{\\boldsymbol{*}}\\right) & \\sqrt{2} g^{\\boldsymbol{*}} h \\\\ \\rule [-3ex]{0pt}{6ex}& \\left(h^{\\boldsymbol{*}}\\right)^{2} & - \\sqrt{2}g^{\\boldsymbol{*}} h^{\\boldsymbol{*}} & \\left(g^{\\boldsymbol{*}}\\right)^{2} \\end{array} \\end{bmatrix}_{\\:\\mathbf{f}} = \\begin{bmatrix} \\begin{array}{c|ccc} ^{\\mathbf{1}}U_{\\boldsymbol{\\left[1\\right]}}&\\rule [0ex]{20pt}{0.0ex}&\\rule [-2.5ex]{0pt}{6.0ex} \\rule [0ex]{16pt}{0ex}& \\rule [0ex]{16pt}{0ex}\\\\ \\hline \\rule [-3ex]{0pt}{6ex}&\\rule [0.0ex]{50pt}{0.0ex}& \\rule [0.0ex]{50pt}{0.0ex} &\\rule [0.0ex]{50pt}{0.0ex}\\\\ \\rule [-3ex]{0pt}{6ex}& & ^{\\mathbf{3}}U_{\\boldsymbol{\\left[2\\right]}} & \\\\ \\rule [-3ex]{0pt}{6ex}& & & \\end{array} \\end{bmatrix}_{\\:\\mathbf{f}} \\tag{ed-06}\\label{eqed-06} \\end{equation} where $\\:^{\\mathbf{1}}U_{\\boldsymbol{\\left[1\\right]}}\\:$ and $\\:^{\\mathbf{3}}U_{\\boldsymbol{\\left[2\\right]}}\\:$ are special unitary matrices in the spaces of the singlet and of the triplet respectively given by \\begin{equation} ^{\\mathbf{1}}U_{\\boldsymbol{\\left[1\\right]}}= \\begin{bmatrix} 1 \\end{bmatrix} \\quad \\in SU(1)\\equiv \\{1\\} \\tag{ed-07}\\label{eqed-07} \\end{equation} \\begin{equation} ^{\\mathbf{3}}U_{\\boldsymbol{\\left[2\\right]}}= \\begin{bmatrix} g^{2}& \\sqrt{2} g h & h^{2} \\rule [-3ex]{0pt}{6ex}\\\\ -\\sqrt{2} g h^{\\boldsymbol{*}} & \\left(g g^{\\boldsymbol{*}}-h h^{\\boldsymbol{*}}\\right) & \\sqrt{2} g^{\\boldsymbol{*}} h \\rule [-3ex]{0pt}{6ex}\\\\ \\left(h^{\\boldsymbol{*}}\\right)^{2} & - \\sqrt{2}g^{\\boldsymbol{*}} h^{\\boldsymbol{*}} & \\left(g^{\\boldsymbol{*}}\\right)^{2} \\rule [-3ex]{0pt}{6ex} \\end{bmatrix} \\quad \\in SU(3) \\tag{ed-08}\\label{eqed-08} \\end{equation} So if we apply the $\\;SU(2)\\;$ transformation $\\:^{\\bf 2}U\\:$ of \\eqref{eqed-04} on both spaces in the product of the lhs of \\eqref{eqed-01} then the spaces of the terms of the direct sum of the rhs side of the same equation remain invariant, the singlet \\eqref{eq02.1} invariant under $\\;SU(1)\\;$ (more exactly unchanged) and the triplet \\eqref{eq02.2} transformed under $\\;SU(3)\\;$ remaining in its invariant space. We say that the symmetry group is $\\;SU(2)$ , NOT $\\;SU(1)\\;$ or $\\;SU(3)\\;$ of the resulting multiplets. Reference link : Total spin of two spin-1/2 particles . $\\color{blue}{\\textbf{Example B :}}$ The quark model of baryons consisting of three quarks. So, suppose we know the existence of three quarks only : $\\boldsymbol{u}$ , $\\boldsymbol{d}$ and $\\boldsymbol{s}$ . Under full symmetry (the same mass) these are the basic states, let \\begin{equation} \\boldsymbol{u}= \\begin{bmatrix} 1\\\\ 0\\\\ 0 \\end{bmatrix} \\qquad \\boldsymbol{d}= \\begin{bmatrix} 0\\\\ 1\\\\ 0 \\end{bmatrix} \\qquad \\boldsymbol{s}= \\begin{bmatrix} 0\\\\ 0\\\\ 1 \\end{bmatrix} \\tag{ed-09}\\label{eqed-09} \\end{equation} of a 3-dimensional complex Hilbert space of quarks, say $\\mathbf{Q}\\equiv \\mathbb{C}^{\\boldsymbol{3}}$ . A quark $\\boldsymbol{\\xi} \\in \\mathbf{Q}$ is expressed in terms of these basic states as \\begin{equation} \\boldsymbol{\\xi}=\\xi_1\\boldsymbol{u}+\\xi_2\\boldsymbol{d}+\\xi_3\\boldsymbol{s}= \\begin{bmatrix} \\xi_1\\\\ \\xi_2\\\\ \\xi_3 \\end{bmatrix} \\qquad \\xi_1,\\xi_2,\\xi_3 \\in \\mathbb{C} \\tag{ed-10}\\label{eqed-10} \\end{equation} Let take 2 more quarks in order to construct baryons from 3 quarks \\begin{equation} \\boldsymbol{\\eta}=\\eta_1\\boldsymbol{u}+\\eta_2\\boldsymbol{d}+\\eta_3\\boldsymbol{s}= \\begin{bmatrix} \\eta_1\\\\ \\eta_2\\\\ \\eta_3 \\end{bmatrix} \\:, \\qquad \\boldsymbol{\\zeta}=\\zeta_1\\boldsymbol{u}+\\zeta_2\\boldsymbol{d}+\\zeta_3\\boldsymbol{s}= \\begin{bmatrix} \\zeta_1\\\\ \\zeta_2\\\\ \\zeta_3 \\end{bmatrix} \\tag{ed-11}\\label{eqed-11} \\end{equation} A baryon state $\\:T\\:$ in the product space \\begin{equation} \\mathbf{B}=\\boldsymbol{3}\\boldsymbol{\\otimes}\\boldsymbol{3}\\boldsymbol{\\otimes}\\boldsymbol{3}=\\mathbf{Q}\\boldsymbol{\\otimes}\\mathbf{Q}\\boldsymbol{\\otimes}\\mathbf{Q}\\equiv \\mathbb{C}^{\\boldsymbol{3}}\\boldsymbol{\\otimes}\\mathbb{C}^{\\boldsymbol{3}}\\boldsymbol{\\otimes}\\mathbb{C}^{\\boldsymbol{3}}=\\mathbb{C}^{\\boldsymbol{27}} \\tag{ed-12}\\label{eqed-12} \\end{equation} is the product of the states of above 3 quarks \\begin{equation} T=\\boldsymbol{\\xi}\\boldsymbol{\\otimes}\\boldsymbol{\\eta}\\boldsymbol{\\otimes}\\boldsymbol{\\zeta} \\tag{ed-13}\\label{eqed-13} \\end{equation} The final result of a full analysis is \\begin{equation} \\boldsymbol{3}\\boldsymbol{\\otimes}\\boldsymbol{3}\\boldsymbol{\\otimes}\\boldsymbol{3}= \\boldsymbol{1}\\boldsymbol{\\oplus}\\boldsymbol{10}\\boldsymbol{\\oplus} \\boldsymbol{8}^{\\boldsymbol{\\prime}}\\boldsymbol{\\oplus}\\boldsymbol{8} \\tag{ed-14}\\label{eqed-14} \\end{equation} that is the space of states of a baryon is the direct sum of a singlet $\\;\\boldsymbol{1}$ , a decuplet $\\;\\boldsymbol{10}$ , a mixed symmetric octet $\\;\\boldsymbol{8'}$ and a mixed anti-symmetric octet $\\;\\boldsymbol{8}$ . Now applying a $\\;SU(3)\\;$ transformation $\\;^{\\bf 3}U\\;$ on the 3-dimensional space $\\mathbf{Q}\\equiv \\mathbb{C}^{\\boldsymbol{3}}$ results in a $\\;SU(27)\\;$ transformation $\\;^{\\bf 27}U\\;$ on the 27-dimensional space $\\;\\mathbf{B}\\;$ of equation \\eqref{eqed-12} \\begin{equation} ^{\\bf 27}U = \\left(^{\\bf 3}U\\right)\\boldsymbol{\\otimes}\\left(^{\\bf 3}U\\right)\\boldsymbol{\\otimes}\\left(^{\\bf 3}U\\right) =\\left(^{\\bf 3}U\\right)^{\\boldsymbol{\\otimes}3} \\tag{ed-15}\\label{eqed-15} \\end{equation} The space of each $\\;m-$ plet remains invariant and a state in this $\\;m-$ plet is transformed under a $\\;SU(m)\\;$ transformation, where $\\;m=1,10,8,8$ . But We say that the symmetry group is $\\;SU(3)$ , NOT $\\;SU(1)\\;$ or $\\;SU(10)\\;$ or $\\;SU(8)\\;$ of the resulting multiplets. Reference link : Symmetry in terms of matrices .",
      "question_latex": [
        "\\mathbf{3}",
        "SU(2)",
        "SU(3)"
      ],
      "answer_latex": [
        "\\newcommand{\\BK}[3]{\\left|{#1},{#2}\\right\\rangle_{#3}}\n   \\newcommand{\\BKB}[3]{\\mathbf{\\left|{#1},{#2}\\right\\rangle_{\\boldsymbol{#3}}}}\n   \\newcommand{\\FR}[2]{{\\textstyle \\frac{#1}{#2}}}\n   \\newcommand{\\BoldExp}[2]{{#1}^{\\boldsymbol{#2}}}\n   \\newcommand{\\CMRR}[2]\n         {   \\begin{bmatrix}\n               #1 \\\\                                       \n               #2           \n             \\end{bmatrix}    }\n   \\newcommand{\\MM}[4] \n         {   \\begin{bmatrix}                                  \n               #1 & #2\\\\                                  \n               #3 & #4\n             \\end{bmatrix}    }\n   \\newcommand{\\MMM}[9] \n         {   \\begin{bmatrix}                     \n               #1 & #2 & #3 \\\\\n               #4 & #5 & #6 \\\\\n               #7 & #8 & #9 \\\\\n             \\end{bmatrix}     }\n   \\newcommand{\\CMRRRR}[4]\n          {  \\begin{bmatrix}                                  \n               #1 \\\\                                       \n               #2 \\\\\n               #3 \\\\\n               #4\n             \\end{bmatrix}    }\n   \\newcommand{\\CMRRR}[3]                           \n         {   \\begin{bmatrix}\n               #1 \\\\                                     \n               #2 \\\\ \n               #3 \n             \\end{bmatrix}    }\n   \\newcommand{\\RMCC}[2]  \n         {   \\begin{bmatrix}\n               #1  &  #2                    \n             \\end{bmatrix}    } \n   \\newcommand{\\RMCCC}[3] \n         {    \\begin{bmatrix}                                \n                #1  &  #2  &  #3                \n              \\end{bmatrix}   }\n   \\newcommand{\\RMCCCC}[4]\n         {    \\begin{bmatrix}\n                #1  &  #2  &  #3  &  #4\n              \\end{bmatrix}     }\n   \\newcommand{\\OSS}[1]\n         {\\overset{\\boldsymbol{\\sim}}{#1}}\n   \\newcommand{\\BoldSub}[2]{{#1}_{\\boldsymbol{#2}}}\n   \\newcommand{\\OSB}[1]\n         {\\overset{\\boldsymbol{-\\!\\!\\!\\!\\!-}}{#1}}",
        "\\boldsymbol{\\lbrace}\\boldsymbol{u},\\boldsymbol{d}\\boldsymbol{\\rbrace}",
        "\\boldsymbol{\\lbrace}\\OSB{\\boldsymbol{u}},\\overline{\\boldsymbol{d}}\\boldsymbol{\\rbrace}",
        "\\;\\boldsymbol{1},\\boldsymbol{3}\\;",
        "\\;SU(2)",
        "\\pi^{-}, \\pi^{0}, \\pi^{+}",
        "SU(2)",
        "u,d,s",
        "SU(3)",
        "\\;n\\;",
        "\\;SU(n)\\;",
        "\\;m\\;",
        "\\;m-",
        "\\;n-",
        "\\color{blue}{\\textbf{Example A :}}",
        "\\;\\alpha\\;",
        "\\;j_{\\alpha}=\\frac12\\;",
        "\\;\\beta\\;",
        "\\;j_{\\beta}=\\frac12\\;",
        "\\;j_{1}=0\\;",
        "\\;j_{2}=1\\;",
        "\\;SU(2)\\;",
        "\\;\\alpha,\\beta\\;",
        "\\;SU(4)\\;",
        "\\;\\mathbb{R}^{3}\\;",
        "\\:^{\\mathbf{1}}U_{\\boldsymbol{\\left[1\\right]}}\\:",
        "\\:^{\\mathbf{3}}U_{\\boldsymbol{\\left[2\\right]}}\\:",
        "\\:^{\\bf 2}U\\:",
        "\\;SU(1)\\;",
        "\\;SU(3)\\;",
        "\\color{blue}{\\textbf{Example B :}}",
        "\\boldsymbol{u}",
        "\\boldsymbol{d}",
        "\\boldsymbol{s}",
        "\\mathbf{Q}\\equiv \\mathbb{C}^{\\boldsymbol{3}}",
        "\\boldsymbol{\\xi} \\in \\mathbf{Q}",
        "\\:T\\:",
        "\\;\\boldsymbol{1}",
        "\\;\\boldsymbol{10}",
        "\\;\\boldsymbol{8'}",
        "\\;\\boldsymbol{8}",
        "\\;^{\\bf 3}U\\;",
        "\\;SU(27)\\;",
        "\\;^{\\bf 27}U\\;",
        "\\;\\mathbf{B}\\;",
        "\\;SU(m)\\;",
        "\\;m=1,10,8,8",
        "\\;SU(3)",
        "\\;SU(10)\\;",
        "\\;SU(8)\\;"
      ],
      "created": "2018-07-29T18:54:29.213",
      "golden_ner_terms": [
        "adjoint",
        "adjoint representation",
        "analysis",
        "angular momentum",
        "anti-symmetric",
        "baryons",
        "basic",
        "basis",
        "complex",
        "composite",
        "construct",
        "decuplet",
        "direct sum",
        "equation",
        "group",
        "h space",
        "hilbert space",
        "independent",
        "invariant",
        "irreducible",
        "l system",
        "link",
        "live",
        "mass",
        "matrix",
        "mesons",
        "model",
        "momentum",
        "number",
        "order",
        "pions",
        "product",
        "quarks",
        "real",
        "representation",
        "rotate",
        "side",
        "similar",
        "space",
        "spin group",
        "state",
        "sum",
        "symmetric",
        "symmetry",
        "transformation",
        "transformations",
        "triplet",
        "unitary",
        "wikipedia"
      ],
      "golden_ner_count": 49,
      "golden_patterns": [
        {
          "pattern": "exploit-symmetry",
          "score": 6.0,
          "hotwords": [
            "symmetry",
            "symmetric",
            "invariant"
          ]
        },
        {
          "pattern": "construct-an-explicit-witness",
          "score": 4.0,
          "hotwords": [
            "construct",
            "build"
          ]
        },
        {
          "pattern": "work-examples-first",
          "score": 2.0,
          "hotwords": [
            "example"
          ]
        }
      ],
      "golden_pattern_names": [
        "exploit-symmetry",
        "construct-an-explicit-witness",
        "work-examples-first"
      ],
      "golden_scopes": [
        {
          "type": "for-any",
          "match": "each $\\;m-$"
        },
        {
          "type": "set-notation",
          "match": "$ are special unitary matrices in the spaces of the singlet and of the triplet r"
        },
        {
          "type": "set-notation",
          "match": "$\\boldsymbol{\\xi} \\in \\mathbf{Q}$"
        }
      ],
      "golden_scope_count": 3
    },
    {
      "id": "se-physics-627879",
      "stratum": "medium",
      "title": "What do field operators in QFT act on?",
      "tags": [
        "quantum-field-theory",
        "hilbert-space",
        "operators",
        "second-quantization"
      ],
      "score": 14,
      "answer_score": 11,
      "question_body": "I have been self-studying physics and QFT for a quite a while now and there are a couple of basic ideas of QFT that I just can't find the answer to no matter how hard I try. I know this might sound silly to some of you but I'm very eager to understand QFT and I have no opportunity to ask teachers and I feel stuck and would greatly appreciate any help. So my question is this: What do field operators do? I have been absolutely unable to find any straightforward answers to this. The only available answers I keep stumbling on (and I take pains to find them) are so heavily technical that I find them hopeless to understand with my humble mathematical background. I understand that in second quantization we treat each mode of a field as a quantum harmonic oscillator and that we define field operators as integrals over creation and annihilation operators for each mode and we define commutation relations between them to make our theory quantum. I have also read at some places (and this is again extremely obscure in most sources - if they even care to mention it) that the wavefunction in QFT is not a function on spacetime coordinates like in regular QM but a functional on a space of possible field configurations that doesn't have time dependence (because we are in the Heisenberg picture). What I don't understand is then what is it exactly that these operators operate on? Do they operate on the wavefunction like operators in regular QM? If, so do they have a set of eigenstates with associated eigenvalues that tell us something about some physical observable related to the field? I saw a video in which they seemed to show just this: that a field operator applied to the wavefunction showed the expectation value of the field at that point. But I also read several times that an operator creates a particle at position x (?!). I would be more than thankful if someone could explain it to me in a non-fancy way so I can just get an idea of what the operators even do and what is the something they act on to start with and thus have a chance to study the technicalities afterwards.",
      "answer_body": "I agree it can be very confusing to appreciate what's going on so hopefully this helps and sync's up with the linked discussions in the comments: There are (at least) two ways to introduce 'quantum fields' which illustrate what they do. One way is to start with a classical system of $N$ coordinates $q_i(t)$ and $N$ momenta $p_i(t)$ satisfying the Heisenberg equations, which in terms of Poisson brackets read as $$ \\frac{d q_i}{dt} = \\{H,q_i\\} $$ $$ \\frac{dp_i}{dt} = \\{H,p_i\\} $$ where e.g. $\\{p_i,q_j\\} = \\delta_{ij}$ , and then promote the coordinates and momenta to operators $$ q_i(t) \\ \\ \\to \\ \\ \\hat{q}_i(t) \\ \\ , \\ \\ p_i(t) \\ \\ \\to \\ \\ \\hat{p}_i(t)$$ and Poisson brackets to commutators, e.g. $$ \\{p_i,q_j\\} = \\delta_{ij} \\ \\ \\to \\ \\ [\\hat{p}_i,\\hat{q}_j] = - i \\delta_{ij}.$$ We then send $$ N \\to \\infty.$$ Before going any further, one should note that we are setting up the transition from a discrete to a continuous system, and this is familiar when setting up the classical dynamics of a continuous string starting from a model of a finite chain of springs (ref [1]), note this is modelled as a finite chain of identical particles . In this model one recalls that the coordinates $q_i(t)$ end up being replaced by a field $\\varphi(x,t)$ i.e. $$ q_i(t) \\ \\ \\to \\ \\ q_{x}(t) = \\varphi(x,t)$$ giving the amplitude from it's rest position at $(x,t)$ , so by analogy we expect that the classical degrees of freedom $q_i(t)$ that we are quantizing with $i$ going to $\\infty$ should end up as operators depending on space and time (if the operators $\\hat{q}(t)$ are Heisenberg picture operators, or just space if they are Schrodinger operators $\\hat{q}$ , let's leave time in here). Thus, modulo a subtlety I will come back to , the transition from classical mechanics of a finite set of positional degrees of freedom to a continuum of quantum 'position' degrees of freedom is $$ q_i(t) \\ \\ \\to \\ \\ \\hat{q}_{\\mathbf{x}}(t) = \\hat{\\varphi}(\\mathbf{x},t)$$ For example, this (in the Schrodinger picture) is roughly how quantum fields are introduced in [3]. Just as your classical mechanics Lagrangian's and Hamiltonian's like $L = \\frac{1}{2}m \\dot{q}_i^2 - V(q_i)$ involve positions and coordinates, so too does e.g. the free Klein-Gordon Lagrangian density $$ \\mathcal{L} = \\frac{1}{2} \\partial_{\\mu} \\hat{\\varphi} \\partial^{\\mu} \\hat{\\varphi} - \\frac{1}{2} m \\hat{\\varphi}^2.$$ So a quantum field operator is (modulo a subtlety) like a position operator at each point of space/space-time, at this stage of setting things up I could have used completely different notation and then you'd probably never confuse this with the familiar Schrodinger picture Schrodinger wave function in $$ i \\frac{\\partial \\psi}{\\partial t} = \\hat{H} \\psi$$ It might be better to think of the Dirac Lagrangian $$ \\mathcal{L} = \\hat{\\overline{\\psi}} (i \\gamma^{\\mu} \\partial_{\\mu} - m) \\hat{\\psi}$$ since this can directly be written in the form of a Schrodinger equation, but now with $\\hat{\\psi}$ an operator $$ i \\frac{\\partial \\hat{\\psi}}{\\partial t} = \\hat{H} \\hat{\\psi}$$ In other words, it's like we took the regular Schrodinger equation and replaced $\\psi$ with $\\hat{q}_k$ $$ i \\frac{\\partial \\hat{q}_k}{\\partial t} =^? \\hat{H} \\hat{q}_k$$ but now we have to have $k = (\\mathbf{x},t)$ i.e. the Schrodinger operator act on the labels that the 'position operator' depends on, but one even could say the same thing about the wave equation one derives in the continuum model of a string so it's not that bizarre. Maybe the bizarre thing is how this is natural even starting from the Schrodinger picture Schrodinger equation as we'll see below. Before going further I will note most of this is the setup in ref [2]. Also, I will note that a common way of introducing the above is via the example of setting up the electromagnetic field Lagrangian/Hamiltonian (e.g. Schwartz ch. 2 or [5]) and reducing it to the analysis of Harmonic Oscillators, thus suggesting promoting the classical modes to quantum creation and annihilation operators. This makes us face up to a subtlety. Above I said a quantum field was basically just a position operator but now with a continuous label, $\\hat{q}_{\\mathbf{x}}(t) = \\hat{\\varphi}(\\mathbf{x},t)$ , and the electromagnetic field example gives things which act like position and momentum operators in the sense that they combine like position and momentum operators do in the Harmonic Oscillator example when we define creation and annihilation operators. But the quantities in the electromagnetic case definitely aren't position and momentum operators, they are just 'conjugate variables' (sometimes, such as in [5], one literally uses the position/momentum notation in the electromagnetic field case due to this). Recall in the classical mechanics Hamiltonian formalism that the very meaning of what is a position variable and what is a momentum variable is actually completely arbitrary, all we need are canonically conjugate variables, we can even do a canonical transformation and interchange what we call position and what we call momentum... So a quantum field is a quantity which classically behaves like one of the 'coordinates' in a canonically conjugate pair of degree of freedom which then gets quantized by promoting Poisson brackets to commutators. For all intents and purposes it behaves similarly to how a position operator acts, which is is why it's introduced as above e.g. in [2] or [3]. So, now that you see all this formalism arose by promoting position $q_i$ to a operator $\\hat{q}_i$ and sending $N \\to \\infty$ so that we get $\\hat{q}_{\\mathbf{x}}(t) = \\hat{\\varphi}(\\mathbf{x},t)$ , or more generally thinking in terms of a canonically conjugate pair of degrees of freedom, it hopefully makes sense that quantum fields act on state vectors the same way that a position operator $\\hat{q}$ in quantum mechanics also acts on state vectors. One can ask (as your post does), why didn't I just say $\\hat{\\varphi}$ acts on a Schrodinger wave function $\\psi(\\mathbf{x},t)$ the way that $\\hat{q}$ acts on a Schrodinger wave function $\\psi(\\mathbf{x},t)$ as usual in introductory quantum mechanics, indeed $\\psi$ is sometimes loosely called as a state vector. The 'cheat' answer is that in Harmonic oscillator example we apply creation and annihilation operators to state vectors that look roughly like $|n> = \\hat{a}^{\\dagger}|0>$ so that's why we do it for the $\\hat{\\varphi}$ 's (recall $\\hat{q} \\approx (\\hat{a} + \\hat{a}^{\\dagger})$ in the Harmonic oscillator case), but one can ask why can't we also blindly just apply it to $\\psi(\\mathbf{x},t)$ , when we examine the Schrodinger wave function below we'll see why the above is the natural thing to do. We can then as usual solve the free Lagrangian or Hamiltonian equations of motion related to a given model using e.g. Fourier methods (e.g. sec. 2.3 and 2.4 of [3]) and we get expansions like $$ \\hat{\\varphi}(t,\\mathbf{x}) = \\int \\frac{d^3 \\mathbf{p}}{(2 \\pi)^3 \\sqrt{2E_{\\mathbf{p}}}} (\\hat{a}_{\\mathbf{p}} e^{-ipx} + \\hat{a}_{\\mathbf{p}}^{\\dagger} e^{+ipx})|_{p_0 = E_{\\mathbf{p}}}$$ where the time evolution is always contained in these scalars $e^{\\pm ipx}$ . In other words, here we just casually note for all intents and purposes if we didn't plug an operator wave function into e.g. Klein-Gordon and instead plugged in scalar wave functions, our solutions would look the exact same just without the hat's. Indeed note with these mode expansions that things like the Hamiltonian reduce to sums like $$ \\hat{H} = \\int \\frac{d^3 \\mathbf{p}}{(2 \\pi)^3} \\omega_{\\mathbf{p}} \\hat{a}^{\\dagger}_{\\mathbf{p}} \\hat{a}_{\\mathbf{p}} (+ \\text{zero point energy})$$ So you should at this stage ask yourself, what is actually going on with the regular Schrodinger equation through all of this, i.e. what is the Schrodinger picture setup of this model. Well, the whole time the Schrodinger picture situation is that we are starting from a multi-particle system of $N$ identical particles and then sending $N \\to \\infty$ in $$ i \\frac{\\partial }{\\partial t}\\psi(\\mathbf{x}_1,...,\\mathbf{x}_N,t) = \\hat{H}(\\mathbf{x}_1,...,\\mathbf{x}_N) \\psi(\\mathbf{x}_1,...,\\mathbf{x}_N,t).$$ But the second we realize we are working with a multi-particle system of identical particles, we immediately go back to the very heart of quantum mechanics [4] (ch. 1), the Heisenberg uncertainty principle, which tells us that working in this position-space wave functions $\\psi(\\mathbf{x}_1,...,\\mathbf{x}_N,t)$ with identical particles is beyond redundant, since the particles are identical what matters is the number of particles in each of the allowable stationary states. Indeed this leads to the (non-relativistic) boson/fermion classification of identical particle wave functions. This is the philosophy behind going to the 'occupation number formalism', or 'second quantization' picture (the term makes sense, but not yet). This is clearly set up from first principles in ref [4]. So, we see there are multiple reasons why the naive Schrodinger picture needs to be modified in QFT, first there is the issue of working with identical particles, something we can even deal with in non-relativistic quantum mechanics, and second that we are sending $N \\to \\infty$ . On top of this, there is the new issue in QFT of dealing with relativity. If we are happy to just admit that we should work with these occupation numbers from the beginning, and more or less just ignore the above Schrodinger equation but still work as if we're starting from the Schrodinger picture, then ref [4] sets it up nicely. It turns out that this process results in the same quantum field operators that were independently introduced above. It's useful, however, to blindly start with the above equation and try to end up with this formalism. This is done for example in reference [6]. First assume everything is non-relativistic for simplicity, and then we'll see where things have to change. If we assume that each of the identical particle stationary states are for example symbolically labelled by energy levels $(1,2,...,\\infty)$ and a given energy eigenvalue $E_k$ is one of these allowable values, then the single particle stationary state wave functions are denoted $\\psi_{E_k}$ and we can expand $\\psi(\\mathbf{x}_1,...,\\mathbf{x}_N,t)$ as $$ \\psi(\\mathbf{x}_1,...,\\mathbf{x}_N,t) = \\sum_{E_1 .. E_N} C(E_1,...,E_N,t) \\psi_{E_1}(\\mathbf{x}_1) .. \\psi_{E_N}(\\mathbf{x}_n)$$ Thus we see that this 'Fourier space' wave function $C(E_1,...,E_N,t)$ more accurately describes the system in the sense that it at least depends on the stationary state energy levels, rather than the space/space-time coordinates and the inherent redundancy (see also [4] for more on this). But it's still not enough, one would like to re-express this in terms of the number of particles in each of the energy levels $(1,2,...)$ , e.g. we would like to re-express $C(E_1,...,E_N,t)$ using an infinite set of variables $(n_1,n_2,...)$ where $n_1$ is the number of particles in the stationary state symbolically denoted $1$ , etc... Without continuing in detail, which ref [6] does, one then just re-expresses the above Schrodinger equation as a Schrodinger equation for $C(E_1,...,E_N,t)$ (you can see one just plugs in the $\\sum_{E_1 .. E_N}$ expression into the Schrodinger equation and then removes the $\\psi_{E_k}$ 's) and then, e.g. for a boson wave function notes we can use the wave function symmetry to re-arrange the wave functions as, to make up some random example $$ C(1,2,1,3,2,..,26,33,t) = C(11..;22..;3;44..;..,t) = \\tilde{C}(n_1;n_2;1;n_4,..,t)$$ so that e.g. there are $n_1$ particles with energy level $1$ , $n_2$ particles with energy level $2$ , one particle with energy level $3$ (in this example), etc... and so instead work with (a suitable normalized version of) a new function $\\tilde{C}(n_1;n_2;1;n_4,...,t)$ which depends on an infinite number of variables since there are an infinite number of stationary states, even though there are only a finite number of particle (recall $C$ depends on a finite number of variables $N$ ). Once you agree that we should care only about the number of particles in a given stationary state, we see that the time evolution of a system where things change amounts to the number of particles in a given stationary state changing with time, in other words, we create or annihilate particles in a each of the stationary states at each time. So, without knowing anything, we are unavoidably led to defining operators called creation and annihilation operators which create or annihilate a particle in a given stationary state, and so it's completely natural to think even of the state of a given system with a given number of particles in some given stationary states in terms of these creation operators acting on a 'vacuum' $|0>$ . Thus if you go through the discussion in reference [6] (for example, equation 1.24 and 1.25 which are too big to type) you will see exactly how even if you never heard of a creation operator or quantum field, you would end up defining them yourself - you can see that even a non-relativistic Hamiltonian can be expressed in terms of these creation and annihilation operators, reducing to familiar expressions like $$ \\hat{H} = \\sum_i \\omega_i \\hat{a}^{\\dagger}_{i} \\hat{a}_{i} $$ which is similar to the above expansion. Note I've assumed a discrete spectrum in all this, and my number of particles $N$ is still fixed, so if you send $N \\to \\infty$ the labels become $\\mathbf{x}$ and $\\mathbf{p}$ etc... and it almost looks exactly like the previous discussion. It's not hard to see from e.g. $\\omega_i = \\sum_k \\int \\psi_k^* \\hat{H} \\psi_i d^3 \\mathbf{x}$ that inserting this into this last Hamiltonian makes it natural to define the combinations $$ \\hat{\\psi}(\\mathbf{x},t) = \\sum_i \\hat{a}_i \\psi_i(\\mathbf{x},t)$$ $$ \\hat{\\psi}^{\\dagger}(\\mathbf{x},t) = \\sum_i \\hat{a}_i^{\\dagger} \\psi_i^*(\\mathbf{x},t)$$ which now very explicitly can be seen to involve the ability of creating or annihilating a particle at a given position $(\\mathbf{x},t)$ at time $t$ , it depends on the state that they act on, exactly like a position operator does (it also really depends on the state it acts on), i.e. syncing up with the previous formalism above, so that the Hamiltonian reads as $$ \\hat{H} = \\int d^3 \\mathbf{x} \\hat{\\psi}^{\\dagger} \\hat{H} \\hat{\\psi}$$ i.e. the mysterious 'quantum field operator' has again fallen out of our non-relativistic formalism. Note it's literally just a linear combination of stationary state solutions to the Schrodinger equation, but now with time-independent operator coefficients, so of course it still satisfies the non-relativistic Schrodinger equation. So we're again lead to this operator Schrodinger equation $$ i \\frac{\\partial \\hat{\\psi}}{\\partial t} = \\hat{H} \\hat{\\psi}$$ Note the Hamiltonian here has the same kind of form that e.g. the Dirac Hamiltonian (which I didn't write explicitly above), an integral of a quantity built out of quantum field operators. So a big chunk of the formalism you see in a QFT book, can equivalently be applied to non-relativistic quantum field theory, equivalent to the familiar Schrodinger picture Schrodinger equation formalism, but it can be introduced directly via the first method thus skipping over all these intermediate steps rationalizing why it's very natural to reformulate even the usual Schrodinger equation picture in this manner due to working with identical particles. There is still this extra behemoth of relativity. Based on everything written above, it looks like the only difference is the choice of Hamiltonian you use, and in a sense that's all it is, almost, but even this has big consequences, and it relates to this point you made about the very meaning of a wave function. In non-relativistic quantum mechanics, the choice of Hamiltonian is fixed by Galilean symmetry. In relativistic quantum mechanics, it is fixed by Lorentz invariance. Thus the fields involved in building up a Hamiltonian that describes identical particles should transform under Lorentz transformations in such a way that the overall theory is Lorentz covariant, i.e. the fields must be representations of the Lorentz group. It turns out that the Hamiltonian describing free electromagnetism can be related to vector representations of the Lorentz group, the Dirac equation (which was historically derived by demanding linear Lorentz covariant equation, thus encoding representation theory from it's inception, that reproduces the mass-shell Klein-Gordon condition) can be expressed using a Hamiltonian built up using spin $1/2$ representations of the Lorentz group (note again that linearity assumption), etc... The way this is usually done is to build up Lorentz invariant Lagrangian's from fields which are representations of the Lorentz group. From this perspective we just note that the free particles that the fields act on can be labelled by their momentum and spin, something we think of as an experimental fact by thinking of the non-relativistic case (e.g. the hydrogen atom). These particle labels, and even the above field representation theory, can be understood as arising in a uniform manner from the representation theory of the Poincare group (which contains the Lorentz group), which is sometimes taken as the absolute starting point of QFT (e.g. [8]). So you can ask, why doesn't everything in relativistic QFT have the same interpretation as that of the non-relativistic case? Consider only free particles. In the non-relativistic and relativistic case, one can define the notion of a number operator $\\hat{N}$ . In the non-relativistic case free particle Hamiltonian $\\hat{H}$ commutes with the number operator $\\hat{N}$ . In the relativistic case free particle Hamiltonian $\\hat{H}$ does not commute with the number operator $\\hat{N}$ . The reason traces back to the fact that special relativity allows for those negative energy solutions, which means more terms get added to the non-relativistic number operator thus preventing $\\hat{N}$ from commuting with $\\hat{H}$ . This issue is sketched for example in [7]. This means that any measurement process involving measuring the position of any free particle will unavoidably lead to the creation and annihilation of particles which cannot be detected by the measurement process, making meaningless the position measurement process itself. Indeed this nearly destroyed the whole subject of QFT as it was being set up, and one should ask why anything at all is measurable in QFT, and if so what is it that can be measured. Even worse, what does this imply in terms of describing an interacting system, if we can't even measure some things about free particles. The transition from non-relativistic to relativistic mechanics involves a fundamental shift in the formalism of classical mechanics, yet here we haven't seen that fundamental shift in the foundations, we've only really seen consequences of quantities like the Hamiltonian and number operator etc... changing in the relativistic case, but even in classical memchanics that happens too in that we replace $S = \\int \\frac{1}{2} m v^2 dt$ by $S = - mc \\int ds$ , but special relativity is deeper than this simple replacement, so on this alone one can expect more than this. If, as in [4] (ch. 1), the Heisenberg Uncertainty Principle is the very core of quantum mechanics, and as seen above things get super complicated in transitioning from one particle to a system of identical particles once we invoke the Uncertainty Principle, you can expect that combining special relativity with the uncertainty principle is the key to the weirdness of (the) relativistic quantum mechanics (of systems of identical particles, which is unavoidable due to the number operator issue mentioned above). I'll just recommend one read [5] to find out what this is. So the final point to make relevant to your post is the Schrodinger functional picture. You see we had to send $N \\to \\infty$ in the naive Schrodinger equation, and we ended up reformulating it in a way that bypasses all the coordinate space identical particle redundancy by going to the occupation number formalism and working with (e.g. in the non-relativistic case) an operator analogue $\\hat{\\psi} = \\sum_n \\hat{a}_n \\psi_n$ of the stationary state mode expansion $\\psi = \\sum_n a_n \\psi_n$ of a single particle wave function (this is obviously why it's called 'second quantization', if we apply all this formalism to a single particle it's literally a second way of working with an expansion in terms of the $\\psi_n$ 's that describes a particle and gets plugged into the Schrodinger equation). So the question is, why can't we just set up a Schrodinger equation using this new Hamiltonian, where e.g. the momentum in this picture acts like a derivative operator the same way it does in the first quantized picture etc... Obviously you're now differentiating with respect to (the eigenvalues of, on suitable eigenvectors) field variables rather than the underlying coordinates, so it becomes a functional formalism, see ref [9]. So hopefully you can see there's a deeper reason why we use functionals. References: Goldstein - Classical Mechanics, Ch. 13. Bjorken, Drell - Relativistic Quantum Fields, Ch. 11. Peskin, Schroeder - Introduction to Quantum Field Theory, Ch 2. Landau, Lifshitz - Quantum Mechanics, Ch. IX. Berestetskii, Lifshitz, Pitaevskii - Quantum Electrodynamics, Ch. I. Fetter, Walecka - Quantum Theory of Many-Particle Systems, Ch. 1. Srednicki - Quantum Field Theory, ch. 1. Weinberg, Quantum Theory of Fields, Ch. 2. Hatfield - Quantum Field Theory of Point Particles and Strings, Ch. 10.",
      "question_latex": [],
      "answer_latex": [
        "\\frac{d q_i}{dt} = \\{H,q_i\\}",
        "\\frac{dp_i}{dt} = \\{H,p_i\\}",
        "q_i(t) \\ \\ \\to \\ \\ \\hat{q}_i(t) \\ \\ , \\ \\ p_i(t) \\ \\ \\to \\ \\ \\hat{p}_i(t)",
        "\\{p_i,q_j\\} = \\delta_{ij} \\ \\ \\to \\ \\ [\\hat{p}_i,\\hat{q}_j] = - i \\delta_{ij}.",
        "N \\to \\infty.",
        "q_i(t) \\ \\ \\to \\ \\ q_{x}(t) = \\varphi(x,t)",
        "q_i(t) \\ \\ \\to \\ \\ \\hat{q}_{\\mathbf{x}}(t) = \\hat{\\varphi}(\\mathbf{x},t)",
        "\\mathcal{L} = \\frac{1}{2} \\partial_{\\mu} \\hat{\\varphi} \\partial^{\\mu} \\hat{\\varphi} - \\frac{1}{2} m \\hat{\\varphi}^2.",
        "i \\frac{\\partial \\psi}{\\partial t} = \\hat{H} \\psi",
        "\\mathcal{L} = \\hat{\\overline{\\psi}} (i \\gamma^{\\mu} \\partial_{\\mu} - m) \\hat{\\psi}",
        "i \\frac{\\partial \\hat{\\psi}}{\\partial t} = \\hat{H} \\hat{\\psi}",
        "i \\frac{\\partial \\hat{q}_k}{\\partial t} =^? \\hat{H} \\hat{q}_k",
        "\\hat{\\varphi}(t,\\mathbf{x}) = \\int \\frac{d^3 \\mathbf{p}}{(2 \\pi)^3 \\sqrt{2E_{\\mathbf{p}}}} (\\hat{a}_{\\mathbf{p}} e^{-ipx} + \\hat{a}_{\\mathbf{p}}^{\\dagger} e^{+ipx})|_{p_0 = E_{\\mathbf{p}}}",
        "\\hat{H} = \\int \\frac{d^3 \\mathbf{p}}{(2 \\pi)^3} \\omega_{\\mathbf{p}} \\hat{a}^{\\dagger}_{\\mathbf{p}} \\hat{a}_{\\mathbf{p}} (+ \\text{zero point energy})",
        "i \\frac{\\partial }{\\partial t}\\psi(\\mathbf{x}_1,...,\\mathbf{x}_N,t) = \\hat{H}(\\mathbf{x}_1,...,\\mathbf{x}_N) \\psi(\\mathbf{x}_1,...,\\mathbf{x}_N,t).",
        "\\psi(\\mathbf{x}_1,...,\\mathbf{x}_N,t) = \\sum_{E_1 .. E_N} C(E_1,...,E_N,t) \\psi_{E_1}(\\mathbf{x}_1) .. \\psi_{E_N}(\\mathbf{x}_n)",
        "C(1,2,1,3,2,..,26,33,t) = C(11..;22..;3;44..;..,t) = \\tilde{C}(n_1;n_2;1;n_4,..,t)",
        "\\hat{H} = \\sum_i \\omega_i \\hat{a}^{\\dagger}_{i} \\hat{a}_{i}",
        "\\hat{\\psi}(\\mathbf{x},t) = \\sum_i \\hat{a}_i \\psi_i(\\mathbf{x},t)",
        "\\hat{\\psi}^{\\dagger}(\\mathbf{x},t) = \\sum_i \\hat{a}_i^{\\dagger} \\psi_i^*(\\mathbf{x},t)",
        "\\hat{H} = \\int d^3 \\mathbf{x} \\hat{\\psi}^{\\dagger} \\hat{H} \\hat{\\psi}",
        "i \\frac{\\partial \\hat{\\psi}}{\\partial t} = \\hat{H} \\hat{\\psi}",
        "N",
        "q_i(t)",
        "p_i(t)",
        "</span>\n<span class=\"math-container\">",
        "</span>\nwhere e.g. <span class=\"math-container\">",
        "</span>, and then promote the coordinates and momenta to operators\n<span class=\"math-container\">",
        "</span>\nand Poisson brackets to commutators, e.g.\n<span class=\"math-container\">",
        "</span>\nWe then send\n<span class=\"math-container\">",
        "</span>\nBefore going any further, one should note that we are setting up the transition from a discrete to a continuous system, and this is familiar when setting up the classical dynamics of a continuous string starting from a model of a finite chain of springs (ref [1]), note this is modelled as a finite chain of <strong>identical particles</strong>. In this model one recalls that the coordinates <span class=\"math-container\">",
        "</span> end up being replaced by a field <span class=\"math-container\">",
        "</span> i.e.\n<span class=\"math-container\">",
        "</span>\ngiving the amplitude from it's rest position at <span class=\"math-container\">",
        "</span>, so by analogy we expect that the classical degrees of freedom <span class=\"math-container\">",
        "</span> that we are quantizing with <span class=\"math-container\">",
        "</span> going to <span class=\"math-container\">",
        "</span> should end up as operators depending on space and time (if the operators <span class=\"math-container\">",
        "</span> are Heisenberg picture operators, or just space if they are Schrodinger operators <span class=\"math-container\">",
        "</span>, let's leave time in here). Thus, <em>modulo a subtlety I will come back to</em>, the transition from classical mechanics of a finite set of positional degrees of freedom to a continuum of quantum 'position' degrees of freedom is\n<span class=\"math-container\">",
        "</span>\nFor example, this (in the Schrodinger picture) is roughly how quantum fields are introduced in [3]. Just as your classical mechanics Lagrangian's and Hamiltonian's like <span class=\"math-container\">",
        "</span> involve positions and coordinates, so too does e.g. the free Klein-Gordon Lagrangian density\n<span class=\"math-container\">",
        "</span>\nSo a quantum field operator is (modulo a subtlety) like a position operator at each point of space/space-time, at this stage of setting things up I could have used completely different notation and then you'd probably never confuse this with the familiar Schrodinger picture Schrodinger wave function in\n<span class=\"math-container\">",
        "</span>\nIt might be better to think of the Dirac Lagrangian\n<span class=\"math-container\">",
        "</span>\nsince this can directly be written in the form of a Schrodinger equation, but now with <span class=\"math-container\">",
        "</span> an operator\n<span class=\"math-container\">",
        "</span>\nIn other words, it's like we took the regular Schrodinger equation and replaced <span class=\"math-container\">",
        "</span> with <span class=\"math-container\">",
        "</span>\nbut now we have to have <span class=\"math-container\">",
        "</span> i.e. the Schrodinger operator act on the labels that the 'position operator' depends on, but one even could say the same thing about the wave equation one derives in the continuum model of a string so it's not that bizarre. Maybe the bizarre thing is how this is natural even starting from the Schrodinger picture Schrodinger equation as we'll see below.</p>\n<p>Before going further I will note most of this is the setup in ref [2]. Also, I will note that a common way of introducing the above is via the example of setting up the electromagnetic field Lagrangian/Hamiltonian (e.g. Schwartz ch. 2 or [5]) and reducing it to the analysis of Harmonic Oscillators, thus suggesting promoting the classical modes to quantum creation and annihilation operators.</p>\n<p>This makes us face up to a subtlety. Above I said a quantum field was basically just a position operator but now with a continuous label, <span class=\"math-container\">",
        "</span>, and the electromagnetic field example gives things which act like position and momentum operators in the sense that they combine like position and momentum operators do in the Harmonic Oscillator  example when we define creation and annihilation operators. But the quantities in the electromagnetic case definitely aren't position and momentum operators, they are just 'conjugate variables' (sometimes, such as in [5], one literally uses the position/momentum notation in the electromagnetic field case due to this).</p>\n<p>Recall in the classical mechanics Hamiltonian formalism that the very meaning of what is a position variable and what is a momentum variable is actually completely arbitrary, all we need are canonically conjugate variables, we can even do a canonical transformation and interchange what we call position and what we call momentum...</p>\n<p>So a quantum field is a quantity which classically behaves like one of the 'coordinates' in a canonically conjugate pair of degree of freedom which then gets quantized by promoting Poisson brackets to commutators. For all intents and purposes it behaves similarly to how a position operator acts, which is is why it's introduced as above e.g. in [2] or [3].</p>\n<p>So, now that you see all this formalism arose by promoting position <span class=\"math-container\">",
        "</span> to a operator <span class=\"math-container\">",
        "</span> and sending <span class=\"math-container\">",
        "</span> so that we get <span class=\"math-container\">",
        "</span>, or more generally thinking in terms of a canonically conjugate pair of degrees of freedom, it hopefully makes sense that quantum fields <strong>act on state vectors</strong> the same way that a position operator <span class=\"math-container\">",
        "</span> in quantum mechanics also acts on state vectors.</p>\n<p>One can ask (as your post does), why didn't I just say <span class=\"math-container\">",
        "</span> acts on a Schrodinger wave function <span class=\"math-container\">",
        "</span> the way that <span class=\"math-container\">",
        "</span> as usual in introductory quantum mechanics, indeed <span class=\"math-container\">",
        "</span> is sometimes loosely called as a state vector.</p>\n<p>The 'cheat' answer is that in Harmonic oscillator example we apply creation and annihilation operators to state vectors that look roughly like <span class=\"math-container\">",
        "</span> so that's why we do it for the <span class=\"math-container\">",
        "</span>'s (recall <span class=\"math-container\">",
        "</span> in the Harmonic oscillator case), but one can ask why can't we also blindly just apply it to <span class=\"math-container\">",
        "</span>, when we examine the Schrodinger wave function below we'll see why the above is the natural thing to do.</p>\n<p>We can then as usual solve the free Lagrangian or Hamiltonian equations of motion related to a given model using e.g. Fourier methods (e.g. sec. 2.3 and 2.4 of [3]) and we get expansions like\n<span class=\"math-container\">",
        "</span>\nwhere the time evolution is always contained in these scalars <span class=\"math-container\">",
        "</span>. In other words, here we just casually note for all intents and purposes if we didn't plug an operator wave function into e.g. Klein-Gordon and instead plugged in scalar wave functions, our solutions would look the exact same just without the hat's. Indeed note with these mode expansions that things like the Hamiltonian reduce to sums like\n<span class=\"math-container\">",
        "</span></p>\n<p>So you should at this stage ask yourself, what is actually going on with the regular Schrodinger equation through all of this, i.e. what is the Schrodinger picture setup of this model. Well, the whole time the Schrodinger picture situation is that we are starting from a multi-particle system of <span class=\"math-container\">",
        "</span> identical particles and then sending <span class=\"math-container\">",
        "</span> in\n<span class=\"math-container\">",
        "</span>\nBut the second we realize we are working with a multi-particle system of identical particles, we immediately go back to the very heart of quantum mechanics [4] (ch. 1), the Heisenberg uncertainty principle, which tells us that working in this position-space wave functions <span class=\"math-container\">",
        "</span> with identical particles is beyond redundant, since the particles are identical what matters is the number of particles in each of the allowable stationary states. Indeed this leads to the (non-relativistic) boson/fermion classification of identical particle wave functions. This is the philosophy behind going to the 'occupation number formalism', or 'second quantization' picture (the term makes sense, but not yet). This is clearly set up from first principles in ref [4].</p>\n<p>So, we see there are multiple reasons why the naive Schrodinger picture needs to be modified in QFT, first there is the issue of working with identical particles, something we can even deal with in non-relativistic quantum mechanics, and second that we are sending <span class=\"math-container\">",
        "</span>. On top of this, there is the new issue in QFT of dealing with relativity.</p>\n<p>If we are happy to just admit that we should work with these occupation numbers from the beginning, and more or less just ignore the above Schrodinger equation but still work as if we're starting from the Schrodinger picture, then ref [4] sets it up nicely. It turns out that this process results in the same quantum field operators that were independently introduced above.</p>\n<p>It's useful, however, to blindly start with the above equation and try to end up with this formalism. This is done for example in reference [6]. First assume everything is non-relativistic for simplicity, and then we'll see where things have to change.</p>\n<p>If we assume that each of the identical particle stationary states are for example symbolically labelled by energy levels <span class=\"math-container\">",
        "</span> and a given energy eigenvalue <span class=\"math-container\">",
        "</span> is one of these allowable values, then the single particle stationary state wave functions are denoted <span class=\"math-container\">",
        "</span> and we can expand <span class=\"math-container\">",
        "</span> as\n<span class=\"math-container\">",
        "</span>\nThus we see that this 'Fourier space' wave function <span class=\"math-container\">",
        "</span> more accurately describes the system in the sense that it at least depends on the stationary state energy levels, rather than the space/space-time coordinates and the inherent redundancy (see also [4] for more on this). But it's still not enough, one would like to re-express this in terms of the <strong>number</strong> of particles in each of the energy levels <span class=\"math-container\">",
        "</span>, e.g. we would like to re-express <span class=\"math-container\">",
        "</span> using an infinite set of variables <span class=\"math-container\">",
        "</span> where <span class=\"math-container\">",
        "</span> is the number of particles in the stationary state symbolically denoted <span class=\"math-container\">",
        "</span>, etc... Without continuing in detail, which ref [6] does, one then just re-expresses the above Schrodinger equation as a Schrodinger equation for <span class=\"math-container\">",
        "</span> (you can see one just plugs in the <span class=\"math-container\">",
        "</span> expression into the Schrodinger equation and then removes the <span class=\"math-container\">",
        "</span>'s) and then, e.g. for a boson wave function notes we can use the wave function symmetry to re-arrange the wave functions as, to make up some random example\n<span class=\"math-container\">",
        "</span>\nso that e.g. there are <span class=\"math-container\">",
        "</span> particles with energy level <span class=\"math-container\">",
        "</span>, <span class=\"math-container\">",
        "</span>, one particle with energy level <span class=\"math-container\">",
        "</span> (in this example), etc... and so instead work with (a suitable normalized version of) a new function <span class=\"math-container\">",
        "</span> which depends on an infinite number of variables since there are an infinite number of stationary states, even though there are only a finite number of particle (recall <span class=\"math-container\">",
        "</span> depends on a finite number of variables <span class=\"math-container\">",
        "</span>).</p>\n<p>Once you agree that we should care only about the number of particles in a given stationary state, we see that the time evolution of a system where things change amounts to the number of particles in a given stationary state changing with time, in other words, we create or annihilate particles in a each of the stationary states at each time. So, without knowing anything, we are unavoidably led to defining operators called creation and annihilation operators which create or annihilate a particle in a given stationary state, and so it's completely natural to think even of the state of a given system with a given number of particles in some given stationary states in terms of these creation operators acting on a 'vacuum' <span class=\"math-container\">",
        "</span>.</p>\n<p>Thus if you go through the discussion in reference [6] (for example, equation 1.24 and 1.25 which are too big to type) you will see exactly how even if you never heard of a creation operator or quantum field, you would end up defining them yourself - you can see that even a non-relativistic Hamiltonian can be expressed in terms of these creation and annihilation operators, reducing to familiar expressions like\n<span class=\"math-container\">",
        "</span>\nwhich is similar to the above expansion. Note I've assumed a discrete spectrum in all this, and my number of particles <span class=\"math-container\">",
        "</span> is still fixed, so if you send <span class=\"math-container\">",
        "</span> the labels become <span class=\"math-container\">",
        "</span> and <span class=\"math-container\">",
        "</span> etc... and it almost looks exactly like the previous discussion.</p>\n<p>It's not hard to see from e.g. <span class=\"math-container\">",
        "</span> that inserting this into this last Hamiltonian makes it natural to define the combinations\n<span class=\"math-container\">",
        "</span>\nwhich now very explicitly can be seen to involve the ability of creating or annihilating a particle at a given position <span class=\"math-container\">",
        "</span> at time <span class=\"math-container\">",
        "</span>, it depends on the state that they act on, exactly like a position operator does (it also really depends on the state it acts on), i.e. syncing up with the previous formalism above, so that the Hamiltonian reads as\n<span class=\"math-container\">",
        "</span>\ni.e. the mysterious 'quantum field operator' has again fallen out of our non-relativistic formalism. Note it's literally just a linear combination of stationary state solutions to the Schrodinger equation, but now with time-independent operator coefficients, so of course it still satisfies the non-relativistic Schrodinger equation. So we're again lead to this operator Schrodinger equation\n<span class=\"math-container\">",
        "</span>\nNote the Hamiltonian here has the same kind of form that e.g. the Dirac Hamiltonian (which I didn't write explicitly above), an integral of a quantity built out of quantum field operators.</p>\n<p>So a big chunk of the formalism you see in a QFT book, can equivalently be applied to non-relativistic quantum field theory, equivalent to the familiar Schrodinger picture Schrodinger equation formalism, but it can be introduced directly via the first method thus skipping over all these intermediate steps rationalizing why it's very natural to reformulate even the usual Schrodinger equation picture in this manner due to working with identical particles.</p>\n<p>There is still this extra behemoth of relativity. Based on everything written above, it looks like the only difference is the choice of Hamiltonian you use, and in a sense that's all it is, almost, but even this has big consequences, and it relates to this point you made about the very meaning of a wave function.</p>\n<p>In non-relativistic quantum mechanics, the choice of Hamiltonian is fixed by Galilean symmetry. In relativistic quantum mechanics, it is fixed by Lorentz invariance. Thus the fields involved in building up a Hamiltonian that describes identical particles should transform under Lorentz transformations in such a way that the overall theory is Lorentz covariant, i.e. the fields must be representations of the Lorentz group. It turns out that the Hamiltonian describing free electromagnetism can be related to vector representations of the Lorentz group, the Dirac equation (which was historically derived by demanding linear Lorentz covariant equation, thus encoding representation theory from it's inception, that reproduces the mass-shell Klein-Gordon condition) can be expressed using a Hamiltonian built up using spin <span class=\"math-container\">",
        "</span> representations of the Lorentz group (note again that linearity assumption), etc...</p>\n<p>The way this is usually done is to build up Lorentz invariant Lagrangian's from fields which are representations of the Lorentz group. From this perspective we just note that the free particles that the fields act on can be labelled by their momentum and spin, something we think of as an experimental fact by thinking of the non-relativistic case (e.g. the hydrogen atom). These particle labels, and even the above field representation theory, can be understood as arising in a uniform manner from the representation theory of the Poincare group (which contains the Lorentz group), which is sometimes taken as the absolute starting point of QFT (e.g. [8]).</p>\n<p>So you can ask, why doesn't everything in relativistic QFT have the same interpretation as that of the non-relativistic case? Consider only free particles. In the non-relativistic and relativistic case, one can define the notion of a number operator <span class=\"math-container\">",
        "</span>. In the non-relativistic case free particle Hamiltonian <span class=\"math-container\">",
        "</span> commutes with the number operator <span class=\"math-container\">",
        "</span>. In the relativistic case free particle Hamiltonian <span class=\"math-container\">",
        "</span> does not commute with the number operator <span class=\"math-container\">",
        "</span>. The reason traces back to the fact that special relativity allows for those negative energy solutions, which means more terms get added to the non-relativistic number operator thus preventing <span class=\"math-container\">",
        "</span> from commuting with <span class=\"math-container\">",
        "</span>. This issue is sketched for example in [7].</p>\n<p>This means that any measurement process involving measuring the position of any free particle will unavoidably lead to the creation and annihilation of particles which cannot be detected by the measurement process, making meaningless the position measurement process itself. Indeed this nearly destroyed the whole subject of QFT as it was being set up, and one should ask why anything at all is measurable in QFT, and if so what is it that can be measured. Even worse, what does this imply in terms of describing an interacting system, if we can't even measure some things about free particles.</p>\n<p>The transition from non-relativistic to relativistic mechanics involves a fundamental shift in the formalism of classical mechanics, yet here we haven't seen that fundamental shift in the foundations, we've only really seen consequences of quantities like the Hamiltonian and number operator etc... changing in the relativistic case, but even in classical memchanics that happens too in that we replace <span class=\"math-container\">",
        "</span> by <span class=\"math-container\">",
        "</span>, but special relativity is deeper than this simple replacement, so on this alone one can expect more than this.</p>\n<p>If, as in [4] (ch. 1), the Heisenberg Uncertainty Principle is the very core of quantum mechanics, and as seen above things get super complicated in transitioning from one particle to a system of identical particles once we invoke the Uncertainty Principle, you can expect that combining special relativity with the uncertainty principle is the key to the weirdness of (the) relativistic quantum mechanics (of systems of identical particles, which is unavoidable due to the number operator issue mentioned above). I'll just recommend one read [5] to find out what this is.</p>\n<p>So the final point to make relevant to your post is the Schrodinger functional picture. You see we had to send <span class=\"math-container\">",
        "</span> in the naive Schrodinger equation, and we ended up reformulating it in a way that bypasses all the coordinate space identical particle redundancy by going to the occupation number formalism and working with (e.g. in the non-relativistic case) an operator analogue <span class=\"math-container\">",
        "</span> of the stationary state mode expansion <span class=\"math-container\">",
        "</span> of a single particle wave function (this is obviously why it's called 'second quantization', if we apply all this formalism to a single particle it's literally a second way of working with an expansion in terms of the <span class=\"math-container\">"
      ],
      "created": "2021-04-08T03:26:10.713",
      "golden_ner_terms": [
        "act on",
        "acts on",
        "analogy",
        "analysis",
        "assumption",
        "atom",
        "basic",
        "canonical",
        "canonical transformation",
        "chain",
        "classical mechanics",
        "classical system",
        "combination",
        "combinations",
        "conjugate",
        "contained",
        "contains",
        "continuous",
        "continuum",
        "coordinate",
        "coordinates",
        "core",
        "degree",
        "degrees of freedom",
        "density",
        "derivative",
        "difference",
        "dirac equation",
        "discrete",
        "eigenvalue",
        "eigenvalues",
        "eigenvectors",
        "electromagnetism",
        "energy",
        "equation",
        "equations of motion",
        "equivalent",
        "even",
        "expand",
        "expansion",
        "expectation",
        "expectation value",
        "expression",
        "face",
        "field",
        "field theory",
        "finite",
        "finite chain",
        "finite set",
        "fixed",
        "formalism",
        "foundations",
        "function",
        "functional",
        "functionals",
        "group",
        "hamiltonian",
        "hamiltonian formalism",
        "harmonic",
        "harmonic oscillator",
        "heisenberg uncertainty principle",
        "hydrogen",
        "identical particles",
        "infinite",
        "infinite set",
        "integral",
        "interpretation",
        "invariant",
        "involved in",
        "key",
        "l system",
        "label",
        "lagrangian",
        "level",
        "linear combination",
        "matter",
        "measurable",
        "measure",
        "mode",
        "model",
        "momentum",
        "multiple",
        "negative",
        "number",
        "numbers",
        "one way",
        "operator",
        "operators",
        "oscillators",
        "physics",
        "point",
        "point particles",
        "poisson bracket",
        "poisson brackets",
        "qft",
        "quantization",
        "quantum electrodynamics",
        "quantum field theory",
        "quantum mechanics",
        "redundant",
        "regular",
        "relativity",
        "representation",
        "representation theory",
        "scalar",
        "second quantization",
        "similar",
        "simple",
        "sound",
        "space",
        "spacetime",
        "special relativity",
        "spectrum",
        "state",
        "stationary",
        "string",
        "symmetry",
        "term",
        "theory",
        "time",
        "time evolution",
        "top",
        "transform",
        "transformation",
        "transformations",
        "type",
        "uncertainty principle",
        "useful",
        "vacuum",
        "variable",
        "vector",
        "vectors",
        "wave equation",
        "wave function",
        "wavefunction",
        "way",
        "work"
      ],
      "golden_ner_count": 137,
      "golden_patterns": [
        {
          "pattern": "work-examples-first",
          "score": 6.0,
          "hotwords": [
            "example",
            "e.g.",
            "illustrate"
          ]
        },
        {
          "pattern": "quotient-by-irrelevance",
          "score": 4.0,
          "hotwords": [
            "modulo",
            "up to"
          ]
        },
        {
          "pattern": "exploit-symmetry",
          "score": 4.0,
          "hotwords": [
            "symmetry",
            "invariant"
          ]
        },
        {
          "pattern": "construct-an-explicit-witness",
          "score": 4.0,
          "hotwords": [
            "explicit",
            "build"
          ]
        },
        {
          "pattern": "find-the-right-abstraction",
          "score": 2.0,
          "hotwords": [
            "perspective"
          ]
        },
        {
          "pattern": "check-the-extreme-cases",
          "score": 2.0,
          "hotwords": [
            "zero"
          ]
        },
        {
          "pattern": "encode-as-algebra",
          "score": 2.0,
          "hotwords": [
            "ring"
          ]
        },
        {
          "pattern": "use-probabilistic-method",
          "score": 2.0,
          "hotwords": [
            "random"
          ]
        },
        {
          "pattern": "unfold-the-definition",
          "score": 2.0,
          "hotwords": [
            "means that"
          ]
        },
        {
          "pattern": "construct-auxiliary-object",
          "score": 2.0,
          "hotwords": [
            "introduce"
          ]
        },
        {
          "pattern": "estimate-by-bounding",
          "score": 2.0,
          "hotwords": [
            "at least"
          ]
        },
        {
          "pattern": "transport-across-isomorphism",
          "score": 2.0,
          "hotwords": [
            "equivalent"
          ]
        }
      ],
      "golden_pattern_names": [
        "work-examples-first",
        "quotient-by-irrelevance",
        "exploit-symmetry",
        "construct-an-explicit-witness",
        "find-the-right-abstraction",
        "check-the-extreme-cases",
        "encode-as-algebra",
        "use-probabilistic-method",
        "unfold-the-definition",
        "construct-auxiliary-object",
        "estimate-by-bounding",
        "transport-across-isomorphism"
      ],
      "golden_scopes": [
        {
          "type": "consider",
          "match": "Consider only free particles"
        },
        {
          "type": "where-binding",
          "match": "where $n_1$ is"
        }
      ],
      "golden_scope_count": 2
    },
    {
      "id": "se-physics-489079",
      "stratum": "medium",
      "title": "$r$ and $R$ difference with Schwarzschild metric",
      "tags": [
        "general-relativity",
        "metric-tensor",
        "notation",
        "history",
        "event-horizon"
      ],
      "score": 6,
      "answer_score": 5,
      "question_body": "Why did Schwarzschild in his article about the solution of Einstein's equation outside a massive sphere use a variable $R$ : $$R=(r^3+\\alpha^3)^{1/3}$$ Whereas, nowadays the variable used is just the radius $r$ ? Doesn't that change the meaning of the equations?",
      "answer_body": "$\\let\\a=\\alpha \\let\\b=\\beta \\let\\th=\\theta \\def\\ra{{(r^3+\\a^3)}} \\def\\sa{{(r_1^3+\\a^3)}}$ Let's begin to put some firm points. The coordinate we today call Schwarzschild's $r$ isn't his $r$ , but $R=\\ra^{1/3}$ . He calls $R$ an \"auxiliary quantity\". E.g. consider the form he gives to Kepler's third law (second-to-last equation of his paper): $$n^2 = {\\a \\over 2\\,\\ra}$$ with $n=d\\phi/dt$ . Schwarzschild writes metric as $$ds^2 = F\\,dt^2 - (G + H\\,r^2)\\,dr^2 - G\\,r^2 (d\\th^2 + \\sin^2\\!\\th\\,d\\phi^2) \\tag1$$ with $F$ , $G$ , $H$ functions of $r$ (eq. 6). From (1) we see that angular part of the metric is different from the one we are accustomed to. In particular, surfaces $r=\\rm const.$ don't have area $4\\pi r^2$ , but $4\\pi r^2 G$ and $G$ is not constant. It's clear from these and other aspects of Schwarzschild's paper that he considers $t$ , $r$ , $\\theta$ , $\\phi$ as true physical space-time coordinates. In particular, he constrains the \"singularity\" to $r=0$ , i,e, out of spacetime. He states that clearly when he writes condition (13), having precisely that aim. Of course both $r$ and $R$ are legitimate radial coordinates. They are however not physically equivalent. In mathematical terms, using $r$ the spacetime manifold exhibits a singularity only at border $r=0$ . Using $R$ that singularity becomes the horizon ( $R=\\a$ ) and it makes sense to ask oneself if points with $R<\\a$ are to be included in spacetime. As is well known, a thorough understanding of that issue would have taken about half a century. OK, maybe this is history of physics. Or it's physics in its own right? Edit From comments and answers I've read I deem necessary to expand somewhat my answer, hoping it will help to solve several doubts and misconceptions. (A short historical note. Einstein's paper exhibiting the final form of his equations is dated Nov 25th 1915. Schw. paper is dated January 13th 1916. Schw. died - by an autoimmune disease still incurable today - on May 11th 1916. He was 43.) First, there is a strong difference between our present way of understanding spacetime in GR and the way of E.'s and S.'s times. I already remarked that one century ago there still weren't clear ideas about the meaning of spacetime coordinates. There still was a tendency to consider them endowed of some physical significance by themselves. In particular, although S. was well aware that spacetime in his solution is curved and space sections are curved too (not euclidean), he writes a formula for angular velocity in a circular orbit $$n^2 = {\\a \\over 2\\,\\ra}$$ and concludes the angular velocity does not, as with Newton's law, grow without limit when the radius of the orbit gets smaller and smaller, but it approaches a determined limit $$n_0 = {1 \\over \\a \\sqrt2}.$$ (For a point with the solar mass the limit frequency will be around $10^4$ per second). (remember that S.'s $\\a$ is what is known as \"S. radius\" and is proportional to the mass of central body). On the contrary, present-day approach, grounded on sounder mathematical bases, is roughly the following. Spacetime is a semi-Riemannian manifold of dimension 4 and signature $+---$ . 1.1 A (real) manifold is a set wherein real coordinates may be defined. This can be done in several ways. Coordinates are nothing but labels for the set's points. 1.2 It's not required that a set of coordinates be able to cover the whole manifold. More sets are allowed - it's only required that together they cover the whole manifold and smoothly overlap between them. Each set is named a card and the ensemble is named an atlas . 1.3 An easy instance. To define a sphere as a (2D) manifold a minimum of two cards are required. Among geographers a lot of cartographic projections are in use and a world's atlas is a good example of the general idea. A Riemannian manifold is a manifold where a metric is defined. Roughly, a formula giving the distance between (infinitesimally) near points. 2.1 More exactly, the distance squared $ds^2$ . So the metric must be positive definite . 2.2 In a semi-Riemannian manifold an extension is allowed: $ds^2$ may also be negative. (A contradiction? Not quite. It's enough to relax the intuitive interpretation as a distance squared.) Minkowski's spacetime of SR is already an instance of that: there are spacelike intervals, timelike ones, and lightlike too. 2.3 The signature refers to how many independent displacements have metric of each sign. In GR there are two conventions in use: $+----$ means timelike has positive $ds^2$ , spacelike negative. $-+++$ is the opposite. There is no real difference - it's only necessary to consistently adhere to one convention. Mixing them in a calculation leads to certain disaster. Leaving aside more sophisticated usages, the metric is the only way we have to give coordinates a physical meaning. It allows to compute the time a clock measures between events or the length of a space interval and so on. 3.1 Assume a coordinate is called $t$ . It's the initial of \"time\" in English, of \"temps\" in French, of \"tempo\" in Italian, of \"tiempo\" in Spanish ... but not of \"Zeit\" in German or \"czas\" in Polish. So why should we assume that coordinate means time? It may (and usually will) be, but not always. Only looking at the metric can we get a safe answer. Now let's come back to S. He makes use of two radial coordinates: $r$ and $R$ . But there's no doubt about which he takes as the \"physical\" radius. His paper's title says On the Gravitational Field of a Mass Point according to Einstein's Theory A \"mass point\". It's obvious that he locates that mass at $r=0$ , that $r$ can take all real positive values, that he'll require no singularity appears for positive $r$ . At paper's end, as I noted above, he computes the revolution period of a planet as a function of radius and comments on a peculiar result; that period doesn't go to zero with $r$ . On the contrary, it goes to a non-zero limit of about $0.1$ ms. He doesn't ask himself what's the meaning of $r$ (physical distance from the central mass?) nor what time would be that $0.1$ ms - which clock would measure it. No wonder: GR had just been born and E. himself wasn't much clearer about such matter. But after a century and a lot of valuable work of eminent theoreticians we can and must have sounder ideas. As to $R$ , I repeat that S. calls it an \"auxiliary\" quantity. In modern terms we would consider it a radial coordinate as good as $r$ - metric can be written both in terms of $R$ (S.'s eq. (14), exactly the same universally denoted today as \"S.'s metric\") and in terms of his $r$ . S. doesn't write the latter but you can see it here: $$ds^2 = \\left[1 -\\a\\,\\ra^{-1/3}\\right] dt^2 - {r^4 \\over \\ra\\,\\left[\\ra^{1/3} - \\a\\right]}\\,dr^2 - \\ra^{2/3} (d\\th^2 + \\sin^2\\!\\th\\,d\\phi^2).\\tag2$$ You can use eq. (2) to answer e.g. the following question: \"Once fixed $t$ and $r$ you're left with a 2D surface (a sphere). What's its area?\" The answer isn't $4\\pi r^2$ , but the more complicated $4\\pi\\,\\ra^{2/3}$ . You could use $R$ instead and then (from S.'s metric) you'd find $4\\pi R^2$ , which is the same. Another question could be: \"What's the radius of that spehere?\" S. wouldn't have hesitated. His answer would have been $$\\int_0^r\\! {r_1^2 \\over \\sqrt{\\sa \\left[\\sa^{1/3} - \\a\\right]}}\\,dr_1.$$ (The integral looks intimidating, but it's easily solved through a substitution - can you see it?) Surely S. would have preferred to use his \"auxiliary quantity\" writing the required radius as $$\\int_\\a^R\\!\\sqrt{R_1 \\over R_1 - \\a}\\,dR_1.$$ He wouldn't have worried about the lower $\\a$ limit, which isn't the origin of $R$ coordinate. To him the real radial coordinate was $r$ . It's up to us to be worried: if $r$ and $R$ are on equal footings as radial coordinates, where is the space origin? At $r=0$ or at $R=0$ ? A mathematician's answer would be straight: if you use $r$ then that coordinate works for all $r>0$ - only at $r=0$ metric (2) is singular as the coefficient of $dr^2$ vanishes. Instead if you want to use $R$ with S.'s metric you must keep $R>\\a$ since metric becomes singular at $R=\\a$ . In both cases - the mathematician would continue - this doesn't mean that your manifold ends there. It could, or it could go on - it's your choice and it doesn't depend on the radial coordinate you initially assumed. It's true that $r$ suggests that $r=0$ is the end of your manifold whereas $R$ naturally leads to think that there is \"something out there\", but here mathematicians take a different view. They would tell us \"what you're looking for is whether the manifold defined by the chart you have built allows for an extension or not. If it doesn't, this is the end of our argument. If it does, it's to you to decide if you want to give a physical meaning to the extension, or not.\" Well, the answer is that the extension exists . How can we say that? Simply by finding other coordinates which are able to cover a region wider than the original one. This was actually done, in several ways, and opened physicists a novel world. A physical meaning to that world was given in 1939, when a paper by Oppenheimer and Snyder [\"On Continued Gravitational Contraction\"] ( https://journals.aps.org/pr/abstract/10.1103/PhysRev.56.455 ) introduced the idea we today know as gravitational collapse .",
      "question_latex": [
        "R=(r^3+\\alpha^3)^{1/3}",
        "R",
        "</span> </p>\n\n<p><a href=\"https://i.stack.imgur.com/5cm0k.png\" rel=\"nofollow noreferrer\"><img src=\"https://i.stack.imgur.com/5cm0k.png\" alt=\"enter image description here\"></a></p>\n\n<p>Whereas, nowadays the variable used is just the radius <span class=\"math-container\">"
      ],
      "answer_latex": [
        "n^2 = {\\a \\over 2\\,\\ra}",
        "ds^2 = F\\,dt^2 - (G + H\\,r^2)\\,dr^2 - \nG\\,r^2 (d\\th^2 + \\sin^2\\!\\th\\,d\\phi^2) \\tag1",
        "n^2 = {\\a \\over 2\\,\\ra}",
        "n_0 = {1 \\over \\a \\sqrt2}.",
        "ds^2 = \\left[1 -\\a\\,\\ra^{-1/3}\\right] dt^2 -\n{r^4 \\over \\ra\\,\\left[\\ra^{1/3} - \\a\\right]}\\,dr^2 -\n\\ra^{2/3} (d\\th^2 + \\sin^2\\!\\th\\,d\\phi^2).\\tag2",
        "\\int_0^r\\!\n{r_1^2 \\over \\sqrt{\\sa \\left[\\sa^{1/3} - \\a\\right]}}\\,dr_1.",
        "\\int_\\a^R\\!\\sqrt{R_1 \\over R_1 - \\a}\\,dR_1.",
        "\\let\\a=\\alpha \\let\\b=\\beta \\let\\th=\\theta \\def\\ra{{(r^3+\\a^3)}} \n\\def\\sa{{(r_1^3+\\a^3)}}",
        "r",
        "R=\\ra^{1/3}",
        "R",
        "</span>\nwith <span class=\"math-container\">",
        "</span>.</p></li>\n<li><p>Schwarzschild writes metric as\n<span class=\"math-container\">",
        "</span>, <span class=\"math-container\">",
        "</span> functions of <span class=\"math-container\">",
        "</span> (eq. 6).</p></li>\n</ul>\n\n<p>From (1) we see that angular part of the metric is different from the\none we are accustomed to. In particular, surfaces <span class=\"math-container\">",
        "</span> don't\nhave area <span class=\"math-container\">",
        "</span>, but <span class=\"math-container\">",
        "</span> and <span class=\"math-container\">",
        "</span> is not constant.</p>\n\n<p>It's clear from these and other aspects of Schwarzschild's paper that\nhe considers <span class=\"math-container\">",
        "</span> as <em>true</em> physical space-time\ncoordinates. In particular, he constrains the \"singularity\" to <span class=\"math-container\">",
        "</span>,\ni,e, out of spacetime. He states that clearly when he writes condition\n(13), having precisely that aim.</p>\n\n<p>Of course both <span class=\"math-container\">",
        "</span> are legitimate radial coordinates. They are\nhowever not physically equivalent. In mathematical terms, using <span class=\"math-container\">",
        "</span>\nthe spacetime manifold exhibits a singularity only at border <span class=\"math-container\">",
        "</span>.\nUsing <span class=\"math-container\">",
        "</span> that singularity becomes the horizon (<span class=\"math-container\">",
        "</span>) and it makes sense\nto ask oneself if points with <span class=\"math-container\">",
        "</span> are to be included in spacetime. As\nis well known, a thorough understanding of that issue would have taken about half a century.</p>\n\n<p>OK, maybe this is history of physics. Or it's physics in its own\nright?</p>\n\n<hr>\n\n<p><strong>Edit</strong></p>\n\n<p>From comments and answers I've read I deem necessary to expand\nsomewhat my answer, hoping it will help to solve several doubts and\nmisconceptions.</p>\n\n<p>(A short historical note. Einstein's paper exhibiting the final form\nof his equations is dated Nov 25th 1915. Schw. paper is dated January\n13th 1916. Schw. died - by an autoimmune disease still incurable\ntoday - on May 11th 1916. He was 43.)</p>\n\n<p>First, there is a strong difference between our present way of\nunderstanding spacetime in GR and the way of E.'s and S.'s times. I\nalready remarked that one century ago there still weren't clear ideas\nabout the meaning of spacetime coordinates. There still was a tendency\nto consider them endowed of some physical significance by themselves.\nIn particular, although S. was well aware that spacetime in his\nsolution is curved and space sections are curved too (not euclidean),\nhe writes a formula for angular velocity in a circular orbit \n<span class=\"math-container\">",
        "</span> and concludes</p>\n\n<blockquote>\n  <p>the angular velocity does not, as with Newton's law, grow without\n  limit when the radius of the orbit gets smaller and smaller, but it\n  approaches a determined limit \n  <span class=\"math-container\">",
        "</span>\n  (For a point with the solar mass the limit frequency will be around\n  <span class=\"math-container\">",
        "</span> per second).</p>\n</blockquote>\n\n<p>(remember that S.'s <span class=\"math-container\">",
        "</span> is what is known as \"S. radius\" and is\nproportional to the mass of central body).</p>\n\n<hr>\n\n<p>On the contrary, present-day approach, grounded on sounder\nmathematical bases, is roughly the following.</p>\n\n<ol>\n<li>Spacetime is a semi-Riemannian manifold of dimension 4 and\nsignature <span class=\"math-container\">",
        "</span>.</li>\n</ol>\n\n<p>1.1 A (real) manifold is a set wherein real coordinates may be\ndefined. This can be done in several ways. Coordinates are nothing but\n<em>labels</em> for the set's points.</p>\n\n<p>1.2 It's not required that a set of coordinates be able to cover the\nwhole manifold. More sets are allowed - it's only required that\ntogether they cover the whole manifold and smoothly overlap between\nthem. Each set is named a <em>card</em> and the ensemble is named an <em>atlas</em>.</p>\n\n<p>1.3 An easy instance. To define a sphere as a (2D) manifold a minimum\nof two cards are required. Among geographers a lot of cartographic\nprojections are in use and a world's atlas is a good example of the\ngeneral idea.</p>\n\n<ol start=\"2\">\n<li>A Riemannian manifold is a manifold where a <em>metric</em> is defined.\nRoughly, a formula giving the distance between (infinitesimally) near\npoints.</li>\n</ol>\n\n<p>2.1 More exactly, the distance squared <span class=\"math-container\">",
        "</span>. So the metric must be\n<em>positive definite</em>.</p>\n\n<p>2.2 In a semi-Riemannian manifold an extension is allowed: <span class=\"math-container\">",
        "</span> may\nalso be negative. (A contradiction? Not quite. It's enough to relax\nthe intuitive interpretation as a distance squared.) Minkowski's\nspacetime of SR is already an instance of that: there are spacelike\nintervals, timelike ones, and lightlike too.</p>\n\n<p>2.3 The signature refers to how many independent displacements have\nmetric of each sign. In GR there are two conventions in use: <span class=\"math-container\">",
        "</span>\nmeans timelike has positive <span class=\"math-container\">",
        "</span>, spacelike negative. <span class=\"math-container\">",
        "</span> is the\nopposite. There is no real difference - it's only necessary to\nconsistently adhere to one convention. Mixing them in a calculation\nleads to certain disaster.</p>\n\n<ol start=\"3\">\n<li>Leaving aside more sophisticated usages, the metric is the only way\nwe have to give coordinates a physical meaning. It allows to compute\nthe time a clock measures between events or the length of a space\ninterval and so on.</li>\n</ol>\n\n<p>3.1 Assume a coordinate is called <span class=\"math-container\">",
        "</span>. It's the initial of \"time\" in\nEnglish, of \"temps\" in French, of \"tempo\" in Italian, of \"tiempo\" in\nSpanish ... but not of \"Zeit\" in German or \"czas\" in Polish. So why\nshould we assume that coordinate means time? It may (and usually will)\nbe, but not always. Only looking at the metric can we get a safe\nanswer.</p>\n\n<hr>\n\n<p>Now let's come back to S. He makes use of two radial coordinates: <span class=\"math-container\">",
        "</span>\nand <span class=\"math-container\">",
        "</span>. But there's no doubt about which he takes as the \"physical\"\nradius. His paper's title says</p>\n\n<blockquote>\n  <p>On the Gravitational Field of a Mass Point according to Einstein's\n  Theory</p>\n</blockquote>\n\n<p>A \"mass point\". It's obvious that he locates that mass at <span class=\"math-container\">",
        "</span>, that\n<span class=\"math-container\">",
        "</span> can take all real positive values, that he'll require no\nsingularity appears for positive <span class=\"math-container\">",
        "</span>. At paper's end, as I noted\nabove, he computes the revolution period of a planet as a function of\nradius and comments on a peculiar result; that period doesn't go to\nzero with <span class=\"math-container\">",
        "</span>. On the contrary, it goes to a non-zero limit of about\n<span class=\"math-container\">",
        "</span> ms. He doesn't ask himself what's the meaning of <span class=\"math-container\">",
        "</span> (physical\ndistance from the central mass?) nor what time would be that <span class=\"math-container\">",
        "</span> ms\n- which clock would measure it.</p>\n\n<p>No wonder: GR had just been born and E. himself wasn't much clearer\nabout such matter. But after a century and a lot of valuable work of\neminent theoreticians we can and must have sounder ideas.</p>\n\n<p>As to <span class=\"math-container\">",
        "</span>, I repeat that S. calls it an \"auxiliary\" quantity. In\nmodern terms we would consider it a radial coordinate as good as <span class=\"math-container\">",
        "</span> -\nmetric can be written both in terms of <span class=\"math-container\">",
        "</span> (S.'s eq. (14), exactly the\nsame universally denoted today as \"S.'s metric\") and in terms of his\n<span class=\"math-container\">",
        "</span>. S. doesn't write the latter but you can see it here:\n<span class=\"math-container\">",
        "</span></p>\n\n<p>You can use eq. (2) to answer e.g. the following question: \"Once fixed\n<span class=\"math-container\">",
        "</span> you're left with a 2D surface (a sphere). What's its area?\"\nThe answer isn't <span class=\"math-container\">",
        "</span>, but the more complicated\n<span class=\"math-container\">",
        "</span>. You could use <span class=\"math-container\">",
        "</span> instead and then (from S.'s\nmetric) you'd find <span class=\"math-container\">",
        "</span>, which is the same. Another question\ncould be: \"What's the radius of that spehere?\" S. wouldn't have\nhesitated. His answer would have been\n<span class=\"math-container\">",
        "</span>\n(The integral looks intimidating, but it's easily solved through a\nsubstitution - can you see it?) Surely S. would have preferred to use\nhis \"auxiliary quantity\" writing the required radius as\n<span class=\"math-container\">",
        "</span>\nHe wouldn't have worried about the lower <span class=\"math-container\">",
        "</span> limit, which isn't the\norigin of <span class=\"math-container\">",
        "</span> coordinate. To him the real radial coordinate was <span class=\"math-container\">",
        "</span>.</p>\n\n<p>It's up to us to be worried: if <span class=\"math-container\">",
        "</span> are on equal footings as\nradial coordinates, where is the space origin? At <span class=\"math-container\">",
        "</span> or at <span class=\"math-container\">",
        "</span>?\nA mathematician's answer would be straight: if you use <span class=\"math-container\">",
        "</span> then that\ncoordinate works for all <span class=\"math-container\">",
        "</span> - only at <span class=\"math-container\">",
        "</span> metric (2) is\n<em>singular</em> as the coefficient of <span class=\"math-container\">",
        "</span> vanishes. Instead if you want to\nuse <span class=\"math-container\">",
        "</span> with S.'s metric you must keep <span class=\"math-container\">",
        "</span> since metric becomes\nsingular at <span class=\"math-container\">",
        "</span>. </p>\n\n<p>In both cases - the mathematician would continue - this doesn't mean\nthat your manifold ends there. It could, or it could go on - it's\nyour choice and it doesn't depend on the radial coordinate you\ninitially assumed. It's true that <span class=\"math-container\">",
        "</span> suggests that <span class=\"math-container\">",
        "</span> is the end\nof your manifold whereas <span class=\"math-container\">"
      ],
      "created": "2019-06-30T22:15:53.717",
      "golden_ner_terms": [
        "angular velocity",
        "area",
        "argument",
        "atlas",
        "bases",
        "body",
        "central",
        "chart",
        "circular",
        "clear",
        "coefficient",
        "constant",
        "contraction",
        "contradiction",
        "coordinate",
        "coordinates",
        "cover",
        "decide",
        "difference",
        "dimension",
        "distance",
        "equation",
        "equivalent",
        "euclidean",
        "expand",
        "extension",
        "field",
        "fixed",
        "formula",
        "frequency",
        "function",
        "gravitational collapse",
        "grounded",
        "independent",
        "instance",
        "integral",
        "interpretation",
        "interval",
        "length",
        "limit",
        "manifold",
        "mass",
        "matter",
        "mean",
        "measure",
        "metric",
        "mixing",
        "near",
        "necessary",
        "negative",
        "nor",
        "obvious",
        "opposite",
        "orbit",
        "origin",
        "period",
        "physics",
        "point",
        "positive",
        "positive definite",
        "proportional",
        "radial",
        "radius",
        "real",
        "region",
        "riemannian manifold",
        "right",
        "signature",
        "singular",
        "solution",
        "space",
        "spacetime",
        "sphere",
        "straight",
        "strong",
        "substitution",
        "surface",
        "theory",
        "time",
        "vanishes",
        "variable",
        "velocity",
        "way",
        "work",
        "zero"
      ],
      "golden_ner_count": 85,
      "golden_patterns": [
        {
          "pattern": "work-examples-first",
          "score": 4.0,
          "hotwords": [
            "example",
            "e.g."
          ]
        },
        {
          "pattern": "construct-auxiliary-object",
          "score": 4.0,
          "hotwords": [
            "auxiliary",
            "introduce"
          ]
        },
        {
          "pattern": "argue-by-contradiction",
          "score": 2.0,
          "hotwords": [
            "contradiction"
          ]
        },
        {
          "pattern": "quotient-by-irrelevance",
          "score": 2.0,
          "hotwords": [
            "up to"
          ]
        },
        {
          "pattern": "check-the-extreme-cases",
          "score": 2.0,
          "hotwords": [
            "zero"
          ]
        },
        {
          "pattern": "construct-an-explicit-witness",
          "score": 2.0,
          "hotwords": [
            "exhibit"
          ]
        },
        {
          "pattern": "local-to-global",
          "score": 2.0,
          "hotwords": [
            "cover"
          ]
        },
        {
          "pattern": "transport-across-isomorphism",
          "score": 2.0,
          "hotwords": [
            "equivalent"
          ]
        },
        {
          "pattern": "monotone-approximation",
          "score": 2.0,
          "hotwords": [
            "limit"
          ]
        }
      ],
      "golden_pattern_names": [
        "work-examples-first",
        "construct-auxiliary-object",
        "argue-by-contradiction",
        "quotient-by-irrelevance",
        "check-the-extreme-cases",
        "construct-an-explicit-witness",
        "local-to-global",
        "transport-across-isomorphism",
        "monotone-approximation"
      ],
      "golden_scopes": [
        {
          "type": "for-any",
          "match": "for all $r>0$"
        }
      ],
      "golden_scope_count": 1
    },
    {
      "id": "se-physics-312703",
      "stratum": "medium",
      "title": "Motivation for covariant derivative axioms in the context of General Relativity",
      "tags": [
        "general-relativity",
        "spacetime",
        "differential-geometry",
        "tensor-calculus",
        "differentiation"
      ],
      "score": 5,
      "answer_score": 1,
      "question_body": "In General Relativity the idea of a covariant derivative on a manifold is quite important and it is usually defined by a set of axioms: Let $M$ be a smooth manifold. A covariant derivative $\\nabla$ on $M$ is a map $\\nabla $ which takes a vector field $X$ and an $(r,s)$-tensors $T$ for all $r,s$ producing the $(r,s)$ tensor $\\nabla_X T$ and satisfying: $\\nabla _X f = Xf,$ when $f\\in \\mathcal{C}^\\infty(M)$, $\\nabla_X (T+S)=\\nabla_X T + \\nabla_X S$ $\\nabla_X T(\\omega, Y)=(\\nabla_X T)(\\omega,Y)+T(\\nabla_X \\omega, Y)+T(\\omega, \\nabla_X Y)$ and similarly for all $(r,s)$-tensor $\\nabla_{f X+g Y}T = f\\nabla_X T+g\\nabla_Y T$ I know that in a more general contexts, this covariant derivative can be recovered from a connection on a principal bundle. That's not what I am talking here. What I am talking here is exactly this definition of covariant derivative, usually the one that is used in General Relativity. The problem is: the covariant derivative is highly important in the context of General Relativity, however, the definition with these axioms is overly abstract. Is there any way in which we can motivate this definition in the context of General Relativity? Or more generaly, is there any way to motivate this definition of a covariant derivative from the point of view of Physics? Again, I could just accept the axioms and move forward, but since this is overly abstract, and I'm dealing with Physics and not math, I'd like to gain a little motivation and insight if possible.",
      "answer_body": "Sure. Let me start with the story up 'til this point (someday I'll write it down in some central spot), so that we can have examples dotted all the way through. Basics Points and scalar fields So you start with a set of objects $\\mathcal M,$ and we're not actually going to peek at the structure of the objects themselves (except, perhaps, for equality) so we just call them \"points\" to indicate that we don't care about their internal structure. Kind of like in category theory, we're going to treat them as black boxes and describe their structure by adding a set of functions: in this case, the scalar fields $\\mathcal S \\subseteq (\\mathcal M \\to \\mathbb R),$ which we want to be \"smooth.\" To get this smoothness, we reinterpret functions $\\mathbb R^k \\to \\mathbb R$ as functions $(\\mathcal M \\to \\mathbb R)^k\\to(\\mathcal M \\to \\mathbb R)$ by applying them \"pointwise\". Let me denote this dual nature with square brackets (scalar field side) and parentheses (function side), formally $$f[s_1, s_2, \\dots s_k] = p\\mapsto f\\big(s_1(p), s_2(p), \\dots s_k(p)\\big),$$ where $\\mapsto$ constructs a function from a symbol ($p$) and a symbolic expression it maps to ($f(\\dots)$). I have not seen a good name for this dual-interpretation of these smooth functions: so, when a smooth function in $C^\\infty(\\mathbb R^k, \\mathbb R)$ is interpreted this way, I like to call them $k$-functors because there's a sorta-cool category diagram out there. So we've got a set of points with another set $\\mathcal S$ of smooth scalar fields defined over it, and $\\mathcal S$ is closed under $k$-functors for all $k$. These actually do a ton of work up-front; $\\operatorname{plus}(a, b) = a + b$ and $\\operatorname{times}(a, b) = a \\cdot b$ are both 2-functors and so under our axiom that $\\mathcal S$ is closed under $k$-functors (they map $\\mathcal S^k \\to \\mathcal S$) these are both allowed pointwise operations on scalar fields. Even better: define that a subset of $\\mathcal M$ is closed if it is a kernel for a field in $\\mathcal S$ or open if its complement is closed, and you have a natural topology: pointwise multiplication gives a union operation, pointwise addition of squares gives an intersection, and you can allow infinite intersections and finite unions without any problem. Using bump functions, you can even prove that all of the scalar fields in $\\mathcal S$ are continuous maps to $\\mathbb R$ on this topology. As an example of this theory-point: we can now demand that the space is connected, which in topology means \"the whole space is not a union of two disjoint open sets.\" Working back through the definitions, we first rewrite the claim to its complement; if $A \\cup B=\\mathcal M$ with $A,B$ disjoint $\\mathcal M - A$ being open means $B$ is closed. So it's equivalently not the union of two disjoint closed sets. So axiomatically we're saying that if $s_1 \\cdot s_2 = 0$ is the zero-field (which must exist because it's a $0$-functor!) then there is some point $p$ such that $s_1(p) = s_2(p) = 0.$ And that's a nice property because these scalar fields do lack this crucial property that we're so used to, \"$ab = 0$ implies either $a=0$ or $b=0.$\" Scalar fields can each be zero on non-overlapping subsets to multiply together to form zero. But as long as the space is connected, at least we recover something similar. So, for actual examples of scalar fields, on the surface of the sphere the points are in fact $\\{(x, y, z) : x^2 + y^2 + z^2=1\\},$ but we refuse to peek inside directly. Instead we start with scalar fields $x,y,z$ that happen to extract these components, and close over smooth functions to get the full set of scalar fields. On the flip side, say, $\\theta$ (the azimuthal angle -- the polar angle I think is OK) is not a valid smooth scalar field, because it has this nasty discontinuity which takes us away from the obvious topology that we'd like to use. You can also see that \"locally\" this will look like $\\mathbb R^2$ and will have similar open sets. We could do a similar treatment with the torus etc. The overlapping coordinate sets. Then we have one of our most important axioms: the statement that around any point $p$ there is an open set containing $p$ and $D$ scalar fields which can (a) be used to distinguish points in that open set, and (b) can be used to expand scalar fields, so that every scalar field on that open set can be expanded as a $D$-functor of the coordinate fields. So again, on the sphere, we can use the fields $x, y$ as our coordinates in the North or South hemispheres (which are open sets if we don't include the equator: use a bump function on $z$ to see this). Similarly we have overlapping hemispheres with respect to $y$ and $z$ which do not include their respective \"equators\". However even if some point is on two of these equators, we can see that it's not on the third: so each point has an open set, and two \"coordinate\" fields, and on that subset all of the scalar fields can be written as functions $f(x, y)$ or what have you. This means $D=2$ and the sphere is 2-dimensional. Easy peasy. Vector fields, tensor fields. Now we introduce the vector fields which are a set $\\mathcal V \\subset (\\mathcal S \\to \\mathcal S)$ obeying the Leibniz law. Say $f_{(m)}$ is the partial derivative of $f$ (which is some function in $C^\\infty(\\mathbb R^k, \\mathbb R)$, mind) with respect to its $m^\\text{th}$ argument. This Leibniz law says that for any $k$-functor $f,$ $$V f[s_1, s_2, s_3, \\dots s_k] = \\sum_{m=1}^k f_{(m)}[s_1, \\dots s_k] ~\\cdot~ V s_m.$$ If this seems to come out of nowhere, keep in mind that it actually is very logically related to everything we said above. Not only does the closure axiom create these operations $\\sum$ and $\\cdot$, but the coordinate axiom means that now on some open set every scalar $s$ is secretly a $D$-functor $s[c_1, c_2, \\dots c_D].$ Define on this subset the scalar fields $v_i = V c_i$, now you have straightforwardly that $V s = \\sum_{i=1}^D v_i \\cdot \\partial_i s.$ So that's why these Leibniz linear maps are \"vector fields\"; locally they are directional derivatives of scalar fields. But, they are defined geometrically: they aren't defined by these components $v_i,$ they just happen to be representable that way locally. It's not hard to see that $U + V$ is well-defined or that $s V$ is well-defined, but there is no obvious $U \\cdot V$ from the above definition. However there is a Lie bracket: $[U, V] = U \\circ V - V\\circ U$ must be Leibniz if both $U$ and $V$ are. (Furthermore this is not a \"vector space\" in the normal mathematical sense; it is a \"module.\" This is just because the scalar fields are not a \"field\" in the normal mathematical sense: just like you can't divide by 0, you can't divide by a scalar field which is 0 in some places, so an axiom [the existence of multiplicative inverses for every nonzero element] fails.) Once we have vector fields, we have covector fields (call this $\\bar {\\mathcal V}$), the linear maps $\\mathcal V \\to \\mathcal S$. and then we can introduce the $[a, b]$ tensor fields as the multilinear maps from $(\\mathcal V^m, \\bar {\\mathcal V}^n) \\to \\mathcal S.$ Call this $\\mathcal V_m^n$ for natural numbers $m, n.$ Now there is a geometric version of Einstein notation, where we just create a lot of copies of this tensor space $\\mathcal V_m^n$ and annotate it with a new letter $\\mathcal T$ plus $n$ distinct upper symbols and $m$ mutually distinct lower symbols. We also annotate any residents of one of these spaces with the corresponding symbols, and we may need to specify those symbols to be in a tensor-dependent order (i.e. not all tensors are symmetric). Outer products are defined the obvious way, e.g. a map from $\\mathcal T^a \\times \\mathcal T^b \\to \\mathcal T^{ab}$. As I recall we need an additional axiom that says that every tensor in, say, $\\mathcal T^{abc}_{de}$ can be written as a sum of outer products of terms in $\\mathcal T^a \\times \\mathcal T^b \\times \\mathcal T^c \\times T_d \\times T_e,$ but this is (if memory serves) apparently a consequence of paracompactness or the existence of the metric or something. The point is that each tensor is officially \"any multilinear map from vectors and covectors to scalars\", but is secretly a finite sum of outer products of vectors and covectors. Anyway, the reason that this last axiom is important is that it lets you do index contractions : expand in terms of the finite sum, then you can apply one of the terms of $\\bar {\\mathcal V}$ to the corresponding term of $\\mathcal V$ to get a scalar field. And as you can expect, we can symbolize this by repeating an index among the top and bottom vectors, to say \"these are being joined.\" So $v^{abc}_{bd}$ lives in $\\mathcal T^{ac}_d$ and has a purely geometric interpretation, there is no \"implicit summation\" of \"components.\" At this point we also have an automatic gradient operation on scalar fields; $\\nabla_\\bullet s = V \\mapsto V s$ maps any scalar field to a covector field. We also introduce the metric tensor, a special $[0, 2]$ and $[2, 0]$ tensor which contract to the identity $[1,1]$ tensor and show a special bijection between covector fields and vector fields. The connection OK, so once we have this whole story, the obvious question is whether there is a meaningful generalization of $\\nabla_a$ to vectors, as it's uniquely defined for scalars. And the answer is, \"Well, it's not quite so unique, but yeah, in many cases that exists.\" But we basically just start with the axioms. For example, we start from $\\nabla_a v^b$ being meaningful, and then we want to generalize to $\\nabla_a (k v^b)$ with the Leibniz rule, and we find that it should be $v^b \\nabla_a k + k \\nabla_a v^b.$ Similarly we want $\\nabla_a (u^b + v^b) = \\nabla_a u^b + \\nabla_a v^b$ as a straightforward linearity requirement. Our definition for its action on covectors is also really straightforward; recall that the contraction $u_b u^b$ is a scalar, and we expect $\\nabla_a(u_b v^b) = u_b \\nabla_a v^b + v^b\\nabla_a u_b.$ Since the first and second terms are already well-defined, we merely define the action of $\\nabla_a$ on a covector as the difference of those two terms, and we get this equation for free. So we assume that some generalization of this form exists. Your equations all concern this operator $\\nabla_a.$ The connection is easily seen when you remember that $v^a (\\nabla_a s)$ is defined as $V s$ by the geometric definition of the covector $\\nabla_a.$ Your expression $\\nabla_V$ is therefore equivalent to $v^a\\nabla_a,$ and we're generalizing $\\nabla_a$ to operate on vectors so it makes sense that then $v^a \\nabla_a$ gets generalized as well. Your first axiom is just \"the ungeneralized form still needs to do what the scalar gradient does, no messing with that please.\" Your second axiom is \"this is a linear operator\" and your third axiom is \"this is a Leibniz operator\", and your fourth is just a direct consequence of the fact that the $v^a$ premultiplier and contraction operation are also linear on $v^a,$ or in other words $\\nabla_a$ maps $\\mathcal T^\\bullet \\to \\mathcal T^\\bullet_a.$ An intuitive understanding for the degeneracy The basic reason that this is not unique in general, is not too hard to understand either. Parallel transport of a scalar makes sense; if you're going in the direction of the gradient it's increasing, in the opposite direction it's decreasing, and it's just a number at the end of the day, so you can believe that you always get to the same number no matter how you walk. But parallel transport of a vector is harder. Let's say that I am in Kansas City in the US and I face North and stretch out my right arm as a vector pointing East. I now walk to the North pole, I'm pointing out South (of course I am, all directions are South from the North Pole), roughly towards Madrid. But suppose that I first side-step East, I should run more or less into Washington, D.C. : now if I walk North to the pole I will be pointing instead at Rome. The path you take matters, and you can roughly predict that it involves 3 tensor indices; there's something there about \"you're taking as input a vector field, and a direction (which is also a vector field) and giving as output a new vector field\" that seems to relate 3 different vector fields, 2 as input and 1 as output. In other words it looks something like a $[1, 2]$-tensor field. Let's do this out formally with the geometry. Suppose you have two different connections $\\nabla$ and $\\nabla'$. Form the difference operator between them, $$\\Delta_a = \\nabla'_a - \\nabla_a.$$ Recall that these both map scalar fields to the same value -- there was no ambiguity about that scalar gradient field! So $\\Delta_a s = 0.$ But that means something very powerful, because $\\Delta_a$ is Leibniz: it means that $\\Delta_a (s~v^b) = s~\\Delta_a v^b$. So it's a linear mapping of vector fields to tensor fields. In particular, this means that $u^a \\Delta_a$ maps a vector field to another vector field linearly. Add in a covector $w_b$ and you get $u^a w_b \\Delta_a v^b$ being a linear mapping from two vector fields $u, v$ and one covector field $w$ to a scalar: and that was precisely our definition of a $[1, 2]$ tensor field. So, in fact, there exists a tensor $D$ such that $u^a w_b \\Delta_a v^b = D_{ac}^b u^a v^c w_b.$ Since this holds for all $u, w$ we can remove those and equivalently say, $$\\nabla'_a v^b = \\nabla_a v^b + D_{ac}^b v^c.$$ And this argument that $D$ needs to exist can also be run backwards, \"assume we add this tensor term to $\\nabla$, then we get another connection.\" So this is both necessary and sufficient. Then of course, we exploit this freedom to get a case where $\\nabla_a \\nabla_b = \\nabla_b \\nabla_a$ and $\\nabla_a g_{bc} = 0$ where $g$ is the metric tensor, and that is the Levi-Civita connection. But this is already a very long answer. I will give you a hint: define $\\Delta_{ab} = \\nabla_a\\nabla_b-\\nabla_b\\nabla_a$ and use a slightly more interesting version of the above argument to argue that this is actually a derivation on scalars and therefore takes the form $T^c_{ab} \\nabla_c$, this $T$ is the torsion tensor. What does changing our connection by adding $D$ do to it?",
      "question_latex": [
        "M",
        "\\nabla",
        "X",
        "(r,s)",
        "T",
        "r,s",
        "\\nabla_X T",
        "\\nabla _X f = Xf,",
        "f\\in \\mathcal{C}^\\infty(M)",
        "\\nabla_X (T+S)=\\nabla_X T + \\nabla_X S",
        "\\nabla_X T(\\omega, Y)=(\\nabla_X T)(\\omega,Y)+T(\\nabla_X \\omega, Y)+T(\\omega, \\nabla_X Y)",
        "\\nabla_{f X+g Y}T = f\\nabla_X T+g\\nabla_Y T"
      ],
      "answer_latex": [
        "f[s_1, s_2, \\dots s_k] = p\\mapsto f\\big(s_1(p), s_2(p), \\dots s_k(p)\\big),",
        "V f[s_1, s_2, s_3, \\dots s_k] = \\sum_{m=1}^k f_{(m)}[s_1, \\dots s_k] ~\\cdot~ V s_m.",
        "\\Delta_a = \\nabla'_a - \\nabla_a.",
        "\\nabla'_a v^b = \\nabla_a v^b + D_{ac}^b v^c.",
        "\\mathcal M,",
        "\\mathcal S \\subseteq (\\mathcal M \\to \\mathbb R),",
        "\\mathbb R^k \\to \\mathbb R",
        "(\\mathcal M \\to \\mathbb R)^k\\to(\\mathcal M \\to \\mathbb R)",
        "where",
        "constructs a function from a symbol (",
        ") and a symbolic expression it maps to (",
        "). I have not seen a good name for this dual-interpretation of these smooth functions: so, when a smooth function in",
        "is interpreted this way, I like to call them",
        "-functors because there's a sorta-cool category diagram out there. </p>\n\n<p>So we've got a set of points with another set",
        "of smooth scalar fields defined over it, and",
        "is closed under",
        "-functors for all",
        ". These actually do a ton of work up-front;",
        "and",
        "are both 2-functors and so under our axiom that",
        "-functors (they map",
        ") these are both allowed pointwise operations on scalar fields. Even better: define that a subset of",
        "is <em>closed</em> if it is a kernel for a field in",
        "or <em>open</em> if its complement is closed, and you have a natural topology: pointwise multiplication gives a union operation, pointwise addition of squares gives an intersection, and you can allow infinite intersections and finite unions without any problem. Using bump functions, you can even prove that all of the scalar fields in",
        "are continuous maps to",
        "on this topology. As an example of this theory-point: we can now demand that the space is connected, which in topology means \"the whole space is not a union of two disjoint open sets.\" Working back through the definitions, we first rewrite the claim to its complement; if",
        "with",
        "disjoint",
        "being open means",
        "is closed. So it's equivalently not the union of two disjoint closed sets. So axiomatically we're saying that if",
        "is the zero-field (which must exist because it's a",
        "-functor!) then there is some point",
        "such that",
        "And that's a nice property because these scalar fields <em>do</em> lack this crucial property that we're so used to, \"",
        "implies either",
        "or",
        "\" Scalar fields can each be zero on non-overlapping subsets to multiply together to form zero. But as long as the space is connected, at least we recover <em>something</em> similar.</p>\n\n<p>So, for actual examples of scalar fields, on the surface of the sphere the points are in fact",
        "but we refuse to peek inside directly. Instead we start with scalar fields",
        "that happen to extract these components, and close over smooth functions to get the full set of scalar fields. On the flip side, say,",
        "(the azimuthal angle -- the polar angle I think is OK) is <strong>not</strong> a valid smooth scalar field, because it has this nasty discontinuity which takes us away from the obvious topology that we'd like to use. You can also see that \"locally\" this will look like",
        "and will have similar open sets. We could do a similar treatment with the torus etc.</p>\n\n<h2>The overlapping coordinate sets.</h2>\n\n<p>Then we have one of our most important axioms: the statement that around any point",
        "there is an open set containing",
        "scalar fields which can (a) be used to distinguish points in that open set, and (b) can be used to expand scalar fields, so that every scalar field on that open set can be expanded as a",
        "-functor of the coordinate fields. So again, on the sphere, we can use the fields",
        "as our coordinates in the North or South hemispheres (which are open sets if we don't include the equator: use a bump function on",
        "to see this). Similarly we have overlapping hemispheres with respect to",
        "which do not include their respective \"equators\". However even if some point is on two of these equators, we can see that it's not on the third: so each point has an open set, and two \"coordinate\" fields, and on that subset all of the scalar fields can be written as functions",
        "or what have you. This means",
        "and the sphere is 2-dimensional. Easy peasy.</p>\n\n<h2>Vector fields, tensor fields.</h2>\n\n<p>Now we introduce the vector fields which are a set",
        "obeying the Leibniz law. Say",
        "is the partial derivative of",
        "(which is some function in",
        ", mind) with respect to its",
        "argument. This Leibniz law says that for any",
        "-functor",
        "If this seems to come out of nowhere, keep in mind that it actually is very logically related to everything we said above. Not only does the closure axiom create these operations",
        ", but the coordinate axiom means that now on some open set every scalar",
        "is secretly a",
        "Define on this subset the scalar fields",
        ", now you have straightforwardly that",
        "So that's why these Leibniz linear maps are \"vector fields\"; locally they are directional derivatives of scalar fields. But, they are defined geometrically: they aren't <em>defined</em> by these components",
        "they just <em>happen to be representable</em> that way locally. It's not hard to see that",
        "is well-defined or that",
        "is well-defined, but there is no obvious",
        "from the above definition. However there is a Lie bracket:",
        "must be Leibniz if both",
        "are. (Furthermore this is not a \"vector space\" in the normal mathematical sense; it is a \"module.\" This is just because the scalar fields are not a \"field\" in the normal mathematical sense: just like you can't divide by 0, you can't divide by a scalar field which is 0 in some places, so an axiom [the existence of multiplicative inverses for every nonzero element] fails.)</p>\n\n<p>Once we have vector fields, we have covector fields (call this",
        "), the linear maps",
        ". and then we can introduce the",
        "tensor fields as the multilinear maps from",
        "Call this",
        "for natural numbers",
        "Now there is a geometric version of Einstein notation, where we just create a lot of copies of this tensor space",
        "and annotate it with a new letter",
        "plus",
        "distinct upper symbols and",
        "mutually distinct lower symbols. We also annotate any residents of one of these spaces with the corresponding symbols, and we may need to specify those symbols to be in a tensor-dependent order (i.e. not all tensors are symmetric). Outer products are defined the obvious way, e.g. a map from",
        ". As I recall we need an additional axiom that says that every tensor in, say,",
        "can be written as a sum of outer products of terms in",
        "but this is (if memory serves) apparently a consequence of paracompactness or the existence of the metric or something. The point is that each tensor is officially \"any multilinear map from vectors and covectors to scalars\", but is secretly a finite sum of outer products of vectors and covectors.</p>\n\n<p>Anyway, the reason that this last axiom is important is that it lets you do <em>index contractions</em>: expand in terms of the finite sum, then you can apply one of the terms of",
        "to the corresponding term of",
        "to get a scalar field. And as you can expect, we can symbolize this by repeating an index among the top and bottom vectors, to say \"these are being joined.\" So",
        "lives in",
        "and has a purely geometric interpretation, there is no \"implicit summation\" of \"components.\"</p>\n\n<p>At this point we also have an automatic gradient operation on scalar fields;",
        "maps any scalar field to a covector field. We also introduce the metric tensor, a special",
        "tensor which contract to the identity",
        "tensor and show a special bijection between covector fields and vector fields. </p>\n\n<h1>The connection</h1>\n\n<p>OK, so once we have this whole story, the obvious question is whether there is a meaningful generalization of",
        "to vectors, as it's uniquely defined for scalars. And the answer is, \"Well, it's not quite so unique, but yeah, in many cases that exists.\" </p>\n\n<p>But we basically just start with the axioms. For example, we start from",
        "being meaningful, and then we want to generalize to",
        "with the Leibniz rule, and we find that it should be",
        "Similarly we want",
        "as a straightforward linearity requirement. Our definition for its action on covectors is also really straightforward; recall that the contraction",
        "is a scalar, and we expect",
        "Since the first and second terms are already well-defined, we merely define the action of",
        "on a covector as the difference of those two terms, and we get this equation for free. So we assume that some generalization of this form exists.</p>\n\n<p>Your equations all concern this operator",
        "The connection is easily seen when you remember that",
        "is defined as",
        "by the geometric definition of the covector",
        "Your expression",
        "is therefore equivalent to",
        "and we're generalizing",
        "to operate on vectors so it makes sense that then",
        "gets generalized as well. Your first axiom is just \"the ungeneralized form still needs to do what the scalar gradient does, no messing with that please.\" Your second axiom is \"this is a linear operator\" and your third axiom is \"this is a Leibniz operator\", and your fourth is just a direct consequence of the fact that the",
        "premultiplier and contraction operation are also linear on",
        "or in other words",
        "maps",
        "</p>\n\n<h2>An intuitive understanding for the degeneracy</h2>\n\n<p>The basic reason that this is not unique in general, is not too hard to understand either. Parallel transport of a scalar makes sense; if you're going in the direction of the gradient it's increasing, in the opposite direction it's decreasing, and it's just a number at the end of the day, so you can believe that you always get to the same number no matter how you walk. But parallel transport of a vector is harder. Let's say that I am in Kansas City in the US and I face North and stretch out my right arm as a vector pointing East. I now walk to the North pole, I'm pointing out South (of course I am, all directions are South from the North Pole), roughly towards Madrid. But suppose that I first side-step East, I should run more or less into Washington, D.C. : now if I walk North to the pole I will be pointing instead at Rome. The path you take matters, and you can roughly predict that it involves 3 tensor indices; there's something there about \"you're taking as input a vector field, and a direction (which is also a vector field) and giving as output a new vector field\" that seems to relate 3 different vector fields, 2 as input and 1 as output. In other words it looks something like a",
        "-tensor field.</p>\n\n<p>Let's do this out formally with the geometry. Suppose you have two different connections",
        ". Form the difference operator between them,",
        "Recall that these both map scalar fields to the same value -- there was no ambiguity about that scalar gradient field! So",
        "But that means something very powerful, because",
        "is Leibniz: it means that",
        ". So it's a linear mapping of vector fields to tensor fields. In particular, this means that",
        "maps a vector field to another vector field linearly. Add in a covector",
        "and you get",
        "being a linear mapping from two vector fields",
        "and one covector field",
        "to a scalar: and that was precisely our definition of a",
        "tensor field. So, in fact, there exists a tensor",
        "Since this holds for all",
        "we can remove those and equivalently say,",
        "And this argument that",
        "needs to exist can also be run backwards, \"assume we add this tensor term to",
        ", then we get another connection.\" So this is both necessary and sufficient.</p>\n\n<p>Then of course, we exploit this freedom to get a case where",
        "is the metric tensor, and that is the Levi-Civita connection. But this is already a very long answer. I will give you a hint: define",
        "and use a slightly more interesting version of the above argument to argue that this is actually a derivation on scalars and therefore takes the form",
        ", this",
        "is the torsion tensor. What does changing our connection by adding"
      ],
      "created": "2017-02-17T02:55:32.590",
      "golden_ner_terms": [
        "action",
        "addition",
        "angle",
        "argument",
        "axiom",
        "azimuthal angle",
        "basic",
        "bijection",
        "bottom",
        "category",
        "category theory",
        "central",
        "closed",
        "closed set",
        "closed under",
        "closure",
        "complement",
        "components",
        "connected",
        "connection",
        "consequence",
        "context",
        "continuous",
        "continuous map",
        "contraction",
        "coordinate",
        "coordinates",
        "covariant derivative",
        "covector",
        "decreasing",
        "derivation",
        "derivative",
        "diagram",
        "difference",
        "directional derivative",
        "disjoint",
        "equality",
        "equation",
        "equivalent",
        "even",
        "expand",
        "expanded",
        "expression",
        "face",
        "field",
        "finite",
        "function",
        "general relativity",
        "generalization",
        "geometry",
        "gradient",
        "identity",
        "implies",
        "increasing",
        "index",
        "indices",
        "infinite",
        "interpretation",
        "intersection",
        "kernel",
        "leibniz rule",
        "levi-civita connection",
        "lie bracket",
        "linear map",
        "linear mapping",
        "linear operator",
        "manifold",
        "map",
        "mapping",
        "matter",
        "metric",
        "metric tensor",
        "module",
        "multilinear",
        "multiplication",
        "multiplicative",
        "multiplicative inverse",
        "natural number",
        "nature",
        "necessary",
        "necessary and sufficient",
        "normal",
        "north pole",
        "number",
        "numbers",
        "obvious",
        "open",
        "open set",
        "operation",
        "operator",
        "opposite",
        "order",
        "outer",
        "outer product",
        "paracompactness",
        "parallel",
        "partial derivative",
        "path",
        "physics",
        "plus",
        "point",
        "pointwise",
        "pointwise addition",
        "pointwise operation",
        "polar",
        "polar angle",
        "pole",
        "principal bundle",
        "property",
        "property b",
        "relativity",
        "representable",
        "right",
        "scalar",
        "side",
        "similar",
        "smooth",
        "smooth function",
        "smooth manifold",
        "space",
        "sphere",
        "square",
        "structure",
        "subset",
        "sufficient",
        "sum",
        "summation",
        "surface",
        "symmetric",
        "tensor",
        "term",
        "theory",
        "top",
        "topology",
        "torsion",
        "torus",
        "union",
        "valid",
        "vector",
        "vector field",
        "vector fields",
        "vector space",
        "vectors",
        "walk",
        "way",
        "well-defined",
        "work",
        "zero",
        "zero element"
      ],
      "golden_ner_count": 149,
      "golden_patterns": [
        {
          "pattern": "local-to-global",
          "score": 6.0,
          "hotwords": [
            "local",
            "locally",
            "cover"
          ]
        },
        {
          "pattern": "unfold-the-definition",
          "score": 6.0,
          "hotwords": [
            "definition of",
            "means that",
            "recall that"
          ]
        },
        {
          "pattern": "work-examples-first",
          "score": 4.0,
          "hotwords": [
            "example",
            "e.g."
          ]
        },
        {
          "pattern": "dualise-the-problem",
          "score": 4.0,
          "hotwords": [
            "dual",
            "complement"
          ]
        },
        {
          "pattern": "transport-across-isomorphism",
          "score": 4.0,
          "hotwords": [
            "equivalent",
            "transport"
          ]
        },
        {
          "pattern": "find-the-right-abstraction",
          "score": 2.0,
          "hotwords": [
            "generalize"
          ]
        },
        {
          "pattern": "check-the-extreme-cases",
          "score": 2.0,
          "hotwords": [
            "zero"
          ]
        },
        {
          "pattern": "exploit-symmetry",
          "score": 2.0,
          "hotwords": [
            "symmetric"
          ]
        },
        {
          "pattern": "construct-an-explicit-witness",
          "score": 2.0,
          "hotwords": [
            "construct"
          ]
        },
        {
          "pattern": "pass-to-a-subsequence",
          "score": 2.0,
          "hotwords": [
            "compactness"
          ]
        },
        {
          "pattern": "encode-as-algebra",
          "score": 2.0,
          "hotwords": [
            "module"
          ]
        },
        {
          "pattern": "construct-auxiliary-object",
          "score": 2.0,
          "hotwords": [
            "introduce"
          ]
        },
        {
          "pattern": "estimate-by-bounding",
          "score": 2.0,
          "hotwords": [
            "at least"
          ]
        }
      ],
      "golden_pattern_names": [
        "local-to-global",
        "unfold-the-definition",
        "work-examples-first",
        "dualise-the-problem",
        "transport-across-isomorphism",
        "find-the-right-abstraction",
        "check-the-extreme-cases",
        "exploit-symmetry",
        "construct-an-explicit-witness",
        "pass-to-a-subsequence",
        "encode-as-algebra",
        "construct-auxiliary-object",
        "estimate-by-bounding"
      ],
      "golden_scopes": [
        {
          "type": "for-any",
          "match": "for all $k$"
        },
        {
          "type": "for-any",
          "match": "for any $k$"
        },
        {
          "type": "for-any",
          "match": "for all $u, w$"
        },
        {
          "type": "where-binding",
          "match": "where $g$ is"
        }
      ],
      "golden_scope_count": 4
    },
    {
      "id": "se-physics-223768",
      "stratum": "medium",
      "title": "How does a charge distribution evolve with time? (in classical electrodynamics)",
      "tags": [
        "electromagnetism",
        "newtonian-mechanics",
        "charge",
        "classical-electrodynamics"
      ],
      "score": 8,
      "answer_score": 3,
      "question_body": "It is often stated that, in classical electrodynamics, the electric and magnetic fields determine uni vocally the dynamics of a charge distribution distribution (how it evolves in time). I can more or less easily see how this applies to a charge distribution composed of point like particles. Basically the trajectories of the particles are given by the Euler-Lagrange equations of motion (or Hamilton's equations, or Newton's equations with the Lorentz force law, pick the ones you like the best). I can't understand how this can work for a continuous distribution, are there analogous equations that the charge distribution has to obey? I can't come up with a satisfactory equation and not due to lack of trying, although I've never taken fluid mechanics or continuum mechanics so I'm kinda lost when dealing with continuous distributions and velocity fields. In short: what is/are the equations that govern the dynamics of the charge distribution and its velocity field? Why haven't I been able to find this in Jackson's classical electrodynamics for example? Is it there in some other form? Is the answer something obvious that I'm missing?",
      "answer_body": "Knowing the current and the charge doesn't tell you the fields. And knowing the fields doesn't tell you how the charge and current evolve. Even throwing in the mass of each species and the velocity of each species and the charge to mass ratio of each species doesn't entirely fix it and still wouldn't tell you the fields. You instead have a coupled system of charges and fields. And all of it needs to be specified and then the evolution of the mutual system needs to be solved. And the continuum version is known to have catastrophes. And even the discrete version has radiation reaction issues. And sometimes those problems only cause small issues and so aren't a big deal. And historically this was an issue, and most people abandoned it to study Quantum Electrodynamics they didn't abandon it becasue they solved it. So classical electrodynamics as normally done had flaws, and most people just walked away from it, which is fine if the current users know the limitations. It is often stated that, in classical electrodynamics, an initial charge distribution along with an initial velocity field determines the electric and magnetic fields, I don't know of a single example of anyone other than you ever saying that. But it is not true. For instance if you had two charges five lightyears apart one held at rest for all $t\\lt t_0$ and the other held at rest for all $t$ with $|t-t_0-3\\rm{yr}|\\lt 1\\rm{yr}$ and during that time it was forced to oscillate harmonically at a fixed small amplitude and a fixed frequency of say the frequency of red light. Then at $t=t_0$ you can release both charges and they will act just like two charges that had always been at rest. For a while. For a year in fact. And then the radiation from the first charge will reach the second charge and it will start to move differently in the case described than the case where they had both always been at rest. They had the same initial charge and current distribution at $t=t_0$ but had different fields (so that part wasn't true) and then they had different evolution (so that part didn't happen either). In general, there are many possible fields given some initial charge and initial current. For instance, with no charges there are many possible vacuum solutions to Maxwell and you can add any one of them to an nonhomogenous solution to Maxwell and get another solution to Maxwell. So many solution to homogeneous Maxwell lead to many solutions to inhomogeneous Maxwell. If you want to get a unique solution you should use Jefimenko or Liénard–Wiechert and both will require knowing the whole past history, not just the initial charge and initial current. which in turn determine univocally the dynamics of said distribution (how it evolves in time). Even if you somehow got the fields (such as being given them) then all this gives you is the force. Knowing how forces determine motions is not trivial. Already, Newton's second law by itself allows multiple solutions (such as Norton's Dome) and throwing in charged particles makes it even more complicated through radiation reaction and other complications. Plus you'd either need the mass of the different charged particles species or else some other similar information. So any dream that you could just have a $\\rho (\\vec r,0)$ and $\\vec J(\\vec r,0)$ and get dynamics (get $\\rho (\\vec r,t)$ and $\\vec J(\\vec r,t)$) is doomed for not specifying the fields (and to use Jefimenko and/or Liénard–Wiechert requires knowing the entire past not just the present). And even if you had that, the dynamics would be nontrivial because of radiation and other effects and you'd need the mass and such. So you'd need a mass distribution and velocity field for each species with a fixed charge to mass ratio. And then you either would have to specify the fields (including any possible vacuum solutions) or you'd need a past history for the charges that includes the accelerations in the past. Or else you'd just have to be given the initial fields. And even then once you have the initial fields, the initial mass distribution of each species and the initial velocity field of each species. Then you'd have to deal with the mutual coupled dynamics of charges and fields, which is not trivial. If a particle species has a tiny charge to mass ratio and there are no other forces then it move in basically straight lines. If they have a huge charge to mass ratio then the lines can be quite bent, and then radiation reaction and other complications become more relevant. Those are all issues that come up for discrete particles. So let's get to the continuum situation. The whole fluid model has problems, and I'll reference *Inconsistency, Asymmetry, and Non-Locality: A Philosophical Investigation of Classical Electrodynamics\" as a (flawed but) general source of many problems with classical electrodynamics. And in particular there is a famous issue where you have a spherical charge distribution so each shell only contributes to the electric field of outer shells and everything can move purely radially. And yet you can get initial shells to cross each other. In general this phenomena is studied in the field called catastrophe theory. Which is merely the technical name. Basically it shows the fluid model breaks down. The fluid model means you break space into regions and for each region you assign a velocity vector that describes the collective motion of all the particles of that species. Then you take groups of regions and have different velocities for different regions in the group and effectively have something like a vector field. This breaks down if particles from one region end up crossing into particles from another region without mixing. Imagine a sparse bunch of cars at 100kpm heading towards a sparsely parked parking lot, as they pass through there is a catastrophe (even though the sparsity means no collision, catastrophe is a technical term, not an emotional or colloquial tetm) the point is that even though the cars are the same species the model of one fluid with one velocity fails. Particles with a low charge to mass ratio going in very straight lines. If they are sparse and a group with high velocity is heading towards a group with low velocity then they can mostly pass through unchanged (sparse so don't get close and low charge to mass, so they act similar to dust, a pressure free gas, again dust is another technical term, not a colloquial word). So later the fast ones should come out the other side pretty much unchanged and same with the slow bunch. A fluid model would try to assign a single velocity to the entire collection during the time they occupied the same region. This would be fine if they were dense enough to interact and had enough time to form a shared collective velocity for each small region. The catastrophe of the fluid theory comes from a continuous version of a similar problem, each shell is pushed outwards but at different rates. The charge density can vary radially. But the surface area also varies at different radii and so you can make it so that the inner surfaces feel a stronger force and thus there is a radial charge distribution and radial velocity field that is perfectly normal that develops over time to have an initially smaller radius shell with an initial velocity move outwards to cross (reach the same altitude) as an initially larger radius shell and so both shells end up at at the same radius at the same time (a finite amount of time) but with different velocities. So the fluid theory fails. So you could try to hypothesize a force density given by $\\rho\\vec E+\\vec J\\times\\vec B$ or an acceleration proportional to $\\rho\\vec E+\\rho\\vec v\\times\\vec B$ where $\\vec v$ is the velocity field value of that species at that point and the proportionality depends on the charge to mass ratio. But since the whole existence of the fluid velocity field requires avoiding catastrophe and in general you can't this is a hopeless problem. But it isn't extra hopeless, the original problem was bad. It is not the case that Newton's second law gives you dynamics. And the Lorentz Force Law is fatally flawed too. What people do is consider situation where these flaws lead to minimally or unimportantly wrong results and then they honestly admit that their assumptions fail in the general situation. For instance using $$m\\vec a=q\\vec E+q\\vec v\\times\\vec B$$ fails to take radiation reaction into account but for many situations that failure only leads to small incorrectness. If you pretend it is an exact result then you dishonesty. But you often are only trying to make a good enough prediction for a particular situation. Some people deal with radiation reaction issues in an iterative manner. They use guessed extrapolations of the evolution of the charges to find the fields due to those charges and currents and then they use those fields to get the forces on the particles then use $\\vec F=m\\vec a$ and the Lorentz Force Law to attempt to get a new predicted charge evolution and a new predicted current evolution. Then from these new charge predictions and these new current predictions they use Maxwell to get new predictions for the fields. And then they repeat: they use the new fields to find new forces and get even newer predictions for the charge and current. And then use those to get even newer predictions for the future fields. And then they repeat: they use the even newer fields to find even newer forces and get even newer newer predictions of the charge and current. And so on and so on. Alternating 1) using dynamical charges, initial fields, and Maxwell to get field dynamical fields and 2) using dynamical fields, initial charges, initial currents, masses, and Lorentz to get dynamical charges and currents. And this isn't about finding iterative predictions for later and later times. It is about make iterations of predictions for all future times, even future times a short time in the future. And as far as I know there is no result saying this iterative process converges. But in many situations of practical interest each of these first few iterations produced very small corrections if the initial guess was good. And so we can call it quits after a finite number and hope it is good enough for practice. This is not a satisfying theoretical framework, and people that tell you that classical electrodynamics as normally done is consistent and straightforward are mistaken or actively lying to you. It is not simple, and the way many people do it is actually mathematically inconsistent. Pretending it is a more perfect theory than it is is dangerous and unwise, for instance you might fail to predict the radiation caused by a large magnetic field bending a high speed charged particle in a circle and that could result in damage to objects and/or cause injury or death. You need to know when you are using a theory with limitations, so that you can handle it with care. Textbooks are going to hand you a toy theory in a special situation. For instance if you pull out a textbook with a test charge point particle in an external field, they are going to write down a Lagrangian that produces the Lorentz Force Law as the Euler-Lagrange equations because they expect you will like that. Or they might right a Lagrangian for fields with a fixed source and then get Maxwell as the Euler-Lagrange equations because they expect you will like that. And then you can pretend $\\vec F=m\\vec a$ is a thing that always works even though examples are known where it isn't. And then you can pretend that knowing an external field and getting a force is all you need and ignore radiation caused by the particle feeling the force. But that would be ignoring the limitations instead of accepting and dealing with them. But in reality you have initial fields and initial charges and you need to coevolve both. And that is a completely separate subject that isn't a standard textbook subject. If you pick a particular problem, like plasma physics people do, then you can find approximate solutions that are good enough for your particular situation. That's what almost everyone does almost all of the time.",
      "question_latex": [],
      "answer_latex": [
        "m\\vec a=q\\vec E+q\\vec v\\times\\vec B",
        "t\\lt t_0",
        "t",
        "|t-t_0-3\\rm{yr}|\\lt 1\\rm{yr}",
        "t=t_0",
        "\\rho (\\vec r,0)",
        "\\vec J(\\vec r,0)",
        "\\rho (\\vec r,t)",
        "\\vec J(\\vec r,t)",
        "\\rho\\vec E+\\vec J\\times\\vec B",
        "\\rho\\vec E+\\rho\\vec v\\times\\vec B",
        "\\vec v",
        "fails to take radiation reaction into account but for many situations that failure only leads to small incorrectness. If you pretend it is an exact result then you dishonesty. But you often are only trying to make a good enough prediction for a particular situation.</p>\n\n<p>Some people deal with radiation reaction issues in an iterative manner. They use guessed extrapolations of the evolution of the charges to find the fields due to those charges and currents and then they use those fields to get the forces on the particles then use",
        "and the Lorentz Force Law to attempt to get a new predicted charge evolution and a new predicted current evolution. Then from these new charge predictions and these new current predictions they use Maxwell to get new predictions for the fields.</p>\n\n<p>And then they repeat: they use the new fields to find new forces and get even newer predictions for the charge and current. And then use those to get even newer predictions for the future fields.</p>\n\n<p>And then they repeat: they use the even newer fields to find even newer forces and get even newer newer predictions of the charge and current.</p>\n\n<p>And so on and so on. Alternating 1) using dynamical charges, initial fields, and Maxwell to get field dynamical fields and 2) using dynamical fields, initial charges, initial currents, masses, and Lorentz to get dynamical charges and currents.</p>\n\n<p>And this isn't about finding iterative predictions for later and later times. It is about make iterations of predictions for all future times, even future times a short time in the future. And as far as I know there is no result saying this iterative process converges. But in many situations of practical interest each of these first few iterations produced very small corrections if the initial guess was good. And so we can call it quits after a finite number and hope it is good enough for practice.</p>\n\n<p>This is not a satisfying theoretical framework, and people that tell you that classical electrodynamics as normally done is consistent and straightforward are mistaken or actively lying to you.</p>\n\n<p>It is not simple, and the way many people do it is actually mathematically inconsistent. Pretending it is a more perfect theory than it is is dangerous and unwise, for instance you might fail to predict the radiation caused by a large magnetic field bending a high speed charged particle in a circle and that could result in damage to objects and/or cause injury or death. You need to know when you are using a theory with limitations, so that you can handle it with care.</p>\n\n<p>Textbooks are going to hand you a toy theory in a special situation. For instance if you pull out a textbook with a test charge point particle in an external field, they are going to write down a Lagrangian that produces the Lorentz Force Law as the Euler-Lagrange equations because they expect you will like that. Or they might right a Lagrangian for fields with a fixed source and then get Maxwell as the Euler-Lagrange equations because they expect you will like that.</p>\n\n<p>And then you can pretend"
      ],
      "created": "2015-12-13T03:44:40.407",
      "golden_ner_terms": [
        "acceleration",
        "almost all",
        "alternating",
        "altitude",
        "area",
        "charge",
        "circle",
        "classical electrodynamics",
        "collection",
        "collision",
        "consistent",
        "continuous",
        "continuum",
        "continuum mechanics",
        "cross",
        "current",
        "dense",
        "density",
        "discrete",
        "distribution",
        "entire",
        "equation",
        "equations of motion",
        "even",
        "field",
        "finite",
        "fix",
        "fixed",
        "forces",
        "frequency",
        "gas",
        "group",
        "harmonically",
        "homogeneous",
        "inconsistent",
        "information",
        "inhomogeneous",
        "inner",
        "instance",
        "interest",
        "l system",
        "lagrangian",
        "magnetic fields",
        "mass",
        "mixing",
        "model",
        "multiple",
        "normal",
        "number",
        "obvious",
        "outer",
        "pass through",
        "perfect",
        "perfectly normal",
        "physics",
        "plasma physics",
        "plus",
        "point",
        "pressure",
        "proportional",
        "proportionality",
        "quantum electrodynamics",
        "radial",
        "radiation",
        "radii",
        "radius",
        "ratio",
        "region",
        "right",
        "side",
        "similar",
        "simple",
        "solution",
        "source",
        "space",
        "speed",
        "straight",
        "stronger",
        "surface",
        "surface area",
        "t distribution",
        "term",
        "theory",
        "time",
        "vacuum",
        "vector",
        "vector field",
        "velocity",
        "way",
        "word",
        "work"
      ],
      "golden_ner_count": 91,
      "golden_patterns": [
        {
          "pattern": "work-examples-first",
          "score": 4.0,
          "hotwords": [
            "example",
            "for instance"
          ]
        },
        {
          "pattern": "monotone-approximation",
          "score": 4.0,
          "hotwords": [
            "approximate",
            "limit"
          ]
        },
        {
          "pattern": "try-a-simpler-case",
          "score": 2.0,
          "hotwords": [
            "toy"
          ]
        },
        {
          "pattern": "find-the-right-abstraction",
          "score": 2.0,
          "hotwords": [
            "framework"
          ]
        },
        {
          "pattern": "exploit-symmetry",
          "score": 2.0,
          "hotwords": [
            "symmetry"
          ]
        },
        {
          "pattern": "local-to-global",
          "score": 2.0,
          "hotwords": [
            "local"
          ]
        },
        {
          "pattern": "encode-as-algebra",
          "score": 2.0,
          "hotwords": [
            "ring"
          ]
        },
        {
          "pattern": "optimise-a-free-parameter",
          "score": 2.0,
          "hotwords": [
            "pick"
          ]
        }
      ],
      "golden_pattern_names": [
        "work-examples-first",
        "monotone-approximation",
        "try-a-simpler-case",
        "find-the-right-abstraction",
        "exploit-symmetry",
        "local-to-global",
        "encode-as-algebra",
        "optimise-a-free-parameter"
      ],
      "golden_scopes": [
        {
          "type": "for-any",
          "match": "for all $t\\lt t_0$"
        },
        {
          "type": "for-any",
          "match": "for all $t$"
        },
        {
          "type": "where-binding",
          "match": "where $\\vec v$ is"
        }
      ],
      "golden_scope_count": 3
    },
    {
      "id": "se-physics-694928",
      "stratum": "medium",
      "title": "Why don't stationary charge feel force from a current carrying wire?",
      "tags": [
        "electromagnetism",
        "special-relativity",
        "magnetic-fields",
        "electricity",
        "electric-current"
      ],
      "score": 6,
      "answer_score": 10,
      "question_body": "The current carrying wire doesn't apply any magnetic force on nearby charge $q$ ( positive stationary charge) because it has 0 velocity in lab frame. We found that there is no force on q by wire. But if we take a moving frame then, q is in relative motion and hence a current carrying wire applies a magnetic force on q. Let's denote this magnetic force in moving frame by $F_B$ . Since net force on charge is still 0 there is some force needed to cancel out this $F_B$ force. This was answered by length contraction. I have seen many videos referring to this as solution but i don't think if it works. I need a calculation which can show how length contraction actually helps in cancelling out $F_B$ . For calculation you are going to do i would like to refer some sign but you can take numerical value if you wish. Area of cross-section of wire, $A$ ; length of wire in lab frame, $L$ ; electron density of wire in lab frame, $n$ ; the average velocity of electrons in lab frame, $v_d$ ; the moving frame is moving in opposite direction of electrons motion as seen from lab frame and the velocity of this frame relative to lab frame is $v_F$ . The charge q is placed at $r$ distance from center of wire that. Moving frame is parallel to straight wire.",
      "answer_body": "$\\newcommand{\\bl}[1]{\\boldsymbol{#1}} \\newcommand{\\e}{\\bl=} \\newcommand{\\p}{\\bl+} \\newcommand{\\m}{\\bl-} \\newcommand{\\mb}[1]{\\mathbf {#1}} \\newcommand{\\mr}[1]{\\mathrm {#1}} \\newcommand{\\gr}{\\bl>} \\newcommand{\\les}{\\bl<} \\newcommand{\\greq}{\\bl\\ge} \\newcommand{\\leseq}{\\bl\\le} \\newcommand{\\plr}[1]{\\left(#1\\right)} \\newcommand{\\blr}[1]{\\left[#1\\right]} \\newcommand{\\vlr}[1]{\\vert#1\\vert} \\newcommand{\\Vlr}[1]{\\Vert#1\\Vert} \\newcommand{\\lara}[1]{\\langle#1\\rangle} \\newcommand{\\lav}[1]{\\langle#1|} \\newcommand{\\vra}[1]{|#1\\rangle} \\newcommand{\\lavra}[2]{\\langle#1|#2\\rangle} \\newcommand{\\lavvra}[3]{\\langle#1|\\,#2\\,|#3\\rangle} \\newcommand{\\vp}{\\vphantom{\\dfrac{a}{b}}} \\newcommand{\\hp}[1]{\\hphantom{#1}} \\newcommand{\\x}{\\bl\\times} \\newcommand{\\qqlraqq}{\\qquad\\bl{-\\!\\!\\!-\\!\\!\\!-\\!\\!\\!\\longrightarrow}\\qquad} \\newcommand{\\qqLraqq}{\\qquad\\boldsymbol{\\e\\!\\e\\!\\e\\!\\e\\!\\Longrightarrow}\\qquad} \\newcommand{\\tl}[1]{\\tag{#1}\\label{#1}} \\newcommand{\\hebl}{\\bl{=\\!=\\!=\\!==\\!=\\!=\\!==\\!=\\!=\\!==\\!=\\!=\\!==\\!=\\!=\\!==\\!=\\!=\\!==\\!=\\!=\\!==\\!=\\!=\\!==\\!=\\!=\\!==\\!=\\!=\\!==\\!=\\!=\\!==\\!=\\!=\\!=}}$ I post below the Figure 5.22 and a paragraph extracted from the book $''$ Electricity and Magnetism $''$ by Edward M. Purcell & David J. Morin, 3rd Edition. $\\hebl$ Figure 5.22. A test charge $\\:q\\:$ moving parallel to a current in a wire. (a) In the lab frame, the wire, in which the positive charges are fixed, is at rest. The current consists of electrons moving to the right with speed $\\:\\nu_0$ . The net charge on the wire is zero. There is no electric field outside the wire. (b) In a frame in which the test charge is at rest, the positive ions are moving to the left with speed $\\:\\nu\\:$ , and the electrons are moving to the right with speed $\\:\\nu'_0$ . The linear density of positive charge is greater than the linear density of negative charge. The wire appears positively charged, with an external field $\\:E'_r$ , which causes a force $\\:qE'_r$ on the stationary test charge $\\:q$ . (c) That force transformed back to the lab frame has the magnitude $\\:qE'_r/\\gamma$ , which is proportional to the product of the speed $\\:\\nu\\:$ of the test charge and the current in the wire, $\\m \\lambda_0\\nu_0$ . $\\hebl$ The case above is not exactly an answer to your question. But it gives some hints : (1) First, since you suppose that the stationary charge in the lab system doesn't feel electric force, $(\\mathbf E\\e\\bl0)$ , the wire must have zero net charge, so we must have its configuration as in (a) of the Figure-01 , that is ions at rest with linear charge density $\\:\\lambda_0\\:$ and electrons with linear charge density $\\:\\m\\lambda_0\\:$ moving to the right with velocity $\\:\\nu_0\\e v_d$ . The current of moving electrons produces a magnetic field $\\:\\mathbf B\\neq\\bl0\\:$ but zero magnetic force $\\:\\mathbf u_q\\x\\mathbf B\\e\\bl0\\:$ on the stationary charge $q\\:(\\mathbf u_q\\e\\bl0)$ . So, in the lab frame we have zero Lorentz force \\begin{equation} \\mathbf f\\e q\\plr{\\mathbf E\\p\\mathbf u_q\\x\\mathbf B}\\e\\bl0 \\tl{01} \\end{equation} (2) Second, in a frame moving to the right the ions are moving to the left and the linear density of positive charge is greater than the linear density of negative charge. The wire appears positively charged, with an external electric field $\\:\\mathbf E'\\neq\\bl0$ , which causes an electric force $\\:q\\mathbf E'\\:$ on the test charge $\\:q$ . This electric force is compensated by the magnetic force $\\:q\\plr{\\mathbf u'_q\\x\\mathbf B'}$ on the moving charge $\\:q$ . So, in the moving to the right frame we have also zero Lorentz force \\begin{equation} \\mathbf f'\\e q\\plr{\\mathbf E'\\p\\mathbf u'_q\\x\\mathbf B'}\\e\\bl0 \\tl{02} \\end{equation} A first quick proof of equation \\eqref{02} is based on the Lorentz transformation of the electromagnetic field, see the Figure and equations (04a),(04b) in my answer therein Is it a typo in David Tong's derivation of spin-orbit interaction? repeated herein for convenience \\begin{align} \\mathbf E' & \\e\\gamma \\mathbf E\\m\\dfrac{\\gamma^2}{c^2 \\plr{\\gamma\\p1}}\\plr{\\mathbf E\\bl\\cdot \\bl\\upsilon}\\bl\\upsilon\\,\\p\\,\\gamma\\plr{\\bl\\upsilon\\x\\mathbf B} \\tl{03a}\\\\ \\mathbf B' & \\e\\gamma \\mathbf B\\m\\dfrac{\\gamma^2}{c^2 \\plr{\\gamma\\p1}}\\plr{\\mathbf B\\bl\\cdot \\bl\\upsilon}\\bl\\upsilon\\,\\m\\,\\dfrac{\\gamma}{c^2}\\plr{\\bl\\upsilon\\x\\mathbf E} \\tl{03b} \\end{align} If at a point in the unprimed system (lab system) we have $\\:\\mathbf E\\e\\bl0\\:$ then in the primed system moving with velocity $\\:\\bl\\upsilon$ \\begin{equation} \\mathbf E' \\e\\bl\\upsilon\\x\\mathbf B' \\tl{04} \\end{equation} see equation (08) and its proof in aforementioned linked answer. But in the new frame we have for the velocity $\\:\\mathbf u'_q\\:$ of the test charge \\begin{equation} \\mathbf u'_q \\e \\m\\bl\\upsilon \\tl{05} \\end{equation} Equations \\eqref{04},\\eqref{05} imply equation \\eqref{02}. Note that based on the Lorentz transformation of the electromagnetic field, equations \\eqref{03a} & \\eqref{03b}, the Lorentz 3-vector force applied on a point charge $\\:q\\:$ moving with velocity $\\:\\mathbf u\\:$ , that is \\begin{equation} \\mathbf f\\e q\\plr{\\mathbf E\\p\\mathbf u\\x\\mathbf B} \\tl{06} \\end{equation} under a Lorentz boost with velocity $\\:\\bl\\upsilon\\:$ is transformed as follows \\begin{equation} \\mathbf f'\\e \\dfrac{\\mathbf f\\p\\dfrac{\\gamma^2}{c^2 \\plr{\\gamma\\p 1}}\\plr{\\mathbf f\\bl\\cdot \\bl\\upsilon}\\bl\\upsilon\\m\\gamma \\bl\\upsilon\\plr{\\dfrac{\\mathbf f\\bl\\cdot\\mathbf u}{c^{2}}}}{\\gamma \\plr{1\\m\\dfrac{\\bl\\upsilon\\bl\\cdot\\mathbf u}{c^{2}\\vphantom{\\tfrac{a}{b}}}}} \\tl{07} \\end{equation} see equation (11) in my answer therein Are magnetic fields just modified relativistic electric fields? . So, if $\\:\\mathbf f\\e\\bl 0\\:$ in an inertial frame $\\:\\rm S\\:$ then $\\:\\mathbf f'\\e\\bl 0\\:$ in any other inertial $\\:\\rm S'$ . $\\hebl$ ADDENDUM A By this ADDENDUM A we respond to the OP's demand to have $''$ ... a calculation which can show how length contraction actually helps in cancelling out $F_B$ $''$ . First we must have in mind that Length Contraction ( LC ) and Time Dilation ( TD ) are consequences of the Lorentz $\\,$ Trans- formation ( LT ). Their use to get results is unsafe. In our calculations we use LT to get safe results while we use LC and TD to interpret these results. This will be done in the following for our case of the infinite straight wire. We will determine the electromagnetic field $\\:\\plr{\\mb E,\\mb B}\\:$ in the inertial frame $\\:\\mr S$ , the rest frame of the wire and the charge $\\:q$ , from the distribution of electric charges and electric currents directly by the Maxwell equations. Using a Lorentz boost transformation, equations \\eqref{03a} & \\eqref{03b}, we will determine the electromagnetic field $\\:\\plr{\\mb E',\\mb B'}\\:$ in an inertial frame $\\:\\mr S'\\:$ moving in the direction of the straight wire. By Lorentz Contraction and relativistic addition of velocities we will interpret the results determining the distribution of electric charges and electric currents in the frame $\\:\\mr S'$ . In Figure-02 we have an infinitely long wire with : (1) ions of linear charge density $\\:\\lambda_{\\bf i}\\e\\lambda_0\\gr 0\\:$ at rest so producing zero electric current $\\:\\mb I_{\\bf i}\\e\\mr I_{\\bf i}\\mb e_{\\bl z}\\e\\bl0\\:$ and (2) electrons of linear charge density $\\:\\lambda_{\\bf e}\\e\\m\\lambda_0\\:$ in motion with velocity $\\:\\mb v\\e \\mr v \\,\\mb e_{\\bl z} , \\mr v \\bl\\in \\plr{\\m c,\\p c}$ so producing electric current $\\:\\mb I_{\\bf e}\\e\\mr I_{\\bf e}\\mb e_{\\bl z}\\e\\lambda_{\\bf e}\\,\\mb v\\e \\m\\lambda_0\\mr v \\,\\mb e_{\\bl z}$ . So in the wire we have : (a) zero linear charge density $\\:\\lambda\\e\\lambda_{\\bf i}\\p\\lambda_{\\bf e}\\e0\\:$ and (b) electric current $\\:\\mb I\\e \\mb I_{\\bf i}\\p\\mb I_{\\bf e}\\e \\m\\lambda_0\\mr v \\,\\mb e_{\\bl z}$ . Although in many textbooks and the web we give in Figure-03 the electromagnetic field $\\:\\plr{\\mb E,\\mb B}\\:$ produced by an infinitely long straight wire with constant uniform linear charge density $\\:\\lambda\\:$ and constant electric current $\\:\\mb I\\e \\mr I\\,\\mb e_{\\bl z}$ . Note that $\\:\\mr I\\:$ is not the magnitude of the vector $\\:\\mb I\\:$ , it's a real number, that is it could take negative values. Because of the rotational around and translational along the $\\:z\\m$ axis symmetries we use cylindrical coordinates $\\:\\rho,\\phi,z\\:$ so we have \\begin{equation} \\mb E\\e\\dfrac{\\lambda}{2\\pi\\epsilon_0\\rho}\\mb e_{\\bl\\rho}=\\dfrac{\\lambda}{2\\pi\\epsilon_0\\rho^2}\\bl\\rho\\,,\\qquad \\mb B=\\dfrac{\\mu_0\\mr I}{2\\pi\\rho}\\mb e_{\\bl\\phi}\\e\\dfrac{\\mu_0}{2\\pi\\rho^2}\\plr{\\mb I\\bl{\\times\\rho}} \\tl{A-01} \\end{equation} where $\\:\\bl\\rho\\e \\rho\\,\\mb e_{\\bl\\rho}\\e \\plr{\\rho\\cos\\phi,\\rho\\sin\\phi,0}$ . In equation \\eqref{A-01} the magnitude of the vector $\\:\\mb E\\:$ is determined from the Maxwell equation \\begin{equation} \\bl{\\nabla\\cdot}\\mb E\\e\\dfrac{\\varrho}{\\epsilon_0}\\quad \\texttt{where } \\varrho\\e\\texttt{volume charge density} \\tl{A-02} \\end{equation} by volume integration in a cylinder of radius $\\:\\rho\\:$ and height $\\:\\mr L\\:$ \\begin{equation} \\begin{split} \\iiint\\limits_V\\bl{\\nabla\\cdot}\\mb E\\,\\mr dV & \\e\\iiint\\limits_V\\dfrac{\\varrho}{\\epsilon_0}\\,\\mr dV\\:\\bl\\Longrightarrow\\:\\iint\\limits_S \\mb E\\bl\\cdot\\mr d\\bl S\\e\\dfrac{\\lambda\\mr L}{\\epsilon_0}\\:\\bl\\Longrightarrow\\\\ \\mr E\\cdot 2\\pi\\rho \\,\\mr L & \\e \\dfrac{\\lambda\\mr L}{\\epsilon_0}\\:\\bl\\Longrightarrow\\: \\mr E\\e\\dfrac{\\lambda}{2\\pi\\epsilon_0\\rho}\\\\ \\end{split} \\tl{A-03} \\end{equation} that is essentially by Gauss Law. Also the magnitude of the vector $\\:\\mb B\\:$ is determined from the Maxwell equation \\begin{equation} \\bl{\\nabla\\times}\\mb B\\e \\mu_0 \\mb j \\p\\dfrac{1}{c^2}\\dfrac{\\partial\\mb E}{\\partial t}\\quad \\texttt{where } \\mb j\\e\\texttt{electric current density} \\tl{A-04} \\end{equation} by surface integration on a circular disk of radius $\\:\\rho\\:$ (note that $\\:\\mb E\\:$ is constant in time) \\begin{equation} \\begin{split} \\iint\\limits_S\\plr{\\bl{\\nabla\\times}\\mb B}\\bl\\cdot\\mr d\\bl S & \\e\\iint\\limits_S\\mu_0 \\mb j \\bl\\cdot\\mr d\\bl S\\:\\bl\\Longrightarrow\\:\\oint\\limits_C\\mb B\\bl\\cdot\\mr d\\bl \\ell\\e \\mu_0\\,\\mr I\\:\\bl\\Longrightarrow\\\\ \\mr B\\cdot 2\\pi\\rho & \\e\\mu_0\\,\\mr I\\:\\bl\\Longrightarrow\\:\\mr B\\e\\dfrac{\\mu_0\\mr I}{2\\pi\\rho} \\\\ \\end{split} \\tl{A-05} \\end{equation} Note that the electromagnetic field $\\:\\plr{\\mb E,\\mb B}\\:$ in equation \\eqref{A-01} as derived directly from Maxwell equations is exactly (not approximately) relativistic. Using above general solution for our case here with zero linear electric charge density $\\:\\lambda\\e0\\:$ and electric current $\\:\\mb I\\e \\m\\lambda_0\\mr v \\,\\mb e_{\\bl z}$ ., that is for the configuration of Figure-02 , we have in the rest frame $\\:\\mr S\\:$ of the wire \\begin{equation} \\mb E\\e\\bl 0\\:(\\texttt{everywhere)}\\,,\\qquad \\mb B\\e\\m\\dfrac{\\mu_0\\lambda_0\\mr v}{2\\pi\\rho}\\mb e_{\\bl\\phi} \\tl{A-06} \\end{equation} as shown in Figure-04 . In an inertial frame $\\:\\mr S'\\:$ moving with velocity $\\:\\bl\\upsilon\\e \\upsilon\\,\\mb e_{\\bl z}\\:$ with respect to the rest frame $\\:\\mr S\\:$ of the wire the electromagnetic field $\\:\\plr{\\mb E',\\mb B'}\\:$ will be derived from that of \\eqref{A-06} via the Lorentz boost transformation, equations \\eqref{03a} & \\eqref{03b}, that is \\begin{equation} \\mb E'\\e\\gamma_{\\bl \\upsilon}\\plr{\\bl{\\upsilon\\times}\\mathbf B}\\,,\\qquad \\mb B'\\e\\gamma_{\\bl \\upsilon}\\mb B\\qquad \\texttt{ where }\\gamma_{\\bl \\upsilon}\\e 1\\bigg/\\sqrt{1\\m\\dfrac{\\upsilon^2}{c^2}} \\tl{A-07} \\end{equation} since $\\:\\mb E\\e\\bl 0\\:$ and $\\:\\plr{\\mb B\\bl{\\cdot\\upsilon }}\\e 0$ . Inserting the expression \\eqref{A-06} of $\\:\\mb B\\:$ in \\eqref{A-07} we have \\begin{equation} \\mb E'\\e\\gamma_{\\bl\\upsilon}\\plr{\\bl{\\upsilon\\times}\\mb B}\\e \\m\\gamma_{\\bl\\upsilon}\\dfrac{\\mu_0\\lambda_0\\upsilon\\,\\mr v}{2\\pi\\rho}\\plr{\\mb e_{\\bl z}\\bl\\times\\mb e_{\\bl\\phi}}\\e \\m\\gamma_{\\bl\\upsilon}\\dfrac{\\mu_0\\lambda_0\\upsilon\\,\\mr v}{2\\pi\\rho'}\\plr{\\mb e'_{\\bl z}\\bl\\times\\mb e'_{\\bl\\phi}} \\tl{A-08} \\end{equation} so \\begin{equation} \\mb E'\\e \\dfrac{\\gamma_{\\bl\\upsilon}\\mu_0\\lambda_0\\upsilon\\,\\mr v}{2\\pi\\rho'}\\mb e'_{\\bl\\rho}\\,,\\qquad \\mb B'\\e\\m\\dfrac{\\gamma_{\\bl\\upsilon}\\mu_0\\lambda_0\\mr v}{2\\pi\\rho'}\\mb e'_{\\bl\\phi} \\tl{A-09} \\end{equation} as shown in Figure-05 . We bring these results in expressions similar to the general solution \\eqref{A-01} as follows \\begin{equation} \\mb E'\\e\\dfrac{\\lambda'}{2\\pi\\epsilon_0\\rho'}\\mb e'_{\\bl\\rho}=\\dfrac{\\lambda'}{2\\pi\\epsilon_0\\rho'^{\\,2}}\\bl\\rho'\\,,\\qquad \\mb B'=\\dfrac{\\mu_0\\mr I'}{2\\pi\\rho'}\\mb e'_{\\bl\\phi}\\e\\dfrac{\\mu_0}{2\\pi\\rho'^{\\,2}}\\plr{\\mb I'\\bl{\\times\\rho'}} \\tl{A-10} \\end{equation} where \\begin{equation} \\boxed{\\:\\:\\lambda'\\e \\dfrac{\\gamma_{\\bl\\upsilon}\\upsilon\\,\\mr v}{c^2}\\lambda_0\\vp\\:\\:}\\quad \\texttt{and}\\quad \\boxed{\\:\\;\\mr I'\\e \\m\\gamma_{\\bl\\upsilon}\\lambda_0\\,\\mr v\\e \\gamma_{\\bl\\upsilon} \\mr I\\vp\\:\\:} \\tl{A-11} \\end{equation} So it looks like on the $\\:z'\\m$ axis of the moving frame $\\:\\mr S'\\:$ there exists a linear electric charge density $\\:\\lambda'\\:$ that produces the electric field $\\:\\mb E'\\:$ and an electric current $\\:\\mr I'\\:$ that produces the magnetic field $\\:\\mb B'$ . But how these facts are explained and especially the values of $\\:\\lambda', \\mr I'\\:$ in equation \\eqref{A-11} ??? These are explained in ADDENDUM B by interpretations using the length contraction and the relativistic addition of velocities. $\\hebl$ ADDENDUM B In the inertial frame $\\:\\mr S'\\:$ for the linear electric charge density $\\:\\lambda'\\:$ we have \\begin{equation} \\begin{split} \\lambda'& \\e\\lambda'_{\\bf i}\\p\\lambda'_{\\bf e}\\\\ \\texttt{where}\\quad\\lambda'_{\\bf i} & \\e \\texttt{charge density of the ion array}\\\\ \\texttt{and }\\quad\\lambda'_{\\bf e} & \\e \\texttt{charge density of the electron array}\\\\ \\end{split} \\tl{B-01} \\end{equation} The ion array is at rest in frame $\\:\\mr S$ , the rest frame of the wire, but it moves with velocity $\\:\\plr{\\m\\bl\\upsilon}\\:$ in frame $\\:\\mr S'$ . Take a straight segment of length $\\:\\mr L_{\\bf i}\\:$ on the ion array in frame $\\:\\mr S$ . Because of length contraction for its length $\\:\\mr L'_{\\bf i}\\:$ in frame $\\:\\mr S'\\:$ we have \\begin{equation} \\mr L'_{\\bf i}\\e \\dfrac{\\mr L_{\\bf i}}{\\gamma_{\\bl\\upsilon}} \\tl{B-02} \\end{equation} Because of the Lorentz invariance of the electric charge we have by equating the charge between $\\:\\mr L_{\\bf i}\\:$ and $\\:\\mr L'_{\\bf i}\\:$ \\begin{equation} \\lambda'_{\\bf i}\\mr L'_{\\bf i}\\e \\lambda_{\\bf i}\\mr L_{\\bf i}\\:\\bl\\Longrightarrow\\: \\lambda'_{\\bf i}\\e \\dfrac{\\mr L_{\\bf i}}{\\mr L'_{\\bf i}}\\lambda_{\\bf i}\\e\\gamma_{\\bl\\upsilon}\\lambda_{\\bf i} \\tl{B-03} \\end{equation} so \\begin{equation} \\boxed{\\:\\:\\lambda'_{\\bf i}\\e\\gamma_{\\bl\\upsilon}\\lambda_0\\vp\\:\\:} \\tl{B-04} \\end{equation} Now, in order to find the linear charge density $\\:\\lambda'_{\\bf e}\\:$ of the electron array in the frame $\\:\\mr S'\\:$ we must determine : (1) First, the velocity $\\:\\mb v'\\:$ of the electron array in the frame $\\:\\mr S'\\:$ and (2) Second, how the length of straight segments of the electron array are contracted between the inertial frames. For (1) we have the following equation for the Lorentz transformation of the velocity 3-vector $\\:\\mb v\\:$ under a Lorentz boost with velocity $\\:\\bl\\upsilon$ \\begin{equation} \\mb v'\\e\\dfrac{\\mb v\\p\\dfrac{\\gamma^2_{\\bl\\upsilon}\\plr{\\bl{\\upsilon\\cdot}\\mb v}}{c^2 \\plr{\\gamma_{\\bl\\upsilon}\\p 1}}\\bl\\upsilon\\m\\gamma_{\\bl\\upsilon}\\bl\\upsilon}{\\gamma_{\\bl\\upsilon}\\plr{1\\m\\dfrac{\\bl{\\upsilon\\cdot}\\mb v}{c^2}\\vphantom{\\dfrac{\\tfrac{a}{b}}{\\tfrac{a}{b}}}}} \\tl{B-05} \\end{equation} which essentially gives the relativistic addition of the velocities $\\:\\mb v\\:$ and $\\:\\m\\bl\\upsilon$ . For (2) we have the following important relation between the $\\:\\gamma\\m$ factors of the velocities $\\:\\bl\\upsilon, \\mb v, \\mb v'\\:$ of equation \\eqref{B-05} \\begin{equation} \\dfrac{\\gamma_{\\mb v'}}{\\gamma_{\\mb v}}\\e\\gamma_{\\bl\\upsilon}\\plr{1\\m\\dfrac{\\bl{\\upsilon\\cdot}\\mb v}{c^2}\\vphantom{\\dfrac{\\tfrac{a}{b}}{\\tfrac{a}{b}}}}\\quad \\texttt{where }\\gamma_{\\bl \\alpha}\\e 1\\bigg/\\sqrt{1\\m\\dfrac{\\alpha^2}{c^2}}\\:\\plr{\\bl\\alpha\\e \\bl\\upsilon,\\mb v,\\mb v'} \\tl{B-06} \\end{equation} For the derivations of equations \\eqref{B-05} and \\eqref{B-06} see my answer here Transformation of 4-velocity , equations (08) and (14) respectively. For our case the velocities $\\:\\bl\\upsilon, \\mb v, \\mb v'\\:$ are collinear on the same $\\:z\\m$ axis so \\eqref{B-05} yields \\begin{equation} \\mb v'\\e\\dfrac{\\mb v\\m\\bl\\upsilon}{1\\m\\dfrac{\\bl{\\upsilon\\cdot}\\mb v}{c^2}\\vphantom{\\dfrac{\\tfrac{a}{b}}{\\tfrac{a}{b}}}} \\tl{B-07a} \\end{equation} expressed also between scalars \\begin{equation} \\mr v'\\e\\dfrac{\\mr v\\m\\upsilon}{1\\m\\dfrac{\\upsilon\\,\\mr v}{c^2}\\vphantom{\\dfrac{\\tfrac{a}{b}}{\\tfrac{a}{b}}}} \\tl{B-07b} \\end{equation} while equation \\eqref{B-06} yields \\begin{equation} \\dfrac{\\gamma_{\\mb v'}}{\\gamma_{\\mb v}}\\e\\gamma_{\\bl\\upsilon}\\plr{1\\m\\dfrac{\\upsilon\\,\\mr v}{c^2}\\vphantom{\\dfrac{\\tfrac{a}{b}}{\\tfrac{a}{b}}}} \\tl{B-08} \\end{equation} Consider now a straight segment $\\:\\mr L^0_{\\bf e}\\:$ on the electron array in its rest frame . Because of length contraction for the lengths $\\:\\mr L_{\\bf e},\\mr L'_{\\bf e}\\:$ in the inertial frames $\\:\\mr S,\\mr S'\\:$ respectively we have \\begin{equation} \\gamma_{\\mb v}\\mr L_{\\bf e}\\e\\gamma_{\\mb v'}\\mr L'_{\\bf e}\\e \\mr L^0_{\\bf e}\\e \\texttt{Lorentz invariant} \\tl{B-09} \\end{equation} Because of the Lorentz invariance of the electric charge we have by equating the charge between $\\:\\mr L_{\\bf e}\\:$ and $\\:\\mr L'_{\\bf e}\\:$ \\begin{equation} \\lambda'_{\\bf e}\\mr L'_{\\bf e}\\e \\lambda_{\\bf e}\\mr L_{\\bf e}\\:\\bl\\Longrightarrow\\: \\lambda'_{\\bf e}\\e \\dfrac{\\mr L_{\\bf e}}{\\mr L'_{\\bf e}}\\lambda_{\\bf e}\\e \\dfrac{\\gamma_{\\mb v'}}{\\gamma_{\\mb v}}\\lambda_{\\bf e} \\e\\gamma_{\\bl\\upsilon}\\plr{1\\m\\dfrac{\\upsilon\\,\\mr v}{c^2}\\vphantom{\\dfrac{\\tfrac{a}{b}}{\\tfrac{a}{b}}}}\\lambda_{\\bf e} \\tl{B-10} \\end{equation} so \\begin{equation} \\boxed{\\:\\:\\lambda'_{\\bf e}\\e\\m\\gamma_{\\bl\\upsilon}\\plr{1\\m\\dfrac{\\upsilon\\,\\mr v}{c^2}\\vphantom{\\dfrac{\\tfrac{a}{b}}{\\tfrac{a}{b}}}}\\lambda_0\\vp\\:\\:} \\tl{B-11} \\end{equation} Adding \\eqref{B-04}, \\eqref{B-11} side by side we have \\begin{equation} \\lambda'\\e\\lambda'_{\\bf i}\\p\\lambda'_{\\bf e}\\e\\gamma_{\\bl\\upsilon}\\lambda_0\\m\\gamma_{\\bl\\upsilon}\\plr{1\\m\\dfrac{\\upsilon\\,\\mr v}{c^2}\\vphantom{\\dfrac{\\tfrac{a}{b}}{\\tfrac{a}{b}}}}\\lambda_0\\e\\dfrac{\\gamma_{\\bl\\upsilon}\\upsilon\\,\\mr v}{c^2}\\lambda_0 \\tl{B-12} \\end{equation} so verifying the first of equations \\eqref{A-11}. For the electric current of the ions in frame $\\:\\mr S'\\:$ we have \\begin{equation} \\mb I'_{\\bf i}\\e \\lambda'_{\\bf i}\\plr{\\m\\bl\\upsilon}\\e \\m \\gamma_{\\bl\\upsilon}\\lambda_0\\bl\\upsilon \\tl{B-13} \\end{equation} so \\begin{equation} \\boxed{\\:\\:\\mr I'_{\\bf i}\\e\\m \\gamma_{\\bl\\upsilon}\\lambda_0\\upsilon\\vp\\:\\:} \\tl{B-14} \\end{equation} while for the electric current of the electrons in frame $\\:\\mr S'\\:$ we have \\begin{equation} \\mb I'_{\\bf e}\\e \\lambda'_{\\bf e}\\mb v'\\e \\m\\gamma_{\\bl\\upsilon}\\plr{1\\m\\dfrac{\\upsilon\\,\\mr v}{c^2}\\vphantom{\\dfrac{\\tfrac{a}{b}}{\\tfrac{a}{b}}}}\\lambda_0\\dfrac{\\mb v\\m\\bl\\upsilon}{\\plr{1\\m\\dfrac{\\upsilon\\,\\mr v}{c^2}\\vphantom{\\dfrac{\\tfrac{a}{b}}{\\tfrac{a}{b}}}}}\\e \\m\\gamma_{\\bl\\upsilon}\\lambda_0\\plr{\\mb v\\m\\bl\\upsilon} \\tl{B-15} \\end{equation} that is \\begin{equation} \\boxed{\\:\\:\\mr I'_{\\bf e}\\e \\m\\gamma_{\\bl\\upsilon}\\lambda_0\\plr{\\mr v\\m\\upsilon} \\vp\\:\\:} \\tl{B-16} \\end{equation} Adding \\eqref{B-14}, \\eqref{B-16} side by side we have \\begin{equation} \\mr I'\\e\\mr I'_{\\bf i}\\p\\mr I'_{\\bf e}\\e\\m \\gamma_{\\bl\\upsilon}\\lambda_0\\upsilon \\m\\gamma_{\\bl\\upsilon}\\lambda_0\\plr{\\mr v\\m\\upsilon}\\e \\m \\gamma_{\\bl\\upsilon}\\lambda_0\\mr v \\tl{B-17} \\end{equation} so verifying the second of equations \\eqref{A-11}. $\\hebl$ Important Note : From the expression of the linear electric charge density $\\:\\lambda'$ , see the first of equations \\eqref{A-11} \\begin{equation} \\boxed{\\:\\:\\lambda'\\e \\dfrac{\\gamma_{\\bl\\upsilon}\\upsilon\\,\\mr v}{c^2}\\lambda_0\\vp\\:\\:} \\nonumber \\end{equation} since $\\gamma_{\\bl\\upsilon}\\gr 0, \\lambda_0\\gr 0$ we have \\begin{equation} \\begin{split} \\lambda' & \\gr 0 \\quad \\texttt{if } \\upsilon\\,\\mr v\\gr 0\\\\ \\lambda' & \\les 0 \\quad \\texttt{if } \\upsilon\\,\\mr v\\les 0\\\\ \\end{split} \\tl{B-18} \\end{equation} that is : the neutral in $\\:\\mr S\\:$ wire appears in the moving system $\\:\\mr S'\\:$ positively charged if the velocities $\\:\\bl\\upsilon, \\mb v\\:$ point to the same direction of the $\\:z\\m$ axis and negatively charged if these velocities point to opposite directions of the $\\:z\\m$ axis.",
      "question_latex": [
        "q",
        "F_B",
        "A",
        "L",
        "n",
        "v_d",
        "v_F",
        "r"
      ],
      "answer_latex": [
        "\\newcommand{\\bl}[1]{\\boldsymbol{#1}} \n\\newcommand{\\e}{\\bl=}\n\\newcommand{\\p}{\\bl+}\n\\newcommand{\\m}{\\bl-}\n\\newcommand{\\mb}[1]{\\mathbf {#1}}\n\\newcommand{\\mr}[1]{\\mathrm {#1}}\n\\newcommand{\\gr}{\\bl>}\n\\newcommand{\\les}{\\bl<}\n\\newcommand{\\greq}{\\bl\\ge}\n\\newcommand{\\leseq}{\\bl\\le}\n\\newcommand{\\plr}[1]{\\left(#1\\right)}\n\\newcommand{\\blr}[1]{\\left[#1\\right]}\n\\newcommand{\\vlr}[1]{\\vert#1\\vert}\n\\newcommand{\\Vlr}[1]{\\Vert#1\\Vert}\n\\newcommand{\\lara}[1]{\\langle#1\\rangle}\n\\newcommand{\\lav}[1]{\\langle#1|}\n\\newcommand{\\vra}[1]{|#1\\rangle}\n\\newcommand{\\lavra}[2]{\\langle#1|#2\\rangle}\n\\newcommand{\\lavvra}[3]{\\langle#1|\\,#2\\,|#3\\rangle}\n\\newcommand{\\vp}{\\vphantom{\\dfrac{a}{b}}}\n\\newcommand{\\hp}[1]{\\hphantom{#1}} \n\\newcommand{\\x}{\\bl\\times}\n\\newcommand{\\qqlraqq}{\\qquad\\bl{-\\!\\!\\!-\\!\\!\\!-\\!\\!\\!\\longrightarrow}\\qquad}\n\\newcommand{\\qqLraqq}{\\qquad\\boldsymbol{\\e\\!\\e\\!\\e\\!\\e\\!\\Longrightarrow}\\qquad}\n\\newcommand{\\tl}[1]{\\tag{#1}\\label{#1}}\n\\newcommand{\\hebl}{\\bl{=\\!=\\!=\\!==\\!=\\!=\\!==\\!=\\!=\\!==\\!=\\!=\\!==\\!=\\!=\\!==\\!=\\!=\\!==\\!=\\!=\\!==\\!=\\!=\\!==\\!=\\!=\\!==\\!=\\!=\\!==\\!=\\!=\\!==\\!=\\!=\\!=}}",
        "''",
        "\\hebl",
        "\\:q\\:",
        "\\:\\nu_0",
        "\\:\\nu\\:",
        "\\:\\nu'_0",
        "\\:E'_r",
        "\\:qE'_r",
        "\\:q",
        "\\:qE'_r/\\gamma",
        "\\m \\lambda_0\\nu_0",
        "(\\mathbf E\\e\\bl0)",
        "\\:\\lambda_0\\:",
        "\\:\\m\\lambda_0\\:",
        "\\:\\nu_0\\e v_d",
        "\\:\\mathbf B\\neq\\bl0\\:",
        "\\:\\mathbf u_q\\x\\mathbf B\\e\\bl0\\:",
        "q\\:(\\mathbf u_q\\e\\bl0)",
        "\\:\\mathbf E'\\neq\\bl0",
        "\\:q\\mathbf E'\\:",
        "\\:q\\plr{\\mathbf u'_q\\x\\mathbf B'}",
        "\\:\\mathbf E\\e\\bl0\\:",
        "\\:\\bl\\upsilon",
        "\\:\\mathbf u'_q\\:",
        "\\:\\mathbf u\\:",
        "\\:\\bl\\upsilon\\:",
        "\\:\\mathbf f\\e\\bl 0\\:",
        "\\:\\rm S\\:",
        "\\:\\mathbf f'\\e\\bl 0\\:",
        "\\:\\rm S'",
        "F_B",
        "\\,",
        "\\:\\plr{\\mb E,\\mb B}\\:",
        "\\:\\mr S",
        "\\:\\plr{\\mb E',\\mb B'}\\:",
        "\\:\\mr S'\\:",
        "\\:\\mr S'",
        "\\:\\lambda_{\\bf i}\\e\\lambda_0\\gr 0\\:",
        "\\:\\mb I_{\\bf i}\\e\\mr I_{\\bf i}\\mb e_{\\bl z}\\e\\bl0\\:",
        "\\:\\lambda_{\\bf e}\\e\\m\\lambda_0\\:",
        "\\:\\mb v\\e \\mr v \\,\\mb e_{\\bl z} , \\mr v \\bl\\in \\plr{\\m c,\\p c}",
        "\\:\\mb I_{\\bf e}\\e\\mr I_{\\bf e}\\mb e_{\\bl z}\\e\\lambda_{\\bf e}\\,\\mb v\\e \\m\\lambda_0\\mr v \\,\\mb e_{\\bl z}",
        "\\:\\lambda\\e\\lambda_{\\bf i}\\p\\lambda_{\\bf e}\\e0\\:",
        "\\:\\mb I\\e \\mb I_{\\bf i}\\p\\mb I_{\\bf e}\\e \\m\\lambda_0\\mr v \\,\\mb e_{\\bl z}",
        "\\:\\lambda\\:",
        "\\:\\mb I\\e \\mr I\\,\\mb e_{\\bl z}",
        "\\:\\mr I\\:",
        "\\:\\mb I\\:",
        "\\:z\\m",
        "\\:\\rho,\\phi,z\\:",
        "\\:\\bl\\rho\\e \\rho\\,\\mb e_{\\bl\\rho}\\e \\plr{\\rho\\cos\\phi,\\rho\\sin\\phi,0}",
        "\\:\\mb E\\:",
        "\\:\\rho\\:",
        "\\:\\mr L\\:",
        "\\:\\mb B\\:",
        "\\:\\lambda\\e0\\:",
        "\\:\\mb I\\e \\m\\lambda_0\\mr v \\,\\mb e_{\\bl z}",
        "\\:\\mr S\\:",
        "\\:\\bl\\upsilon\\e \\upsilon\\,\\mb e_{\\bl z}\\:",
        "\\:\\mb E\\e\\bl 0\\:",
        "\\:\\plr{\\mb B\\bl{\\cdot\\upsilon }}\\e 0",
        "\\:z'\\m",
        "\\:\\lambda'\\:",
        "\\:\\mb E'\\:",
        "\\:\\mr I'\\:",
        "\\:\\mb B'",
        "\\:\\lambda', \\mr I'\\:",
        "\\:\\plr{\\m\\bl\\upsilon}\\:",
        "\\:\\mr L_{\\bf i}\\:",
        "\\:\\mr L'_{\\bf i}\\:",
        "\\:\\lambda'_{\\bf e}\\:",
        "\\:\\mb v'\\:",
        "\\:\\mb v\\:",
        "\\:\\m\\bl\\upsilon",
        "\\:\\gamma\\m",
        "\\:\\bl\\upsilon, \\mb v, \\mb v'\\:",
        "\\:\\mr L^0_{\\bf e}\\:",
        "\\:\\mr L_{\\bf e},\\mr L'_{\\bf e}\\:",
        "\\:\\mr S,\\mr S'\\:",
        "\\:\\mr L_{\\bf e}\\:",
        "\\:\\mr L'_{\\bf e}\\:",
        "\\:\\lambda'",
        "\\gamma_{\\bl\\upsilon}\\gr 0, \\lambda_0\\gr 0",
        "\\:\\bl\\upsilon, \\mb v\\:"
      ],
      "created": "2022-02-16T21:14:40.327",
      "golden_ner_terms": [
        "addition",
        "area",
        "average",
        "axis",
        "center",
        "charge",
        "circular",
        "collinear",
        "configuration",
        "constant",
        "contraction",
        "coordinates",
        "cross-section",
        "current",
        "cylinder",
        "cylindrical coordinates",
        "density",
        "derivation",
        "dilation",
        "distance",
        "distribution",
        "electric current",
        "electric fields",
        "electricity",
        "electrons",
        "equation",
        "expression",
        "field",
        "fixed",
        "frame",
        "gauss",
        "gauss law",
        "general solution",
        "height",
        "inertial frames",
        "infinite",
        "integration",
        "ions",
        "length",
        "length contraction",
        "magnetic fields",
        "maxwell equations",
        "moving frame",
        "negative",
        "net",
        "number",
        "opposite",
        "order",
        "parallel",
        "point",
        "positive",
        "product",
        "proportional",
        "radius",
        "real",
        "real number",
        "relation",
        "relative motion",
        "right",
        "segment",
        "side",
        "similar",
        "solution",
        "speed",
        "stationary",
        "straight",
        "surface",
        "time",
        "time dilation",
        "transformation",
        "vector",
        "velocity",
        "volume",
        "zero"
      ],
      "golden_ner_count": 74,
      "golden_patterns": [
        {
          "pattern": "monotone-approximation",
          "score": 4.0,
          "hotwords": [
            "approximate",
            "limit"
          ]
        },
        {
          "pattern": "check-the-extreme-cases",
          "score": 2.0,
          "hotwords": [
            "zero"
          ]
        },
        {
          "pattern": "exploit-symmetry",
          "score": 2.0,
          "hotwords": [
            "invariant"
          ]
        },
        {
          "pattern": "encode-as-algebra",
          "score": 2.0,
          "hotwords": [
            "ring"
          ]
        }
      ],
      "golden_pattern_names": [
        "monotone-approximation",
        "check-the-extreme-cases",
        "exploit-symmetry",
        "encode-as-algebra"
      ],
      "golden_scopes": [
        {
          "type": "consider",
          "match": "Consider now a straight segment "
        },
        {
          "type": "set-notation",
          "match": "$\\:\\mb v\\e \\mr v \\,\\mb e_{\\bl z} , \\mr v \\bl\\in \\plr{\\m c,\\p c}$"
        }
      ],
      "golden_scope_count": 2
    },
    {
      "id": "se-physics-484817",
      "stratum": "medium",
      "title": "How to understand the makeup of neutral pi and eta mesons?",
      "tags": [
        "particle-physics",
        "standard-model",
        "superposition",
        "quarks",
        "mesons"
      ],
      "score": 5,
      "answer_score": 8,
      "question_body": "I know that mesons are bosons made up of quark-antiquark pairs. But when I see the list of mesons , I can see that the makeup of neutral pions and eta mesons are noted in a strange way. $$\\pi^0=(u\\bar{u}-d\\bar{d})/\\sqrt{2}$$ $$\\eta^0=(u\\bar{u}+d\\bar{d}-2s\\bar{s})/\\sqrt{6}$$ How am I supposed to understand their compositions? Interpretation 1: a neutral pion should be understand as a quantum superposition and is actually composed of 2 pairs, sometimes appearing as an up pair, some other times as a down pair. Interpretation 2: a neutral pion can be an up pair or a down pair. Both compositions lead to mesons with the exact same characteristics and behaviours. What is the meaning of those square roots? If it's too complicated to be explained within a few lines, can anyone recommend me a website or a book?",
      "answer_body": "Note that in the 3-dimensional complex space spanned by basis $\\boldsymbol{\\lbrace}\\boldsymbol{u}\\overline{\\boldsymbol{u}},\\boldsymbol{d}\\overline{\\boldsymbol{d}},\\boldsymbol{s}\\overline{\\boldsymbol{s}}\\boldsymbol{\\rbrace}$ , this basis is replaced by $\\boldsymbol{\\lbrace}\\boldsymbol{\\pi^{0},\\boldsymbol{\\eta},\\boldsymbol{\\eta}^{\\prime}}\\boldsymbol{\\rbrace}$ through a special unitary transformation $\\mathrm{V}\\in SU(3)$ , \\begin{equation} \\begin{bmatrix} \\boldsymbol{\\pi^{0}} \\vphantom{\\dfrac{a}{\\tfrac{a}{b}}}\\\\ \\boldsymbol{\\eta} \\vphantom{\\dfrac{a}{\\tfrac{a}{b}}}\\\\ \\boldsymbol{\\eta}^{\\prime} \\vphantom{\\dfrac{a}{\\tfrac{a}{b}}} \\end{bmatrix} \\boldsymbol{=} \\begin{bmatrix} \\sqrt{\\tfrac{1}{2}} & \\boldsymbol{-} \\sqrt{\\tfrac{1}{2}} & \\hphantom{\\boldsymbol{-}}0 \\vphantom{\\tfrac{a}{\\tfrac{a}{b}}}\\\\ \\sqrt{\\tfrac{1}{6}} & \\hphantom{\\boldsymbol{-}}\\sqrt{\\tfrac{1}{6}} & \\boldsymbol{-}\\sqrt{\\tfrac{2}{3}} \\vphantom{\\tfrac{a}{\\tfrac{a}{b}}}\\\\ \\sqrt{\\tfrac{1}{3}} & \\hphantom{\\boldsymbol{-}}\\sqrt{\\tfrac{1}{3}} & \\hphantom{\\boldsymbol{-}}\\sqrt{\\tfrac{1}{3}} \\vphantom{\\tfrac{a}{\\tfrac{a}{b}}} \\end{bmatrix} \\begin{bmatrix} \\boldsymbol{u}\\overline{\\boldsymbol{u}} \\vphantom{\\dfrac{a}{\\tfrac{a}{b}}}\\\\ \\boldsymbol{d}\\overline{\\boldsymbol{d}} \\vphantom{\\dfrac{a}{\\tfrac{a}{b}}}\\\\ \\boldsymbol{s}\\overline{\\boldsymbol{s}} \\vphantom{\\dfrac{a}{\\tfrac{a}{b}}} \\end{bmatrix} =\\mathrm{V} \\begin{bmatrix} \\boldsymbol{u}\\overline{\\boldsymbol{u}} \\vphantom{\\dfrac{a}{\\tfrac{a}{b}}}\\\\ \\boldsymbol{d}\\overline{\\boldsymbol{d}} \\vphantom{\\dfrac{a}{\\tfrac{a}{b}}}\\\\ \\boldsymbol{s}\\overline{\\boldsymbol{s}} \\vphantom{\\dfrac{a}{\\tfrac{a}{b}}} \\end{bmatrix} \\tag{1}\\label{1} \\end{equation} see Figure. $ \\newcommand{\\FR}[2]{{\\textstyle \\frac{#1}{#2}}} \\newcommand{\\BK}[3]{\\left|{#1},{#2}\\right\\rangle_{#3}} \\newcommand{\\BoldExp}[2]{{#1}^{\\boldsymbol{#2}}} \\newcommand{\\BoldSub}[2]{{#1}_{\\boldsymbol{#2}}} \\newcommand{\\MM}[4] {\\begin{bmatrix} #1 & #2\\\\ #3 & #4\\\\ \\end{bmatrix}} \\newcommand{\\MMM}[9] {\\textstyle \\begin{bmatrix} #1 & #2 & #3 \\\\ #4 & #5 & #6 \\\\ #7 & #8 & #9 \\\\ \\end{bmatrix}} \\newcommand{\\CMRR}[2] {\\begin{bmatrix} #1 \\\\ #2 \\end{bmatrix}} \\newcommand{\\CMRRR}[3] {\\begin{bmatrix} #2 \\\\ #3 \\end{bmatrix}} \\newcommand{\\CMRRRR}[4] {\\begin{bmatrix} #1 \\\\ #2 \\\\ #3 \\\\ #4 \\end{bmatrix}} \\newcommand{\\RMCC}[2] {\\begin{bmatrix} #1 & #2 \\end{bmatrix}} \\newcommand{\\RMCCC}[3] {\\begin{bmatrix} #1 & #2 & #3 \\end{bmatrix}} \\newcommand{\\RMCCCC}[4] {\\begin{bmatrix} #1 & #2 & #3 & #4 \\end{bmatrix}} $ $\\boldsymbol{\\S\\:}\\textbf{A. Mesons from three quarks}$ $\\boldsymbol{u},\\boldsymbol{d},\\boldsymbol{s} : \\boldsymbol{3}\\boldsymbol{\\otimes}\\overline{\\boldsymbol{3}}\\boldsymbol{=}\\boldsymbol{1}\\boldsymbol{\\oplus}\\boldsymbol{8}$ Suppose we know the existence of three quarks only : $\\boldsymbol{u}$ , $\\boldsymbol{d}$ and $\\boldsymbol{s}$ . Under full symmetry these are the basic states, let \\begin{equation} \\boldsymbol{u}= \\begin{bmatrix} 1\\\\ 0\\\\ 0 \\end{bmatrix} \\qquad \\boldsymbol{d}= \\begin{bmatrix} 0\\\\ 1\\\\ 0 \\end{bmatrix} \\qquad \\boldsymbol{s}= \\begin{bmatrix} 0\\\\ 0\\\\ 1 \\end{bmatrix} \\tag{001}\\label{001} \\end{equation} of a 3-dimensional complex Hilbert space of quarks, say $\\mathbf{Q}\\equiv \\mathbb{C}^{\\boldsymbol{3}}$ . A quark $\\boldsymbol{\\xi} \\in \\mathbf{Q}$ is expressed in terms of these basic states as \\begin{equation} \\boldsymbol{\\xi}=\\xi_u\\boldsymbol{u}+\\xi_d\\boldsymbol{d}+\\xi_s\\boldsymbol{s}= \\begin{bmatrix} \\xi_u\\\\ \\xi_d\\\\ \\xi_s \\end{bmatrix} \\qquad \\xi_u,\\xi_d,\\xi_s \\in \\mathbb{C} \\tag{002}\\label{002} \\end{equation} For a quark $\\boldsymbol{\\zeta} \\in \\mathbf{Q}$ \\begin{equation} \\boldsymbol{\\zeta}=\\zeta_u\\boldsymbol{u}+\\zeta_d\\boldsymbol{d}+\\zeta_s\\boldsymbol{s}= \\begin{bmatrix} \\zeta_u\\\\ \\zeta_d\\\\ \\zeta_s \\end{bmatrix} \\tag{003}\\label{003} \\end{equation} the respective antiquark $\\overline{\\boldsymbol{\\zeta}}$ is expressed by the complex conjugates of the coordinates \\begin{equation} \\overline{\\boldsymbol{\\zeta}}=\\overline{\\zeta}_u \\overline{\\boldsymbol{u}}+\\overline{\\zeta}_d\\overline{\\boldsymbol{d}}+\\overline{\\zeta}_s\\overline{\\boldsymbol{s}}= \\begin{bmatrix} \\overline{\\zeta}_u\\\\ \\overline{\\zeta}_d\\\\ \\overline{\\zeta}_s \\end{bmatrix} \\tag{004}\\label{004} \\end{equation} with respect to the basic states \\begin{equation} \\overline{\\boldsymbol{u}}= \\begin{bmatrix} 1\\\\ 0\\\\ 0 \\end{bmatrix} \\qquad \\overline{\\boldsymbol{d}}= \\begin{bmatrix} 0\\\\ 1\\\\ 0 \\end{bmatrix} \\qquad \\overline{\\boldsymbol{s}}= \\begin{bmatrix} 0\\\\ 0\\\\ 1 \\end{bmatrix} \\tag{005}\\label{005} \\end{equation} the antiquarks of $\\boldsymbol{u},\\boldsymbol{d}$ and $\\boldsymbol{s}$ respectively. The antiquarks belong to a different space, the space of antiquarks $\\overline{\\mathbf{Q}}\\equiv \\mathbb{C}^{\\boldsymbol{3}}$ . Since a meson is a quark-antiquark pair, we'll try to find the product space \\begin{equation} \\mathbf{M}=\\mathbf{Q}\\boldsymbol{\\otimes}\\overline{\\mathbf{Q}}\\: \\left(\\equiv \\mathbb{C}^{\\boldsymbol{9}}\\right) \\tag{006}\\label{006} \\end{equation} Using the expressions \\eqref{002} and \\eqref{004} of the quark $\\boldsymbol{\\xi} \\in \\mathbf{Q}$ and the antiquark $\\overline{\\boldsymbol{\\zeta}} \\in \\overline{\\mathbf{Q}}$ respectively, we have for the product meson state $ \\mathrm{X} \\in \\mathbf{M}$ \\begin{equation} \\begin{split} \\mathrm{X}=\\boldsymbol{\\xi}\\boldsymbol{\\otimes}\\overline{\\boldsymbol{\\zeta}}=&\\xi_u\\overline{\\zeta}_u \\left(\\boldsymbol{u}\\boldsymbol{\\otimes}\\overline{\\boldsymbol{u}}\\right)+\\xi_u\\overline{\\zeta}_d \\left( \\boldsymbol{u}\\boldsymbol{\\otimes}\\overline{\\boldsymbol{d}}\\right)+\\xi_u\\overline{\\zeta}_s \\left(\\boldsymbol{u}\\boldsymbol{\\otimes}\\overline{\\boldsymbol{s}}\\right)+ \\\\ &\\xi_d\\overline{\\zeta}_u \\left(\\boldsymbol{d}\\boldsymbol{\\otimes}\\overline{\\boldsymbol{u}}\\right)+\\xi_d\\overline{\\zeta}_d \\left( \\boldsymbol{d}\\boldsymbol{\\otimes}\\overline{\\boldsymbol{d}}\\right)+\\xi_d\\overline{\\zeta}_s \\left(\\boldsymbol{d}\\boldsymbol{\\otimes}\\overline{\\boldsymbol{s}}\\right)+\\\\ &\\xi_s\\overline{\\zeta}_u \\left(\\boldsymbol{s}\\boldsymbol{\\otimes}\\overline{\\boldsymbol{u}}\\right)+\\xi_s\\overline{\\zeta}_d \\left( \\boldsymbol{s}\\boldsymbol{\\otimes}\\overline{\\boldsymbol{d}}\\right)+\\xi_s\\overline{\\zeta}_s \\left(\\boldsymbol{s}\\boldsymbol{\\otimes}\\overline{\\boldsymbol{s}}\\right) \\end{split} \\tag{007}\\label{007} \\end{equation} In order to simplify the expressions, the product symbol $\"\\boldsymbol{\\otimes}\"$ is omitted and so \\begin{equation} \\begin{split} \\mathrm{X}=\\boldsymbol{\\xi}\\overline{\\boldsymbol{\\zeta}}=&\\xi_u\\overline{\\zeta}_u \\boldsymbol{u}\\overline{\\boldsymbol{u}}+\\xi_u\\overline{\\zeta}_d \\boldsymbol{u}\\overline{\\boldsymbol{d}}+\\xi_u\\overline{\\zeta}_s \\boldsymbol{u}\\overline{\\boldsymbol{s}}+ \\\\ &\\xi_d\\overline{\\zeta}_u \\boldsymbol{d}\\overline{\\boldsymbol{u}}+\\xi_d\\overline{\\zeta}_d \\boldsymbol{d}\\overline{\\boldsymbol{d}}+\\xi_d\\overline{\\zeta}_s \\boldsymbol{d}\\overline{\\boldsymbol{s}}+\\\\ &\\xi_s\\overline{\\zeta}_u \\boldsymbol{s}\\overline{\\boldsymbol{u}}+\\xi_s\\overline{\\zeta}_d \\boldsymbol{s}\\overline{\\boldsymbol{d}}+\\xi_s\\overline{\\zeta}_s \\boldsymbol{s}\\overline{\\boldsymbol{s}} \\end{split} \\tag{008}\\label{008} \\end{equation} Due to the fact that $\\mathbf{Q}$ and $\\overline{\\mathbf{Q}}$ are of the same dimension, it's convenient to represent the meson states in the product 9-dimensional complex space $\\:\\mathbf{M}=\\mathbf{Q}\\boldsymbol{\\otimes}\\overline{\\mathbf{Q}}\\:$ by square $3 \\times 3$ matrices instead of row or column vectors \\begin{equation} \\mathrm{X}=\\boldsymbol{\\xi}\\overline{\\boldsymbol{\\zeta}}= \\begin{bmatrix} \\xi_u\\overline{\\zeta}_u & \\xi_u\\overline{\\zeta}_d & \\xi_u\\overline{\\zeta}_s\\\\ \\xi_d\\overline{\\zeta}_u & \\xi_d\\overline{\\zeta}_d & \\xi_d\\overline{\\zeta}_s\\\\ \\xi_s\\overline{\\zeta}_u & \\xi_s\\overline{\\zeta}_d & \\xi_s\\overline{\\zeta}_s \\end{bmatrix}= \\begin{bmatrix} \\xi_u\\vphantom{\\overline{\\zeta}_u}\\\\ \\xi_d\\vphantom{\\overline{\\zeta}_u}\\\\ \\xi_s\\vphantom{\\overline{\\zeta}_u} \\end{bmatrix} \\begin{bmatrix} \\overline{\\zeta}_u \\\\ \\overline{\\zeta}_d \\\\ \\overline{\\zeta}_s \\end{bmatrix}^{\\mathsf{T}} = \\begin{bmatrix} \\xi_u\\vphantom{\\overline{\\zeta}_u}\\\\ \\xi_d\\vphantom{\\overline{\\zeta}_u}\\\\ \\xi_s\\vphantom{\\overline{\\zeta}_u} \\end{bmatrix} \\begin{bmatrix} \\overline{\\zeta}_u & \\overline{\\zeta}_d & \\overline{\\zeta}_s \\end{bmatrix} \\tag{009}\\label{009} \\end{equation} The product space $\\:\\mathbf{M}=\\mathbf{Q}\\boldsymbol{\\otimes}\\overline{\\mathbf{Q}}\\:$ is created by completion of the set of states \\eqref{008} with arbitrary complex coefficients \\begin{equation} \\begin{split} \\mathrm{X}=&\\mathrm{x}_{_{11}}\\boldsymbol{u}\\overline{\\boldsymbol{u}}+\\mathrm{x}_{_{12}} \\boldsymbol{u}\\overline{\\boldsymbol{d}}+\\mathrm{x}_{_{13}} \\boldsymbol{u}\\overline{\\boldsymbol{s}}+ \\\\ &\\mathrm{x}_{_{21}}\\boldsymbol{d}\\overline{\\boldsymbol{u}}+\\mathrm{x}_{_{22}} \\boldsymbol{d}\\overline{\\boldsymbol{d}}+\\mathrm{x}_{_{23}} \\boldsymbol{d}\\overline{\\boldsymbol{s}}+ \\qquad \\mathrm{x}_{_{ij}} \\in \\mathbb{C}\\\\ &\\mathrm{x}_{_{31}} \\boldsymbol{s}\\overline{\\boldsymbol{u}}+\\mathrm{x}_{_{32}} \\boldsymbol{s}\\overline{\\boldsymbol{d}}+\\mathrm{x}_{_{33}} \\boldsymbol{s}\\overline{\\boldsymbol{s}} \\end{split} \\tag{010}\\label{010} \\end{equation} that is \\begin{equation} \\mathrm{X}= \\begin{bmatrix} \\mathrm{x}_{_{11}} & \\mathrm{x}_{_{12}} & \\mathrm{x}_{_{13}}\\\\ \\mathrm{x}_{_{21}} & \\mathrm{x}_{_{22}} & \\mathrm{x}_{_{23}}\\\\ \\mathrm{x}_{_{31}} & \\mathrm{x}_{_{32}} & \\mathrm{x}_{_{33}} \\end{bmatrix} \\:, \\qquad \\mathrm{x}_{_{ij}} \\in \\mathbb{C} \\tag{011}\\label{011} \\end{equation} So $\\:\\mathbf{M}=\\mathbf{Q}\\boldsymbol{\\otimes}\\overline{\\mathbf{Q}}\\:$ is identical to $\\mathbb{C}^{\\boldsymbol{9}}$ with base states \\begin{align} &\\boldsymbol{u}\\overline{\\boldsymbol{u}}= \\begin{bmatrix} 1 & 0 & 0\\\\ 0 & 0 & 0\\\\ 0 & 0 & 0 \\end{bmatrix} \\quad \\boldsymbol{u}\\overline{\\boldsymbol{d}}= \\begin{bmatrix} 0 & 1 & 0\\\\ 0 & 0 & 0\\\\ 0 & 0 & 0 \\end{bmatrix} \\quad \\boldsymbol{u}\\overline{\\boldsymbol{s}}= \\begin{bmatrix} 0 & 0 & 1\\\\ 0 & 0 & 0\\\\ 0 & 0 & 0 \\end{bmatrix} \\tag{012a}\\label{012a}\\\\ &\\boldsymbol{d}\\overline{\\boldsymbol{u}}= \\begin{bmatrix} 0 & 0 & 0\\\\ 1 & 0 & 0\\\\ 0 & 0 & 0 \\end{bmatrix} \\quad \\boldsymbol{d}\\overline{\\boldsymbol{d}}= \\begin{bmatrix} 0 & 0 & 0\\\\ 0 & 1 & 0\\\\ 0 & 0 & 0 \\end{bmatrix} \\quad \\:\\boldsymbol{d}\\overline{\\boldsymbol{s}}= \\begin{bmatrix} 0 & 0 & 0\\\\ 0 & 0 & 1\\\\ 0 & 0 & 0 \\end{bmatrix} \\tag{012b}\\label{012b}\\\\ &\\boldsymbol{s}\\overline{\\boldsymbol{u}}= \\begin{bmatrix} 0 & 0 & 0\\\\ 0 & 0 & 0\\\\ 1 & 0 & 0 \\end{bmatrix} \\quad \\:\\boldsymbol{s}\\overline{\\boldsymbol{d}}= \\begin{bmatrix} 0 & 0 & 0\\\\ 0 & 0 & 0\\\\ 0 & 1 & 0 \\end{bmatrix} \\quad \\:\\boldsymbol{s}\\overline{\\boldsymbol{s}}= \\begin{bmatrix} 0 & 0 & 0\\\\ 0 & 0 & 0\\\\ 0 & 0 & 1 \\end{bmatrix} \\tag{012c}\\label{012c} \\end{align} This basis is represented symbolically by a $3\\times 3$ array \\begin{equation} \\mathcal{F}_{\\mathbf{M}}= \\begin{bmatrix} \\boldsymbol{u}\\overline{\\boldsymbol{u}} & \\boldsymbol{u}\\overline{\\boldsymbol{d}} & \\boldsymbol{u}\\overline{\\boldsymbol{s}}\\\\ \\boldsymbol{d}\\overline{\\boldsymbol{u}} & \\boldsymbol{d}\\overline{\\boldsymbol{d}} & \\boldsymbol{d}\\overline{\\boldsymbol{s}}\\\\ \\boldsymbol{s}\\overline{\\boldsymbol{u}} & \\boldsymbol{s}\\overline{\\boldsymbol{d}} & \\boldsymbol{s}\\overline{\\boldsymbol{s}} \\end{bmatrix} \\tag{013}\\label{013} \\end{equation} In this Hilbert space the usual inner product between states \\begin{equation} \\mathrm{X}= \\begin{bmatrix} \\mathrm{x}_{_{11}} & \\mathrm{x}_{_{12}} & \\mathrm{x}_{_{13}}\\\\ \\mathrm{x}_{_{21}} & \\mathrm{x}_{_{22}} & \\mathrm{x}_{_{23}}\\\\ \\mathrm{x}_{_{31}} & \\mathrm{x}_{_{32}} & \\mathrm{x}_{_{33}} \\end{bmatrix} \\:, \\qquad \\mathrm{Y}= \\begin{bmatrix} \\mathrm{y}_{_{11}} & \\mathrm{y}_{_{12}} & \\mathrm{y}_{_{13}}\\\\ \\mathrm{y}_{_{21}} & \\mathrm{y}_{_{22}} & \\mathrm{y}_{_{23}}\\\\ \\mathrm{y}_{_{31}} & \\mathrm{y}_{_{32}} & \\mathrm{y}_{_{33}} \\end{bmatrix} \\tag{014}\\label{014} \\end{equation} is \\begin{equation} \\begin{split} \\langle \\mathrm{X},\\mathrm{Y}\\rangle \\equiv &\\mathrm{x}_{_{11}}\\overline{\\mathrm{y}}_{_{11}}+\\mathrm{x}_{_{12}}\\overline{\\mathrm{y}}_{_{12}}+\\mathrm{x}_{_{13}}\\overline{\\mathrm{y}}_{_{13}}+\\\\ &\\mathrm{x}_{_{21}}\\overline{\\mathrm{y}}_{_{21}}+\\mathrm{x}_{_{22}}\\overline{\\mathrm{y}}_{_{22}}+\\mathrm{x}_{_{23}}\\overline{\\mathrm{y}}_{_{23}}+\\\\ &\\mathrm{x}_{_{31}}\\overline{\\mathrm{y}}_{_{31}}+\\mathrm{x}_{_{32}}\\overline{\\mathrm{y}}_{_{32}}+\\mathrm{x}_{_{33}}\\overline{\\mathrm{y}}_{_{33}} \\end{split} \\tag{015}\\label{015} \\end{equation} which, using the $3\\times 3$ matrix representation of states, is the trace of the matrix product $\\mathrm{X}\\BoldExp{\\mathrm{Y}}{*}$ \\begin{equation} \\langle \\mathrm{X},\\mathrm{Y}\\rangle =\\mathrm{Tr}\\left[\\mathrm{X}\\BoldExp{\\mathrm{Y}}{*}\\right] \\tag{016}\\label{016} \\end{equation} given that $\\BoldExp{\\mathrm{Y}}{*}$ is the complex conjugate of the transpose of $\\mathrm{Y}$ \\begin{equation} \\BoldExp{\\mathrm{Y}}{*}\\equiv \\BoldExp{ \\begin{bmatrix} \\mathrm{y}_{_{11}} & \\mathrm{y}_{_{12}} & \\mathrm{y}_{_{13}}\\\\ \\mathrm{y}_{_{21}} & \\mathrm{y}_{_{22}} & \\mathrm{y}_{_{23}}\\\\ \\mathrm{y}_{_{31}} & \\mathrm{y}_{_{32}} & \\mathrm{y}_{_{33}} \\end{bmatrix}} {*} = \\overline{\\begin{bmatrix} \\mathrm{y}_{_{11}} & \\mathrm{y}_{_{12}} & \\mathrm{y}_{_{13}}\\\\ \\mathrm{y}_{_{21}} & \\mathrm{y}_{_{22}} & \\mathrm{y}_{_{23}}\\\\ \\mathrm{y}_{_{31}} & \\mathrm{y}_{_{32}} & \\mathrm{y}_{_{33}} \\end{bmatrix}^{\\mathsf{T}}} = \\begin{bmatrix} \\overline{\\mathrm{y}}_{_{11}} & \\overline{\\mathrm{y}}_{_{21}} & \\overline{\\mathrm{y}}_{_{31}}\\\\ \\overline{\\mathrm{y}}_{_{12}} & \\overline{\\mathrm{y}}_{_{22}} & \\overline{\\mathrm{y}}_{_{32}}\\\\ \\overline{\\mathrm{y}}_{_{13}} & \\overline{\\mathrm{y}}_{_{23}} & \\overline{\\mathrm{y}}_{_{33}} \\end{bmatrix} \\tag{017}\\label{017} \\end{equation} Now, under a unitary transformation $\\;W \\in SU(3)\\;$ in the 3-dimensional space of quarks $\\;\\mathbf{Q}\\;$ , we have \\begin{equation} \\BoldExp{\\boldsymbol{\\xi}}{'} = W\\boldsymbol{\\xi} \\tag{018}\\label{018} \\end{equation} so in the space of antiquarks $\\overline{\\mathbf{Q}}\\;$ , since $\\;\\BoldExp{\\boldsymbol{\\zeta}}{'}=W \\boldsymbol{\\zeta}\\;$ \\begin{equation} \\overline{\\BoldExp{\\boldsymbol{\\zeta}}{'}}= \\overline{W}\\;\\overline{\\boldsymbol{\\zeta}} \\tag{019}\\label{019} \\end{equation} and for the meson state \\begin{align} \\BoldExp{\\mathrm{X}}{'} & =\\BoldExp{\\boldsymbol{\\xi}}{'}\\boldsymbol{\\otimes}\\overline{\\BoldExp{\\boldsymbol{\\zeta}}{'}}=\\left(W\\boldsymbol{\\xi}\\vphantom{\\overline{W}\\overline{\\boldsymbol{\\zeta}} }\\right)\\left(\\overline{W}\\overline{\\boldsymbol{\\zeta}} \\right) = \\Biggl(W\\begin{bmatrix} \\xi_u\\vphantom{\\overline{\\zeta}_u}\\\\ \\xi_d\\vphantom{\\overline{\\zeta}_u}\\\\ \\xi_s\\vphantom{\\overline{\\zeta}_u} \\end{bmatrix}\\Biggr) \\Biggl(\\overline{W}\\begin{bmatrix} \\overline{\\zeta}_u\\\\ \\overline{\\zeta}_d\\\\ \\overline{\\zeta}_s \\end{bmatrix}\\Biggr)^{\\mathsf{T}} \\nonumber\\\\ & = W\\Biggl(\\begin{bmatrix} \\xi_u\\vphantom{\\overline{\\zeta}_u}\\\\ \\xi_d\\vphantom{\\overline{\\zeta}_u}\\\\ \\xi_s\\vphantom{\\overline{\\zeta}_u} \\end{bmatrix} \\begin{bmatrix} \\overline{\\zeta}_u & \\overline{\\zeta}_d & \\overline{\\zeta}_s \\end{bmatrix}\\Biggr)\\overline{W}^{\\mathsf{T}} =W\\left(\\boldsymbol{\\xi}\\boldsymbol{\\otimes}\\overline{\\boldsymbol{\\zeta}}\\right)\\BoldExp{W}{*}=W\\;\\mathrm{X}\\;\\BoldExp{W}{*} \\nonumber \\tag{020}\\label{020} \\end{align} that is \\begin{equation} \\BoldExp{\\mathrm{X}}{'} = W\\;\\mathrm{X}\\;\\BoldExp{W}{*} \\tag{021}\\label{021} \\end{equation} Above equation \\eqref{021} is the transformation law of meson states in the 9-dimensional space $\\;\\mathbf{M}=\\mathbf{Q}\\boldsymbol{\\otimes}\\overline{\\mathbf{Q}}\\;$ induced by a unitary transformation $\\;W \\in SU(3)\\;$ in the 3-dimensional space of quarks $\\mathbf{Q}$ . Under this transformation law the inner product of two meson states is invariant because its relation with the trace, equation \\eqref{016}, yields \\begin{equation} \\langle \\BoldExp{\\mathrm{X}}{'},\\BoldExp{\\mathrm{Y}}{'}\\rangle =\\mathrm{Tr}\\left[\\BoldExp{\\mathrm{X}}{'}\\BoldExp{\\BoldExp{\\mathrm{Y}}{'}}{*}\\right]=\\mathrm{Tr}\\Bigl[\\left(W\\mathrm{X}\\BoldExp{W}{*}\\right) \\BoldExp{\\left(W\\mathrm{Y}\\BoldExp{W}{*}\\right)}{*}\\Bigr]=\\mathrm{Tr}\\Bigl[W \\left( \\mathrm{X}\\BoldExp{Y}{*}\\right)\\BoldExp{W}{*}\\Bigr]=\\mathrm{Tr}\\Bigl[\\mathrm{X}\\BoldExp{Y}{*}\\Bigr] \\tag{022}\\label{022} \\end{equation} The last equality in above equation \\eqref{022} is valid since under the transformation law \\eqref{021} the trace remains invariant. More generally, for unitary $\\;W \\in SU(n)\\;$ and $\\;A\\;$ a $\\;n \\times n\\;$ complex matrix the transformation \\begin{equation} \\BoldExp{\\mathrm{A}}{'} = W\\;\\mathrm{A}\\;\\BoldExp{W}{*} \\tag{023}\\label{023} \\end{equation} if expressed in terms of elements, yields (we use the Einstein summation convention) \\begin{equation} \\BoldExp{a_{ij}}{'} = w_{i\\rho}a_{\\rho\\sigma}\\BoldExp{w_{\\sigma j}}{*} \\tag{024}\\label{0242} \\end{equation} so \\begin{equation} \\mathrm{Tr}\\Bigl[\\BoldExp{\\mathrm{A}}{'}\\Bigr]=\\BoldExp{a_{ii}}{'} = w_{i\\rho}a_{\\rho\\sigma}\\BoldExp{w_{\\sigma i}}{*}=(\\BoldExp{w_{\\sigma i}}{*}w_{i\\rho})a_{\\rho\\sigma}=\\delta_{\\sigma\\rho}a_{\\rho\\sigma}=a_{\\rho\\rho}=\\mathrm{Tr}\\Bigl[A\\Bigr] \\tag{025}\\label{025} \\end{equation} proving the invariance of inner product under the transformation law \\eqref{021} \\begin{equation} \\langle \\BoldExp{\\mathrm{X}}{'},\\BoldExp{\\mathrm{Y}}{'}\\rangle =\\langle \\mathrm{X},\\mathrm{Y}\\rangle \\tag{026}\\label{026} \\end{equation} Now, obviously the meson state represented by the identity matrix \\begin{equation} \\mathrm{I}= \\begin{bmatrix} 1 & 0 & 0\\\\ 0 & 1 & 0\\\\ 0 & 0 & 1 \\end{bmatrix} \\tag{027}\\label{027} \\end{equation} is unchanged under the transformation \\eqref{021} and if normalized yields \\begin{equation} \\BoldSub{\\mathrm{F}}{0}=\\sqrt{\\tfrac{1}{3}} \\begin{bmatrix} 1 & 0 & 0\\\\ 0 & 1 & 0\\\\ 0 & 0 & 1 \\end{bmatrix} =\\sqrt{\\tfrac{1}{3}}\\left(\\boldsymbol{u}\\overline{\\boldsymbol{u}}+\\boldsymbol{d}\\overline{\\boldsymbol{d}}+\\boldsymbol{s}\\overline{\\boldsymbol{s}} \\right)\\equiv \\BoldExp{\\boldsymbol{\\eta}}{\\prime} \\tag{028}\\label{028} \\end{equation} that is, it represents the $\\;\\BoldExp{\\boldsymbol{\\eta}}{\\prime}\\;$ meson. The 1-dimensional subspace $\\;\\boldsymbol{\\lbrace}\\BoldSub{\\mathrm{F}}{0}\\boldsymbol{\\rbrace}\\;$ spanned by this state is invariant. Note that $\\;\\BoldExp{\\boldsymbol{\\eta}}{\\prime}=\\sqrt{3}\\cdot \\mathrm{Tr}\\left[\\mathcal{F}_{\\mathbf{M}}\\right]$ . Any meson state orthogonal to this space, $\\mathrm{X}\\perp\\boldsymbol{\\lbrace}\\BoldSub{\\mathrm{F}}{0}\\boldsymbol{\\rbrace} $ , remains orthogonal under the transformation. But \\begin{equation} \\mathrm{X}\\perp \\boldsymbol{\\lbrace}\\BoldSub{\\mathrm{F}}{0}\\boldsymbol{\\rbrace}\\Leftrightarrow\\langle \\mathrm{X},\\BoldSub{\\mathrm{F}}{0}\\rangle =0\\Leftrightarrow\\mathrm{Tr}\\left[\\mathrm{X}\\BoldSub{\\mathrm{F}}{0}^{\\boldsymbol{*}}\\right]=0\\Leftrightarrow\\mathrm{Tr}\\left[\\mathrm{X}\\right]=0 \\tag{029}\\label{029} \\end{equation} So, the 8-dimensional linear subspace of all meson states with traceless matrix representation is the orthogonal complement of the 1-dimensional subspace $\\;\\boldsymbol{\\lbrace}\\BoldSub{\\mathrm{F}}{0}\\boldsymbol{\\rbrace}\\;$ and if $\\;\\boldsymbol{\\lbrace}\\BoldSub{\\mathrm{F}}{1},\\BoldSub{\\mathrm{F}}{2},\\cdots,\\BoldSub{\\mathrm{F}}{8}\\boldsymbol{\\rbrace}\\;$ is any basis which spans this space then \\begin{equation} \\boldsymbol{\\lbrace}\\BoldSub{\\mathrm{F}}{1},\\BoldSub{\\mathrm{F}}{2},\\cdots,\\BoldSub{\\mathrm{F}}{8}\\boldsymbol{\\rbrace}=\\boldsymbol{\\lbrace}\\BoldSub{\\mathrm{F}}{0}\\boldsymbol{\\rbrace}^{\\boldsymbol{\\perp}}=\\Bigl\\{ \\mathrm{X} \\in \\mathbf{Q}\\boldsymbol{\\otimes}\\overline{\\mathbf{Q}}\\; :\\; \\mathrm{Tr}\\left[X\\right]=0 \\; \\Bigr\\} \\tag{030}\\label{030} \\end{equation} This space is invariant under the transformation \\eqref{021}. There are arbitrary many choices of the basis $\\;\\left(\\BoldSub{\\mathrm{F}}{1},\\BoldSub{\\mathrm{F}}{2},\\cdots,\\BoldSub{\\mathrm{F}}{8}\\right)\\;$ but a proper one must correspond to mesons in the real world and be orthonormal if possible. So, the normalized traceless meson state \\begin{equation} \\BoldSub{\\mathrm{F}}{3}=\\sqrt{\\tfrac{1}{2}} \\begin{bmatrix} 1 & \\hphantom{\\boldsymbol{-}}0 & \\hphantom{\\boldsymbol{-}}0\\\\ 0 & \\boldsymbol{-}1 & \\hphantom{\\boldsymbol{-}}0\\\\ 0 & \\hphantom{\\boldsymbol{-}} 0 & \\hphantom{\\boldsymbol{-}}0 \\end{bmatrix} =\\sqrt{\\tfrac{1}{2}}\\left(\\boldsymbol{u}\\overline{\\boldsymbol{u}}-\\boldsymbol{d}\\overline{\\boldsymbol{d}} \\right)\\equiv \\BoldExp{\\boldsymbol{\\pi}}{0} \\tag{031}\\label{031} \\end{equation} represents of course the $\\;\\BoldExp{\\boldsymbol{\\pi}}{0}\\;$ meson (pion). The basis $\\mathcal{F}_{\\mathbf{M}}$ may be expressed symbolically as sum of a diagonal and a traceless component \\begin{equation} \\begin{split} &\\mathcal{F}_{\\mathbf{M}}=\\Bigl(\\tfrac{1}{3}\\mathrm{Tr}\\left[\\mathcal{F}_{\\mathbf{M}}\\right]\\Bigr)\\mathcal{I}+\\Bigl[\\mathcal{F}_{\\mathbf{M}}-\\Bigl(\\tfrac{1}{3}\\mathrm{Tr}\\left[\\mathcal{F}_{\\mathbf{M}}\\right]\\Bigr)\\mathcal{I}\\Bigr]\\\\ &=\\begin{bmatrix} \\dfrac{\\BoldExp{\\boldsymbol{\\eta}}{\\prime}}{\\sqrt{3}} & \\mathbf{0} & \\mathbf{0}\\\\ \\mathbf{0} & \\dfrac{\\BoldExp{\\boldsymbol{\\eta}}{\\prime}}{\\sqrt{3}} & \\mathbf{0}\\\\ \\mathbf{0} & \\mathbf{0} & \\dfrac{\\BoldExp{\\boldsymbol{\\eta}}{\\prime}}{\\sqrt{3}} \\end{bmatrix} + \\begin{bmatrix} \\dfrac{\\left(2\\boldsymbol{u}\\overline{\\boldsymbol{u}}-\\boldsymbol{d}\\overline{\\boldsymbol{d}}-\\boldsymbol{s}\\overline{\\boldsymbol{s}}\\right) }{3}{\\rule[0ex]{-10pt}{0ex}} & \\boldsymbol{u}\\overline{\\boldsymbol{d}} & \\boldsymbol{u}\\overline{\\boldsymbol{s}}\\\\ \\boldsymbol{d}\\overline{\\boldsymbol{u}} & \\dfrac{\\left(-\\boldsymbol{u}\\overline{\\boldsymbol{u}}+2\\boldsymbol{d}\\overline{\\boldsymbol{d}}-\\boldsymbol{s}\\overline{\\boldsymbol{s}}\\right) }{3} & \\boldsymbol{d}\\overline{\\boldsymbol{s}} \\\\ \\boldsymbol{s}\\overline{\\boldsymbol{u}} & \\boldsymbol{s}\\overline{\\boldsymbol{d}} & {\\rule[-2ex]{-10pt}{6ex}} \\dfrac{\\left(-\\boldsymbol{u}\\overline{\\boldsymbol{u}}-\\boldsymbol{d}\\overline{\\boldsymbol{d}}+2\\boldsymbol{s}\\overline{\\boldsymbol{s}}\\right)}{3} \\end{bmatrix} \\end{split} \\tag{032}\\label{032} \\end{equation} The 3rd diagonal element of the traceless component of $\\mathcal{F}_{\\mathbf{M}}$ , if opposed and normalized, yields \\begin{equation} \\BoldSub{\\mathrm{F}}{8}=\\sqrt{\\tfrac{1}{6}} \\begin{bmatrix} 1 & \\hphantom{\\boldsymbol{-}}0 & \\hphantom{\\boldsymbol{-}}0\\\\ 0 & \\hphantom{\\boldsymbol{-}}1 & \\hphantom{\\boldsymbol{-}}0\\\\ 0 & \\hphantom{\\boldsymbol{-}}0 & \\boldsymbol{-}2 \\end{bmatrix} =\\sqrt{\\tfrac{1}{6}}\\left(\\boldsymbol{u}\\overline{\\boldsymbol{u}}+\\boldsymbol{d}\\overline{\\boldsymbol{d}}-2\\boldsymbol{s}\\overline{\\boldsymbol{s}} \\right)\\equiv \\boldsymbol{\\eta} \\tag{033}\\label{033} \\end{equation} that is, it represents the $\\;\\boldsymbol{\\eta}\\;$ meson. (to be continued in $\\boldsymbol{\\S\\:}\\textbf{B}$ )",
      "question_latex": [
        "\\pi^0=(u\\bar{u}-d\\bar{d})/\\sqrt{2}",
        "\\eta^0=(u\\bar{u}+d\\bar{d}-2s\\bar{s})/\\sqrt{6}",
        "</span></p>\n\n<p><span class=\"math-container\">"
      ],
      "answer_latex": [
        "\\boldsymbol{\\lbrace}\\boldsymbol{u}\\overline{\\boldsymbol{u}},\\boldsymbol{d}\\overline{\\boldsymbol{d}},\\boldsymbol{s}\\overline{\\boldsymbol{s}}\\boldsymbol{\\rbrace}",
        "\\boldsymbol{\\lbrace}\\boldsymbol{\\pi^{0},\\boldsymbol{\\eta},\\boldsymbol{\\eta}^{\\prime}}\\boldsymbol{\\rbrace}",
        "\\mathrm{V}\\in SU(3)",
        "\\newcommand{\\FR}[2]{{\\textstyle \\frac{#1}{#2}}}\n\\newcommand{\\BK}[3]{\\left|{#1},{#2}\\right\\rangle_{#3}} \n\\newcommand{\\BoldExp}[2]{{#1}^{\\boldsymbol{#2}}}                                                    \n\\newcommand{\\BoldSub}[2]{{#1}_{\\boldsymbol{#2}}}\n\\newcommand{\\MM}[4]\n    {\\begin{bmatrix}                                   \n          #1 & #2\\\\                                    \n          #3 & #4\\\\\n     \\end{bmatrix}}\n\\newcommand{\\MMM}[9]  \n     {\\textstyle \\begin{bmatrix}                       \n          #1 & #2 & #3 \\\\\n          #4 & #5 & #6 \\\\\n          #7 & #8 & #9 \\\\\n     \\end{bmatrix}}\n\\newcommand{\\CMRR}[2] \n    {\\begin{bmatrix}                                     \n          #1 \\\\                                       \n          #2           \n     \\end{bmatrix}}\n\\newcommand{\\CMRRR}[3]                      \n    {\\begin{bmatrix}  \n          #2 \\\\ \n          #3 \n     \\end{bmatrix}}\n\\newcommand{\\CMRRRR}[4]\n    {\\begin{bmatrix}    \n          #1 \\\\                                        \n          #2 \\\\\n          #3 \\\\\n          #4\n     \\end{bmatrix}}\n\\newcommand{\\RMCC}[2]                                \n    {\\begin{bmatrix}                                 \n          #1  &  #2                                                    \n     \\end{bmatrix}} \n\\newcommand{\\RMCCC}[3]  \n    {\\begin{bmatrix}                                                             \n          #1  &  #2  &  #3                                   \n     \\end{bmatrix}}\n\\newcommand{\\RMCCCC}[4]                                \n    {\\begin{bmatrix}                                                                 \n          #1  &  #2  &  #3  &  #4                                         \n     \\end{bmatrix}}",
        "\\boldsymbol{\\S\\:}\\textbf{A. Mesons from three quarks}",
        "\\boldsymbol{u},\\boldsymbol{d},\\boldsymbol{s} : \\boldsymbol{3}\\boldsymbol{\\otimes}\\overline{\\boldsymbol{3}}\\boldsymbol{=}\\boldsymbol{1}\\boldsymbol{\\oplus}\\boldsymbol{8}",
        "\\boldsymbol{u}",
        "\\boldsymbol{d}",
        "\\boldsymbol{s}",
        "\\mathbf{Q}\\equiv \\mathbb{C}^{\\boldsymbol{3}}",
        "\\boldsymbol{\\xi} \\in \\mathbf{Q}",
        "\\boldsymbol{\\zeta} \\in \\mathbf{Q}",
        "\\overline{\\boldsymbol{\\zeta}}",
        "\\boldsymbol{u},\\boldsymbol{d}",
        "\\overline{\\mathbf{Q}}\\equiv \\mathbb{C}^{\\boldsymbol{3}}",
        "\\overline{\\boldsymbol{\\zeta}} \\in \\overline{\\mathbf{Q}}",
        "\\mathrm{X} \\in \\mathbf{M}",
        "\"\\boldsymbol{\\otimes}\"",
        "\\mathbf{Q}",
        "\\overline{\\mathbf{Q}}",
        "\\:\\mathbf{M}=\\mathbf{Q}\\boldsymbol{\\otimes}\\overline{\\mathbf{Q}}\\:",
        "3 \\times 3",
        "\\mathbb{C}^{\\boldsymbol{9}}",
        "3\\times 3",
        "\\mathrm{X}\\BoldExp{\\mathrm{Y}}{*}",
        "\\BoldExp{\\mathrm{Y}}{*}",
        "\\mathrm{Y}",
        "\\;W \\in SU(3)\\;",
        "\\;\\mathbf{Q}\\;",
        "\\overline{\\mathbf{Q}}\\;",
        "\\;\\BoldExp{\\boldsymbol{\\zeta}}{'}=W \\boldsymbol{\\zeta}\\;",
        "\\;\\mathbf{M}=\\mathbf{Q}\\boldsymbol{\\otimes}\\overline{\\mathbf{Q}}\\;",
        "\\;W \\in SU(n)\\;",
        "\\;A\\;",
        "\\;n \\times n\\;",
        "\\;\\BoldExp{\\boldsymbol{\\eta}}{\\prime}\\;",
        "\\;\\boldsymbol{\\lbrace}\\BoldSub{\\mathrm{F}}{0}\\boldsymbol{\\rbrace}\\;",
        "\\;\\BoldExp{\\boldsymbol{\\eta}}{\\prime}=\\sqrt{3}\\cdot \\mathrm{Tr}\\left[\\mathcal{F}_{\\mathbf{M}}\\right]",
        "\\mathrm{X}\\perp\\boldsymbol{\\lbrace}\\BoldSub{\\mathrm{F}}{0}\\boldsymbol{\\rbrace}",
        "\\;\\boldsymbol{\\lbrace}\\BoldSub{\\mathrm{F}}{1},\\BoldSub{\\mathrm{F}}{2},\\cdots,\\BoldSub{\\mathrm{F}}{8}\\boldsymbol{\\rbrace}\\;",
        "\\;\\left(\\BoldSub{\\mathrm{F}}{1},\\BoldSub{\\mathrm{F}}{2},\\cdots,\\BoldSub{\\mathrm{F}}{8}\\right)\\;",
        "\\;\\BoldExp{\\boldsymbol{\\pi}}{0}\\;",
        "\\mathcal{F}_{\\mathbf{M}}",
        "\\;\\boldsymbol{\\eta}\\;",
        "\\boldsymbol{\\S\\:}\\textbf{B}"
      ],
      "created": "2019-06-07T15:46:05.157",
      "golden_ner_terms": [
        "base",
        "basic",
        "basis",
        "bosons",
        "column",
        "column vector",
        "complement",
        "completion",
        "complex",
        "complex conjugate",
        "component",
        "conjugate",
        "coordinates",
        "diagonal",
        "dimension",
        "einstein summation convention",
        "elements",
        "equality",
        "equation",
        "hilbert space",
        "identity",
        "identity matrix",
        "induced",
        "inner",
        "inner product",
        "interpretation",
        "invariant",
        "linear subspace",
        "matrix",
        "matrix product",
        "matrix representation",
        "mesons",
        "order",
        "orthogonal",
        "orthogonal complement",
        "orthonormal",
        "pions",
        "product",
        "quarks",
        "real",
        "relation",
        "representation",
        "represents",
        "row",
        "space",
        "spanned by",
        "square",
        "square root",
        "state",
        "subspace",
        "sum",
        "summation",
        "superposition",
        "symmetry",
        "trace",
        "transformation",
        "transpose",
        "unitary",
        "unitary transformation",
        "valid",
        "vectors",
        "way"
      ],
      "golden_ner_count": 62,
      "golden_patterns": [
        {
          "pattern": "exploit-symmetry",
          "score": 4.0,
          "hotwords": [
            "symmetry",
            "invariant"
          ]
        },
        {
          "pattern": "dualise-the-problem",
          "score": 4.0,
          "hotwords": [
            "complement",
            "transpose"
          ]
        },
        {
          "pattern": "the-diagonal-argument",
          "score": 2.0,
          "hotwords": [
            "diagonal"
          ]
        }
      ],
      "golden_pattern_names": [
        "exploit-symmetry",
        "dualise-the-problem",
        "the-diagonal-argument"
      ],
      "golden_scopes": [
        {
          "type": "set-notation",
          "match": "$\\mathrm{V}\\in SU(3)$"
        },
        {
          "type": "set-notation",
          "match": "$\\boldsymbol{\\xi} \\in \\mathbf{Q}$"
        },
        {
          "type": "set-notation",
          "match": "$\\boldsymbol{\\zeta} \\in \\mathbf{Q}$"
        },
        {
          "type": "set-notation",
          "match": "$\\boldsymbol{\\xi} \\in \\mathbf{Q}$"
        },
        {
          "type": "set-notation",
          "match": "$\\overline{\\boldsymbol{\\zeta}} \\in \\overline{\\mathbf{Q}}$"
        },
        {
          "type": "set-notation",
          "match": "$ \\mathrm{X} \\in \\mathbf{M}$"
        },
        {
          "type": "set-notation",
          "match": "$ is created by completion of the set of states \\eqref{008} with arbitrary compl"
        },
        {
          "type": "set-notation",
          "match": "$\\;W \\in SU(3)\\;$"
        },
        {
          "type": "set-notation",
          "match": "$\\;W \\in SU(3)\\;$"
        },
        {
          "type": "set-notation",
          "match": "$\\;W \\in SU(n)\\;$"
        },
        {
          "type": "set-notation",
          "match": "$ is any basis which spans this space then \\begin{equation} \\boldsymbol{\\lbrace}"
        }
      ],
      "golden_scope_count": 11
    },
    {
      "id": "se-physics-191222",
      "stratum": "medium",
      "title": "Observation of gauge in artificial magnetic fields",
      "tags": [
        "atomic-physics",
        "gauge-theory",
        "synthetic-gauge-fields"
      ],
      "score": 5,
      "answer_score": 4,
      "question_body": "In the ultracold atom community, an \"artificial gauge field\" or \"artificial magnetic field\" is a spatially varying hopping phase somehow engineered into the system, so that atoms hopping around an optical lattice gain a non-integrable phase factor in (seemingly) precise analogy with a gauge field. However, it would appear that there is a major caveat to this comparison. In a recent paper , it is claimed: A common belief is that all observables are gauge-independent. However, gauge-dependent observations can be made in time-of-flight images of ultracold atoms when the momentum distribution of the wavefunction is observed. The sudden switch-off of all laser beams transforms canonical momentum, which is gauge dependent, into mechanical momentum, which is readily observed [29]. And, indeed, they observe a change in lattice symmetry that reflects the non-translation-invariant gauge choice, rather than the translation-invariant artificial magnetic field that it creates. Clearly, when taken at face value this is a major difference from a true gauge field. So my questions are then: Is the above really a reasonable description of what happens in these artificial gauge field experiments, or does it just somehow signal a breakdown of the analogy between these systems and real gauge fields? Whatever the difference is between artificial and real gauge fields, does this imply any effects of real gauge fields that would not be observable in these artificial gauge experiments? Or do these artificial gauge fields somehow improve on real gauge fields, in that they reproduce all the phenomena of true gauge fields and on top of that have the additional property of being directly measurable?",
      "answer_body": "Disclaimer: I do particle physics / cosmology, so this is definitely outside my field, apply grains of salt to this answer appropriately. I think Reference [29] (Lin et al, arxiv reference: 1008.4864) honestly does a better job of explaining what is going on (which makes sense, the impression I get is that 1008.4864 is a foundational paper in this subfield). The gist, as I understand it from Lin et al, is that the hamiltonian for the system they are interested in--neutral atoms in a BEC state coupled to two intersecting lasers--can be written like: \\begin{equation} H_{BEC} = (\\vec{p} - \\vec{p}_{min})^2, \\end{equation} where $\\vec{p}_{min}$ is a quantity that can be controlled by the experimenters (adjusting the lasers). Lin et al note that this is formally similar to the hamiltonian for a charged particle moving in a background electromagnetic field \\begin{equation} H_{charged} = (\\vec{p} - q \\vec{A})^2 + q \\phi, \\end{equation} where $\\vec{A}$ and $\\phi$ are the vector and scalar potentials respectively. Of course, $H_{charged}$ is gauge invariant. To implement this analogy, they introduce an analogue vector potential $\\vec{A^*}$ and scalar potential $\\phi^*$ (they don't really introduce $\\phi*$ but let's run with it for now). Then Lin et al identify \\begin{equation} \\vec{p}_{min} = q^* \\vec{A^*}, \\end{equation} and also work in a gauge where \\begin{equation} \\phi^* = 0. \\end{equation} Then in that gauge, the analogue electric field is given by $\\vec{E^*} = -\\dot{\\vec{A^*}}$. Thus, Lin et al point out that there are effects from setting up a time dependent $\\vec{A}$ (ie a time dependent $\\vec{p}_{min}$). From the perspective of the analogue gauge theory, this is due to the fact that the analogue electric field $\\vec{E}^*$ is non-zero. The analogue electric field is a gauge invariant, observable quantity. The passage you cite (from 1503.08243, Kennedy et al) suggests that the effect they measure comes from a time dependent $\\vec{A^*}$. Again this would lead to a non-zero $\\vec{E^*}$. Of course, from the perspective of the analogue gauge theory, they are free to perform a gauge transformation, and they must get the same answer because physical observables must be gauge invariant (this point is ironclad--if the rest of this answer is wrong, this one point can't be wrong unless the analogy to gauge fields completely breaks down). However, a gauge transformation will necessarily turn on $\\phi^*$. In other words, $\\phi^*$ will be nonzero in any other gauge. This requires one to change the original Hamiltonian to take this into account. What will still be true is that \\begin{equation} H_{BEC}=(\\vec{p}-\\vec{p}_{min})^2 = (\\vec{p} - q\\vec{A^*})^2 + q \\phi^* \\end{equation} so obviously in this new gauge we can no longer identify $q\\vec{A^*} = \\vec{p}_{min}$. I think this is really what Kennedy et al are getting at, this relationship between $\\vec{A^*}$ and $\\vec{p}_{min}$ is not gauge invariant. When the new hamiltonian is used correctly, the final answer will be the same in any gauge. However I think that actually showing this works would be overkill--the bottom line I think is that $\\vec{E^*}$ is non-zero, so everyone in the end is making measurements of a gauge invariant quantity. Update 7/4 So I had a chance to look at this more. In the end I just have to disagree with the quote you cited--the observables do not depend on the gauge. However the gauge they choose is particularly nice, and finding a manifestly gauge invariant formulation of what they are doing might not be worth it. The bottom line is that once you gauge fix, every combination of operators you write down is gauge invariant (since there is no gauge freedom left), and therefore there is guaranteed to be a gauge invariant combination of operators that reduces to the combination you wrote down in the gauge you picked. In other words, one completely valid way to describe a gauge invariant quantity is to say what it looks like in a well defined gauge. What I think is going on is that the observable Kennedy et al are measuring (column density in momentum space) is very natural in one gauge fixed version of the problem, but finding the manifestly gauge invariant version would be unnecessarily complicated. More details: Powell et al (1009.1389) is a really good paper that discusses the theoretical aspects of what is going on in setups like the one used in Kennedy et al. The underlying formalism you need is gauge theory on a lattice. The basic idea is that there are fermion fields living on a given lattice site that create particles at that lattice site. In Kennedy et al these are referred to as $a_{m,n}$ (where $m,n$ are indices on a 2D lattice). There are also link fields, which are given by Wilson lines that connect the lattice site $(m,n)$ to the lattice site $(m',n')$ \\begin{equation} W_{(m,n),(m',n')} = \\exp \\left(i\\int_{(x_m,y_n)}^{(x_{m'},y_{n'})} d\\vec{x}\\cdot\\vec{A}\\right) = e^{i\\phi_{(m,n),(m',n')}} \\end{equation} The last line works because this is a $U(1)$ gauge field, so the integrals are numbers. Both the $a_{m,n}$ operators, and the phases $\\phi_{m,n}$, transform under gauge transformations. The gauge transformations occur on each site independently, so we can write the parameters of the gauge transformation as $\\lambda_{m,n}$. The $a$ operators transform as \\begin{equation} a_{m,n} \\rightarrow e^{i \\lambda_{m,n}} a_{m,n} \\end{equation} and the Wilson lines transform as \\begin{equation} W_{(m,n),(m',n')} \\rightarrow e^{i \\lambda_{m,n}} W_{(m,n),(m',n')} e^{-i \\lambda_{m',n'}} \\end{equation} or equivalently \\begin{equation} \\phi_{(m,n),(m',n')} \\rightarrow \\phi_{(m,n),(m',n')} + \\lambda_{m,n} - \\lambda_{m',n'} \\end{equation} As a particle theorist / cosmologist, I am more familiar with the above formulas in their continuum form, where I would call $a$ by the name $\\psi$, so $\\psi(x)\\rightarrow e^{i\\lambda(x)}\\psi(x)$ and $W(x,y) \\rightarrow e^{i \\lambda(x)} W(x,y) e^{-i\\lambda(y)}$. One key observation is that the \"hopping Hamiltonian\" from Kennedy et al is gauge invariant \\begin{equation} H = -t \\sum_{m,n} a_{m+1,n}^\\dagger e^{i\\phi_{(m+1,n),(m,n)}} a_{m,n} \\end{equation} which can be seen using the transformation rules above. Incidentally, my particle-y instincts are to think of the above as a discretized version of the fermionic part of the QED lagrangian, $\\bar{\\psi} i \\gamma^\\mu D_\\mu \\psi$, where $D$ is the gauge covariant derivative. The fact that the hopping Hamiltonian is gauge invariant really means that nothing physical is going to end up depending on the choice of gauge. To the extent that this Hamiltonian describes the system Kennedy et al are measuring, nothing can end up depending on a choice of gauge because the underlying hamlitonian describing all of the dynamics does not. (This could be broken, for example, if (1) the approximation of the full dynamics of the system by this gauge theory breaks down in a way that breaks gauge invariance, or (2) if the way that the experimental apparatus is coupled to the BEC breaks the gauge symmetry. I am assuming both of those don't happen--if they do that is more the fault on the experimental side than the theoretical side, ie it is a boring breaking of gauge invariance). For example, the number of particles on each site \\begin{equation} n_{m,n} = a_{m,n}^\\dagger a_{m,n} \\end{equation} The number of particles is gauge invariant (as you can check from the rules and physically has to be the case since the number of particles is observable). Both Powell et al and Kennedy et all find it convenient to work in a gauge where the phases only depend on one lattice direction, so \\begin{equation} \\phi_{(m,n),(m',n')} \\rightarrow \\phi_{m,m'} \\end{equation} This is a very nice gauge for what they want to do. In particular, the translations in the $y$ direction commute with the Wilson line operators, but translations in $x$ do not. Their basic point is that all gauge fixings will force translation invariance to be broken somehow, so full translation invariance is not a real symmetry of the system. Now the measurements in Kennedy et al, as far as I can tell, are really done in momentum space (from a theoretical point of view momentum space is nice for this problem because this diagonalizes the Hamiltonian). The momentum space operators are \\begin{equation} \\tilde{a}_{p,q} = \\sum_{m,n} e^{i 2\\pi (p m+qn) / N} a_{m,n} \\end{equation} where $N$ is the number of lattice sites. Things now get complicated because the momentum space operators don't have obviously nice transformation properties under gauge transformations (the gauge transformation of the $\\tilde{a}_{p,q}$ will end up being some convolution of the gauge parameters with the real space operators $a_{m,n}$). This is related to the fact that the commutator of the Hamiltonian with the translation operator will be complicated in a general gauge. So, what I think is going on is that Kennedy et al construct a column density in momentum space, which I am guessing amounts to the probability which you can compute in a given state by $\\langle \\tilde{a}^\\dagger_{p,q}\\tilde{a}_{p,q} \\rangle$, where the $\\tilde{a}_{p,q}$ are defined in the gauge that they describe. One frustrating thing is that I am not 100% sure what specific combination corresponds to the plots they make, so I can't be more explicit about what I'm saying, but conceptually it doesn't matter what precise combination of $\\tilde{a}$'s they are plotting. This does not make the observable gauge dependent, however it does mean that showing the gauge invariance is tricky. There is guaranteed to be some gauge invariant combination of Wilson lines and fermion operators that reduces to the combination that Kennedy et al plot, in the gauge that they pick. One avenue to discover the precise gauge invaraint combination is by guessing--if you find one gauge invariant combination that reduces to their observable, that is the correct one. Another more systematic approach is to take their observable, written in the gauge that they chose, and perform an arbitrary gauge transformation. The result will likely be messy (since the momentum space operators don't have nice transformation properties), but you are guaranteed to be able to write the result in terms of manifestly gauge invariant objects if you do everything correctly (you will probably have to add in gauge transformed combinations of operators that were zero in the original gauge, and the net goal is to cancel out all of the dependence on the gauge parameter). In other words, once you gauge fix, you can write down any arbitrarily complicated combination you like of the operators you have and you are guaranteed to be talking about gauge invariant quantities, since there's no gauge freedom left. However, finding the manifestly gauge invariant form can be hard. In the problem that Kennedy et al are considering, there is such a natural choice of gauge that I think they basically want to argue that there's no point in finding the gauge invariant form of what they are measuring--the main pragmatic reason to find a gauge invariant form would be if different groups were using different gauges and needed to compare their answers. The gauge invariant form could be interesting theoretically to get more insight into the system. Based on what Powell says in section II, I think the gauge invariant formulation involves studying the properties of the projective symmetry group of the system. But that would definitely be beyond the scope of an experimental paper.",
      "question_latex": [],
      "answer_latex": [
        "\\vec{p}_{min}",
        "\\vec{A}",
        "\\phi",
        "H_{charged}",
        "\\vec{A^*}",
        "\\phi^*",
        "\\phi*",
        "\\vec{E^*} = -\\dot{\\vec{A^*}}",
        "\\vec{E}^*",
        "\\vec{E^*}",
        "q\\vec{A^*} = \\vec{p}_{min}",
        "a_{m,n}",
        "m,n",
        "(m,n)",
        "(m',n')",
        "U(1)",
        "\\phi_{m,n}",
        "\\lambda_{m,n}",
        "a",
        "\\psi",
        "\\psi(x)\\rightarrow e^{i\\lambda(x)}\\psi(x)",
        "W(x,y) \\rightarrow e^{i \\lambda(x)} W(x,y) e^{-i\\lambda(y)}",
        "\\bar{\\psi} i \\gamma^\\mu D_\\mu \\psi",
        "D",
        "y",
        "x",
        "N",
        "\\tilde{a}_{p,q}",
        "\\langle \\tilde{a}^\\dagger_{p,q}\\tilde{a}_{p,q} \\rangle",
        "\\tilde{a}"
      ],
      "created": "2015-06-25T05:11:45.413",
      "golden_ner_terms": [
        "analogy",
        "approximation",
        "atom",
        "atoms",
        "basic",
        "bottom",
        "canonical",
        "choose",
        "column",
        "combination",
        "combinations",
        "commutator",
        "construct",
        "continuum",
        "convolution",
        "cosmology",
        "covariant derivative",
        "density",
        "derivative",
        "difference",
        "distribution",
        "extent",
        "face",
        "factor",
        "field",
        "fix",
        "fixed",
        "formalism",
        "gauge",
        "gauge invariance",
        "gauge symmetry",
        "gauge theory",
        "group",
        "hamiltonian",
        "indices",
        "invariant",
        "invariant form",
        "key",
        "lagrangian",
        "laser",
        "lattice",
        "line",
        "link",
        "matter",
        "mean",
        "measurable",
        "measure",
        "measurements",
        "momentum",
        "net",
        "number",
        "numbers",
        "observables",
        "observation",
        "operator",
        "operators",
        "parameter",
        "particle physics",
        "physics",
        "point",
        "potential",
        "probability",
        "property",
        "qed",
        "real",
        "scalar",
        "scalar potential",
        "scope",
        "section",
        "side",
        "similar",
        "site",
        "space",
        "state",
        "subfield",
        "symmetry",
        "theory",
        "time",
        "top",
        "transform",
        "transformation",
        "transformations",
        "translation",
        "valid",
        "vector",
        "vector potential",
        "wavefunction",
        "way",
        "well defined",
        "work",
        "zero"
      ],
      "golden_ner_count": 91,
      "golden_patterns": [
        {
          "pattern": "exploit-symmetry",
          "score": 4.0,
          "hotwords": [
            "symmetry",
            "invariant"
          ]
        },
        {
          "pattern": "construct-an-explicit-witness",
          "score": 4.0,
          "hotwords": [
            "construct",
            "explicit"
          ]
        },
        {
          "pattern": "optimise-a-free-parameter",
          "score": 4.0,
          "hotwords": [
            "choose",
            "pick"
          ]
        },
        {
          "pattern": "transport-across-isomorphism",
          "score": 4.0,
          "hotwords": [
            "equivalent",
            "identify"
          ]
        },
        {
          "pattern": "work-examples-first",
          "score": 2.0,
          "hotwords": [
            "example"
          ]
        },
        {
          "pattern": "find-the-right-abstraction",
          "score": 2.0,
          "hotwords": [
            "perspective"
          ]
        },
        {
          "pattern": "check-the-extreme-cases",
          "score": 2.0,
          "hotwords": [
            "zero"
          ]
        },
        {
          "pattern": "local-to-global",
          "score": 2.0,
          "hotwords": [
            "cover"
          ]
        },
        {
          "pattern": "the-diagonal-argument",
          "score": 2.0,
          "hotwords": [
            "diagonal"
          ]
        },
        {
          "pattern": "encode-as-algebra",
          "score": 2.0,
          "hotwords": [
            "ring"
          ]
        },
        {
          "pattern": "use-probabilistic-method",
          "score": 2.0,
          "hotwords": [
            "probability"
          ]
        },
        {
          "pattern": "unfold-the-definition",
          "score": 2.0,
          "hotwords": [
            "means that"
          ]
        },
        {
          "pattern": "construct-auxiliary-object",
          "score": 2.0,
          "hotwords": [
            "introduce"
          ]
        }
      ],
      "golden_pattern_names": [
        "exploit-symmetry",
        "construct-an-explicit-witness",
        "optimise-a-free-parameter",
        "transport-across-isomorphism",
        "work-examples-first",
        "find-the-right-abstraction",
        "check-the-extreme-cases",
        "local-to-global",
        "the-diagonal-argument",
        "encode-as-algebra",
        "use-probabilistic-method",
        "unfold-the-definition",
        "construct-auxiliary-object"
      ],
      "golden_scopes": [
        {
          "type": "where-binding",
          "match": "where $\\vec{p}_{min}$ is"
        },
        {
          "type": "where-binding",
          "match": "where $D$ is"
        },
        {
          "type": "where-binding",
          "match": "where $N$ is"
        }
      ],
      "golden_scope_count": 3
    },
    {
      "id": "se-physics-237249",
      "stratum": "medium",
      "title": "Did merging Black Holes in GW150914 give up entropy and information to the gravitational waves, since they lost 3 solar masses?",
      "tags": [
        "general-relativity",
        "black-holes",
        "gravitational-waves",
        "event-horizon",
        "information"
      ],
      "score": 14,
      "answer_score": 8,
      "question_body": "Since the final Black Hole (BH) had 3 solar masses less of mass than the original binary BH, it seems the 2 BHs lost mass, and with it event surface area, entropy, and information. If that came from inside the BHs it seems that it would be a contradiction of what a BH is --'nothing escapes its event horizon'. Some may say that came from the rotational energy of the pair, but the new BH now seems to have less than the two original ones summed. So, did some of that escape the horizons?",
      "answer_body": "It might be popular to oversimplify general relativity and say things like these two black holes have masses $M_1$ and $M_2$ and velocities $\\vec v_1$ and $\\vec v_2$ and potential energy $U.$ And it can sound appealing to imagine the general relativity is just like Newtonian mechanics but with some corrections and limitations for high speeds and strong forces. But it isn't right. Firstly, in general relativity when you say an object has mass $M$ , you do not mean that if you summed up the mass $m_i$ of each part, they add up to $M.$ In fact $$M\\neq m_1+m_2+\\dots +m_N.$$ Secondly, energy and mass aren't conserved, and there isn't even always an unambiguous way to even talk about a total energy. The word general in general relativity comes from the general case when you don't have that kinds of global coordinate systems and global frames that allowed you to turn energy densities into energies. It forces the theory to become a local theory. So lets get at what it means to lose mass by looking at an example of how it could happen without black holes and without waves. Let's say you have a spherical bowling ball shaped planet and a spherical shell of matter surrounding that spherical bowling ball shaped planet. There are lots of ways spacetime can be curved, and sometimes you can label them by parameters with units of mass. So what if spacetime starts out curved like a Schwarzschild type of parameter $M$ in the region between the shell and the ball (so outside the planet but inside the shell) and is curved like a Schwarzschild type of parameter $M+80m$ in the region outside both. This is totally possible and in fact if $m>0$ this can be done with regular ordinary matter and the more of it we put there the larger we can make $m.$ Now let's say that shell has some springs on the bottom. If we let the shell fall, then when the shell starts to make contact with the planet the springs hit first. The whole time the shell falls you continue to have a solution of Schwarzschild type with a parameter of $M+80m$ in the region outside both. And this is actually amazing becasue Einstein's Equation says that it is energy, not mass that is the source of gravity and the shell is falling, falling faster and faster. But what we call a solution of Schwarzschild type with parameter $M+80m$ refers to solutions that could be generated by particles with less energy that are farther out, or by particles with more energy that in closer. So as the shell falls it changes from a low energy shell that is farther out into a higher energy shell that is closer in. But those things generate the same type, specifically the same parameter. As the springs make contact the shell slows down. But the energy of the springs goes up by an equal amount and you continue to have a solution of Schwarzschild type with a parameter of $M+80m$ in the region outside the shell. Now you can take all the springs and use them to power a fancy particle accelerator and use it to create a bunch of matter and antimatter using the energy. Save some of the energy too, don't use it all now. Matter and antimatter both have regular positive energy. And both have regular positive mass. And you continue to have a solution of Schwarzschild type with a parameter of $M+80m$ in the region outside the shell. You just have a larger number of particles because you have these additional particles you didn't have before and you have all their antiparticles which you also didn't have before. But you continue to have a solution of Schwarzschild type with a parameter of $M+80m$ in the region outside the shell. Now you use the new particles to make a shell of matter and give it some energy, enough to have escape velocity. And you use the new antiparticles to make a shell of antimatter and give it some energy, enough to have escape velocity. And each shell has positive energy and positive mass and so now you have three shells, the matter shell, the antimatter shell, the old shell, and there is the original planet too. Outside all of the shells you continue to have a solution of Schwarzschild type with a parameter of $M+80m$ in the region outside all the shells. But you have a solution of Schwarzschild type with a parameter of $M+79m$ in the region between the matter shell and the antimatter shell. And you have a solution of Schwarzschild type with a parameter of $M+78m$ in the region between the antimatter shell and the old shell. And those shells are taking off like a rocket. To someone far away everything looked like a regular parameter $M+80m$ solution with the images getting a bit smaller as the old shell collapsed and then got larger as you sent out the matter shell. But the matter shell gets thinner as it goes up and eventually like a balloon that gets so big it reaches you the shell reaches that far away person. And once it does, they don't notice very much since it is so very very thin (they were far away) but they are now inside so they see a solution of parameter $M+79m$ and they see a second shell (though it is super thin too) and when that passes they are now inside so they see a solution of parameter $M+78m.$ So they see every single particle of the old shell is still there and every single particle of the ball is still there. But the parameter value decreased from $M+80m$ to $M+78m$ and recall that $M$ and $m$ were in kilograms, so they might say something like the mass of that planetary system decreased. Even though every particle is still there and the mass of none of them changed. They are just closer together now and some of the energy they would naturally get when they got closer together is gone now. And that makes a solution with a smaller parameter, it makes $M+78m$ instead of $M+80m.$ So we learned a bit and we learned that the parameter value assigned to that planetary system wasn't just based on how many particles there were and what their individual masses was, but it depended and the arrangement and motion of the parts relative to each other as well. The same issue happens with the black holes. There was a system of both holes and the system's parameters depended not just on the parameters of each hole, but also on how far apart they were from each other and how they were moving relative to each other. As they moved, the waves are like the springs they make the holes move slower in a sense than they would without waves, and that energy in a sense is now part of the wave. And if the wave is still there it could all look the same to us. Just like that planet system looked the same until that spring energy was turned into something (the matter and antimatter shells) that could be sent to us. Those black holes are actually stars, they are collections of hydrogen and helium and electrons and neutrons and protons and such. They just act super similar to what a black hole with a certain parameter would act like. And we are influenced by those objects all from the times and places before an event horizon formed. We don't know that event horizons form. Maybe instead of crossing an event horizon, particle disappear when they reach it. We don't actually know for sure. And this is key. It's because we are always affected by things before they cross and never during or after. So you might want to say they crossed. But the things affecting you are the things from before they crossed. That's the things that matter. Do what happens is the things that make up the two stars don't speed up as much as they would without waves, and the waves travel outwards. So if far away things were very very close to Kerr type of parameter $(M+m,J+j)$ then in between the wave and the stars you might get something closer to Kerr type of parameter $(M,J).$ So us far away we keep seeing Kerr type of parameter $(M+m,J+j)$ up until that wave passed us and then we see a Kerr type of parameter $(M,J).$ Very much like planetary example I gave. Now that I've address most of the common misconceptions (though this is still pretty handwavy) we could look at your questions. Since the final Black Hole (BH) had 3 solar masses less of mass than the original binary BH, it seems the 2 BHs lost mass, and with it event surface area, entropy, and information. Lets denote the surface area of a black hole of mass $M$ by $A(M).$ Then it is basic black hole thermodynamics that $$A(M_1+M_2)>A(M_1)+A(M_2).$$ When the black holes of mass $M_1$ and $M_2$ merge into a black hole of mass $M$ and some waves you'll find that $A(M)>A(M_1)+A(M_2).$ So the surface area increases even if you had eternal black holes instead of stars that are well approximated by black holes. If this seems unintuitive, imagine a point in between the two black holes. You could imagine time as the z direction. Then the event horizons are like tubes. So there is a tube and another tube and they spiral around like a pair of paper towel rolls placed on opposite ends of record being played. But as they spread out and spiral in there us a point where the center no longer has a chance to escape. At that event a point grows into its own cylinder and expands to be a bigger radius cylinder and a bigger radius cylinder until it reaches the other event horizons. It was the other two tilting and entwining around each other like braids that made the barrier. And the existence of the barrier is what defined the event inside that can't escape. If you had some bars in a bird cage, you can escape if you are small enough. But if you twist the bars around tighter and tighter then eventually they touch. The place they touch is the barrier and like an ice cream cone with the point pointing towards the past the cone backwards defines where the new horizon forms. So a new horizon starts to form way in the center of the black holes and it jumps outwards at the speed of light and the waves escape between the bars before they touch. Since there was lots of volume between the black holes there was lots of room to create a bigger horizon out of the joint system. And besides, its normal for black holes to create space so its not hard to create more area as they whirl.",
      "question_latex": [],
      "answer_latex": [
        "M\\neq m_1+m_2+\\dots +m_N.",
        "A(M_1+M_2)>A(M_1)+A(M_2).",
        "M_1",
        "M_2",
        "\\vec v_1",
        "\\vec v_2",
        "U.",
        "M",
        "m_i",
        "M.",
        "</span></p>\n<p>Secondly, energy and mass aren't conserved, and there isn't even always an unambiguous way to even talk about a total energy. The word general in general relativity comes from the general case when you don't have that kinds of global coordinate systems and global frames that allowed you to turn energy densities into energies. It forces the theory to become a local theory.</p>\n<p>So lets get at what it means to lose mass by looking at an example of how it could happen without black holes and without waves. Let's say you have a spherical bowling ball shaped planet and a spherical shell of matter surrounding that spherical bowling ball shaped planet. There are lots of ways spacetime can be curved, and sometimes you can label them by parameters with units of mass. So what if spacetime starts out curved like a Schwarzschild type of parameter <span class=\"math-container\">",
        "</span> in the region between the shell and the ball (so outside the planet but inside the shell) and is curved like a Schwarzschild type of parameter <span class=\"math-container\">",
        "</span> in the region outside both.</p>\n<p>This is totally possible and in fact if <span class=\"math-container\">",
        "</span> this can be done with regular ordinary matter and the more of it we put there the larger we can make <span class=\"math-container\">",
        "</span></p>\n<p>Now let's say that shell has some springs on the bottom. If we let the shell fall, then when the shell starts to make contact with the planet the springs hit first. The whole time the shell falls you continue to have a solution of Schwarzschild type with a parameter of <span class=\"math-container\">",
        "</span> in the region outside both.</p>\n<p>And this is actually amazing becasue Einstein's Equation says that it is energy, not mass that is the source of gravity and the shell is falling, falling faster and faster. But what we call a solution of Schwarzschild type with parameter <span class=\"math-container\">",
        "</span> refers to solutions that could be generated by particles with less energy that are farther out, or by particles with more energy that in closer.</p>\n<p>So as the shell falls it changes from a low energy shell that is farther out into a higher energy shell that is closer in. But those things generate the same type, specifically the same parameter.</p>\n<p>As the springs make contact the shell slows down. But the energy of the springs goes up by an equal amount and you continue to have a solution of Schwarzschild type with a  parameter of <span class=\"math-container\">",
        "</span> in the region outside the shell. Now you can take all the springs and use them to power a fancy particle accelerator and use it to create a bunch of matter and antimatter using the  energy. Save some of the energy too, don't use it all now.</p>\n<p>Matter and antimatter both have regular positive energy. And both have regular positive mass. And you continue to have a solution of Schwarzschild type with a parameter of <span class=\"math-container\">",
        "</span> in the region outside the shell. You just have a larger <strong>number</strong> of particles because you have these additional particles you didn't have before and you have all their antiparticles which you also didn't have before.  But you continue to have a solution of Schwarzschild type with a parameter of <span class=\"math-container\">",
        "</span> in the region outside the shell.</p>\n<p>Now you use the new particles to make a shell of matter and give it some energy, enough to have escape velocity. And you use the new antiparticles to make a shell of antimatter and give it some energy, enough to have escape velocity. And each shell has positive energy and positive mass and so now you have three shells, the matter shell, the antimatter shell, the old shell, and there is the original planet too.</p>\n<p>Outside all of the shells you continue to have a solution of Schwarzschild type with a parameter of <span class=\"math-container\">",
        "</span> in the region outside all the shells. But you have a solution of Schwarzschild type with a parameter of <span class=\"math-container\">",
        "</span> in the region between the matter shell and the antimatter shell.  And you have a solution of Schwarzschild type with a parameter of <span class=\"math-container\">",
        "</span> in the region between the antimatter shell and the old shell. And those shells are taking off like a rocket.</p>\n<p>To someone far away everything looked like a regular parameter <span class=\"math-container\">",
        "</span> solution with the images getting a bit smaller as the old shell collapsed and then got larger as you sent out the matter shell. But the matter shell gets thinner as it goes up and eventually like a balloon that gets so big it reaches you the shell reaches that far away person. And once it does, they don't notice very much since it is so very very thin (they were far away) but they are now inside so they see a solution of parameter <span class=\"math-container\">",
        "</span> and they see a second shell (though it is super thin too) and when that passes they are now inside so they see a solution of parameter <span class=\"math-container\">",
        "</span></p>\n<p>So they see every single particle of the old shell is still there and every single particle of the ball is still there. But the parameter value decreased from <span class=\"math-container\">",
        "</span> to <span class=\"math-container\">",
        "</span> and recall that <span class=\"math-container\">",
        "</span> and <span class=\"math-container\">",
        "</span> were in kilograms, so they might say something like the mass of that planetary system decreased. Even though every particle is still there and the mass of none of them changed. They are just closer together now and some of the energy they would naturally get when they got closer together is gone now. And that <em>makes</em> a solution with a smaller parameter, it makes <span class=\"math-container\">",
        "</span> instead of <span class=\"math-container\">",
        "</span></p>\n<p>So we learned a bit and we learned that the parameter value assigned to that planetary system wasn't just based on how many particles there were and what their individual masses was, but it depended and the arrangement and motion of the parts relative to each other as well.</p>\n<p>The same issue happens with the black holes. There was a system of both holes and the system's parameters depended not just on the parameters of each hole, but also on how far apart they were from each other and how they were moving relative to each other.</p>\n<p>As they moved, the waves are like the springs they make the holes move slower in a sense than they would without waves, and that energy in a sense is now part of the wave. And if the wave is still there it could all look the same to us. Just like that planet system looked the same until that spring energy was turned into something (the matter and antimatter shells) that could be sent to us.</p>\n<p>Those black holes are actually stars, they are collections of hydrogen and helium and electrons and neutrons and protons and such. They just act super similar to what a black hole with a certain parameter would act like. And we are influenced by those objects all from the times and places before an event horizon formed.</p>\n<p>We don't know that event horizons form. Maybe instead of crossing an event horizon, particle disappear when they reach it. We don't actually know for sure. And this is key. It's because we are always affected by things before they cross and never during or after. So you might want to say they crossed. But the things affecting you are the things from before they crossed. That's the things that matter.</p>\n<p>Do what happens is the things that make up the two stars don't speed up as much as they would without waves, and the waves travel outwards. So if far away things were very very close to Kerr type of parameter <span class=\"math-container\">",
        "</span> then in between the wave and the stars you might get something closer to Kerr type of parameter <span class=\"math-container\">",
        "</span></p>\n<p>So us far away we keep seeing Kerr type of parameter <span class=\"math-container\">",
        "</span> up until that wave passed us and then we see a Kerr type of parameter <span class=\"math-container\">",
        "</span> Very much like planetary example I gave.</p>\n<p>Now that I've address most of the common misconceptions (though this is still pretty handwavy) we could look at your questions.</p>\n<p>Since the final Black Hole (BH) had 3 solar masses less of mass than the original binary BH,</p>\n<blockquote>\n<p>it seems the 2 BHs lost mass, and with it event surface area, entropy, and information.</p>\n</blockquote>\n<p>Lets denote the surface area of a black hole of mass <span class=\"math-container\">",
        "</span> by <span class=\"math-container\">",
        "</span> Then it is basic black hole thermodynamics that <span class=\"math-container\">",
        "</span></p>\n<p>When the black holes of mass <span class=\"math-container\">",
        "</span> merge into a black hole of mass <span class=\"math-container\">",
        "</span> and some waves you'll find that <span class=\"math-container\">"
      ],
      "created": "2016-02-15T05:06:04.850",
      "golden_ner_terms": [
        "antimatter",
        "area",
        "ball",
        "basic",
        "binary",
        "black hole thermodynamics",
        "black holes",
        "bottom",
        "center",
        "cone",
        "contradiction",
        "coordinate",
        "coordinate systems",
        "cross",
        "cylinder",
        "electrons",
        "energy",
        "entropy",
        "equation",
        "escape velocity",
        "even",
        "event",
        "event horizon",
        "eventually",
        "forces",
        "general relativity",
        "generate",
        "generated by",
        "gravity",
        "hydrogen",
        "ice",
        "information",
        "key",
        "label",
        "mass",
        "matter",
        "mean",
        "neutrons",
        "newtonian mechanics",
        "normal",
        "number",
        "object",
        "opposite",
        "parameter",
        "place",
        "point",
        "positive",
        "potential",
        "potential energy",
        "power",
        "protons",
        "radius",
        "region",
        "regular",
        "relativity",
        "right",
        "similar",
        "solution",
        "sound",
        "source",
        "space",
        "spacetime",
        "speed",
        "speed of light",
        "spring",
        "stars",
        "strong",
        "strong force",
        "surface",
        "surface area",
        "theory",
        "thermodynamics",
        "thin",
        "time",
        "type",
        "unambiguous",
        "velocity",
        "volume",
        "waves",
        "way",
        "word"
      ],
      "golden_ner_count": 81,
      "golden_patterns": [
        {
          "pattern": "local-to-global",
          "score": 4.0,
          "hotwords": [
            "local",
            "global"
          ]
        },
        {
          "pattern": "monotone-approximation",
          "score": 4.0,
          "hotwords": [
            "approximate",
            "limit"
          ]
        },
        {
          "pattern": "work-examples-first",
          "score": 2.0,
          "hotwords": [
            "example"
          ]
        },
        {
          "pattern": "quotient-by-irrelevance",
          "score": 2.0,
          "hotwords": [
            "up to"
          ]
        },
        {
          "pattern": "dualise-the-problem",
          "score": 2.0,
          "hotwords": [
            "dual"
          ]
        },
        {
          "pattern": "encode-as-algebra",
          "score": 2.0,
          "hotwords": [
            "ring"
          ]
        },
        {
          "pattern": "unfold-the-definition",
          "score": 2.0,
          "hotwords": [
            "recall that"
          ]
        }
      ],
      "golden_pattern_names": [
        "local-to-global",
        "monotone-approximation",
        "work-examples-first",
        "quotient-by-irrelevance",
        "dualise-the-problem",
        "encode-as-algebra",
        "unfold-the-definition"
      ],
      "golden_scopes": [],
      "golden_scope_count": 0
    },
    {
      "id": "se-physics-502851",
      "stratum": "medium",
      "title": "Are Fresnel Equations with Complex Indices and Angles always valid?",
      "tags": [
        "optics",
        "electromagnetic-radiation",
        "classical-electrodynamics",
        "refraction"
      ],
      "score": 5,
      "answer_score": 5,
      "question_body": "This question-answer pair came after i was asked the following question and realized i had to do some research of my own to answer it fully, and to be sure that the answer is „yes“, in the sense described in my answer. One fairly often sees the assertion that the Fresnel equations are unconditionally valid for linear isotropic mediums, even when the refractive indices are complex, so that they can be freely used for calculations even with metal-metal and metal-dielectric interfaces. For example, a well known „Photonics Knowledge“ site (RP Photonics) makes this claim on its „Fresnel Equations“ entry in its „Encyclopedia of Photonics“ . I have been unable to find a justification for the truth of this statement; can someone please point me to a reference, or give a concise, rigorous justification of its truth?",
      "answer_body": "First let’s define „unconditionally“. The answer is yes in the sense that, as long as the mediums concerned and (a) linear and (b) isotropic the standard derivation of the Fresnel equations works, that is, (1) Assume a plane wave solutions to Maxwell’s equations for (a) an incident wave, (b) a reflected wave on the incoming wave side of the interface and (c) a transmitted plane wave beyond the interface then (2) derive the complex ratios of the amplitudes of these three waves by imposing the continuity conditions for the electromagnetic fields across the interface. One can even relax the isotropy condition and appropriate simple (but extremely messy) generalizations of following reasoning still works perfectly for complex component dielectric tensor, or even the full blown rank 4 tensor that maps the Faraday tensor to the Displacement tensor, but the equations analogous to the wonted Fresnel equations are vastly more complicated tensor equations, albeit linear. Another way of putting this statement is that when we are dealing with linear materials with general complex-component wavevectors, the Fresnel equations always describe a valid solution of Maxwell’s equations comprising an incident plane wave, a reflected plane wave on the incoming wave side of the interface and a lone transmitted plane wave in the medium beyond the interface. Ok, let’s move onto justification. As always, if you want to know whether a certain set of equations is valid in certain circumstances, one must look at the derivation of those equations and check whether all the assumptions used to make any and all inferences hold give the said circumstances. Let us take, as a canonical derivation of the Fresnel equations, that to be found in Section 1.5.2 of Born and Wolf entitled „Fresnel Formulae“; in my (the Seventh) edition, it begins on page 40: Max Born and Emil Wolf, \"Principles of Optics: Electromagnetic Theory of Propagation, Interference and Diffraction of Light\" To make this derivation work, one needs the following steps: The spatial variation of three assumed plane wave fields (incident (I), reflected (R), transmitted) (T) can all be written in the form $\\left(\\mathbf{E},\\,\\mathbf{H}\\right) = \\left(\\mathbf{E}_0,\\,\\mathbf{H}_0\\right)\\,\\exp\\left(i\\,\\left<\\mathbf{k},\\,\\mathbf{r} \\right>\\right)$ . That is, the spatial variation of each plane is encoded by the wavevector $\\mathbf{k}$ . The components of this wavevector can naturally be complex to account for absorptive materials. So, so far, nothing in Born and Wolf derivation changes. We can either assume arbitrary directions for these three (I, R, T) fields and find that their wavevectors are forced to be coplanar in step 3 below, or we can make this assumption at the outset and find that Maxwell’s equations can be successfully solved with the further assumption in place; Given this assumptions, Maxwell’s Equations reduce to: $$\\begin{array}{lcl} \\mathbf{k} \\times \\mathbf{E}_0 &=& \\omega\\,\\mu\\,\\mathbf{H}_0\\\\ \\mathbf{k} \\times \\mathbf{H}_0 &=& -\\left(\\omega\\,\\epsilon+i\\,\\sigma\\right)\\,\\mathbf{E}_0\\\\ \\left<\\mathbf{k},\\,\\mathbf{E}_0\\right>&=& \\left<\\mathbf{k},\\,\\mathbf{H}_0\\right>=0 \\end{array}\\tag{1}$$ and this is not changed by the components of $\\mathbf{k}$ ’s being complex rather than real; To complete the derivation, one writes down the continuity conditions for the tangential electromagnetic fields and uses Gaussian elimination to find the required amplitude ratios for the incident, reflected and transmitted fields. Both $\\mathbb{R}$ and $\\mathbb{C}$ are commutative fields, thus both equally allow Gaussian elimination and so nothing about this step in the derivation is changed by a complex component $\\mathbf{k}$ . With electromagnetic waves, one must work through the derivation twice, once for each of a basis pair of polarization states. The wonted and easiest choice for the latter are linear polarization states, firstly (a) the Transverse Magnetic (TM) or Senkrecht state (s) where the magnetic field lies everywhere parallel to the interface and normal to the plane of incidence (the plane spanned by the incident wavevector and the unit normal to the interface) and (b) the Transverse Electric, or Parallel state (p), where the electric field is everywhere parallel to the interface plane and lies normal to the plane of incidence. There are a few points that need special care: Magnetic Constants Most derivations of the Fresnel formulas assume that the magnetic constant for both mediums is the same. This doesn’t have to be so for the derivation to hold, but the final equations are subtly different if there is a magnetic constant difference between the two media and indeed i can’t actually find a convenient reference for them right now. So you may need to be careful of some exotic materials and include the magnetic constant if need be; Unconjugated Dispersion Condition and Complex Wavenumber The wavevector magnitude condition is no longer a condition on the magnitude but a condition on the unconjugated inner product of $\\mathbf{k}$ with itself: $$\\left<\\mathbf{k},\\,\\mathbf{k}\\right> = \\omega\\,\\mu\\,\\left(\\omega\\,\\epsilon+i\\,\\sigma\\right)\\tag{2}$$ a condition which follows simply from the vector wave equation that results from substituting one of the curl equations in (1) into the other. I have in the past made some considerably strife for myself in making this mistake. One then defines the wavenumber of the medium in question as the complex square root $$k = \\stackrel{def}{=}\\sqrt{\\left<\\mathbf{k},\\,\\mathbf{k}\\right>}\\tag{3}$$ of the medium in question. The branch of the square root is such that the imaginary part is of consistent sign. In the convention (as used in (1)) that takes $\\exp(-i\\,\\omega\\,t)$ to be the positive frequency variation, the imaginary part of k is chosen to be positive. The index of the medium is then defined as: $$n \\stackrel{def}{=} \\frac{\\lambda\\,k}{2\\,\\pi} = \\frac{\\lambda\\,\\sqrt{\\left<\\mathbf{k},\\,\\mathbf{k}\\right>} }{2\\,\\pi}\\tag{4}$$ Snell’s Law Lastly, Snell’s law needs some care to interpret, for it implies some quite nonintuitive things. As i explain in my answer here , all three of: Snell’s law; The law of reflexion and; The principle that the wavevectors of all three plane waves must be coplanar are the direct result of equating the components of all wave vectors when the wavevectors are projected into the interface plane. The only way that the components of $\\mathbf{E},\\, \\mathbf{H}$ parallel to the interface and of $\\mathbf{D},\\,\\mathbf{B}$ normal to the plane can be continuous is if the components of the wavevector $\\mathbf{k}$ that defines the variation in the spatial variation $\\exp\\left(i\\,\\left<\\mathbf{k},\\,\\mathbf{r}\\right>\\right)$ are the same for all three incident, transmitted and reflected fields. This assertion immediately constrains all three wavevectors to be coplanar and contained in the plane spanned by the incident wavevector and the interface normal, i.e. the plane normal to $\\mathbf{k}_i\\times \\hat{\\mathbf{n}}$ , the so called plane of incidence and it constrains components of all three vectors along the interface in this plane of incidence to be equal. Thus the analysis is always simplified by aligning our co-ordinates with the the plane of incidence, and so wavevectors all have only two nonzero Cartesian components $k_x,\\,k_y$ . The above constraint on exponential variation in the direction of the interface in the plane of incidence also means that the the angle of incidence must always equal the angle of reflexion and, across the interface it also gives us: $$n_i\\, \\sin\\theta_i = n_r\\,\\sin\\theta_r\\tag{5}$$ where $n_i$ and $n_r$ are the indices defined by (4) for the two mediums on either side of the interface the brach convention mentioned after (3). When the values of $n_i$ and $n_r$ are complex, to make (5) hold we now in general require a complex refraction angle $\\theta_r$ . In multilayer problems the incidence angle can also be complex. This means that the Cartesian vector components of $\\mathbf{k}$ are out of phase. It betokens evanescence . Clearly in general general a complex angle is needed to match the variation for different values of $\\arg(n_i)$ and $\\arg(n_r)$ in (5). This phenomenon is the presence of oscillating stores of energy at the interface that arise to match the EM field variations across the interface. This is a very like phenomenon to the oscillator reactive power in inductive / capacitive lumped circuit elements. The purely imaginary angle, for example, arising when $k_i, k_r$ and $\\theta_i$ are all real but leave $\\sin\\theta_r > 1$ , can corresponds to the purely evanescent situation that arises in total internal reflexion as discussed in my answer here . Complex refraction angles are extremely hard if not impossible to visualize, but it is evident that with such angles (5) can be fulfilled and then the normal derivation of the Fresnel equations runs to completion. One last word of warning. Snell’s law is not always as powerful and useful as it seems. Strictly speaking, it always holds when it refers to wavevector components as in (5), but it may not always correspond to the correct relationship between directions of propagation of the fields. The wavevector concept defines phase fronts of the wave fields, and in evanescent and anisotropic cases this can be radically different from the direction of power flow. So phase front stops being a useful notion in defining the direction of propagation of the electromagnetic field: if we trace the path of the field with our power meters and detectors, we can in general end up following directions that are quite different from those defined by the wavevector.",
      "question_latex": [],
      "answer_latex": [
        "\\begin{array}{lcl}\n\\mathbf{k} \\times \\mathbf{E}_0 &=& \\omega\\,\\mu\\,\\mathbf{H}_0\\\\\n\\mathbf{k} \\times \\mathbf{H}_0 &=& -\\left(\\omega\\,\\epsilon+i\\,\\sigma\\right)\\,\\mathbf{E}_0\\\\\n\\left<\\mathbf{k},\\,\\mathbf{E}_0\\right>&=& \\left<\\mathbf{k},\\,\\mathbf{H}_0\\right>=0\n\\end{array}\\tag{1}",
        "\\left<\\mathbf{k},\\,\\mathbf{k}\\right> = \\omega\\,\\mu\\,\\left(\\omega\\,\\epsilon+i\\,\\sigma\\right)\\tag{2}",
        "k = \\stackrel{def}{=}\\sqrt{\\left<\\mathbf{k},\\,\\mathbf{k}\\right>}\\tag{3}",
        "n \\stackrel{def}{=} \\frac{\\lambda\\,k}{2\\,\\pi} = \\frac{\\lambda\\,\\sqrt{\\left<\\mathbf{k},\\,\\mathbf{k}\\right>} }{2\\,\\pi}\\tag{4}",
        "n_i\\, \\sin\\theta_i = n_r\\,\\sin\\theta_r\\tag{5}",
        "\\left(\\mathbf{E},\\,\\mathbf{H}\\right) = \\left(\\mathbf{E}_0,\\,\\mathbf{H}_0\\right)\\,\\exp\\left(i\\,\\left<\\mathbf{k},\\,\\mathbf{r} \\right>\\right)",
        "\\mathbf{k}",
        "</span></p>\n\n<p>and this is not changed by the components of <span class=\"math-container\">",
        "</span>’s being complex rather than real;</p>\n\n<ol start=\"3\">\n<li>To complete the derivation, one writes down the continuity conditions for the tangential electromagnetic fields and uses Gaussian elimination to find the required amplitude ratios for the incident, reflected and transmitted fields. Both <span class=\"math-container\">",
        "</span> and <span class=\"math-container\">",
        "</span> are commutative fields, thus both equally allow Gaussian elimination and so nothing about this step in the derivation is changed by a complex component <span class=\"math-container\">",
        "</span>. With electromagnetic waves, one must work through the derivation twice, once for each of a basis pair of polarization states. The wonted and easiest choice for the latter are linear polarization states, firstly (a) the Transverse Magnetic (TM) or Senkrecht state (s) where the magnetic field lies everywhere parallel to the interface and normal to the plane of incidence (the plane spanned by the incident wavevector and the unit normal to the interface) and (b) the Transverse Electric, or Parallel state (p), where the electric field is everywhere parallel to the interface plane and lies normal to the plane of incidence.</li>\n</ol>\n\n<p>There are a few points that need special care:</p>\n\n<p><strong>Magnetic Constants</strong></p>\n\n<p>Most derivations of the Fresnel formulas assume that the magnetic constant for both mediums is the same. This doesn’t have to be so for the derivation to hold, but the final equations are subtly different if there is a magnetic constant difference between the two media and indeed i can’t actually find a convenient reference for them right now. So you may need to be careful of some exotic materials and include the magnetic constant if need be;</p>\n\n<p><strong>Unconjugated Dispersion Condition and Complex Wavenumber</strong></p>\n\n<p>The wavevector magnitude condition is no longer a condition on the magnitude but a condition on the <em>unconjugated</em> inner product of <span class=\"math-container\">",
        "</span> with itself:</p>\n\n<p><span class=\"math-container\">",
        "</span></p>\n\n<p>a condition which follows simply from the vector wave equation that results from substituting one of the curl equations in (1) into the other. I have in the past made some considerably strife for myself in making this mistake.</p>\n\n<p>One then defines the wavenumber of the medium in question as the complex square root </p>\n\n<p><span class=\"math-container\">",
        "</span> </p>\n\n<p>of the medium in question. The branch of the square root is such that the imaginary part is of  consistent sign. In the convention  (as used in (1)) that takes <span class=\"math-container\">",
        "</span> to be the positive frequency variation, the imaginary part of k is chosen to be positive.</p>\n\n<p>The index of the medium is then defined as:</p>\n\n<p><span class=\"math-container\">",
        "</span></p>\n\n<p><strong>Snell’s Law</strong></p>\n\n<p>Lastly, Snell’s law needs some care to interpret, for it implies some quite nonintuitive things.  As i explain <a href=\"https://physics.stackexchange.com/a/97897\">in my answer here</a>, all three of:</p>\n\n<ol>\n<li>Snell’s law;</li>\n<li>The law of reflexion and;</li>\n<li>The principle that the wavevectors of all three plane waves must be coplanar are the direct result of equating the components of all wave vectors when the wavevectors are projected into the interface plane. </li>\n</ol>\n\n<p>The only way that the components of <span class=\"math-container\">",
        "</span> parallel to the interface and of <span class=\"math-container\">",
        "</span> normal to the plane can be continuous is if the components of the wavevector <span class=\"math-container\">",
        "</span> that defines the variation in the spatial variation <span class=\"math-container\">",
        "</span> are <em>the same</em> for all three incident, transmitted and reflected fields. This assertion immediately constrains all three wavevectors to be coplanar and contained in the plane spanned by the incident wavevector and the interface normal, i.e. the plane normal to <span class=\"math-container\">",
        "</span>, the so called <em>plane of incidence</em> and it constrains components of all three vectors along the interface in this plane of incidence to be equal. Thus the analysis is always simplified by aligning our co-ordinates with the the plane of incidence, and so wavevectors all have only two nonzero Cartesian components <span class=\"math-container\">",
        "</span>.</p>\n\n<p>The above constraint on exponential variation in the direction of the interface in the plane of incidence also means that the the angle of incidence must always equal the angle of reflexion and, across the interface it also gives us:</p>\n\n<p><span class=\"math-container\">",
        "</span></p>\n\n<p>where <span class=\"math-container\">",
        "</span> are the indices defined by (4) for the two mediums on either side of the interface the brach convention mentioned after (3). When the values of <span class=\"math-container\">",
        "</span> are complex, to make (5) hold we now in general require a complex refraction angle <span class=\"math-container\">",
        "</span>. In multilayer problems the incidence angle can also be complex. This means that the Cartesian vector components of <span class=\"math-container\">",
        "</span> are out of phase. It betokens <em>evanescence</em>. Clearly in general general a complex angle is needed to match the variation for different values of <span class=\"math-container\">",
        "</span> in (5). This phenomenon is the presence of oscillating stores of energy at the interface that arise to match the EM field variations across the interface. This is a very like phenomenon to the oscillator reactive power in inductive / capacitive lumped circuit elements. The purely imaginary angle, for example, arising when <span class=\"math-container\">",
        "</span> are all real but leave <span class=\"math-container\">"
      ],
      "created": "2019-09-14T19:14:46.993",
      "golden_ner_terms": [
        "absorptive",
        "analysis",
        "angle",
        "assumption",
        "basis",
        "branch",
        "canonical",
        "circuit",
        "commutative",
        "complete",
        "completion",
        "complex",
        "component",
        "components",
        "concept",
        "consistent",
        "constant",
        "contained",
        "continuous",
        "coplanar",
        "curl",
        "derivation",
        "dielectric",
        "difference",
        "diffraction",
        "dispersion",
        "displacement",
        "elements",
        "energy",
        "equation",
        "even",
        "exponential",
        "field",
        "flow",
        "frequency",
        "fresnel formulae",
        "fresnel formulas",
        "gaussian",
        "gaussian elimination",
        "imaginary",
        "imaginary part",
        "implies",
        "incident",
        "index",
        "indices",
        "inner",
        "inner product",
        "interference",
        "magnetic fields",
        "normal",
        "onto",
        "optics",
        "parallel",
        "path",
        "photonics",
        "place",
        "plane",
        "plane normal",
        "plane wave",
        "point",
        "polarization",
        "positive",
        "power",
        "product",
        "purely imaginary",
        "rank",
        "real",
        "reflexion",
        "refraction",
        "right",
        "root",
        "section",
        "side",
        "simple",
        "site",
        "solution",
        "spanned by",
        "square",
        "square root",
        "state",
        "step",
        "strictly",
        "tensor",
        "theory",
        "trace",
        "transverse",
        "unit",
        "unit normal",
        "useful",
        "valid",
        "variation",
        "vector",
        "vector component",
        "vectors",
        "wave equation",
        "waves",
        "way",
        "word",
        "work"
      ],
      "golden_ner_count": 99,
      "golden_patterns": [
        {
          "pattern": "check-the-extreme-cases",
          "score": 4.0,
          "hotwords": [
            "extreme",
            "zero"
          ]
        },
        {
          "pattern": "work-examples-first",
          "score": 2.0,
          "hotwords": [
            "example"
          ]
        },
        {
          "pattern": "argue-by-contradiction",
          "score": 2.0,
          "hotwords": [
            "impossible"
          ]
        },
        {
          "pattern": "induction-and-well-ordering",
          "score": 2.0,
          "hotwords": [
            "inductive"
          ]
        },
        {
          "pattern": "encode-as-algebra",
          "score": 2.0,
          "hotwords": [
            "encode"
          ]
        },
        {
          "pattern": "unfold-the-definition",
          "score": 2.0,
          "hotwords": [
            "means that"
          ]
        }
      ],
      "golden_pattern_names": [
        "check-the-extreme-cases",
        "work-examples-first",
        "argue-by-contradiction",
        "induction-and-well-ordering",
        "encode-as-algebra",
        "unfold-the-definition"
      ],
      "golden_scopes": [],
      "golden_scope_count": 0
    },
    {
      "id": "se-physics-398062",
      "stratum": "medium",
      "title": "Laser beam alignment: best practices",
      "tags": [
        "optics",
        "laser",
        "experimental-technique"
      ],
      "score": 10,
      "answer_score": 11,
      "question_body": "I have to build a complex optical setup consisting of many mirrors, lenses, beam-splitters, multiple wavelengths lasers to be coupled, etc. I am looking for the \"best practices\" to efficiently (and with the best precision) align a laser beam and the procedure to follow (tips, tricks, conventions). Edit: The used lasers are in the visible range (blue & red). My question is more like a \"How to\" in order to optimize the overall alignment, the lens placing for beam collimation, and to avoid aberrations. Of course, safety is extremely important (be extra careful while dealing with non-visible lasers), but this is not what I am interested in. In which order are the components added to the setup? How to best check the alignment of each individual component?",
      "answer_body": "Safe Alignment Since you're using visible wavelengths, you can align everything looking at it through webcams. They're so cheap and they see very well for many applications out to 11oonm. Put them all over your apparatus and only look at the computer screen as you align. There's little to no temptation to peek as you can almost always arrange a webcam to see far better than you could if you put your face right up near what you're aligning. The only \"peek\" risk is the urge to look at your hands when they're out of sight. This can be overcome by (1) wearing the appropriate laser safety googles and (2) practicing alignment with a low power class 1 laser so you get into the habit of focussing solely on the computer screen image. If you can't get a computer screen near your apparatus, simply set up a web server on your computer giving a feed of the webcam's output. Then you can browse to the website on your phone or tablet that you stand next to you. Make sure that the latter is on a stand so that it can't fall over into the beam. Another solution is to get hold of some 3D goggles and either download an app to project the webcam output into them or write one yourself. I use a \"Google Cardboard\" holder to turn my phone into goggles, and, since its a Windows phone, it's very easy to access the relevant APIs through MS Visual Studio written apps. I'm sure Android is just as easy. Then you can stand next to your apparatus and \"look direct at it\" with zero chance of a laser eye injury. For class 4 you'll need to be a little careful with your skin, especially when your hands are out of sight whilst you're looking in goggles or at a computer screen. You may wish to consider all remote, motorized actuation of your adjustment. I would say this is compulsory for anything above 1 watt power. For low end class 4 under a watt and longer wavelengths (down to about 550nm), I personally deem the risk acceptable to use my hands. You must decide for yourself. The beam is about as harmful as a small flame - and also as painful - which is good because one pulls ones hand away if it gets in the beam and any burn is thus minor. Shorter wavelengths 550nm and under bring the risk of photochemical damage and the attendant - small but nonzero - risk of skin cancer from a laser burn. As I said, I am comfortable with the risk for lower power, longer wavelength class 4 lasers but you must decide for yourself - I don't earn any money as a hand product model for Estée Lauder or Elizabeth Aarden and I'm in good health so small injuries are of little bother to me. Also, ONLY decide for yourself - never for a colleague and NEVER for a subordinate. If you have a student or subordinate doing this work for you, you MUST use motorized remote stages for any class 4 laser. Don't even ask a subordinate, it's grossly unfair: they simply cannot, by dint of your relationship with them, answer impartially. If you do use your hands, make sure to take all rings and watches off so that the beam doesn't end up in someone else's eyes. There should, however, be no idle bystanders throughout alignment - only people who are truly needed for the procedure and who understand laser safety. That said, an experimenter whose job it is to switch the laser on and off as you need it and who can watch what you're doing is a good idea. It is also best practice to do laser alignment with the laboratory lights switched on . Use bright lighting. The aim here is to shrink the eye's pupil to about 1mm, the small aperture it uses on a bright, sunlit day. If you're aligning in the dark, the pupil is about 7mm in diameter: that means that the probability of a laser's getting in the eye is fifty fold higher. Below are some shots of webcam arrangements I have used in the past. You need either to get a friendly machinist to make brackets for you to attach them to the various Newport/ Thorlabs/ Edmund /.... stages and supports and allow them to swivvel so that you can adjust their view easily. It's easy to screw directly into the webcam's plastic housing. You should get some ideas from the below. Telescopes User Martin Beckett makes the point that I've also used: If you have space it's often easier to replace the laser with a telescope and a camera (look for survey level telescopes) then you can adjust a component while looking at the image of a target at the output. Martin means here a builder's Dumpy Level . They are compact and good quality ones can now be bought for a few hundred dollars. They are also available with standard camera tripod mounts: check this before you get one because the builder's tripods are generally too bulky to use in a laboratory. I usually dispense with the tripod altogether and mount the level on the optical bench on a tip /tilt + XYZ stage. A riflescope of comparable magnification (say 32x and over) can also be used here - this may be a better shape (long and thin) for some applications. If you use a dumpy level, you may also wish to get hold of some fiber bundle lighting kits. These let you shine light into your apparatus so that you can see through the level at alignment targets. Surprisingly little light gets through a high magnification telescope (well duh! it's gathering light from a tiny solid angle, but still it always catches me by surprise) so you will probably need lighting to see properly. The dumpy level is used exactly as you intuitively think - to \"fare the photon's path\". Wherever the crosshairs land will be where the laser beam will go, when the laser's cavity is aligned with the dumpy level's optical axis. This last point shows a second aspect to this technique: you must design a method to bring the laser's cavity to alignment with the dumpy level's optical axis after you've aligned everything with the level. Naïvely, this means mounting the laser on a tip/tilt XY stage - the simplest and the best method - but this is impracticable for many bulky laboratory lasers. If so, you will need to pass the beam from the laser through a crossed periscope for your X and Y translational degrees of freedom as well as a tip/tilt mirror. All of these degrees of freedom are coupled, so this is not a trivial exercise. You will need an alignment plan (see below) for this step - it can end up being harder than skipping the level altogether. But sometimes, this last step is easy - once aligned with each other, the components are sometimes quite tolerant of the laser's alignment. The general rule here is: the use of a level only saves work to set up spatial relationships between components other than the laser. It does not help align the laser. For example, a Michelson interferometer requires an exquisitely precise angular relationships between the planes of the reflectors and the plane of the beamsplitter. A level will help you set this up beautifully. The Michelson, once aligned, is pretty tolerant of the laser's direction. You can easily have a sizeable fraction of a degree perturbation to the laser's alignment and an aligned Michelson will simply self correct for it. So some applications will benefit from a level, others not. Alignment Lasers Similar to the use of a level is the use of alignment lasers. That is, one uses a IEC60825 Class 1 laser (NOT Class 1M) of the same wavelength as the final to do one's alignment with. One does have the same problem at the end as with the level: you must design an alignment procedure to bring the final laser's optical axis into alignment with that of the alignment laser when the latter is replaced with the former. I generally have a package made where I package an alignment laser with a collimator and other optics to achieve the same beamwidth as the final laser will output. Alignment lasers have obvious safety benefits and you can work so much faster when there is only a Class 1 laser threat. Another benefit is that the beam is often of much higher quality - and coherence - than the final. This lets you use extremely useful tools such as the Shearing Interferometer (which I talk about below) for collimation. Large, high power lasers often do not have the coherence length to allow the use of this vital tool. Before Procedure Begins With any complicated system, you need to design an alignment procedure at the outset and calculate theoretically exactly what you will see as you bring each part of the kit into alignment. This, naturally, also includes an alignment criterion - how you decide when alignment is done. Fringe shapes and how they change as you twiddle knobs, intensity profiles, the lot. When you get to a stage in your alignment that does not agree with theory, stop, make sure you understand what's wrong and reconcile the two (theory and practice) before you go forward. You'll learn a great deal about your setup this way. Collimation By far the most effective - and safest - way to collimate a beam is through the use of a Shearing Interferometer . You don't have to aim a class 4 beam across a laboratory to check its divergence and with a shearplate the accuracy of the collimation you can achieve is equivalent to that gotten by checking the beam divergence over hundreds of meters or even kilometers of axial beam length. So it's waaay more effective and safer than many of the collimation setups aiming beams across laboratories I see. Again, the fringe pattern must be viewed with a camera. You simply use the CCD chip in the camera and remove the webcam's lens: the fringe pattern is a structured collimated beam. You can use the shutter speed to help you adjust brightness but you will almost certainly need to add an attenuator to prevent CCD chip damage. The only catch is that you need to check that the lasers are coherent enough to use the shearplate: this means that they need to have an axial coherence length of a centimeter or so, since most shearplates are several millimeters thick. The $1/e$ coherence length for a Lorentzian lineshape is: $$L_c=\\frac{\\lambda_0^2}{\\pi\\,\\Delta\\lambda} = \\frac{c}{\\pi\\,\\Delta\\nu}$$ where $\\lambda_0$ is the center wavelength, $\\Delta\\lambda$ the full width half maximum wavelength spread and $\\Delta\\nu$ the full width half maximum frequency spread. Therefore, for a 1cm coherence length, you need a 633nm laser to have a wavelength spread of about 0.013nm, equivalent to 9.5GHz linewidth. This may be a tall ask for some of the more powerful lasers. If so, you will need to use an alignment laser discussed above as the imperative for avoiding the across-the-laboratory collimation method becomes stronger with increasing laser power. You can make your own shearing interferometer from a microscope slide but the easiest ones to use have a wedged shearplate so you're probably going to have to buy one for the reasons I discuss below. A diagram from the wikipedia page I linked is below so that the device is creating interference between the two Fresnel reflected beams from either side of a parallel plate. The device is basically plotting a fringe pattern for the phase map $\\Delta\\,\\nabla_\\vec{n} \\varphi$ where $\\varphi$ is the phase field in the incoming beam, $\\Delta$ the magnitude and $\\vec{n}$ the direction of the displacement between the two beams. Converging/ diverging beams have, to lowest order: $$\\varphi(\\vec{r}) = \\varphi + \\vec{k}_\\perp \\cdot \\vec{r} + \\frac{1}{2}\\,\\vec{r}^T \\kappa\\,\\vec{r}\\tag{1}$$ where $\\vec{k}_\\perp$ is the component of the wavevector normal to the nominal direction (and thus measures the beam's tilt) and $\\kappa$ is the curvature matrix (related to the curvature tensor). Therefore, the shearplate's fringes are set by the field $$\\Delta\\,\\nabla_\\vec{n} \\varphi \\propto \\vec{k}_\\perp \\cdot \\vec{n} + \\langle \\kappa\\vec{n},\\,\\vec{r}\\rangle\\tag{2}$$ which gives a linear fringe pattern. Collimation is the elimination of $\\kappa$. So, from the above equation, you adjust your optics until the linear fringes disappear. That's for a flat shearplate. It is hard to find the point where the last fringe disappears accurately, so commercial shearplates have a wedge between the two surfaces. That way, there is always a linear term in (2): the absence of the $\\langle \\kappa\\vec{n},\\,\\vec{r}\\rangle$ means that the fringe direction is set by the wedge and when $\\kappa$ is nonzero they rotate to some other direction so you simply adjust until you have linear fringes pointing in the direction indicated on the instrument. One thing to be aware of, especially with lasers, is that they cannot always be collimated with an axisymmetric lens like a microscope objective. This happens if the principal curvatures in $\\kappa$ above are unequal, and therefore the introduction of a constant curvature into the beam by the use of a collimating axisymmetric lens cannot annul both principal curvatures at once. This is the phenomenon of astigmatism and if you suspect this situation, you must use the shearing interferometer in two positions, rotated through $90^\\circ$ about the input optical axis, from one another to check that both principal wavefront curvatures have been annulled. If there is astigmatism, it can only be corrected using cylindrical lenses. Here is a cross section of a setup I have used with a camera, whowing the relative positions. The tested beam comes in through the enclosed tube from the left and there is a beam stop on the right. A companion instrument is the autocollimator . I don't use this much as it's pretty much redundant if one uses a dumpy level and shearing interferometer. But it can be compact and convenient, so it's worth looking into. The Astronomer's Star Test Beam quality - lack of aberration - is ideally assessed by a point diffraction interferometer . Here's my own writeup based on a commercial device here . This is a device that uses a pinhole to create a reference beam from the input beam, and thus can measure wavefront aberration of a laser beam. An interferometer is a bulky and awkward instrument to get into a system, so you may be tempted to use a Hartmann wavefront sensor ; see also the Wiki page here . In my experience, although they require little experience to use, one can get a much more accurate confirmation of low aberration with the Astronomer's Star Test and you need only a CCD screen, a focussing lens and your brain to do it. Again, I emphasize, you must do the star test either with a camera or IEC60825 class 1 (NOT class 1M) alignment laser! Read up on this from the many amateur astronomer's websites. With experience, you will get confirmation of aberration down to as low as 0.02 waves RMS easily. A Hartmann sensor is good down to about 0.07 waves RMS. A refinement is to compare the intensity of a focal spot to one generated from a beam of the same wavelength and beamwidth, thus measuring the Strehl ratio directly. This can help if you are inexperienced, but an experienced operator will measure the equivalent of about 0.97 Strehl from the shape of the point spread function alone. For alignment, in most cases, you will get as good a result as with the use of an interferometer.",
      "question_latex": [],
      "answer_latex": [
        "L_c=\\frac{\\lambda_0^2}{\\pi\\,\\Delta\\lambda} = \\frac{c}{\\pi\\,\\Delta\\nu}",
        "\\varphi(\\vec{r}) = \\varphi + \\vec{k}_\\perp \\cdot \\vec{r} + \\frac{1}{2}\\,\\vec{r}^T \\kappa\\,\\vec{r}\\tag{1}",
        "\\Delta\\,\\nabla_\\vec{n} \\varphi \\propto \\vec{k}_\\perp \\cdot \\vec{n} + \\langle \\kappa\\vec{n},\\,\\vec{r}\\rangle\\tag{2}",
        "1/e",
        "</p>\n\n<p>where",
        "is the center wavelength,",
        "the full width half maximum wavelength spread and",
        "the full width half maximum frequency spread. Therefore, for a 1cm coherence length, you need a 633nm laser to have a wavelength spread of about 0.013nm, equivalent to 9.5GHz linewidth. This may be a tall ask for some of the more powerful lasers. If so, you will need to use an alignment laser discussed above as the imperative for avoiding the across-the-laboratory collimation method becomes stronger with increasing laser power.</p>\n\n<p>You can make your own shearing interferometer from a microscope slide but the easiest ones to use have a wedged shearplate so you're probably going to have to buy one for the reasons I discuss below. A diagram from the wikipedia page I linked is below</p>\n\n<p><a href=\"https://i.stack.imgur.com/QTmWj.png\" rel=\"noreferrer\"><img src=\"https://i.stack.imgur.com/QTmWj.png\" alt=\"enter image description here\"></a></p>\n\n<p>so that the device is creating interference between the two Fresnel reflected beams from either side of a parallel plate. The device is basically plotting a fringe pattern for the phase map",
        "where",
        "is the phase field in the incoming beam,",
        "the magnitude and",
        "the direction of the displacement between the two beams. Converging/ diverging beams have, to lowest order:</p>\n\n<p>",
        "is the component of the wavevector normal to the nominal direction (and thus measures the beam's tilt) and",
        "is the curvature matrix (related to the curvature tensor). Therefore, the shearplate's fringes are set by the field </p>\n\n<p>",
        "</p>\n\n<p>which gives a linear fringe pattern.  Collimation is the elimination of",
        ". So, from the above equation, you adjust your optics until the linear fringes disappear. That's for a flat shearplate. It is hard to find the point where the last fringe disappears accurately, so commercial shearplates have a wedge between the two surfaces. That way, there is always a linear term in (2): the absence of the",
        "means that the fringe direction is set by the wedge and when",
        "is nonzero they rotate to some other direction so you simply adjust until you have linear fringes pointing in the direction indicated on the instrument.</p>\n\n<p>One thing to be aware of, especially with lasers, is that they cannot always be collimated with an axisymmetric lens like a microscope objective. This happens if the principal curvatures in",
        "above are unequal, and therefore the introduction of a constant curvature into the beam by the use of a collimating axisymmetric lens cannot annul both principal curvatures at once. This is the phenomenon of astigmatism and if you suspect this situation, you must use the shearing interferometer in two positions, rotated through"
      ],
      "created": "2018-04-06T09:18:25.687",
      "golden_ner_terms": [
        "angle",
        "axial",
        "axis",
        "blue",
        "calculate",
        "camera",
        "center",
        "class",
        "class 1",
        "coherence",
        "compact",
        "comparable",
        "complex",
        "component",
        "components",
        "computer",
        "constant",
        "cross",
        "cross section",
        "curvature",
        "decide",
        "degree",
        "degrees of freedom",
        "design",
        "diagram",
        "diameter",
        "diffraction",
        "displacement",
        "divergence",
        "diverging",
        "effective",
        "equation",
        "equivalent",
        "even",
        "face",
        "fiber",
        "fiber bundle",
        "field",
        "flat",
        "fold",
        "fraction",
        "frequency",
        "function",
        "holder",
        "hundreds",
        "image",
        "increasing",
        "instrument",
        "intensity",
        "interference",
        "laser",
        "length",
        "lenses",
        "level",
        "map",
        "matrix",
        "measure",
        "minor",
        "model",
        "multiple",
        "near",
        "normal",
        "obvious",
        "operator",
        "optics",
        "order",
        "parallel",
        "path",
        "perturbation",
        "plane",
        "point",
        "power",
        "principal curvature",
        "probability",
        "product",
        "project",
        "range",
        "ratio",
        "redundant",
        "refinement",
        "right",
        "rms",
        "rotate",
        "section",
        "sensor",
        "side",
        "similar",
        "solid",
        "solid angle",
        "solution",
        "space",
        "speed",
        "star",
        "step",
        "stronger",
        "telescope",
        "telescopes",
        "tensor",
        "term",
        "theory",
        "thin",
        "useful",
        "wavelength",
        "waves",
        "way",
        "wedge",
        "width",
        "wikipedia",
        "work",
        "zero"
      ],
      "golden_ner_count": 110,
      "golden_patterns": [
        {
          "pattern": "check-the-extreme-cases",
          "score": 4.0,
          "hotwords": [
            "extreme",
            "zero"
          ]
        },
        {
          "pattern": "try-a-simpler-case",
          "score": 2.0,
          "hotwords": [
            "simplest"
          ]
        },
        {
          "pattern": "work-examples-first",
          "score": 2.0,
          "hotwords": [
            "example"
          ]
        },
        {
          "pattern": "exploit-symmetry",
          "score": 2.0,
          "hotwords": [
            "symmetric"
          ]
        },
        {
          "pattern": "construct-an-explicit-witness",
          "score": 2.0,
          "hotwords": [
            "build"
          ]
        },
        {
          "pattern": "encode-as-algebra",
          "score": 2.0,
          "hotwords": [
            "ring"
          ]
        },
        {
          "pattern": "use-probabilistic-method",
          "score": 2.0,
          "hotwords": [
            "probability"
          ]
        },
        {
          "pattern": "unfold-the-definition",
          "score": 2.0,
          "hotwords": [
            "means that"
          ]
        },
        {
          "pattern": "transport-across-isomorphism",
          "score": 2.0,
          "hotwords": [
            "equivalent"
          ]
        },
        {
          "pattern": "show-both-inequalities",
          "score": 2.0,
          "hotwords": [
            "other direction"
          ]
        }
      ],
      "golden_pattern_names": [
        "check-the-extreme-cases",
        "try-a-simpler-case",
        "work-examples-first",
        "exploit-symmetry",
        "construct-an-explicit-witness",
        "encode-as-algebra",
        "use-probabilistic-method",
        "unfold-the-definition",
        "transport-across-isomorphism",
        "show-both-inequalities"
      ],
      "golden_scopes": [
        {
          "type": "where-binding",
          "match": "where $\\lambda_0$ is"
        },
        {
          "type": "where-binding",
          "match": "where $\\varphi$ is"
        },
        {
          "type": "where-binding",
          "match": "where $\\vec{k}_\\perp$ is"
        }
      ],
      "golden_scope_count": 3
    },
    {
      "id": "se-physics-448761",
      "stratum": "medium",
      "title": "Rigorous derivation of the mean free path in a gas",
      "tags": [
        "statistical-mechanics",
        "kinetic-theory",
        "mean-free-path"
      ],
      "score": 19,
      "answer_score": 8,
      "question_body": "Can anyone supply me with a derivation of the mean free path, of particles in a Maxwell Boltzmann Gas? Cited in various literature is the formula, \\begin{align} \\begin{split} \\ell&=\\frac{1}{\\sqrt{2}n\\sigma}, \\end{split} \\end{align} which describes the mean free path of an atom/molecule in a Maxwell Boltzmann gas. Within it, $n$ is the gas density (assumed to be homogeneous) and $\\sigma$ is the cross section of colliding particles. Derivations of the mean free path tend to include a comment about the relative velocity of particles. However, most literature will not treat this as a distribution, and will instead follow a two part approximate method. First, magnitude of the relative velocity is expressed using equation (1). Within it, $\\theta$ is the angle between relative velocities $2$ and $1$ . \\begin{align} \\tag{1} |\\mathbf{v}_{r}|&=\\sqrt{\\mathbf{v}_{r}\\cdot\\mathbf{v}_{r}}\\\\ &=\\sqrt{(\\mathbf{v_2}-\\mathbf{v_1})\\cdot(\\mathbf{v_2}-\\mathbf{v_1})}\\\\ &=\\sqrt{\\mathbf{v_1}^2+\\mathbf{v_2}^2-2\\mathbf{v_1}\\cdot\\mathbf{v_2}}\\\\ v_{r}&=\\sqrt{v_1^2+v_2^2-2v_1v_2cos(\\theta)}\\\\ \\end{align} Once equation (1) has been found, most authors take an average of both sides. Instead of evaluating this by treating each velocity as a distribution however, they instead take the average under the square root and onto each summed term (equation 2) \\begin{align} \\tag{2} \\langle v_{r}\\rangle&=\\big\\langle\\sqrt{v_1^2+v_2^2-2v_1v_2cos(\\theta)}\\big\\rangle\\\\ &=\\sqrt{\\big\\langle v_1^2+v_2^2-2v_1v_2cos(\\theta)\\big\\rangle}\\\\ &=\\sqrt{\\big\\langle v_1^2\\big\\rangle+\\big\\langle v_2^2\\big\\rangle-\\big\\langle2v_1v_2cos(\\theta)\\big\\rangle}\\\\ &=\\sqrt{\\langle v_1\\rangle^2+\\langle v_2\\rangle^2-0}\\\\ &=\\sqrt{\\langle v_1\\rangle^2+\\langle v_2\\rangle^2}\\\\ &=\\sqrt{2\\langle v_1\\rangle^2}\\\\ &=\\sqrt{2}~\\langle v_1\\rangle\\\\ \\end{align} A large amount of (2) is wrong. Exceptions are the forth and fifth steps, where an average of the dot product between two arbitrary velocities is zero, justified by the velocity in a Maxwell Boltzmann distribution having no preferred direction. The remaining step involves the average velocity of any particle in a Boltzmann gas being the same ( $\\langle v_1\\rangle=\\langle v_2\\rangle$ ). One might think to replace the average of the square of velocity with an rms velocity, however doing so yields the rms relative velocity distribution, which is out by $8\\%$ from the mean relative velocity. I would like to see a derivation of the same result without these approximate methods. Failing that, reference to a textbook that provides them would be helpful as well. References ( all use the approximation ): Reif, F. (1965) Fundamentals of Statistical and Thermal Physics. McGraw Hill, New York, 273-278. formula for mean free path in two dimensions Why is the mean free path divided by $\\sqrt{2}$ ? RMS Free Path vs Mean Free Path",
      "answer_body": "The following is a self answer concerning the rigorous derivation of the relative velocity in a Maxwellian gas. It is here for comparison against other answers. Assumptions are Two particle collisions are the most probable ones, and collisions involving more than this, contribute to the mean relative velocity by a negligible amount. The mean relative velocity is strictly positive. I think that this derivation differs from the one by @Thorondor because within it, $\\mathbf{v_1}^2+\\mathbf{v_2}^2\\neq\\mathbf{v_r}^2$ . Which is to say that the dot product of $\\mathbf{v_1}$ , and $\\mathbf{v_2}$ is not assumed to be zero. This forces us to integrate over all possible angles between the velocities, which may account for the ~0.07 difference. It is also why the derivation is significantly longer. The mean free path of an atom/molecule in a Maxwellian gas, depends upon the average relative velocity of each particle to one another. In order to obtain it, we first find the magnitude of velocity $\\mathbf{v}_1$ relative to all other particles moving with $\\mathbf{v}_2$ . This expression is then averaged for all values of $\\mathbf{v}_2$ , from zero to infinity, producing a mean relative speed given $\\mathbf{v}_1$ exists. Following modus ponens, the mean is multiplied by the probability that an atom/molecule has velocity $\\mathbf{v}_1$ , and then averaged for all $\\mathbf{v}_1$ from zero to infinity. Consider two velocities $\\mathbf{v}_1$ and $\\mathbf{v}_2$ inclined at an angle $\\theta$ , there relative velocity will be described by \\begin{align} (v_r)_{12}&=\\sqrt{v_1^2+v_2^2-2v_1v_2cos(\\theta)}. \\end{align} All directions are equally probable for $\\mathbf{v}_2$ . To calculate the average of $(v_r)_{12}$ , we multiply it by the probability that it lies within some solid angle $d\\Omega$ . \\begin{align} \\langle(v_r)_{12}\\rangle&=\\int_{\\Omega}\\frac{d\\Omega}{4\\pi}(v_r)_{12}\\\\ &=\\int_{\\Omega}\\frac{2\\pi sin(\\theta)d\\theta}{4\\pi}(v_r)_{12}\\\\ &=\\frac{1}{2}\\int_{0}^{\\pi}sin(\\theta)d\\theta(v_1^2+v_2^2-2v_1v_2cos(\\theta))^\\frac{1}{2} \\end{align} Consider a change of basis, where $cos(\\theta)=x$ , so $sin(\\theta)d\\theta=-dx$ . The limits of integration will change to $cos(0)=1$ and $cos(\\pi)=-1$ . \\begin{align} \\therefore~\\langle(v_r)_{12}\\rangle&=\\frac{1}{2}\\int_{1}^{-1}dx(v_1^2+v_2^2-2v_1v_2x)^\\frac{1}{2}\\\\ &=\\frac{1}{3}\\frac{(v_1^2+v_2^2-2v_1v_2x)^\\frac{3}{2}}{2v_1v_2}\\Biggr|_{1}^{-1}\\\\ &=\\frac{1}{6v_1v_2}\\left((v_1^2+v_2^2+2v_1v_2)^\\frac{3}{2}-(v_1^2+v_2^2-2v_1v_2)^\\frac{3}{2}\\right)\\\\ &=\\frac{1}{6v_1v_2}\\left((v_1+v_2)^\\frac{3}{2}(v_1+v_2)^\\frac{3}{2}+(v_1-v_2)^\\frac{3}{2}(v_1-v_2)^\\frac{3}{2}\\right)\\\\ &=\\frac{1}{6v_1v_2}\\left((v_1+v_2)^3+|v_1-v_2|^3\\right)\\\\ \\end{align} We take the magnitude of $v_1-v_2$ because $\\langle(v_r)_{12}\\rangle$ should always be positive, and therefore $\\left((v_1-v_2)^2\\right)^\\frac{3}{2}$ must also be. This splits the average into two parts. \\begin{align} \\langle(v_r)_{12}\\rangle&=\\frac{1}{6v_1v_2}\\left((v_1+v_2)^3-(v_1-v_2)^3\\right),~when~v_1\\geq v_2,\\\\ &=\\frac{1}{6v_1v_2}\\left[2v_2^3+6v_1^2v_2\\right]\\\\ &=\\frac{3v_1^2+v_2^2}{3v_1}.\\\\ \\langle(v_r)_{12}\\rangle&=\\frac{1}{6v_1v_2}\\left((v_1+v_2)^3+(v_1-v_2)^3\\right),~when~v_2>v_1,\\\\ &=\\frac{3v_2^2+v_1^2}{3v_2}. \\end{align} The average relative velocity of an atom/molecule possessing magnitude and direction $\\mathbf{v_1}$ , with respect to all other particles moving with $\\mathbf{v_2}$ , lying within speeds of zero to infinity is then \\begin{align} \\langle(v_r)_1\\rangle&=\\int_{0}^{\\infty}P(v_2)\\langle(v_r)_{12}\\rangle dv_2,\\\\ P(v_2)dv_2&=4\\pi\\left(\\frac{m}{2\\pi k_BT}\\right)^\\frac{3}{2}e^{-\\frac{mv_2^2}{2k_BT}}v_2^2dv_2. \\end{align} Note that $\\langle(v_r)_1\\rangle$ has two distinct forms depending on the difference between speeds. Because of this we break the integral into two parts. \\begin{align} \\langle(v_r)_1\\rangle=4\\pi\\left(\\frac{m}{2\\pi k_BT}\\right)^\\frac{3}{2}&\\left[\\int_{0}^{v_1}dv_2e^{-\\frac{mv_2^2}{2k_BT}}\\left(\\frac{3v_1^2+v_2^2}{3v_1}\\right)v_2^2\\right.\\\\ &+\\left.\\int_{v_1}^{\\infty}dv_2e^{-\\frac{mv_2^2}{2k_BT}}\\left(\\frac{3v_2^2+v_1^2}{3v_2}\\right)v_2^2\\right] \\end{align} To arrive at the relative velocity of any atom with another, we form a product of the above, and the probability a particle has velocity $v_1+dv_1$ . \\begin{align} \\langle v_r \\rangle&=\\int_{0}^{\\infty}P(v_1)\\langle(v_r)_1\\rangle dv_1 \\end{align} \\begin{align} \\langle v_r \\rangle=\\left(4\\pi\\left[\\frac{m}{2\\pi k_BT}\\right]^\\frac{3}{2}\\right)^2\\int_{0}^{\\infty}dv_1e^{-\\frac{mv_1^2}{2k_BT}}v_1^2&\\left[\\int_{0}^{v_1}dv_2e^{-\\frac{mv_2^2}{2k_BT}}\\left(\\frac{3v_1^2+v_2^2}{3v_1}\\right)v_2^2\\right.\\\\ &+\\left.\\int_{v_1}^{\\infty}dv_2e^{-\\frac{mv_2^2}{2k_BT}}\\left(\\frac{3v_2^2+v_1^2}{3v_2}\\right)v_2^2\\right] \\end{align} which may be re-written as \\begin{align} \\langle v_r\\rangle=\\left(\\frac{2m}{k_BT}\\right)^3&\\left[\\int_{0}^{\\infty}dv_1e^{-\\frac{mv_1^2}{2k_BT}}v_1^2\\int_{0}^{v_1}dv_2e^{-\\frac{mv_2^2}{2k_BT}}\\left(\\frac{3v_1^2+v_2^2}{3v_1}\\right)v_2^2\\right.\\\\ &+\\left.\\int_{0}^{\\infty}dv_1e^{-\\frac{mv_1^2}{2k_BT}}v_1^2\\int_{v_1}^{\\infty}dv_2e^{-\\frac{mv_2^2}{2k_BT}}\\left(\\frac{3v_2^2+v_1^2}{3v_2}\\right)v_2^2\\right], \\end{align} or \\begin{align} \\langle v_r\\rangle&=\\left(\\frac{2m}{k_BT}\\right)^3\\left[I_1+I_2\\right], \\end{align} where $I_1$ stands for the first integral in the square brackets of the above, and $I_2$ the second one. \\begin{align} I_1&=\\int_{0}^{\\infty}dv_1e^{-\\frac{mv_1^2}{2k_BT}}v_1^2\\int_{0}^{v_1}dv_2e^{-\\frac{mv_2^2}{2k_BT}}\\left(\\frac{3v_1^2+v_2^2}{3v_1}\\right)v_2^2\\\\ I_2&=\\int_{0}^{\\infty}dv_1e^{-\\frac{mv_1^2}{2k_BT}}v_1^2\\int_{v_1}^{\\infty}dv_2e^{-\\frac{mv_2^2}{2k_BT}}\\left(\\frac{3v_2^2+v_1^2}{3v_2}\\right)v_2^2 \\end{align} We wish to show that these two definite integrals are equivalent. To do so, a substitution, then change in the order of integration following Fubini's theorem is employed. Consider the piecewise function, \\begin{align} \\mathbb{I}_{\\{v_2\\leq v_1\\}}&=\\begin{cases} 1&v_1\\leq v_2\\\\ 0&v_1>v_2\\\\ \\end{cases} \\end{align} which is zero outside the limit of the second integral of $I_1$ . Using it, we can rewrite equation $I_1$ as \\begin{align} I_1&=\\int_{0}^{\\infty}dv_1e^{-\\frac{mv_1^2}{2k_BT}}v_1^2\\int_{0}^{\\infty}dv_2\\mathbb{I}_{\\{v_2\\leq v_1\\}}e^{-\\frac{mv_2^2}{2k_BT}}\\left(\\frac{3v_1^2+v_2^2}{3v_1}\\right)v_2^2.\\\\ \\end{align} We can combine these two integrals with a substitution $a=v_1$ , allowing us to write the following \\begin{align} I_1&=\\int_{0}^{\\infty}\\int_{0}^{\\infty}\\mathbb{I}_{\\{v_2\\leq v_1\\}}e^{-\\frac{ma^2}{2k_BT}}a^2e^{-\\frac{mv_2^2}{2k_BT}}\\left(\\frac{3v_1^2+v_2^2}{3v_1}\\right)v_2^2dv_2da.\\\\ \\end{align} Since the integrals limits are constant, we can apply Fubini's theorem exchange there order. \\begin{align} I_1&=\\int_{0}^{\\infty}\\int_{0}^{\\infty}\\mathbb{I}_{\\{v_2\\leq v_1\\}}e^{-\\frac{ma^2}{2k_BT}}a^2e^{-\\frac{mv_2^2}{2k_BT}}\\left(\\frac{3v_1^2+v_2^2}{3v_1}\\right)v_2^2dadv_2\\\\ &=\\int_{0}^{\\infty}dv_2e^{-\\frac{mv_2^2}{2k_BT}}v_2^2\\int_{0}^{\\infty}dv_1\\mathbb{I}_{\\{v_2\\leq v_1\\}}e^{-\\frac{mv_1^2}{2k_BT}}\\left(\\frac{3v_1^2+v_2^2}{3v_1}\\right)v_1^2\\\\ &=\\int_{0}^{\\infty}dv_2e^{-\\frac{mv_2^2}{2k_BT}}v_2^2\\int_{v_2}^{\\infty}dv_1e^{-\\frac{mv_1^2}{2k_BT}}\\left(\\frac{3v_1^2+v_2^2}{3v_1}\\right)v_1^2\\\\ \\end{align} Then, because the integral is definite, we can interchange $v_1$ with $v_2$ , and obtain an equivalent volume. \\begin{align} I_1&=\\int_{0}^{\\infty}dv_1e^{-\\frac{mv_1^2}{2k_BT}}v_1^2\\int_{v_1}^{\\infty}dv_2e^{-\\frac{mv_2^2}{2k_BT}}\\left(\\frac{3v_2^2+v_1^2}{3v_2}\\right)v_2^2\\\\ &=I_2 \\end{align} Hence, the average relative velocity can be rewritten as \\begin{align} \\langle v_r\\rangle&=2\\left(\\frac{2m}{k_BT}\\right)^3I_2. \\end{align} To evaluate $I_2$ , first consider the integral \\begin{align} I_0&=\\int_{v_1}^{\\infty}dv_2e^{-\\frac{mv_2^2}{2k_BT}}\\left(\\frac{3v_2^2+v_1^2}{3v_2}\\right)v_2^2. \\end{align} We can break this into parts, \\begin{align} I_0&=\\int_{v_1}^{\\infty}dv_2e^{-\\frac{mv_2^2}{2k_BT}}v_2^3+\\frac{v_1^2}{3}\\int_{v_1}^{\\infty}dv_2e^{-\\frac{mv_2^2}{2k_BT}}v_2, \\end{align} then consider the substitution $\\frac{m}{2k_BT}v_2^2=x$ , so $dv_2=\\frac{k_BT}{mv_2}$ , and the lower limit of integration changes to $\\frac{mv_1^2}{2k_BT}$ . \\begin{align} I_0&=\\int_{\\frac{mv_1^2}{2k_BT}}^{\\infty}dv_2e^{-\\frac{mv_2^2}{2k_BT}}v_2^3+\\frac{v_1^2}{3}\\int_{\\frac{mv_1^2}{2k_BT}}^{\\infty}dv_2e^{-\\frac{mv_2^2}{2k_BT}}v_2\\\\ &=2\\left(\\frac{k_BT}{m}\\right)^2\\int_{\\frac{mv_1^2}{2k_BT}}^{\\infty}e^{-x}xdx+\\frac{v_1^2}{3}\\frac{k_BT}{m}\\int_{\\frac{mv_1^2}{2k_BT}}^{\\infty}e^{-x}dx \\end{align} Since \\begin{align} \\int_{a}^{b}xe^{-x}dx&=-(x+1)e^{-x}\\Biggr|_{a}^{b}, \\end{align} $I_0$ simplifies to \\begin{align} I_0&=2\\left(\\frac{k_BT}{m}\\right)^2\\left[\\frac{m}{2k_BT}v_1^2+1\\right]e^{\\frac{-mv_1^2}{2k_BT}}\\\\ &=e^{-\\frac{mv_1^2}{2k_BT}}\\left[\\frac{4}{3}\\left(\\frac{k_BT}{m}\\right)v_1^2+2\\left(\\frac{k_BT}{m}\\right)^2\\right] \\end{align} Substituting this into $I_2$ yields \\begin{align} I_2=&\\int_{0}^{\\infty}dv_1e^{-\\frac{mv_1^2}{2k_BT}}v_1^2\\left[\\frac{4}{3}\\left(\\frac{k_BT}{m}\\right)v_1^2+2\\left(\\frac{k_BT}{m}\\right)^2\\right] \\\\=&2\\left(\\frac{k_BT}{m}\\right)^2\\int_{0}^{\\infty}dv_1e^{-\\frac{mv_1^2}{2k_BT}}v_1^2\\\\ &+\\frac{4}{3}\\frac{k_BT}{m}\\int_{0}^{\\infty}dv_1e^{-\\frac{mv_1^2}{2k_BT}}v_1^4\\\\ \\end{align} $I_2$ is actually a standard integral of the form, \\begin{align} \\int_{0}^{\\infty}e^{-ax^2}x^n&=\\frac{1}{2a^{\\frac{n+1}{2}}}\\Gamma\\left(\\frac{n+1}{2}\\right), \\end{align} where $\\Gamma$ is the gamma function, a generalization of the factorial. $I_2$ now simplifies to \\begin{align} I_2&=\\left(\\frac{k_BT}{m}\\right)^{\\frac{7}{2}}\\left[\\Gamma\\left(\\frac{3}{2}\\right)+\\frac{2}{3}\\Gamma\\left(\\frac{5}{2}\\right)\\right]\\\\ &=\\left(\\frac{k_BT}{m}\\right)^{\\frac{7}{2}}\\left[\\frac{\\sqrt{\\pi}}{2}+\\frac{2}{3}\\frac{3\\sqrt{\\pi}}{4}\\right]\\\\ &=\\left(\\frac{k_BT}{m}\\right)^{\\frac{7}{2}}\\sqrt{\\pi} \\end{align} Hence, the average relative speed of any Maxwellian gas particle with respect to any other is \\begin{align} \\langle v_r\\rangle&=2\\left[4\\pi\\left(\\frac{m}{2\\pi k_BT}\\right)^\\frac{3}{2}\\right]^2\\left(\\frac{k_BT}{m}\\right)^\\frac{7}{2}\\sqrt{\\pi},\\\\ &=\\frac{2\\sqrt{\\pi}\\cdot16\\pi^2}{8\\pi^3}\\left(\\frac{k_BT}{m}\\right)^{-3}\\left(\\frac{k_BT}{m}\\right)^\\frac{7}{2},\\\\ &=\\frac{4}{\\sqrt{\\pi}}\\sqrt{\\frac{k_BT}{m}},\\\\ &=\\sqrt{\\frac{16}{\\pi}\\frac{k_BT}{m}}. \\end{align} And the ratio of mean velocity $\\langle v_r\\rangle$ to mean relative velocity $\\langle v\\rangle$ is \\begin{align} \\frac{\\langle v\\rangle}{\\langle v_r\\rangle}&=\\frac{\\sqrt{\\frac{8}{\\pi}\\frac{k_BT}{m}}}{\\sqrt{\\frac{16}{\\pi}\\frac{k_BT}{m}}}\\\\ &=\\frac{1}{\\sqrt{2}} \\end{align} And thus the mean free path $\\ell$ is \\begin{align} \\ell=\\frac{1}{n\\sigma\\sqrt{2}} \\end{align}",
      "question_latex": [
        "n",
        "\\sigma",
        "\\theta",
        "2",
        "1",
        "\\langle v_1\\rangle=\\langle v_2\\rangle",
        "8\\%",
        "\\sqrt{2}"
      ],
      "answer_latex": [
        "\\mathbf{v_1}^2+\\mathbf{v_2}^2\\neq\\mathbf{v_r}^2",
        "\\mathbf{v_1}",
        "\\mathbf{v_2}",
        "\\mathbf{v}_1",
        "\\mathbf{v}_2",
        "\\theta",
        "(v_r)_{12}",
        "d\\Omega",
        "cos(\\theta)=x",
        "sin(\\theta)d\\theta=-dx",
        "cos(0)=1",
        "cos(\\pi)=-1",
        "v_1-v_2",
        "\\langle(v_r)_{12}\\rangle",
        "\\left((v_1-v_2)^2\\right)^\\frac{3}{2}",
        "\\langle(v_r)_1\\rangle",
        "v_1+dv_1",
        "I_1",
        "I_2",
        "a=v_1",
        "v_1",
        "v_2",
        "\\frac{m}{2k_BT}v_2^2=x",
        "dv_2=\\frac{k_BT}{mv_2}",
        "\\frac{mv_1^2}{2k_BT}",
        "I_0",
        "\\Gamma",
        "\\langle v_r\\rangle",
        "\\langle v\\rangle",
        "\\ell"
      ],
      "created": "2018-12-16T23:50:06.170",
      "golden_ner_terms": [
        "angle",
        "approximation",
        "atom",
        "average",
        "basis",
        "calculate",
        "change of basis",
        "constant",
        "cross",
        "cross section",
        "definite integral",
        "density",
        "derivation",
        "difference",
        "distribution",
        "dot product",
        "equation",
        "equivalent",
        "expression",
        "factorial",
        "forces",
        "formula",
        "fubini's theorem",
        "function",
        "gamma function",
        "gas",
        "generalization",
        "homogeneous",
        "infinity",
        "integral",
        "integrate",
        "integration",
        "limit",
        "lower limit",
        "mean",
        "mean free path",
        "modus ponens",
        "onto",
        "order",
        "path",
        "physics",
        "piecewise",
        "positive",
        "probability",
        "product",
        "ratio",
        "rms",
        "root",
        "section",
        "solid",
        "solid angle",
        "speed",
        "square",
        "square root",
        "step",
        "strictly",
        "substitution",
        "term",
        "theorem",
        "velocity",
        "volume",
        "zero"
      ],
      "golden_ner_count": 62,
      "golden_patterns": [
        {
          "pattern": "check-the-extreme-cases",
          "score": 4.0,
          "hotwords": [
            "zero",
            "infinity"
          ]
        },
        {
          "pattern": "use-probabilistic-method",
          "score": 2.0,
          "hotwords": [
            "probability"
          ]
        },
        {
          "pattern": "transport-across-isomorphism",
          "score": 2.0,
          "hotwords": [
            "equivalent"
          ]
        },
        {
          "pattern": "monotone-approximation",
          "score": 2.0,
          "hotwords": [
            "limit"
          ]
        }
      ],
      "golden_pattern_names": [
        "check-the-extreme-cases",
        "use-probabilistic-method",
        "transport-across-isomorphism",
        "monotone-approximation"
      ],
      "golden_scopes": [
        {
          "type": "consider",
          "match": "Consider two velocities "
        },
        {
          "type": "consider",
          "match": "Consider a change of basis, where "
        },
        {
          "type": "consider",
          "match": "Consider the piecewise function, \\begin{align} \\mathbb{I}_{\\{v_2\\leq v_1\\"
        },
        {
          "type": "for-any",
          "match": "for all $\\mathbf{v}_1$"
        },
        {
          "type": "where-binding",
          "match": "where $\\Gamma$ is"
        }
      ],
      "golden_scope_count": 5
    },
    {
      "id": "se-physics-644342",
      "stratum": "medium",
      "title": "Why do we treat the whole capacitor as if it would be a single conductor?",
      "tags": [
        "electric-circuits",
        "voltage",
        "capacitance",
        "batteries"
      ],
      "score": 6,
      "answer_score": 3,
      "question_body": "I am a high school and I am very confused about redistribution of charges when we connect 2 capacitors, my problems are: why are we treating the whole capacitor as if it would be a single conductor and say that charge will distribute equally in both plates (which doesn't make sense to me) unless the potential/ better to say a potential difference of both plates becomes equal so, if that's the case let's take the scenario of what I have shown in the image. why the charge redistribution doesn't occur here? there is a potential difference between both the plates connected, so the charges should flow? why is always the charge redistribution occurs symmetrically on both plates? say the potential of the right plate of below capacitor be \"V/2\" instead of 0 then the potential difference between both plates connected is different so the charge redistribution should be asymmetrical I,e both plates doesn't necessarily have equal and opposite charges? In my textbook, for series combination it is written that the capacitors must have equal and opposite charges on both plates because if they wouldn't then there will be an electric field inside the conductor connecting both capacitors and that would redistribute the charges until they acquire equal and opposite charge because as the plates as very close the electric field due to positive plate and negative plate are almost equal and opposite at any point inside the conductor, which seems to be logical but there is a problem in it. if we go by this explanation then when two capacitors having equal and opposite charges on both plates are connected then the charge redistribution should never occur because the net electric field inside the conductors connecting the two is 0 always, isn't it? please help me to understand this at my level, please provide me with a logical explanation for my confusions. EDIT -since my people are confused with what I am thinking I am adding one more image with what I meant by 0 electric field due to equal and opposite charges on the plates,",
      "answer_body": "From one of your comments I read In my second statement I am asking why do always the plates of capacitor gets an equal and opposite charge no matter how we connect it in any circuit Allow me to show that this is in general (and in general I mean not in a circuit) not the case. The isolated capacitor The plates of an isolated capacitor can host different amounts of charge, if the net charge of the system is not zero to begin with. Using parallel plate capacitors makes it easy to see that what is equal (and opposite in sign) is the charge on the facing sides of each plate. This should actually represent portions of infinite facing planes, to the electric field lines should all be vertical They must have taught you the phenomenon of electrostatic induction. So try to think what happens when - in the distant voids of sidereal space, far from any other conductors - you bring a charged conducting plate with positive net charge Q near another identical, but neutral, plate. Let's say we just materialize it there, at a distance d, like so: The positive charge on the first plate will attract the negative charge on the nearest face of the neutral plate, leaving the opposite face positively charged. At equilibrium (we need a little bit of time for the electric field to propagate - say d/c - and the charge to rearrange - say a multiple of the relaxation time of the conductor's material), only half of the total charge Q will be present (with opposite sign) on the inner sides of the newly formed capacitor. If I understand you right, you have a problem in accepting the fact that the 'inner' charges are equal (and opposite). Well, mathematically this results from solving a system of equations that relate voltage and charge in a multi-conductor system. This might be above the level taught in high school (but you can find a lucid description in Pollack and Stump's \"Electromagnetism\" textbook, if you wish), so let's try to see it in a more intuitive way. Faraday Lines and Tubes of Flux Are you familiar with the concept of Faraday's lines? They should be taught in a high school course. Basically, they are a way to represent the electric field orientation and strength in space. Electric field lines emerge from positive charges and sink into negative charges. The more lines are present in a region of space, the stronger is the electric field. A better way to see this is through the concept of tubes of flux. If you have been introduced to such a concept, then you should know that The charges Q1 and Q2 on the conductor's areas that are at the extremes of a tube of flux are equal in magnitude and opposite in sign. $$Q1= Q, Q2 = -Q$$ The electric flux through an arbitrary cross section of the tube of flux is given by $$flux = Q/{\\epsilon}_0$$ The relevant consequences are summarized in this quote from Branko Popovic's \"Introductory Engineering Electromagnetism\" textbook (p. 49): \"The entire region in which the electric flux exists can be divided into tubes of equal flux. Each tube can then be represented by a single line of force (say, the line of force along its axis). Since the tube are supposed to be of equal flux, the charges on which they end are also equal. We can represent these charges by a single plus or minus sign. If these conventions are adopted, the magnitude of the electric field intensity is proportional to the density of the lines of force at a point, and the charge density is proportional to the density of the plus and minus signs.\" Now, try to imagine the field lines leaving the inner side of the first plate originating from a given fraction of charge there, they will have to end on the the inner side of the second plate on the same amount of charge there. If it can help you, imagine that each charge represented by a single plus or minus sign can only shoot out (if positive) or sink (if negative) one field line, do you see now why you need to have the same amount of charge facing in the interior of the capacitor? You can't have dangling electric field lines starting from a charge and not ending in another charge. Likewise, the field lines reaching a negative charge cannot come from an empty point in space. The connected capacitor Now, when you connect one plate to Earth - i.e. a reservoir of charge that can supply and balance any charge you need without changing its potential, you end up losing the extra charge on the outer sides of the plates and all you are left with is the equal and opposing charges on the inner faces. The rationale behind this is that the Earth is so big (i.e. it has a humongous self-capacitance) that whatever charge resides on its surface (if you are curios follow this link ) ends up so diluted that it appears to have none on the small partial surfaces offered by everyday objects and electronic components connected to it. It just appears that earthing a conducting object drives away any excess charge on it, leaving only the charge that is electrostatically induced by the nearby non-grounded objects. When we connect a capacitor in a circuit, even if it is not grounded, there is another mechanism that ensures that there won't be excess charge on the outer sides of the plates. Batteries are inherently neutral, so when they offer a charge +Q at the plus terminal, they will have a charge -Q at the minus terminal. If the capacitor you connect is neutral to begin with, you will necessarily reach an equilibrium where positive and negative plates of the capacitor will have identical but opposite charge - charge that once equilibrium is established will end up on the internal facing plates. The plates ends up as being an extension of the battery contacts. That's why in a circuit context we say that capacitors do NOT store charge, but instead they displace it. In the following I will only consider neutral capacitors, meaning that each capacitor is 'charged' with equal and opposite charges on its facing plates and no residual charge is to be found (in the approximation of negligible fringe effects or of infinite parallel plated capacitor) on the exterior surfaces of the plates. Multiple connected capacitors I believe part of your confusion stems from the fact that much depends on how the connection happens: are the capacitors isolated and already charged? Are they connected in a (possibly but not necessarily earthed) circuit and then 'charged'? Let's see what happens in the parallel and series configuration when isolated, pre-charged capacitors are connected and then when the same 'uncharged' configuration is connected to a battery (with a convenient internal resistance to avoid un-physical behavior). Let's start with two isolated capacitors, each charged independently with charges Q1 for the first one, and Q2>Q1 for the second one. The caps are overall neutral, so the charge will be on the opposing internal faces of the plates. For the intensity of the electric field I am using the number of lines per unit area (the caps are flat and since we neglect fringe effects the field lines are perpendicuar to the plates' surface, ie the flux is the product of the field strenght with the area - if you want to know the voltage, integrate the field or just multiply by the distance, with the correct sign, I do not want to be bothered by these details) Parallel capacitors Now, let's see what happens when we connect them in parallel with the same polarity. The left plate of the first cap, which carried charge +Q1, and the left plate of the second cap, which carried charge +Q2 are now basically a single plate with total charge +(Q1+Q2). The charge cannot go anywhere, and all you have by connecting the plate with a piece of conductor is another conductor. So, in this sense you consider each couple of plates as a single conductor. The charge will in general redistribute in order to give a uniform electric field between the newly formed plates and zero field inside the conductor. If the caps have the same area, and we do not mess with the distance d between plates, we end up with a two-plates structure with charge Q1+Q2 on a plate of double area on one side, and the opposite charge on the other side. What would the field be in our units of lines per unit area? Correct, the average of the field of the two isolated caps. I have added two more pictures where the area of the capacitors are different (with the same overall amount of total charge Q1+Q2) to show that the field inside is the same, and so is the potential difference across the distance d. That's what you would expect from a parallel connection: both devices are subject to the same voltage which, for identical capacitors, happens to be the average of the voltages of the separated charged caps. If we connect the isolated caps with reverse polarity, we get a partial cancellation of charge on each plate (the total charge on each newly formed plate would be Q2-Q1 (if we invert the first one wrt to the previous configuration) and the field inside the newly formed cap will be considerably weakened (zero, if the separated charges were identical). In this case the total field witl be only 2 lines per unit area and the resulting voltage will be half the difference of the voltages of the separated caps. Now, when you connect the parallel to a battery you won't see anything particularly different because the bottom line of the situation is that of having identical opposite charge on the two plates of the parallel configuration. Of course, now it's the battery that imposes the voltage, so the charge will follow from that. Things can be different though when we consider a series configuration, as your instinct told you (so, here is my +1 to your question) Series capacitors Again, let's start with two isolated charged capacitor, one with charge Q1 (meaning +Q1 on one plate and -Q1 on the other) and the other with charge Q2 > Q1. We bring them together putting one after another, but still as an isolated system. When we put them in series with the same polarity we get something like this (I added vectors that represent the field generated by the sheets of charge) Note that we have different charges on the end plates: its Q1 on the leftmost plate and -Q2 on the rightmost plate. It has to be this way because the plates are isolated and they cannot change their charge (this is an ideal system without any leakages). The middle section, composed of two joined plates now is a single piece of conductor with charge Q2-Q1. In the case of Q1=Q2 the net charge on this section would be zero, but charge will still be separated due to the electrostatic induction effect of the charged exterior plates. So, in this case I would say that there is no charge redistribution, but you could see charge separation if you assembled this capacitor by bringing close to each other the two outermost charged plates and the neutral middle section. Whether or not charge will be distributed will depend on how you assemble the final configuration. Now, something different happens when we have a series capacitor connected to a battery. In this case the charge on the outermost plates is imposed by the battery connection and we can no longer have different charges on the outer plates. We start from +Q on the leftmost plate and -Q on the rightmost one, and the inner section responds by polarizing itself via electrostatic induction. fig series of two caps - connected to battery At equilibrium, the neutral section will see its charge displaced so that -Q faces the +Q on the left, while a charge +Q faces the -Q charge on the right. This will also happen with any series of capacitors: the charge is the same (sign apart) on all the plates: what changes when the capacity is different, is the electric field between plates, hence the voltage drop across each capacitor. One more thing: What happens in the isolated case when we place the series capacitors in opposition? If we place the caps in series with opposite polarity, we will have a new structure with +Q1 on the first plate, -Q1+Q2 on the central section, and +Q2 on the rightmost plate. If Q2>Q1, we have a central section that has a negative overall charge, while both exterior plates carry a positive charge. The system is overall neutral, as were the two separated capacitors before connection, but the new isolated structure will show a field like this fig isolated series of two caps - opposite polarity As you can see, the final result depends on how the constituent parts are put together, and on the connections - or more generally, interactions - with the rest of the world. If we bring a non-neutral isolated capacitor (or even a neutral real capacitor with fringe effects) near a bigger conducting body connected to the Earth - something we could call a ground plane - even if we do not connect any part of it to the body, electrostatic induction will displace charges in the bigger body producing a different configuration in the distribution of charge and the values of the potentials. When you have several conductors, it is best to approach the problem by writing a system of equations in the coefficients of potential or in the coefficients of electrostatic induction (or, as some call them, coefficients of capacity). Trying to solve such problems with intuition could easily lead to the wrong solution (and this post is no exception!).",
      "question_latex": [],
      "answer_latex": [
        "Q1= Q, Q2 = -Q",
        "flux = Q/{\\epsilon}_0",
        "</span></p>\n<ol start=\"2\">\n<li>The electric flux through an arbitrary cross section of the tube of flux is given by</li>\n</ol>\n<p><span class=\"math-container\">"
      ],
      "created": "2021-06-09T07:49:46.043",
      "golden_ner_terms": [
        "approximation",
        "area",
        "average",
        "axis",
        "balance",
        "batteries",
        "behavior",
        "body",
        "bottom",
        "capacity",
        "central",
        "charge",
        "circuit",
        "combination",
        "components",
        "concept",
        "conductor",
        "conductors",
        "configuration",
        "connected",
        "connection",
        "context",
        "cross",
        "cross section",
        "density",
        "difference",
        "distance",
        "distribution",
        "earth",
        "easy to see",
        "electromagnetism",
        "entire",
        "equilibrium",
        "even",
        "extension",
        "exterior",
        "face",
        "field",
        "flat",
        "flow",
        "flux",
        "fraction",
        "generated by",
        "grounded",
        "ideal",
        "image",
        "induced",
        "induction",
        "infinite",
        "inner",
        "integrate",
        "intensity",
        "interactions",
        "interior",
        "intuition",
        "isolated",
        "l system",
        "level",
        "line",
        "link",
        "matter",
        "mean",
        "multiple",
        "near",
        "negative",
        "net",
        "number",
        "object",
        "opposite",
        "order",
        "orientation",
        "outer",
        "parallel",
        "place",
        "plane",
        "plus",
        "point",
        "polarity",
        "polarizing",
        "positive",
        "potential",
        "product",
        "proportional",
        "real",
        "region",
        "residual",
        "right",
        "scenario",
        "section",
        "separated",
        "separation",
        "series",
        "side",
        "sink",
        "solution",
        "space",
        "strength",
        "stronger",
        "structure",
        "surface",
        "terminal",
        "time",
        "unit",
        "vectors",
        "voltage",
        "way",
        "zero"
      ],
      "golden_ner_count": 107,
      "golden_patterns": [
        {
          "pattern": "check-the-extreme-cases",
          "score": 4.0,
          "hotwords": [
            "extreme",
            "zero"
          ]
        },
        {
          "pattern": "dualise-the-problem",
          "score": 2.0,
          "hotwords": [
            "dual"
          ]
        },
        {
          "pattern": "induction-and-well-ordering",
          "score": 2.0,
          "hotwords": [
            "induction"
          ]
        },
        {
          "pattern": "encode-as-algebra",
          "score": 2.0,
          "hotwords": [
            "ring"
          ]
        },
        {
          "pattern": "construct-auxiliary-object",
          "score": 2.0,
          "hotwords": [
            "introduce"
          ]
        }
      ],
      "golden_pattern_names": [
        "check-the-extreme-cases",
        "dualise-the-problem",
        "induction-and-well-ordering",
        "encode-as-algebra",
        "construct-auxiliary-object"
      ],
      "golden_scopes": [],
      "golden_scope_count": 0
    },
    {
      "id": "se-physics-272971",
      "stratum": "medium",
      "title": "Does light from a super continuum laser remain coherent?",
      "tags": [
        "electromagnetism",
        "optics",
        "laser",
        "non-linear-optics",
        "coherence"
      ],
      "score": 5,
      "answer_score": 4,
      "question_body": "Is the broad spectrum light from a super continuum (white) laser system when filtered for a particular wave length still temporally coherent to a similar degree as the source laser? i.e. Does the part of the resultant beam of a specific wavelength still have the same temporal characteristics of a laser or is it more like an incoherent source such as monochromatic light filtered from the sun.",
      "answer_body": "Temporal coherence$^{\\text{1}}$ in supercontinuum generation (SCG) is a very active area of research in ultrafast optics at the moment. This is in part because there are a lot of exciting applications which rely on broadband coherent light, but also because there are so many nonlinear processes involved in supercontinuum generation that a full understanding of how they each interact to influence the coherence will take time. This means your question is a very exciting one, but also that it is extremely broad and difficult to answer. This answer is largely based on information which can be found in Agrawal , this review article on SCG using anomalous dispersion pumping, and this article on SCG pumping in the normal dispersion regime, and includes simulations covering just four relatively broad example cases: Two where the coherence of the supercontinuum pump source is conserved, and two where it is destroyed. As there are plenty of other sources which go into detail about the underlying physics of each nonlinear process involved, I've left this information out except where it's necessary$^{\\text{2}}$. Unfortunately this means assuming a degree of familiarity with SCG, but I will link some references at least. There are also a few extra modes of coherence degradation which I have not described, but are also detailed here . Firstly, in the SCG community coherence is generally defined in the spectral domain as follows: $\\left|g_{1,2}^{(1)}(\\lambda, t_{1}-t_{2})\\right| = \\left|\\frac{\\langle A_{1}^{*}(\\lambda, t_{1})A_{2}(\\lambda, t_{2}) \\rangle}{\\sqrt{\\langle |A_{1}(\\lambda, t_{1})|^{2} \\rangle\\langle |A_{2}(\\lambda, t_{2})|^{2} \\rangle} } \\right|$ $\\langle \\rangle$ denote an ensemble average over independently generated pairs of spectra $A(\\lambda)$, emitted by the supercontinuum source at different times $t_{1}$ and $t_{2}$. As SCG is most often carried out using pretty stable modelocked lasers, I will assume that these pairs of spectra are nearly identical at the input of the optical fibre except for a noise contribution. $|g_{1,2}^{(1)}|$ is a measure of amplitude and $^{\\text{3}}$ phase stability as a function of wavelength, defined over the interval [0; 1] (with 1 indicating perfect coherence and 0 complete decoherence), and quantifies the sensitivity of the spectral broadening to the noise on the input signal. PCF is widely used for supercontinuum generation because the fibre structure can be adjusted to customize the dispersion curve, and because the mode confinement can be very small allowing for extremely high optical intensities which provide a lot of nonlinearity. I will also assume single-mode silica PCF here. Propagation in these fibres is usually modelled using the generalized nonlinear Schrödinger equation. In short, this is a partial differential equation including two terms, one for loss and dispersion (linear term), and the other for the intensity-dependent material response (nonlinear term). Both terms contribute to whether the coherence is preserved or destroyed. Although the mechanism responsible for this can be loosely interpreted as a competition between the following nonlinear processes (not exhaustive): Self-phase modulation Soliton fission The Raman effect Modulation instability it is arguably better to categorize it by the dispersion regime, i.e., whether the input optical signal is launched in the anomalous side of the PCF dispersion slope, or the normal side. Anomalous dispersion is characterised by short wavelengths having a higher group velocity than longer wavelengths, and normal dispersion is characterised by long wavelengths having a higher group velocity than shorter wavelengths. Some useful quantities which I will refer to: $L_{NL} = \\frac{2n_{2}}{P_{0}\\lambda(\\text{MFD}/2)^{2}}$ is the nonlinear length, and is the propagation distance over which the accumulated nonlinear phase reaches $2\\pi$. $n_{2}=2.7\\times 10^{-20}$ m$^{2}$/W is the nonlinear refractive index for silica, MFD is the mode field diameter, $P_{0}$ is the peak power of the pulse which has a wavelength $\\lambda$. $L_{D} = \\frac{T_{0}^{2}}{|\\beta_{2}|}$ is the dispersion length, and is the propagation distance over which the accumulated dispersive phase reaches $2\\pi$. $T_{0}$ is the FWHM duration of the pulse, and $\\beta_{2}=\\frac{-\\lambda^{2}D(\\lambda)}{2\\pi c}$ is the group velocity dispersion in units of fs$^{2}$/m ($D(\\lambda)$ is equivalent and has units of ps/(nmkm)). Dispersion regimes and (very brief) summary of the dynamics. Here's a couple of typical PCF dispersion curves designed for pumping around 1 $\\mu$m (e.g., using an Yb-doped fibre laser): The one on the left has both normal and anomalous dispersion regions, with a zero-dispersion wavelength at 1 $\\mu$m inbetween. For this dispersion curve, supercontinuum is usually generated by pumping on the anomalous dispersion side, close to the zero-dispersion wavelength. Both coherent and incoherent spectral broadening are then driven by soliton propagation and dispersive wave generation in the fibre ( ref ). Whether the output spectral coherence is high or low depends on how the initial pulse divides into fundamental solitons, and this will be described in the first section below. In reference to the competition between nonlinear processes outlined above, the input pulse bandwidth and duration determine whether it divides by soliton fission (coherent), or due to modulation instability (MI, incoherent). The dispersion curve on the right has only normal dispersion, and fibres with this kind of dispersion profile are referred to as all-normal dispersion (ANDi) fibre. Supercontinuum is usually generated by pumping at the dispersion minimum to maximize the peak intensity over the length of the fibre. In reference to the competition between nonlinear processes, coherence is either preserved or destroyed depending on whether optical wave breaking occurs (coherent), or whether there is significant Raman gain over the fibre length (incoherent). This process is described in the second section below. Example 1: Incoherent and coherent broadening when pumping in the anomalous dispersion regime. The image below shows incoherent supercontinuum generation over a 6 cm fibre length with a dispersion curve similar to that shown in the left-hand dispersion figure above. The input pulse was transform limited with a duration of 1 ps, central wavelength of 1040 nm, and energy of 30 nJ. The top figure shows the spectral development as a function of fibre length (spectral power density units of dBm/nm), and the bottom figure shows the coherence as a function of fibre length for the same simulation. Initially the pulse undergoes SPM, which broadens the spectrum coherently with a linear dependence on fibre length. Modulation instability (MI) quickly follows, characterized by two symmetric sidebands appearing either side of the SPM-broadened pump. MI is an amplification process, and goes like $\\text{exp}(g_{\\text{MI}}z)$ causing rapid amplification of the sidebands after ~1 cm. After around 2.5 cm, four-wave mixing causes the MI process to cascade leading to very rapid bandwidth expansion. MI arises because high peak power pulse propagation in the anomalous dispersion regime is unstable under the right conditions, and a perturbation analysis of the nonlinear Schrödinger equation shows that the SCG output is highly sensitive to intensity modulations (e.g., from input noise), which is amplified according to the following gain curve: $g=2\\sqrt{-\\beta_{2}^{2}\\omega_{m}-2\\gamma P\\beta_{2}\\omega_{m}^{2}}$ for $\\omega_{m}<\\frac{-2\\gamma P}{\\beta_{2}}$ $g=0$ for $\\omega_{m}\\ge \\frac{-2\\gamma P}{\\beta_{2}}$ where $\\omega_m$ is the difference between the modulation angular frequency and the pulse central angular frequency, $P$ is the peak power, and $\\gamma=(2\\pi n_{2})/(\\lambda (\\text{MFD}/2)^{2})$. This gain curve shapes the MI sidebands shown in the figure. This amplification of out-of-band noise results in interference with the residual input signal to create noisy, ultrafast and high peak power modulations in the time domain. The cascaded MI increases the time-domain interference and noise, resulting in modulations which can evolve into fundamental solitons. These then shed energy to high-frequency dispersive waves as they propagate and also undergo soliton self-frequency shifting to longer wavelengths, giving extended spectral broadening beyond that from MI. This latter process isn't shown in the figure, but would occur if the fibre length was extended. As this process is seeded by noise, the supercontinuum output will be incoherent. The coherence figure shows this, as the MI sidebands have zero coherence. The coherence of the residual pump also degrades rapidly as MI randomly strips energy from the signal. The figure below shows the output time and wavelength domain ensemble distributions. The bold line is the ensemble average, and the shaded areas show the individual simulations in the ensemble. Although each pulse had the same time and spectral domain shapes, the different input noise has had a drastic influence on the propagation dynamics, with no two output pulses being alike. This is the consequence of low coherence, and filtering this spectrum at any particular wavelength would not give a stable time domain output. MI can be suppressed to give highly coherent spectral broadening by using shorter pump pulses, which have a correspondingly broader bandwidth. This suppression occurs partly because the broad, coherent input bandwidth seeds the MI gain spectrum, reducing the influence of noise, and also because the broader bandwidth is more susceptible to higher-order dispersion and inter-pulse Raman scattering, which perturb the input pulse and cause it to fission into fundamental solitons more quickly than MI can amplify noise. This happens over a length scale given by: $L_{\\text{fission}}=\\sqrt{L_{D}L_{NL}}$ As these two perturbations are insensitive to noise, the resulting SCG is coherent. If $L_{\\text{fission}}$ is smaller than the length required for significant MI buildup, the output spectral coherence will be high. This is precisely what is happening in the figure above, where the input pulse was 80 fs long, had a 1040 nm central wavelength, and energy of 1 nJ. The broad input bandwidth of the short pulse undergoes a small amount of SPM before soliton fission occurs, causing explosive spectral broadening just millimetres along the fibre length. The pulse gradually fissions into its constituent fundamental soliton parts, shedding excess energy as a high-frequency dispersive wave on the other side of the zero dispersion wavelength as it does so. Inter-pulse Raman scattering causes these solitons to gradually shift to lower frequencies which gradually broadens the spectrum with propagation distance. This process is called soliton self-frequency shifting ( SSFS in the figure). The lower figure shows that these processes preserve coherence, which is very high over the whole supercontinuum bandwidth. Filtering this spectrum over a smaller wavelength band will give a stable output in the time domain. The time and wavelength domain supercontinuum shapes are shown in the figure above. Unlike the case where MI was dominant, the individuals in the ensemble are indistinguishable indicating that the broadening process was insensitive to input noise. Example 2: Incoherent and coherent broadening when pumping in ANDi fibre. As with anomalous dispersion pumping, longer pulses more readily lead to incoherent spectral broadening in the ANDi regime. The figure below shows how a 7 ps input pulse with 60 nJ energy and central wavelength of 1040 nm propagates in the ANDi fibre shown in the right-hand dispersion figure. Again, SPM acts first to coherently broaden the input spectrum linearly with propagation distance. However, as ANDi fibres generally support dissipative high-intensity solutions, MI doesn't play a role in the spectral broadening in this dispersion regime and the Raman effect becomes significant instead. This is evident in the spectral figure, where a broad Stokes peak develops at the low-frequency side of the input pulse spectrum. As was the case with MI, the Raman effect in the ANDi regime generally leads to the amplification of out-of-band noise, with a gain bandwidth set by the material Raman response (gain peak at 13.2 THz detuning below the peak frequency of the input spectrum for silica). The Stokes wave is quickly followed by an anti-Stokes wave at the high-frequency side of the input spectrum. After approximately 60 cm of propagation, four-wave mixing causes the Raman effect to cascade, forming higher-order (anti-)Stokes peaks. As the (anti-)Stokes peaks are seeded by noise, they have zero coherence. They are amplified during propagation and use the input pulse as a pump source, so the input pulse is randomly depleted over the fibre length causing its coherence to degrade as well. This is evident in the coherence figure at approximately 90 cm, where the coherence is beginning to degrade at the low-frequency side of the SPM-broadened input spectrum. Looking at the time and wavelength domain output also shows how sensitive the propagation dynamics are to the input noise. The output stability degrades at the leading edge of the pulse in the time domain because the dominant red-shifted Stokes peaks in the output spectrum are brought to the front of the pulse by the normal dispersion. The Stokes and anti-Stokes portions of the spectrum show large-scale instability as well at the long and short wavelength sides of the spectrum. Filtering a smaller wavelength band would not produce a stable output. It is possible to define a length over which Raman gain becomes significant, and this requires a gain coefficient with a dependence on the ratio of chromatic dispersion to nonlinearity, and describes the coupling between Raman and four wave mixing: $g=2\\gamma \\text{Re}\\left[ \\sqrt{K(2q-K)} \\right]$ where $K = -(\\beta_{2}\\Omega_{R}^{2})/(2\\gamma P)$ is the linear phase mismatch between the pump and (anti-)Stokes wave with respect to the nonlinear contribution to the mismatch. $q$ is the (anti-)Stokes order, $\\Omega_{R}$ is the frequency detuning which maximizes the Raman gain (13.2 THz for silica). For low phase mismatches ($|K|<1$), Raman is suppressed. As $|K|$ approaches infinity, Raman can contribute more. This is interesting in its own right, because it shows that the four wave mixing which cascades the Raman effect in the example above can also be very effective in suppressing it entirely if the peak power is very high. Using this, the Raman length is given by: $L_{R}=1/(gP)$. By pumping with shorter pulses, coherence degradation in the ANDi regime can largely be prevented. As the pulse duration is decreased, dispersion and SPM have a greater influence and can lead to optical wave breaking (OWB), shown in the figure below. The output spectrum is extremely flat, and has very high coherence over the entire bandwidth. Filtering the spectrum for a small wavelength band will produce a stable time domain output. Looking at each simulation in the ensemble shows not only that the broadening is completely insensitive to input noise, but also that the process preserves the pulse time domain distribution, which is single-peaked. Wave breaking occurs over a length scale approximated by: $L_{\\text{WB}}\\approx 1.1T\\sqrt{\\frac{1}{\\gamma P\\beta_{2}}}=1.1\\sqrt{L_{D}L_{NL}}$ If the pulse duration at the input is gradually increased, there will be a point where wave breaking requires a longer length of fibre than is required for significant Raman gain. As the competition between these two processes is central to the coherence conservation in this example, it is important to find which combination of input pulse duration and fibre length will give incoherent/coherent output. A coherence length will do this, and is given by taking the ratio of $L_{R}$ to $L_{WB}$: $L_{c}\\propto \\frac{L_{R}}{L_{WB}}\\propto \\frac{1}{f_{R}\\Omega_{R}T}$, where $f_{R}=0.18$ for fused silica. So, if the pulse duration is reduced such that $L_{WB}$ is less than $L_{R}$, the output spectral coherence will be high. Note also that the parameter $K$ depends inversely on the peak power. High peak powers are commonly associated with short pulse durations, so when the input pulse duration is short Raman will be suppressed at the start of the propagation before dispersion becomes significant. This restriction doesn't apply to wave breaking, so this becomes the favourable route for spectral broadening in ANDi fiber at high peak powers and short pulse durations. This holds for peak powers in excess of 100 kW, and even if the peak power is only reduced by a factor of ~2 by dispersion after wave breaking takes place the peak power can still be high enough to suppress Raman very efficiently, giving excellent output spectral coherence. 1 Shortened to \"coherence\" from now on. 2 An explanation of supercontinuum generation is out of the scope of the question, which is about coherence. 3 I have seen a few instances of spectral amplitude being plugged into this equation with no phase information, and this is inappropriate .",
      "question_latex": [],
      "answer_latex": [
        "^{\\text{1}}",
        "^{\\text{2}}",
        "\\left|g_{1,2}^{(1)}(\\lambda, t_{1}-t_{2})\\right| = \\left|\\frac{\\langle A_{1}^{*}(\\lambda, t_{1})A_{2}(\\lambda, t_{2}) \\rangle}{\\sqrt{\\langle |A_{1}(\\lambda, t_{1})|^{2} \\rangle\\langle |A_{2}(\\lambda, t_{2})|^{2} \\rangle} } \\right|",
        "\\langle \\rangle",
        "A(\\lambda)",
        "t_{1}",
        "t_{2}",
        "|g_{1,2}^{(1)}|",
        "^{\\text{3}}",
        "L_{NL} = \\frac{2n_{2}}{P_{0}\\lambda(\\text{MFD}/2)^{2}}",
        "2\\pi",
        "n_{2}=2.7\\times 10^{-20}",
        "^{2}",
        "P_{0}",
        "\\lambda",
        "L_{D} = \\frac{T_{0}^{2}}{|\\beta_{2}|}",
        "T_{0}",
        "\\beta_{2}=\\frac{-\\lambda^{2}D(\\lambda)}{2\\pi c}",
        "D(\\lambda)",
        "\\mu",
        "\\text{exp}(g_{\\text{MI}}z)",
        "g=2\\sqrt{-\\beta_{2}^{2}\\omega_{m}-2\\gamma P\\beta_{2}\\omega_{m}^{2}}",
        "\\omega_{m}<\\frac{-2\\gamma P}{\\beta_{2}}",
        "g=0",
        "\\omega_{m}\\ge \\frac{-2\\gamma P}{\\beta_{2}}",
        "\\omega_m",
        "P",
        "\\gamma=(2\\pi n_{2})/(\\lambda (\\text{MFD}/2)^{2})",
        "L_{\\text{fission}}=\\sqrt{L_{D}L_{NL}}",
        "L_{\\text{fission}}",
        "g=2\\gamma \\text{Re}\\left[ \\sqrt{K(2q-K)} \\right]",
        "K = -(\\beta_{2}\\Omega_{R}^{2})/(2\\gamma P)",
        "q",
        "\\Omega_{R}",
        "|K|<1",
        "|K|",
        "L_{R}=1/(gP)",
        "L_{\\text{WB}}\\approx 1.1T\\sqrt{\\frac{1}{\\gamma P\\beta_{2}}}=1.1\\sqrt{L_{D}L_{NL}}",
        "L_{R}",
        "L_{WB}",
        "L_{c}\\propto \\frac{L_{R}}{L_{WB}}\\propto \\frac{1}{f_{R}\\Omega_{R}T}",
        "f_{R}=0.18",
        "K"
      ],
      "created": "2016-08-07T00:03:37.540",
      "golden_ner_terms": [
        "analysis",
        "area",
        "average",
        "band",
        "bottom",
        "central",
        "coefficient",
        "coherence",
        "combination",
        "complete",
        "confinement",
        "consequence",
        "continuum",
        "covering",
        "curve",
        "decoherence",
        "degree",
        "density",
        "development",
        "diameter",
        "difference",
        "differential",
        "differential equation",
        "dispersion",
        "distance",
        "distribution",
        "divides",
        "domain",
        "dominant",
        "edge",
        "effective",
        "energy",
        "entire",
        "equation",
        "equivalent",
        "even",
        "expansion",
        "factor",
        "fiber",
        "fibre",
        "field",
        "flat",
        "frequency",
        "function",
        "generated by",
        "group",
        "image",
        "index",
        "indistinguishable",
        "infinity",
        "information",
        "intensity",
        "interference",
        "interval",
        "involved in",
        "laser",
        "length",
        "line",
        "linear dependence",
        "link",
        "measure",
        "mixing",
        "mode",
        "moment",
        "noise",
        "normal",
        "optics",
        "order",
        "parameter",
        "partial differential equation",
        "perfect",
        "perturbation",
        "physics",
        "place",
        "point",
        "power",
        "preserve",
        "ratio",
        "reduced",
        "residual",
        "restriction",
        "resultant",
        "right",
        "scattering",
        "scope",
        "section",
        "side",
        "similar",
        "simulation",
        "simulations",
        "slope",
        "soliton",
        "solitons",
        "source",
        "spectrum",
        "stability",
        "stable",
        "structure",
        "sun",
        "support",
        "symmetric",
        "term",
        "time",
        "top",
        "transform",
        "unstable",
        "useful",
        "velocity",
        "wavelength",
        "waves",
        "zero"
      ],
      "golden_ner_count": 111,
      "golden_patterns": [
        {
          "pattern": "check-the-extreme-cases",
          "score": 6.0,
          "hotwords": [
            "extreme",
            "zero",
            "infinity"
          ]
        },
        {
          "pattern": "work-examples-first",
          "score": 4.0,
          "hotwords": [
            "example",
            "e.g."
          ]
        },
        {
          "pattern": "monotone-approximation",
          "score": 4.0,
          "hotwords": [
            "approximate",
            "limit"
          ]
        },
        {
          "pattern": "find-the-right-abstraction",
          "score": 2.0,
          "hotwords": [
            "generalize"
          ]
        },
        {
          "pattern": "exploit-symmetry",
          "score": 2.0,
          "hotwords": [
            "symmetric"
          ]
        },
        {
          "pattern": "construct-an-explicit-witness",
          "score": 2.0,
          "hotwords": [
            "build"
          ]
        },
        {
          "pattern": "dualise-the-problem",
          "score": 2.0,
          "hotwords": [
            "dual"
          ]
        },
        {
          "pattern": "local-to-global",
          "score": 2.0,
          "hotwords": [
            "cover"
          ]
        },
        {
          "pattern": "encode-as-algebra",
          "score": 2.0,
          "hotwords": [
            "ring"
          ]
        },
        {
          "pattern": "use-probabilistic-method",
          "score": 2.0,
          "hotwords": [
            "random"
          ]
        },
        {
          "pattern": "estimate-by-bounding",
          "score": 2.0,
          "hotwords": [
            "at least"
          ]
        },
        {
          "pattern": "split-into-cases",
          "score": 2.0,
          "hotwords": [
            "cases:"
          ]
        },
        {
          "pattern": "optimise-a-free-parameter",
          "score": 2.0,
          "hotwords": [
            "maximize"
          ]
        },
        {
          "pattern": "transport-across-isomorphism",
          "score": 2.0,
          "hotwords": [
            "equivalent"
          ]
        }
      ],
      "golden_pattern_names": [
        "check-the-extreme-cases",
        "work-examples-first",
        "monotone-approximation",
        "find-the-right-abstraction",
        "exploit-symmetry",
        "construct-an-explicit-witness",
        "dualise-the-problem",
        "local-to-global",
        "encode-as-algebra",
        "use-probabilistic-method",
        "estimate-by-bounding",
        "split-into-cases",
        "optimise-a-free-parameter",
        "transport-across-isomorphism"
      ],
      "golden_scopes": [
        {
          "type": "where-binding",
          "match": "where $\\omega_m$ is"
        },
        {
          "type": "where-binding",
          "match": "where $K = -(\\beta_{2}\\Omega_{R}^{2})/(2\\gamma P)$ is"
        }
      ],
      "golden_scope_count": 2
    },
    {
      "id": "se-physics-473093",
      "stratum": "hard",
      "title": "Expressing the Schrödinger equation in 2nd quantised language",
      "tags": [
        "homework-and-exercises",
        "quantum-field-theory",
        "operators",
        "schroedinger-equation",
        "second-quantization"
      ],
      "score": 2,
      "answer_score": 2,
      "question_body": "For times sake, I will only write about the non-interacting part of the Hamiltonian, $$H_0=\\sum_{j=1}\\left(-\\frac{\\hbar^2}{2m}\\frac{\\partial}{\\partial x_j^2}+U(x_j)\\right)$$ where $U(x_j)$ is some scalar potential. In second quantised language the wave function is given by $$|\\phi_\\alpha\\rangle=\\int dx_1\\cdots dx_N\\,\\phi_\\alpha(x_1,\\cdots,x_N)\\psi^T(x_1)\\cdots\\psi^T(x_N)|0\\rangle.$$ The Hamiltonian: $$H_{0,s}=\\int dx\\,\\psi^T(x)H_0\\psi(x).$$ SHOW $$H_{0,s}|\\phi_\\alpha\\rangle=\\int dx_1\\cdots dx_N\\,(H_0\\phi_\\alpha(x_1,\\cdots,x_N))\\psi^T(x_1)\\cdots\\psi^T(x_N)|0\\rangle.$$ I then set the basis vectors to be the wavefunction $\\psi_\\alpha$ for conveinence. So, I started by simply writing out: $$\\int dx\\,\\psi^T(x)H_0\\psi(x)\\int dx_1\\cdots dx_N\\,\\phi_\\alpha(x_1,\\cdots,x_N)\\psi^T(x_1)\\cdots\\psi^T(x_N)|0\\rangle$$ and using the definition of the position dependent density operator $\\rho(x)=\\psi^T(x)\\psi(x)$ . Since the basis vectors are the wave function, we can commute $H_o$ around and get $$\\int dx\\,\\psi^T(x)\\psi(x)H_0=\\int dx\\,\\rho(x)H_0=H_0$$ since we are in 1D in a 1 particle state, the volume integral over the density will just 1. Then one can just use the same thing of the basis vectors to get $H_0$ into place in the final expression. This is a homework question, and I don't feel satisfied with this as an answer; is it correct, or completely wrong? I'm not sure about the justification for the commutation of $H_0$ , it feels too flimsy and my grasp of the linear algebra required here is somewhat lacking. Also where here can we consider the difference between bosons and fermions? I have derived the commutation law $[\\psi(x),\\psi^T(y)]=\\delta(x-y)$ , but i don't know where to use it. Effectively, I have some idea of whats going on, but not a complete one - and I think I've been sat with it for too long to see where I'm going wrong",
      "answer_body": "Let us define: $$ \\boxed{\\hat{H}_0(x) = -\\frac{\\hbar^2}{2m}\\frac{d^2}{dx^2}+U(x)} $$ and $$ \\boxed{\\hat{H}_{0,s} = \\int dx\\,\\Psi^T(x)\\hat{H}_0(x)\\Psi(x)} $$ When there are $N$ decoupled particles the total Hamiltonian takes the form: $$ \\boxed{\\hat{H}_0 = \\sum_{i=1}^N\\hat{H}_0(x_i)} $$ Notice that it is the single particle Hamiltonian, $\\hat{H}_0(x)$ , that appears in the definition of $\\hat{H}_{0,s}$ and not $\\hat{H}_0$ as the OP writes. Indeed, (as has been pointed out in the comments) since there is an integral over $x$ in $\\hat{H}_{0,s}$ it does not make sense to insert $\\hat{H}_0$ in $\\hat{H}_{0,s}$ . We then define the multiparticle state, $|\\phi_a\\rangle$ , with corresponding wavefunction, $\\phi_a(x_1,\\dots,x_N)$ , by: $$ \\boxed{|\\phi_a\\rangle=\\int dx_1\\dots dx_N\\phi_a(x_1,\\dots,x_N)\\Psi^T(x_1)\\dots \\Psi^T(x_N)|0\\rangle} $$ Then, using that $[\\Psi(x),\\Psi^T(y)]=\\delta(x-y)$ , \\begin{equation} \\begin{aligned} \\hat{H}_{0,s}&|\\phi_a\\rangle = \\int dx\\,\\Psi^T(x)\\hat{H}_0(x)\\Psi(x)\\int dx_1\\dots dx_N\\phi_a(x_1,\\dots,x_N)\\Psi^T(x_1)\\dots \\Psi^T(x_N)|0\\rangle\\\\ &=\\int dx\\,\\Psi^T(x)\\hat{H}_0(x)\\int dx_1\\dots dx_N\\phi_a(x_1,\\dots,x_N)\\sum_{i=1}^N\\delta(x-x_i)\\Psi^T(x_1)\\dots\\Psi^T(x_i\\!\\!\\!/)\\dots \\Psi^T(x_N)|0\\rangle\\\\ &=\\int dx\\,\\Psi^T(x)\\hat{H}_0(x)\\sum_{i=1}^N\\int dx_1\\dots dx_i\\!\\!\\!/\\dots dx_N\\phi_a(x_1,\\dots,x,\\dots,x_N)\\Psi(x_1)\\dots\\Psi^T(x_i\\!\\!\\!/)\\dots \\Psi^T(x_N)|0\\rangle\\\\ &=\\int dx\\,\\Psi^T(x)\\sum_{i=1}^N\\int dx_1\\dots dx_i\\!\\!\\!/\\dots dx_N\\big(\\hat{H}_0(x)\\phi_a(x_1,\\dots,x,\\dots,x_N)\\big)\\Psi^T(x_1)\\dots\\Psi^T(x_i\\!\\!\\!/)\\dots \\Psi^T(x_N)|0\\rangle\\\\ &=\\int dx_1\\dots dx_N\\Big(\\sum_{i=1}^N\\hat{H}_0(x_i)\\phi_a(x_1,\\dots,x_N)\\Big)\\Psi^T(x_1)\\dots \\Psi^T(x_N)|0\\rangle\\\\ &=\\int dx_1\\dots dx_N\\Big(\\hat{H}_0\\phi_a(x_1,\\dots,x_N)\\Big)\\Psi^T(x_1)\\dots \\Psi^T(x_N)|0\\rangle\\\\ \\end{aligned} \\end{equation} which is what was meant to be shown. In the second to last step one changes variables of integration and in the last step we used the definition of $\\hat{H}_0$ given above. By request I'm adding more detail: An important step in the above derivation is going from the first to the second equality. Note primarily that $\\hat{H}_0(x)$ acts on $\\Psi(x)$ , which is the only quantity to its right that depends on $x$ . Isolating the relevant terms then, and using only the commutator, $[\\Psi(x),\\Psi^T(y)]=\\delta(x-y)$ , \\begin{equation} \\begin{aligned} \\big(\\hat{H}_0(x)\\Psi(x)\\big)\\Psi^T(x_1)&=\\Psi^T(x_1)\\hat{H}_0(x)\\Psi(x)+\\hat{H}_0(x)\\delta(x-x_1)\\\\ &=\\Psi^T(x_1)\\hat{H}_0(x)\\Psi(x)+\\hat{H}_0(x)\\delta(x-x_1)\\\\ \\big(\\hat{H}_0(x)\\Psi(x)\\big)\\Psi^T(x_1)\\Psi^T(x_2)&=\\big(\\Psi^T(x_1)\\hat{H}_0(x)\\Psi(x)+\\hat{H}_0(x)\\delta(x-x_1)\\big)\\Psi^T(x_2)\\\\ &=\\Psi^T(x_1)\\big(\\hat{H}_0(x)\\Psi^T(x_2)\\Psi(x)+\\hat{H}_0(x)\\delta(x-x_2)\\big)+\\Psi^T(x_2)\\hat{H}_0(x)\\delta(x-x_1)\\\\ &=\\Psi^T(x_1)\\Psi^T(x_2)\\hat{H}_0(x)\\Psi(x)+\\Psi^T(x_1)\\hat{H}_0(x)\\delta(x-x_2)+\\Psi^T(x_2)\\hat{H}_0(x)\\delta(x-x_1)\\\\ &\\vdots \\end{aligned} \\end{equation} Now $\\Psi(y)$ is an annihilation operator, so that $\\Psi(y)|0\\rangle=0$ . The above two equations when acting on the vacuum then read: \\begin{equation} \\begin{aligned} \\big(\\hat{H}_0(x)\\Psi(x)\\big)\\Psi^T(x_1)|0\\rangle &=\\Big(\\hat{H}_0(x)\\delta(x-x_1)\\Big)|0\\rangle\\\\ \\big(\\hat{H}_0(x)\\Psi(x)\\big)\\Psi^T(x_1)\\Psi^T(x_2)|0\\rangle &=\\Big(\\Psi^T(x_1)\\hat{H}_0(x)\\delta(x-x_2)+\\Psi^T(x_2)\\hat{H}_0(x)\\delta(x-x_1)\\Big)|0\\rangle\\\\ \\end{aligned} \\end{equation} so the general result reads, \\begin{equation} \\big(\\hat{H}_0(x)\\Psi(x)\\big)\\Psi^T(x_1)\\dots \\Psi^T(x_N)|0\\rangle= \\sum_{i=1}^N\\Big(\\hat{H}_0(x)\\delta(x-x_i)\\Big)\\Psi^T(x_1)\\dots\\Psi^T(x_i\\!\\!\\!/)\\dots \\Psi^T(x_N)|0\\rangle \\end{equation} Notice here that the combination $\\hat{H}_0(x)\\delta(x-x_i)$ can pass through the quantity $\\Psi^T(x_1)\\dots\\Psi^T(x_i\\!\\!\\!/)\\dots \\Psi^T(x_N)$ without obstruction: they commute. Including now the multi-particle wavefunction contribution, \\begin{equation} \\begin{aligned} &\\big(\\hat{H}_0(x)\\Psi(x)\\big)\\phi_a(x_1,\\dots,x_N)\\Psi^T(x_1)\\dots \\Psi^T(x_N)|0\\rangle\\\\ &= \\sum_{i=1}^N\\Big(\\hat{H}_0(x)\\delta(x-x_i)\\Big)\\phi_a(x_1,\\dots,x_N)\\Psi^T(x_1)\\dots\\Psi^T(x_i\\!\\!\\!/)\\dots \\Psi^T(x_N)|0\\rangle\\\\ &= \\sum_{i=1}^N\\Big(\\hat{H}_0(x)\\delta(x-x_i)\\phi_a(x_1,\\dots,x_N)\\Big)\\Psi^T(x_1)\\dots\\Psi^T(x_i\\!\\!\\!/)\\dots \\Psi^T(x_N)|0\\rangle\\\\ &= \\sum_{i=1}^N\\Big(\\hat{H}_0(x)\\delta(x-x_i)\\phi_a(x_1,\\dots,x,\\dots,x_N)\\Big)\\Psi^T(x_1)\\dots\\Psi^T(x_i\\!\\!\\!/)\\dots \\Psi^T(x_N)|0\\rangle\\\\ \\end{aligned} \\end{equation} where in the last equality we used the delta function to replace the $x_i$ argument of the wavefunction by $x$ . Let me mention in passing that one can also write $\\hat{H}_0(x)\\delta(x-x_i)=\\hat{H}_0(x_i)\\delta(x-x_i)$ and then integrate by parts twice in $x_i$ after having included the $x_i$ integral, and this would then also make manifest that $\\hat{H}_0(x_i)$ acts on the wavefunction -- this leads to the same result. To show that all such approaches lead to the same result it suffices to realise that all of these are equivalent: $$ \\int dx_i\\Big(\\frac{d^2}{dx^2}\\delta(x-x_i)\\phi(x_i)\\Big)=\\frac{d^2}{dx^2}\\int dx_i\\Big(\\delta(x-x_i)\\phi(x_i)\\Big)=\\frac{d^2}{dx^2}\\phi(x) $$ $$ \\int dx_i\\Big(\\frac{d^2}{dx^2}\\delta(x-x_i)\\phi(x_i)\\Big)=\\int dx_i\\Big(\\frac{d^2}{dx^2_i}\\delta(x-x_i)\\phi(x_i)\\Big)=\\int dx_i\\Big(\\delta(x-x_i)\\frac{d^2}{dx^2_i}\\phi(x_i)\\Big)=\\frac{d^2}{dx^2}\\phi(x) $$ $$ \\int dx_i\\Big(\\frac{d^2}{dx^2}\\delta(x-x_i)\\phi(x_i)\\Big)=\\int dx_i\\frac{d^2}{dx^2}\\Big(\\delta(x-x_i)\\phi(x)\\Big)=\\frac{d^2}{dx^2}\\int dx_i\\delta(x-x_i)\\phi(x)=\\frac{d^2}{dx^2}\\phi(x) $$ In the last equation there is no inconsistency (as one can check by using the product rule for differentiation and then evaluating the $x_i$ integral taking into account that the integral commutes with the derivatives - all terms vanish except for the term yielding $\\frac{d^2}{dx^2}\\phi(x)$ .) Since all of these lead to the same result after integrating out $x_i$ , we can effectively replace $(\\frac{d^2}{dx^2}\\delta(x-x_i))\\phi(x_i)$ by $\\frac{d^2}{dx^2}(\\delta(x-x_i)\\phi(x))$ which is what we did above. Next include also the measure contributions, \\begin{equation} \\begin{aligned} &\\big(\\hat{H}_0(x)\\Psi(x)\\big)\\int dx_1\\dots dx_N\\,\\phi_a(x_1,\\dots,x_N)\\Psi^T(x_1)\\dots \\Psi^T(x_N)|0\\rangle\\\\ &= \\sum_{i=1}^N\\int dx_1\\dots dx_i\\!\\!\\!/\\,\\,\\,\\dots dx_N\\Big(\\int dx_i\\hat{H}_0(x)\\delta(x-x_i)\\phi_a(x_1,\\dots,x,\\dots,x_N)\\Big)\\Psi^T(x_1)\\dots\\Psi^T(x_i\\!\\!\\!/)\\dots \\Psi^T(x_N)|0\\rangle\\\\ \\end{aligned} \\end{equation} and integrate out $x_i$ in the $i^{\\rm th}$ term in the sum (using the delta function), \\begin{equation} \\begin{aligned} &\\big(\\hat{H}_0(x)\\Psi(x)\\big)\\int dx_1\\dots dx_N\\,\\phi_a(x_1,\\dots,x_N)\\Psi^T(x_1)\\dots \\Psi^T(x_N)|0\\rangle\\\\ &= \\sum_{i=1}^N\\int dx_1\\dots dx_i\\!\\!\\!/\\,\\,\\,\\dots dx_N\\Big(\\hat{H}_0(x)\\phi_a(x_1,\\dots,x,\\dots,x_N)\\Big)\\Psi^T(x_1)\\dots\\Psi^T(x_i\\!\\!\\!/)\\dots \\Psi^T(x_N)|0\\rangle\\\\ \\end{aligned} \\end{equation} Including now the factor $\\Psi^T(x)$ and integrating over $x$ yields $\\hat{H}_{0,s}|\\phi_a\\rangle$ , \\begin{equation} \\begin{aligned} \\hat{H}_{0,s}|\\phi_a\\rangle=&\\big(\\int dx \\,\\Psi^T(x)\\hat{H}_0(x)\\Psi(x)\\big)\\int dx_1\\dots dx_N\\,\\phi_a(x_1,\\dots,x_N)\\Psi^T(x_1)\\dots \\Psi^T(x_N)|0\\rangle\\\\ &= \\sum_{i=1}^N\\int dx_1\\dots dx_i\\!\\!\\!/\\,\\,\\,\\dots dx_N\\int dx\\,\\Psi^T(x)\\Big(\\hat{H}_0(x)\\phi_a(x_1,\\dots,x,\\dots,x_N)\\Big)\\Psi^T(x_1)\\dots\\Psi^T(x_i\\!\\!\\!/)\\dots \\Psi^T(x_N)|0\\rangle\\\\ \\end{aligned} \\end{equation} Next redefine the integration variable, $x\\rightarrow x_i$ , use that the $\\Psi^T(x_i)$ 's commute among themselves, and that the combination $\\hat{H}_0(x)\\phi_a(x_1,\\dots,x,\\dots,x_N)$ is just a function, so that $\\Psi^T(x)$ can pass through it without obstruction: \\begin{equation} \\begin{aligned} &\\hat{H}_{0,s}|\\phi_a\\rangle= \\sum_{i=1}^N\\int dx_1\\dots dx_i\\!\\!\\!/\\,\\,\\,\\dots dx_N\\int dx_i\\,\\Big(\\hat{H}_0(x_i)\\phi_a(x_1,\\dots,x_i,\\dots,x_N)\\Big)\\Psi^T(x_i)\\Psi^T(x_1)\\dots\\Psi^T(x_i\\!\\!\\!/)\\dots \\Psi^T(x_N)|0\\rangle\\\\ &= \\sum_{i=1}^N\\int dx_1\\dots dx_N\\,\\Big(\\hat{H}_0(x_i)\\phi_a(x_1,\\dots,x_N)\\Big)\\Psi^T(x_1)\\dots \\Psi^T(x_N)|0\\rangle\\\\ &= \\int dx_1\\dots dx_N\\,\\Big(\\sum_{i=1}^N\\hat{H}_0(x_i)\\phi_a(x_1,\\dots,x_N)\\Big)\\Psi^T(x_1)\\dots \\Psi^T(x_N)|0\\rangle\\\\ &= \\int dx_1\\dots dx_N\\,\\Big(\\hat{H}_0\\phi_a(x_1,\\dots,x_N)\\Big)\\Psi^T(x_1)\\dots \\Psi^T(x_N)|0\\rangle\\\\ \\end{aligned} \\end{equation} where we used the definition of $\\hat{H}_0$ given above. The resulting equation, $$ \\boxed{\\hat{H}_{0,s}|\\phi_a\\rangle= \\int dx_1\\dots dx_N\\,\\Big(\\hat{H}_0\\phi_a(x_1,\\dots,x_N)\\Big)\\Psi^T(x_1)\\dots \\Psi^T(x_N)|0\\rangle} $$ is the desired result.",
      "question_latex": [
        "H_0=\\sum_{j=1}\\left(-\\frac{\\hbar^2}{2m}\\frac{\\partial}{\\partial x_j^2}+U(x_j)\\right)",
        "|\\phi_\\alpha\\rangle=\\int dx_1\\cdots dx_N\\,\\phi_\\alpha(x_1,\\cdots,x_N)\\psi^T(x_1)\\cdots\\psi^T(x_N)|0\\rangle.",
        "H_{0,s}=\\int dx\\,\\psi^T(x)H_0\\psi(x).",
        "H_{0,s}|\\phi_\\alpha\\rangle=\\int dx_1\\cdots dx_N\\,(H_0\\phi_\\alpha(x_1,\\cdots,x_N))\\psi^T(x_1)\\cdots\\psi^T(x_N)|0\\rangle.",
        "\\int   dx\\,\\psi^T(x)H_0\\psi(x)\\int dx_1\\cdots dx_N\\,\\phi_\\alpha(x_1,\\cdots,x_N)\\psi^T(x_1)\\cdots\\psi^T(x_N)|0\\rangle",
        "\\int dx\\,\\psi^T(x)\\psi(x)H_0=\\int dx\\,\\rho(x)H_0=H_0",
        "</span> where <span class=\"math-container\">",
        "</span> is some scalar potential. </p>\n\n<p>In second quantised language the wave function  is given by \n<span class=\"math-container\">",
        "</span></p>\n\n<p>The Hamiltonian: \n<span class=\"math-container\">",
        "</span></p>\n\n<blockquote>\n  <p>SHOW \n  <span class=\"math-container\">",
        "</span></p>\n</blockquote>\n\n<p>I then set the basis vectors to be the wavefunction <span class=\"math-container\">",
        "</span> for conveinence. </p>\n\n<p>So, I started by simply writing out: \n<span class=\"math-container\">",
        "</span>\nand using the definition of the position dependent density operator <span class=\"math-container\">",
        "</span>. Since the basis vectors are the wave function, we can commute <span class=\"math-container\">",
        "</span> around and get\n<span class=\"math-container\">",
        "</span>\nsince we are in 1D in a 1 particle state, the volume integral over the density will just 1. Then one can just use the same thing of the basis vectors to get <span class=\"math-container\">",
        "</span> into place in the final expression. </p>\n\n<p>This is a homework question, and I don't feel satisfied with this as an answer; is it correct, or completely wrong? I'm not sure about the justification for the commutation of <span class=\"math-container\">",
        "</span>, it feels too flimsy and my grasp of the linear algebra required here is somewhat lacking. </p>\n\n<p>Also where here can we consider the difference between bosons and fermions? I have derived the commutation law <span class=\"math-container\">"
      ],
      "answer_latex": [
        "\\boxed{\\hat{H}_0(x) = -\\frac{\\hbar^2}{2m}\\frac{d^2}{dx^2}+U(x)}",
        "\\boxed{\\hat{H}_{0,s} = \\int dx\\,\\Psi^T(x)\\hat{H}_0(x)\\Psi(x)}",
        "\\boxed{\\hat{H}_0 = \\sum_{i=1}^N\\hat{H}_0(x_i)}",
        "\\boxed{|\\phi_a\\rangle=\\int dx_1\\dots dx_N\\phi_a(x_1,\\dots,x_N)\\Psi^T(x_1)\\dots \\Psi^T(x_N)|0\\rangle}",
        "\\int dx_i\\Big(\\frac{d^2}{dx^2}\\delta(x-x_i)\\phi(x_i)\\Big)=\\frac{d^2}{dx^2}\\int dx_i\\Big(\\delta(x-x_i)\\phi(x_i)\\Big)=\\frac{d^2}{dx^2}\\phi(x)",
        "\\int dx_i\\Big(\\frac{d^2}{dx^2}\\delta(x-x_i)\\phi(x_i)\\Big)=\\int dx_i\\Big(\\frac{d^2}{dx^2_i}\\delta(x-x_i)\\phi(x_i)\\Big)=\\int dx_i\\Big(\\delta(x-x_i)\\frac{d^2}{dx^2_i}\\phi(x_i)\\Big)=\\frac{d^2}{dx^2}\\phi(x)",
        "\\int dx_i\\Big(\\frac{d^2}{dx^2}\\delta(x-x_i)\\phi(x_i)\\Big)=\\int dx_i\\frac{d^2}{dx^2}\\Big(\\delta(x-x_i)\\phi(x)\\Big)=\\frac{d^2}{dx^2}\\int dx_i\\delta(x-x_i)\\phi(x)=\\frac{d^2}{dx^2}\\phi(x)",
        "\\boxed{\\hat{H}_{0,s}|\\phi_a\\rangle=\n\\int dx_1\\dots  dx_N\\,\\Big(\\hat{H}_0\\phi_a(x_1,\\dots,x_N)\\Big)\\Psi^T(x_1)\\dots \\Psi^T(x_N)|0\\rangle}",
        "</span>\nand\n<span class=\"math-container\">",
        "</span>\nWhen there are <span class=\"math-container\">",
        "</span> decoupled particles the total Hamiltonian takes the form:\n<span class=\"math-container\">",
        "</span>\nNotice that it is the single particle Hamiltonian, <span class=\"math-container\">",
        "</span>, that appears in the definition of <span class=\"math-container\">",
        "</span> and not <span class=\"math-container\">",
        "</span> as the OP writes. Indeed, (as has been pointed out in the comments) since there is an integral over <span class=\"math-container\">",
        "</span> in <span class=\"math-container\">",
        "</span> it does not make sense to insert <span class=\"math-container\">",
        "</span>.</p>\n\n<p>We then define the multiparticle state, <span class=\"math-container\">",
        "</span>, with corresponding wavefunction, <span class=\"math-container\">",
        "</span>, by:\n<span class=\"math-container\">",
        "</span></p>\n\n<p>Then, using that <span class=\"math-container\">",
        "</span>, \n<span class=\"math-container\">\\begin{equation}\n\\begin{aligned}\n\\hat{H}_{0,s}&|\\phi_a\\rangle = \\int dx\\,\\Psi^T(x)\\hat{H}_0(x)\\Psi(x)\\int dx_1\\dots dx_N\\phi_a(x_1,\\dots,x_N)\\Psi^T(x_1)\\dots \\Psi^T(x_N)|0\\rangle\\\\\n&=\\int dx\\,\\Psi^T(x)\\hat{H}_0(x)\\int dx_1\\dots dx_N\\phi_a(x_1,\\dots,x_N)\\sum_{i=1}^N\\delta(x-x_i)\\Psi^T(x_1)\\dots\\Psi^T(x_i\\!\\!\\!/)\\dots \\Psi^T(x_N)|0\\rangle\\\\\n&=\\int dx\\,\\Psi^T(x)\\hat{H}_0(x)\\sum_{i=1}^N\\int dx_1\\dots dx_i\\!\\!\\!/\\dots dx_N\\phi_a(x_1,\\dots,x,\\dots,x_N)\\Psi(x_1)\\dots\\Psi^T(x_i\\!\\!\\!/)\\dots \\Psi^T(x_N)|0\\rangle\\\\\n&=\\int dx\\,\\Psi^T(x)\\sum_{i=1}^N\\int dx_1\\dots dx_i\\!\\!\\!/\\dots dx_N\\big(\\hat{H}_0(x)\\phi_a(x_1,\\dots,x,\\dots,x_N)\\big)\\Psi^T(x_1)\\dots\\Psi^T(x_i\\!\\!\\!/)\\dots \\Psi^T(x_N)|0\\rangle\\\\\n&=\\int dx_1\\dots dx_N\\Big(\\sum_{i=1}^N\\hat{H}_0(x_i)\\phi_a(x_1,\\dots,x_N)\\Big)\\Psi^T(x_1)\\dots \\Psi^T(x_N)|0\\rangle\\\\\n&=\\int dx_1\\dots dx_N\\Big(\\hat{H}_0\\phi_a(x_1,\\dots,x_N)\\Big)\\Psi^T(x_1)\\dots \\Psi^T(x_N)|0\\rangle\\\\\n\\end{aligned}\n\\end{equation}</span>\nwhich is what was meant to be shown. In the second to last step one changes variables of integration and in the last step we used the definition of <span class=\"math-container\">",
        "</span> given above.</p>\n\n<hr>\n\n<p><strong>By request I'm adding more detail:</strong></p>\n\n<p>An important step in the above derivation is going from the first to the second equality. Note primarily that <span class=\"math-container\">",
        "</span> acts on <span class=\"math-container\">",
        "</span>, which is the only quantity to its right that depends on <span class=\"math-container\">",
        "</span>. Isolating the relevant terms then, and using only the commutator, <span class=\"math-container\">",
        "</span>, \n<span class=\"math-container\">\\begin{equation}\n\\begin{aligned}\n\\big(\\hat{H}_0(x)\\Psi(x)\\big)\\Psi^T(x_1)&=\\Psi^T(x_1)\\hat{H}_0(x)\\Psi(x)+\\hat{H}_0(x)\\delta(x-x_1)\\\\\n&=\\Psi^T(x_1)\\hat{H}_0(x)\\Psi(x)+\\hat{H}_0(x)\\delta(x-x_1)\\\\\n\\big(\\hat{H}_0(x)\\Psi(x)\\big)\\Psi^T(x_1)\\Psi^T(x_2)&=\\big(\\Psi^T(x_1)\\hat{H}_0(x)\\Psi(x)+\\hat{H}_0(x)\\delta(x-x_1)\\big)\\Psi^T(x_2)\\\\\n&=\\Psi^T(x_1)\\big(\\hat{H}_0(x)\\Psi^T(x_2)\\Psi(x)+\\hat{H}_0(x)\\delta(x-x_2)\\big)+\\Psi^T(x_2)\\hat{H}_0(x)\\delta(x-x_1)\\\\\n&=\\Psi^T(x_1)\\Psi^T(x_2)\\hat{H}_0(x)\\Psi(x)+\\Psi^T(x_1)\\hat{H}_0(x)\\delta(x-x_2)+\\Psi^T(x_2)\\hat{H}_0(x)\\delta(x-x_1)\\\\\n&\\vdots\n\\end{aligned}\n\\end{equation}</span>\nNow <span class=\"math-container\">",
        "</span> is an annihilation operator, so that <span class=\"math-container\">",
        "</span>. The above two equations when acting on the vacuum then read:\n<span class=\"math-container\">\\begin{equation}\n\\begin{aligned}\n\\big(\\hat{H}_0(x)\\Psi(x)\\big)\\Psi^T(x_1)|0\\rangle\n&=\\Big(\\hat{H}_0(x)\\delta(x-x_1)\\Big)|0\\rangle\\\\\n\\big(\\hat{H}_0(x)\\Psi(x)\\big)\\Psi^T(x_1)\\Psi^T(x_2)|0\\rangle\n&=\\Big(\\Psi^T(x_1)\\hat{H}_0(x)\\delta(x-x_2)+\\Psi^T(x_2)\\hat{H}_0(x)\\delta(x-x_1)\\Big)|0\\rangle\\\\\n\\end{aligned}\n\\end{equation}</span>\nso the general result reads,\n<span class=\"math-container\">\\begin{equation}\n\\big(\\hat{H}_0(x)\\Psi(x)\\big)\\Psi^T(x_1)\\dots \\Psi^T(x_N)|0\\rangle=\n\\sum_{i=1}^N\\Big(\\hat{H}_0(x)\\delta(x-x_i)\\Big)\\Psi^T(x_1)\\dots\\Psi^T(x_i\\!\\!\\!/)\\dots \\Psi^T(x_N)|0\\rangle\n\\end{equation}</span>\nNotice here that the combination <span class=\"math-container\">",
        "</span> can pass through the quantity <span class=\"math-container\">",
        "</span> without obstruction: they commute. \nIncluding now the multi-particle wavefunction contribution,\n<span class=\"math-container\">\\begin{equation}\n\\begin{aligned}\n&\\big(\\hat{H}_0(x)\\Psi(x)\\big)\\phi_a(x_1,\\dots,x_N)\\Psi^T(x_1)\\dots \\Psi^T(x_N)|0\\rangle\\\\\n&=\n\\sum_{i=1}^N\\Big(\\hat{H}_0(x)\\delta(x-x_i)\\Big)\\phi_a(x_1,\\dots,x_N)\\Psi^T(x_1)\\dots\\Psi^T(x_i\\!\\!\\!/)\\dots \\Psi^T(x_N)|0\\rangle\\\\\n&=\n\\sum_{i=1}^N\\Big(\\hat{H}_0(x)\\delta(x-x_i)\\phi_a(x_1,\\dots,x_N)\\Big)\\Psi^T(x_1)\\dots\\Psi^T(x_i\\!\\!\\!/)\\dots \\Psi^T(x_N)|0\\rangle\\\\\n&=\n\\sum_{i=1}^N\\Big(\\hat{H}_0(x)\\delta(x-x_i)\\phi_a(x_1,\\dots,x,\\dots,x_N)\\Big)\\Psi^T(x_1)\\dots\\Psi^T(x_i\\!\\!\\!/)\\dots \\Psi^T(x_N)|0\\rangle\\\\\n\\end{aligned}\n\\end{equation}</span>\nwhere in the last equality we used the delta function to replace the <span class=\"math-container\">",
        "</span> argument of the wavefunction by <span class=\"math-container\">",
        "</span>. </p>\n\n<p>Let me mention in passing that one can also write <span class=\"math-container\">",
        "</span> and then integrate by parts twice in <span class=\"math-container\">",
        "</span> after having included the <span class=\"math-container\">",
        "</span> integral, and this would then also make manifest that <span class=\"math-container\">",
        "</span> acts on the wavefunction -- this leads to the same result. To show that all such approaches lead to the same result it suffices to realise that all of these are equivalent:\n<span class=\"math-container\">",
        "</span>\n<span class=\"math-container\">",
        "</span>\nIn the last equation there is no inconsistency (as one can check by using the product rule for differentiation and then evaluating the <span class=\"math-container\">",
        "</span> integral taking into account that the integral commutes with the derivatives - all terms vanish except for the term yielding <span class=\"math-container\">",
        "</span>.) \nSince all of these lead to the same result after integrating out <span class=\"math-container\">",
        "</span>, we can effectively replace <span class=\"math-container\">",
        "</span> by <span class=\"math-container\">",
        "</span> which is what we did above. </p>\n\n<p>Next include also the measure contributions,\n<span class=\"math-container\">\\begin{equation}\n\\begin{aligned}\n&\\big(\\hat{H}_0(x)\\Psi(x)\\big)\\int dx_1\\dots dx_N\\,\\phi_a(x_1,\\dots,x_N)\\Psi^T(x_1)\\dots \\Psi^T(x_N)|0\\rangle\\\\\n&=\n\\sum_{i=1}^N\\int dx_1\\dots dx_i\\!\\!\\!/\\,\\,\\,\\dots dx_N\\Big(\\int dx_i\\hat{H}_0(x)\\delta(x-x_i)\\phi_a(x_1,\\dots,x,\\dots,x_N)\\Big)\\Psi^T(x_1)\\dots\\Psi^T(x_i\\!\\!\\!/)\\dots \\Psi^T(x_N)|0\\rangle\\\\\n\\end{aligned}\n\\end{equation}</span>\nand integrate out <span class=\"math-container\">",
        "</span> in the <span class=\"math-container\">",
        "</span> term in the sum (using the delta function),\n<span class=\"math-container\">\\begin{equation}\n\\begin{aligned}\n&\\big(\\hat{H}_0(x)\\Psi(x)\\big)\\int dx_1\\dots dx_N\\,\\phi_a(x_1,\\dots,x_N)\\Psi^T(x_1)\\dots \\Psi^T(x_N)|0\\rangle\\\\\n&=\n\\sum_{i=1}^N\\int dx_1\\dots dx_i\\!\\!\\!/\\,\\,\\,\\dots dx_N\\Big(\\hat{H}_0(x)\\phi_a(x_1,\\dots,x,\\dots,x_N)\\Big)\\Psi^T(x_1)\\dots\\Psi^T(x_i\\!\\!\\!/)\\dots \\Psi^T(x_N)|0\\rangle\\\\\n\\end{aligned}\n\\end{equation}</span>\nIncluding now the factor <span class=\"math-container\">",
        "</span> and integrating over <span class=\"math-container\">",
        "</span> yields <span class=\"math-container\">",
        "</span>,\n<span class=\"math-container\">\\begin{equation}\n\\begin{aligned}\n\\hat{H}_{0,s}|\\phi_a\\rangle=&\\big(\\int dx \\,\\Psi^T(x)\\hat{H}_0(x)\\Psi(x)\\big)\\int dx_1\\dots dx_N\\,\\phi_a(x_1,\\dots,x_N)\\Psi^T(x_1)\\dots \\Psi^T(x_N)|0\\rangle\\\\\n&=\n\\sum_{i=1}^N\\int dx_1\\dots dx_i\\!\\!\\!/\\,\\,\\,\\dots dx_N\\int dx\\,\\Psi^T(x)\\Big(\\hat{H}_0(x)\\phi_a(x_1,\\dots,x,\\dots,x_N)\\Big)\\Psi^T(x_1)\\dots\\Psi^T(x_i\\!\\!\\!/)\\dots \\Psi^T(x_N)|0\\rangle\\\\\n\\end{aligned}\n\\end{equation}</span>\nNext redefine the integration variable, <span class=\"math-container\">",
        "</span>, use that the <span class=\"math-container\">",
        "</span>'s commute among themselves, and that the combination <span class=\"math-container\">",
        "</span> is just a function, so that <span class=\"math-container\">",
        "</span> can pass through it without obstruction:\n<span class=\"math-container\">\\begin{equation}\n\\begin{aligned}\n&\\hat{H}_{0,s}|\\phi_a\\rangle=\n\\sum_{i=1}^N\\int dx_1\\dots dx_i\\!\\!\\!/\\,\\,\\,\\dots dx_N\\int dx_i\\,\\Big(\\hat{H}_0(x_i)\\phi_a(x_1,\\dots,x_i,\\dots,x_N)\\Big)\\Psi^T(x_i)\\Psi^T(x_1)\\dots\\Psi^T(x_i\\!\\!\\!/)\\dots \\Psi^T(x_N)|0\\rangle\\\\\n&=\n\\sum_{i=1}^N\\int dx_1\\dots  dx_N\\,\\Big(\\hat{H}_0(x_i)\\phi_a(x_1,\\dots,x_N)\\Big)\\Psi^T(x_1)\\dots \\Psi^T(x_N)|0\\rangle\\\\\n&=\n\\int dx_1\\dots  dx_N\\,\\Big(\\sum_{i=1}^N\\hat{H}_0(x_i)\\phi_a(x_1,\\dots,x_N)\\Big)\\Psi^T(x_1)\\dots \\Psi^T(x_N)|0\\rangle\\\\\n&=\n\\int dx_1\\dots  dx_N\\,\\Big(\\hat{H}_0\\phi_a(x_1,\\dots,x_N)\\Big)\\Psi^T(x_1)\\dots \\Psi^T(x_N)|0\\rangle\\\\\n\\end{aligned}\n\\end{equation}</span>\nwhere we used the definition of <span class=\"math-container\">",
        "</span> given above. The resulting equation,\n<span class=\"math-container\">"
      ],
      "created": "2019-04-16T12:24:38.447",
      "golden_ner_terms": [
        "acts on",
        "algebra",
        "argument",
        "basis",
        "bosons",
        "combination",
        "commutator",
        "complete",
        "delta function",
        "density",
        "density operator",
        "derivation",
        "difference",
        "differentiation",
        "equality",
        "equation",
        "equivalent",
        "expression",
        "factor",
        "fermions",
        "function",
        "hamiltonian",
        "integral",
        "integrate",
        "integration",
        "language",
        "linear algebra",
        "measure",
        "operator",
        "pass through",
        "place",
        "potential",
        "product",
        "product rule",
        "right",
        "scalar",
        "scalar potential",
        "state",
        "step",
        "sum",
        "term",
        "vacuum",
        "vanish",
        "variable",
        "vectors",
        "volume",
        "wave function",
        "wavefunction"
      ],
      "golden_ner_count": 48,
      "golden_patterns": [
        {
          "pattern": "unfold-the-definition",
          "score": 2.0,
          "hotwords": [
            "definition of"
          ]
        },
        {
          "pattern": "transport-across-isomorphism",
          "score": 2.0,
          "hotwords": [
            "equivalent"
          ]
        }
      ],
      "golden_pattern_names": [
        "unfold-the-definition",
        "transport-across-isomorphism"
      ],
      "golden_scopes": [],
      "golden_scope_count": 0
    },
    {
      "id": "se-physics-158849",
      "stratum": "hard",
      "title": "Harmonic oscillator coherent state expectation values",
      "tags": [
        "quantum-mechanics",
        "homework-and-exercises",
        "harmonic-oscillator",
        "coherent-states"
      ],
      "score": 3,
      "answer_score": 4,
      "question_body": "I'm looking to calculate the expected values of a coherent state (of a harmonic oscillator) evolving in time. I know that the $x$ and $p$ expectation values are as in classical motion, but I'm wondering about $x^2$ and $p^2$. Let's say I'm starting with the coherent state $| b \\rangle$, with $b \\in \\mathbb{R}$, so the wavefunction is the ground state displaced by $bx_0\\sqrt{2}$: $$\\psi_b (x) = \\psi_0(x-bx_0\\sqrt{2})$$ Or similarly the Wigner function will be $$W_b(x,p) = W_0(x-bx_0\\sqrt{2},p)$$ Now I know the expected values of $x$ and $p$ are classical: $$\\langle x(t) \\rangle = bx_0\\sqrt{2}\\cos(-\\omega t)$$ $$\\langle p(t) \\rangle = bp_0\\sqrt{2}\\sin(-\\omega t)$$ But what about $\\langle x^2(t) \\rangle$ and $\\langle p^2(t) \\rangle$ and ?",
      "answer_body": "Let $\\alpha \\in {\\Bbb C}$, and let $\\vert{n}\\rangle $ be the harmonic oscillator state with energy $(n+\\textstyle\\frac{1}{2})\\hbar\\omega$. At $t=0$, the coherent state $\\vert {\\alpha(0)}\\rangle $ is defined by $$ \\vert{\\alpha(0)}\\rangle= e^{-\\vert \\alpha \\vert^2/2}\\,\\left( \\sum_{n=0}^{\\infty} \\displaystyle{\\alpha^n\\over \\sqrt{n!}}\\,\\vert{n}\\rangle\\right) \\tag{1} $$ What is $\\vert{\\alpha(t)}\\rangle$, the coherent state at time $t$? Start with (1). Since $\\left\\vert n\\right\\rangle $ is an eigenstate of the harmonic oscillator hamiltonian $\\hat{H}=\\left( \\hat a^{\\dagger }\\hat a+\\frac{1}{2}\\right) \\hbar \\omega $ with eigenvalue $\\left( n+\\frac{1}{2}\\right) \\hbar \\omega ,$ the time evolution of $\\left\\vert n\\right\\rangle $ is simply $\\left\\vert n(t)\\right\\rangle =e^{-i(n+\\frac{1}{2})\\omega t}\\left\\vert n\\right\\rangle $ and thus \\begin{equation} \\left\\vert \\alpha (t)\\right\\rangle =e^{-\\left\\vert \\alpha \\right\\vert ^{2}/2}\\left( \\sum_{n=0}^{\\infty }\\frac{\\alpha ^{n}}{\\sqrt{n!}}e^{-i(n+\\frac{% 1}{2})\\omega t}\\left\\vert n\\right\\rangle \\right) . \\end{equation} It is easy to show that $\\left\\vert \\alpha (t)\\right\\rangle $ is normalized. Now we first need to show that $a\\vert{\\alpha(t)}\\rangle=\\alpha e^{i\\hbar \\omega t}\\vert{\\alpha(t)}\\rangle$. Recall that $\\hat{a}\\left\\vert n\\right\\rangle =\\sqrt{n}\\left\\vert n-1\\right\\rangle .$ \\ Then, since $\\hat{a}$ is linear, \\begin{eqnarray} \\hat{a}\\left\\vert \\alpha (t)\\right\\rangle &=&e^{-\\left\\vert \\alpha \\right\\vert ^{2}/2}\\left( \\sum_{n=0}^{\\infty }\\frac{\\alpha ^{n}}{\\sqrt{n!}}% e^{-i(n+\\frac{1}{2})\\omega t}\\hat{a}\\left\\vert n\\right\\rangle \\right) , \\\\ &=&e^{-\\left\\vert \\alpha \\right\\vert ^{2}/2}\\left( \\sum_{n=0}^{\\infty }\\frac{% \\alpha ^{n}}{\\sqrt{n!}}e^{-i(n+\\frac{1}{2})\\omega t}\\sqrt{n}\\left\\vert n-1\\right\\rangle \\right) , \\\\ &=&e^{-\\left\\vert \\alpha \\right\\vert ^{2}/2}\\left( \\sum_{n=0}^{\\infty }\\frac{% \\alpha ^{n}}{\\sqrt{\\left( n-1\\right) !}}e^{-i(n+\\frac{1}{2})\\omega t}\\left\\vert n-1\\right\\rangle \\right) , \\\\ &=&\\alpha e^{-i\\omega t}e^{-\\left\\vert \\alpha \\right\\vert ^{2}/2}\\left( \\sum_{n=0}^{\\infty }\\frac{\\alpha ^{n-1}}{\\sqrt{\\left( n-1\\right) !}}% e^{-i(n-1+\\frac{1}{2})\\omega t}\\left\\vert n-1\\right\\rangle \\right) .\\quad \\end{eqnarray} The sum properly starts at $n=1$ since the $n=0$ term does not exist. Thus, setting $m=n-1,$ we can rewrite this sum in terms of $m,$ with $m$ starting at $m=0.$ Hence \\begin{eqnarray} \\hat{a}\\left\\vert \\alpha (t)\\right\\rangle &=&\\alpha e^{-i\\omega t}\\left[ e^{-\\left\\vert \\alpha \\right\\vert ^{2}/2}\\left( \\sum_{m=0}^{\\infty }\\frac{% \\alpha ^{m}}{\\sqrt{m!}}e^{-i(m+\\frac{1}{2})\\omega t}\\left\\vert m\\right\\rangle \\right) \\right] \\\\ &=&\\alpha e^{-i\\omega t}\\left\\vert \\alpha (t)\\right\\rangle . \\end{eqnarray} A useful secondary result, which follows immediately from above, is \\begin{eqnarray} \\left[ \\hat{a}\\left\\vert \\alpha (t)\\right\\rangle \\right] ^{\\dagger } &=&\\left\\langle \\alpha (t)\\right\\vert \\hat{a}^{\\dagger } \\\\ &=&\\left[ \\alpha e^{-i\\omega t}\\left\\vert \\alpha (t)\\right\\rangle \\right] ^{\\dagger }=\\alpha ^{\\ast }e^{i\\omega t}\\left\\langle \\alpha (t)\\right\\vert \\end{eqnarray} Now $\\langle \\hat p(t) \\rangle$ and $\\langle \\hat x(t)\\rangle$ for $\\vert{\\alpha(t)}\\rangle$. Starting from the definitions $$ \\hat{a} =\\sqrt{\\frac{m\\omega }{2\\hbar }}\\left( \\hat{x}+\\frac{i}{% m\\omega }\\hat{p}\\right) , \\qquad \\hat{a}^{\\dagger } =\\sqrt{\\frac{m\\omega }{2\\hbar }}\\left( \\hat{x}-% \\frac{i}{m\\omega }\\hat{p}\\right) , $$ we have $$ \\hat{x} =\\sqrt{\\frac{\\hbar }{2m\\omega }}\\left( \\hat{a}^{\\dagger }+% \\hat{a}\\right) , \\qquad \\hat{p} =i\\sqrt{\\frac{m\\omega \\hbar }{2}}\\left( \\hat{a}^{\\dagger }-% \\hat{a}\\right) , $$ and thus \\begin{eqnarray} \\left\\langle x(t)\\right\\rangle &=&\\sqrt{\\frac{\\hbar }{2m\\omega }}\\left[ \\left\\langle \\alpha (t)\\right\\vert \\hat{a}^{\\dagger }\\left\\vert \\alpha (t)\\right\\rangle +\\left\\langle \\alpha (t)\\right\\vert \\hat{a}\\left\\vert \\alpha (t)\\right\\rangle \\right]\\, , \\\\ &=&\\sqrt{\\frac{\\hbar }{2m\\omega }}\\left[ \\alpha ^{\\ast }e^{i\\omega t}+\\alpha e^{-i\\omega t}\\right] \\left\\langle \\alpha (t)\\right. \\left\\vert \\alpha (t)\\right\\rangle \\\\ &=&\\sqrt{\\frac{\\hbar }{2m\\omega }}\\left[ \\alpha ^{\\ast }e^{i\\omega t}+\\alpha e^{-i\\omega t}\\right] , \\end{eqnarray} which is real, as expected. We can clean this up by writing $\\alpha =\\left\\vert \\alpha \\right\\vert e^{i\\theta }$ to obtain% \\begin{equation} \\left\\langle x(t)\\right\\rangle =\\sqrt{\\frac{2\\hbar }{m\\omega }}\\left\\vert \\alpha \\right\\vert \\cos \\left( \\omega t-\\theta \\right) \\tag{2} \\end{equation} Likewise, \\begin{eqnarray} \\left\\langle p(t)\\right\\rangle &=&i\\sqrt{\\frac{m\\omega \\hbar }{2}}\\left[ \\left\\langle \\alpha (t)\\right\\vert \\hat{a}^{\\dagger }\\left\\vert \\alpha (t)\\right\\rangle -\\left\\langle \\alpha (t)\\right\\vert \\hat{a}\\left\\vert \\alpha (t)\\right\\rangle \\right] \\\\ &=&i\\sqrt{\\frac{m\\omega \\hbar }{2}}\\left[ \\alpha ^{\\ast }e^{i\\omega t}-\\alpha e^{-i\\omega t}\\right] \\left\\langle \\alpha (t)\\right. \\left\\vert \\alpha (t)\\right\\rangle \\\\ &=&-\\sqrt{2m\\omega \\hbar }\\left\\vert \\alpha \\right\\vert \\sin \\left( \\omega t-\\theta \\right) \\tag{3} \\end{eqnarray} which is again real. In your specific case you are starting with a coherent state for which, at $t=0$, we have $$ \\langle x(0)\\rangle= b\\sqrt{2}x_0\\, ,\\qquad \\langle p(0)\\rangle=0 $$ so this implies from (2) and (3) evaluated at $t=0$ that $$ b\\sqrt{2}x_0=\\sqrt{\\frac{2\\hbar }{m\\omega }}\\left\\vert \\alpha \\right\\vert \\cos \\left(\\theta \\right)\\, , \\qquad 0= \\sqrt{2m\\omega \\hbar }\\left\\vert \\alpha \\right\\vert \\sin \\left(\\theta \\right) $$ Comparing with your initial conditions gives $\\theta=0$ and $b\\sqrt{2}x_0=\\sqrt{\\frac{2\\hbar }{m\\omega }} \\alpha $ with $\\alpha$ real. Finally, $\\hat{x}^{2}$ and $\\hat{p}^{2}.$ From $\\hat{x}$ and $\\hat{p},$ we find \\begin{eqnarray} \\hat{x}^{2} &=&\\frac{\\hbar }{2m\\omega }\\left( \\hat{a}^{\\dagger }+ \\hat{a}\\right) ^{2}=\\frac{\\hbar }{2m\\omega }\\left( \\left( \\hat{a} ^{\\dagger }\\right) ^{2}+\\hat{a}^{\\dagger }\\hat{a}+\\hat{a}\\hat{a} ^{\\dagger }+\\left( \\hat{a}\\right) ^{2}\\right) , \\\\ &=&\\frac{\\hbar }{2m\\omega }\\left( \\left( \\hat{a}^{\\dagger }\\right) ^{2}+2 \\hat{a}^{\\dagger }\\hat{a}+1+\\left( \\hat{a}\\right) ^{2}\\right) , \\\\ \\hat{p}^{2} &=&-\\frac{m\\omega \\hbar }{2}\\left( \\hat{a}-\\hat{a} ^{\\dagger }\\right) ^{2}=-\\frac{m\\omega \\hbar }{2}\\left( \\left( \\hat{a} ^{\\dagger }\\right) ^{2}-\\hat{a}^{\\dagger }\\hat{a}-\\hat{a}\\hat{a}% ^{\\dagger }+\\left( \\hat{a}\\right) ^{2}\\right) , \\\\ &=&-\\frac{m\\omega \\hbar }{2}\\left( \\left( \\hat{a}^{\\dagger }\\right) ^{2}-2% \\hat{a}^{\\dagger }\\hat{a}-1+\\left( \\hat{a}\\right) ^{2}\\right) , \\end{eqnarray} where \\begin{equation} \\hat{a}\\hat{a}^{\\dagger }=\\hat{a}\\hat{a}^{\\dagger }-\\hat{a}% ^{\\dagger }\\hat{a}+\\hat{a}^{\\dagger }\\hat{a}=\\left[ \\hat{a},% \\hat{a}^{\\dagger }\\right] +\\hat{a}^{\\dagger }\\hat{a}=1+\\hat{a}% ^{\\dagger }\\hat{a} \\end{equation} has been used. Thus, \\begin{eqnarray} \\left\\langle x^{2}(t)\\right\\rangle &=&\\frac{\\hbar }{2m\\omega }\\left[ \\left\\langle \\alpha (t)\\right\\vert \\left( \\hat{a}^{\\dagger }\\right) ^{2}\\left\\vert \\alpha (t)\\right\\rangle +2\\left\\langle \\alpha (t)\\right\\vert \\hat{a}^{\\dagger }\\hat{a}\\left\\vert \\alpha (t)\\right\\rangle\\right.\\nonumber \\\\ &&\\left.\\qquad\\qquad\\qquad\\qquad\\qquad\\quad +1+\\left\\langle \\alpha (t)\\right\\vert \\hat{a}^{2}\\left\\vert \\alpha (t)\\right\\rangle \\right] , \\\\ &=&\\frac{\\hbar }{2m\\omega }\\left[ \\left( \\alpha ^{\\ast }e^{i\\omega t}\\right) ^{2}+2\\alpha ^{\\ast }\\alpha +1+\\left( \\alpha e^{-i\\omega t}\\right) ^{2}% \\right]\\, ,\\\\ &=&\\frac{\\hbar }{2m\\omega }\\left[ \\left( \\alpha ^{\\ast }e^{i\\omega t}+\\alpha e^{-i\\omega t}\\right) ^{2}+1\\right] , \\\\ &=&\\frac{\\hbar }{2m\\omega }\\left[ 4\\left\\vert \\alpha \\right\\vert ^{2}\\cos ^{2}\\left( \\omega t-\\theta \\right) +1\\right] . \\\\ \\left\\langle p^{2}(t)\\right\\rangle &=&-\\frac{m\\omega \\hbar }{2}\\left[ \\left\\langle \\alpha (t)\\right\\vert \\left( \\hat{a}^{\\dagger }\\right) ^{2}\\left\\vert \\alpha (t)\\right\\rangle -2\\left\\langle \\alpha (t)\\right\\vert \\hat{a}^{\\dagger }\\hat{a}\\left\\vert \\alpha (t)\\right\\rangle\\right.\\nonumber \\\\ &&\\left.\\qquad\\qquad\\qquad\\qquad\\qquad\\quad -1+\\left\\langle \\alpha (t)\\right\\vert \\hat{a}^{2}\\left\\vert \\alpha (t)\\right\\rangle \\right] , \\\\ &=&-\\frac{m\\omega \\hbar }{2}\\left[ \\left( \\alpha ^{\\ast }e^{i\\omega t}\\right) ^{2}-2\\alpha ^{\\ast }\\alpha -1+\\left( \\alpha e^{-i\\omega t}\\right) ^{2}\\right]\\, ,\\\\ &=&-\\frac{m\\omega \\hbar }{2}\\left[ \\left( \\alpha ^{\\ast }e^{i\\omega t}-\\alpha e^{-i\\omega t}\\right) ^{2}-1\\right] , \\\\ &=&-\\frac{m\\omega \\hbar }{2}\\left[ -4\\left\\vert \\alpha \\right\\vert ^{2}\\sin ^{2}\\left( \\omega t-\\theta \\right) -1\\right]\\, ,\\\\ &=&\\frac{m\\omega \\hbar }{2}\\left[ 4\\left\\vert \\alpha \\right\\vert ^{2}\\sin ^{2}\\left( \\omega t-\\theta \\right) +1% \\right] . \\end{eqnarray}",
      "question_latex": [
        "\\psi_b (x) = \\psi_0(x-bx_0\\sqrt{2})",
        "W_b(x,p) = W_0(x-bx_0\\sqrt{2},p)",
        "\\langle x(t) \\rangle = bx_0\\sqrt{2}\\cos(-\\omega t)",
        "\\langle p(t) \\rangle = bp_0\\sqrt{2}\\sin(-\\omega t)",
        "x",
        "p",
        "x^2",
        "p^2",
        "| b \\rangle",
        "b \\in \\mathbb{R}",
        "bx_0\\sqrt{2}",
        "</p>\n\n<p>Or similarly the Wigner function will be </p>\n\n<p>",
        "</p>\n\n<p>Now I know the expected values of",
        "and",
        "are classical:</p>\n\n<p>",
        "</p>\n\n<p>But what about"
      ],
      "answer_latex": [
        "\\vert{\\alpha(0)}\\rangle= e^{-\\vert \\alpha \\vert^2/2}\\,\\left(\n\\sum_{n=0}^{\\infty} \\displaystyle{\\alpha^n\\over \\sqrt{n!}}\\,\\vert{n}\\rangle\\right) \\tag{1}",
        "\\hat{a} =\\sqrt{\\frac{m\\omega }{2\\hbar }}\\left( \\hat{x}+\\frac{i}{%\nm\\omega }\\hat{p}\\right) , \\qquad\n\\hat{a}^{\\dagger } =\\sqrt{\\frac{m\\omega }{2\\hbar }}\\left( \\hat{x}-%\n\\frac{i}{m\\omega }\\hat{p}\\right) ,",
        "\\hat{x} =\\sqrt{\\frac{\\hbar }{2m\\omega }}\\left( \\hat{a}^{\\dagger }+%\n\\hat{a}\\right) , \\qquad\n\\hat{p} =i\\sqrt{\\frac{m\\omega \\hbar }{2}}\\left( \\hat{a}^{\\dagger }-%\n\\hat{a}\\right) ,",
        "\\langle x(0)\\rangle= b\\sqrt{2}x_0\\, ,\\qquad \\langle p(0)\\rangle=0",
        "b\\sqrt{2}x_0=\\sqrt{\\frac{2\\hbar }{m\\omega }}\\left\\vert\n\\alpha \\right\\vert \\cos \\left(\\theta \\right)\\, , \\qquad 0= \\sqrt{2m\\omega \\hbar }\\left\\vert \\alpha \\right\\vert \\sin \\left(\\theta \\right)",
        "\\alpha \\in {\\Bbb C}",
        "\\vert{n}\\rangle",
        "(n+\\textstyle\\frac{1}{2})\\hbar\\omega",
        "t=0",
        "\\vert {\\alpha(0)}\\rangle",
        "</p>\n\n<p>What is",
        ", the coherent state at time",
        "?  Start with (1).  Since",
        "is an eigenstate of the harmonic oscillator hamiltonian",
        "with eigenvalue",
        "the time evolution of",
        "is simply",
        "and thus\n\\begin{equation}\n\\left\\vert \\alpha (t)\\right\\rangle =e^{-\\left\\vert \\alpha \\right\\vert\n^{2}/2}\\left( \\sum_{n=0}^{\\infty }\\frac{\\alpha ^{n}}{\\sqrt{n!}}e^{-i(n+\\frac{%\n1}{2})\\omega t}\\left\\vert n\\right\\rangle \\right) .\n\\end{equation}\nIt is easy to show that",
        "is normalized.</p>\n\n<p>Now we first need to show that",
        ". Recall that",
        "\\ Then, since",
        "is linear,\n\\begin{eqnarray}\n\\hat{a}\\left\\vert \\alpha (t)\\right\\rangle &=&e^{-\\left\\vert \\alpha\n\\right\\vert ^{2}/2}\\left( \\sum_{n=0}^{\\infty }\\frac{\\alpha ^{n}}{\\sqrt{n!}}%\ne^{-i(n+\\frac{1}{2})\\omega t}\\hat{a}\\left\\vert n\\right\\rangle \\right) ,\n\\\\\n&=&e^{-\\left\\vert \\alpha \\right\\vert ^{2}/2}\\left( \\sum_{n=0}^{\\infty }\\frac{%\n\\alpha ^{n}}{\\sqrt{n!}}e^{-i(n+\\frac{1}{2})\\omega t}\\sqrt{n}\\left\\vert\nn-1\\right\\rangle \\right) , \\\\\n&=&e^{-\\left\\vert \\alpha \\right\\vert ^{2}/2}\\left( \\sum_{n=0}^{\\infty }\\frac{%\n\\alpha ^{n}}{\\sqrt{\\left( n-1\\right) !}}e^{-i(n+\\frac{1}{2})\\omega\nt}\\left\\vert n-1\\right\\rangle \\right) , \\\\\n&=&\\alpha e^{-i\\omega t}e^{-\\left\\vert \\alpha \\right\\vert ^{2}/2}\\left(\n\\sum_{n=0}^{\\infty }\\frac{\\alpha ^{n-1}}{\\sqrt{\\left( n-1\\right) !}}%\ne^{-i(n-1+\\frac{1}{2})\\omega t}\\left\\vert n-1\\right\\rangle \\right) .\\quad\n\\end{eqnarray}\nThe sum properly starts at",
        "since the",
        "term does not exist. Thus, setting",
        "we can rewrite this sum in terms of",
        "with",
        "starting at",
        "Hence\n\\begin{eqnarray}\n\\hat{a}\\left\\vert \\alpha (t)\\right\\rangle &=&\\alpha e^{-i\\omega t}\\left[\ne^{-\\left\\vert \\alpha \\right\\vert ^{2}/2}\\left( \\sum_{m=0}^{\\infty }\\frac{%\n\\alpha ^{m}}{\\sqrt{m!}}e^{-i(m+\\frac{1}{2})\\omega t}\\left\\vert\nm\\right\\rangle \\right) \\right] \\\\\n&=&\\alpha e^{-i\\omega t}\\left\\vert \\alpha (t)\\right\\rangle .\n\\end{eqnarray}\nA useful secondary result, which follows immediately from above, is\n\\begin{eqnarray}\n\\left[ \\hat{a}\\left\\vert \\alpha (t)\\right\\rangle \\right] ^{\\dagger }\n&=&\\left\\langle \\alpha (t)\\right\\vert \\hat{a}^{\\dagger } \\\\\n&=&\\left[ \\alpha e^{-i\\omega t}\\left\\vert \\alpha (t)\\right\\rangle \\right]\n^{\\dagger }=\\alpha ^{\\ast }e^{i\\omega t}\\left\\langle \\alpha (t)\\right\\vert\n\\end{eqnarray}</p>\n\n<p>Now",
        "and",
        "for",
        ". Starting from the definitions",
        "we have",
        "and thus\n\\begin{eqnarray}\n\\left\\langle x(t)\\right\\rangle  &=&\\sqrt{\\frac{\\hbar }{2m\\omega }}\\left[\n\\left\\langle \\alpha (t)\\right\\vert \\hat{a}^{\\dagger }\\left\\vert \\alpha\n(t)\\right\\rangle +\\left\\langle \\alpha (t)\\right\\vert \\hat{a}\\left\\vert\n\\alpha (t)\\right\\rangle \\right]\\, ,  \\\\\n&=&\\sqrt{\\frac{\\hbar }{2m\\omega }}\\left[ \\alpha ^{\\ast }e^{i\\omega t}+\\alpha\ne^{-i\\omega t}\\right] \\left\\langle \\alpha (t)\\right. \\left\\vert \\alpha\n(t)\\right\\rangle  \\\\\n&=&\\sqrt{\\frac{\\hbar }{2m\\omega }}\\left[ \\alpha ^{\\ast }e^{i\\omega t}+\\alpha\ne^{-i\\omega t}\\right] ,\n\\end{eqnarray}\nwhich is real, as expected.  We can clean this up by writing",
        "to obtain%\n\\begin{equation}\n\\left\\langle x(t)\\right\\rangle =\\sqrt{\\frac{2\\hbar }{m\\omega }}\\left\\vert\n\\alpha \\right\\vert \\cos \\left( \\omega t-\\theta \\right) \\tag{2}\n\\end{equation}</p>\n\n<p>Likewise,\n\\begin{eqnarray}\n\\left\\langle p(t)\\right\\rangle  &=&i\\sqrt{\\frac{m\\omega \\hbar }{2}}\\left[\n\\left\\langle \\alpha (t)\\right\\vert \\hat{a}^{\\dagger }\\left\\vert \\alpha\n(t)\\right\\rangle -\\left\\langle \\alpha (t)\\right\\vert \\hat{a}\\left\\vert\n\\alpha (t)\\right\\rangle \\right]  \\\\\n&=&i\\sqrt{\\frac{m\\omega \\hbar }{2}}\\left[ \\alpha ^{\\ast }e^{i\\omega\nt}-\\alpha e^{-i\\omega t}\\right] \\left\\langle \\alpha (t)\\right. \\left\\vert\n\\alpha (t)\\right\\rangle  \\\\\n&=&-\\sqrt{2m\\omega \\hbar }\\left\\vert \\alpha \\right\\vert \\sin \\left( \\omega\nt-\\theta \\right) \\tag{3}\n\\end{eqnarray}\nwhich is again real.</p>\n\n<p>In your specific case you are starting with a coherent state for which, at",
        ", we have",
        "so this implies from (2) and (3) evaluated at",
        "that",
        "Comparing with your initial conditions gives",
        "real.</p>\n\n<p>Finally,",
        "From"
      ],
      "created": "2015-01-11T20:50:48.970",
      "golden_ner_terms": [
        "calculate",
        "eigenvalue",
        "energy",
        "expectation",
        "expectation value",
        "expected value",
        "function",
        "ground state",
        "hamiltonian",
        "harmonic",
        "harmonic oscillator",
        "implies",
        "initial condition",
        "real",
        "state",
        "sum",
        "term",
        "time",
        "time evolution",
        "useful",
        "wavefunction"
      ],
      "golden_ner_count": 21,
      "golden_patterns": [
        {
          "pattern": "encode-as-algebra",
          "score": 2.0,
          "hotwords": [
            "ring"
          ]
        },
        {
          "pattern": "unfold-the-definition",
          "score": 2.0,
          "hotwords": [
            "recall that"
          ]
        }
      ],
      "golden_pattern_names": [
        "encode-as-algebra",
        "unfold-the-definition"
      ],
      "golden_scopes": [
        {
          "type": "set-notation",
          "match": "$\\alpha \\in {\\Bbb C}$"
        }
      ],
      "golden_scope_count": 1
    },
    {
      "id": "se-physics-145959",
      "stratum": "hard",
      "title": "Stress-Energy Tensor of EM Field",
      "tags": [
        "electromagnetism",
        "special-relativity",
        "stress-energy-momentum-tensor"
      ],
      "score": 3,
      "answer_score": 1,
      "question_body": "Stress-energy tensor for electromagnetic field is given by $$T^{\\mu\\nu}=\\frac1{4\\pi}(F^{\\mu\\alpha}F^{\\nu}{}_\\alpha-\\frac14 g^{\\mu\\nu} F_{\\alpha\\beta}F^{\\alpha\\beta}).$$ My textbook (unpublished textbook by Blandford and Kip Thorne) says that the stress tensor consists of: A pressure $P_\\perp=\\textbf E^2/8\\pi$ orthogonal to $\\textbf E$ and a pressure $P_\\perp=\\textbf B^2/8\\pi$ orthogonal to $\\textbf B$ . A tension $P_\\parallel= \\textbf E^2/8\\pi$ along $\\textbf E$ and a tension $P_\\parallel=\\textbf B^2/8\\pi$ along $\\textbf B^2$ . I had written down the full expression of the components of the stress-energy tensor $T$ and i haven't got a clue of what these statements mean, both qualitatively and quantatively. Can someone clarify this issue by explaining me the meaning of the aforementioned \"pressure orthogonal to $E$ and $B$ \" and \"stress along $E$ and $B$ \"?",
      "answer_body": "Your version of the stress tensor matches what's currently on the Wikipedia link under Electromagnetic stress–energy tensor https://en.wikipedia.org/wiki/Electromagnetic_stress%E2%80%93energy_tensor except for having $μ_0$ in place of your $4π$ : $$T^{μν} = \\frac{1}{μ_0}\\left(F^{μα}{F^ν}_α - \\frac{1}{4} g^{μν} F_{αβ}F^{αβ}\\right),$$ and with your metric $g_{μν}$ being written there as the Minkowski metric $η_{μν}$ . A close examination of the context indicates that it is using the conventions (which it didn't spell out, explicitly): $$ \\left(x^0, x^1, x^2, x^3\\right) = (ct, x, y, z), \\hspace 1em F_{0i} = \\frac {E_i} c, \\hspace 1em F_{ij} = B^k, \\\\ η_{00} = -1, \\hspace 1em η_{0i} = 0 = η_{i0}, \\hspace 1em η_{ij} = δ_{ij} $$ as $i = 1, 2, 3$ and $(i,j,k)$ range cyclically over $(1,2,3)$ , $(2,3,1)$ and $(3,1,2)$ ; and with $F_{μν} = -F_{νμ}$ . This yields the components listed in the article: $$ T^{00} = \\frac 1 2 \\left(ε_0 E^2 + \\frac{B^2}{μ_0}\\right) = \\frac {ε_0} 2 \\left(E^2 + B^2 c^2\\right), \\\\ \\left(T^{10}, T^{20}, T^{30}\\right) = \\frac{×}{μ_0 c} = ε_0c× = \\left(T^{01}, T^{02}, T^{03}\\right), \\\\ T^{ij} = -ε_0 E_i E_j - \\frac{B_i B_j}{μ_0} + δ^{ij} \\left(ε_0 E^2 + \\frac{B^2}{μ_0}\\right) = -ε_0 \\left(E_iE_j + B^iB^jc^2 - \\frac 1 2 δ^{ij} \\left(E^2 + B^2 c^2\\right)\\right), $$ where $ε_0 = 1/(c^2 μ_0)$ . Under your convention, $ε_0 = 1/(8πc^2)$ . The last term can be written in tensor-dyad form as: $$-ε_0 \\left( + c^2 - \\frac 1 2 \\left(E^2 + B^2 c^2\\right)\\right).$$ The authors' analysis is, at best, sloppy, if that's what they really wrote. They're saying, just look at the $$ and $$ parts separately: $$-ε_0 \\left( - \\frac 1 2 E^2\\right), \\hspace 1em -\\frac{1}{μ_0} \\left( - \\frac 1 2 B^2\\right).$$ More genrally, consider for any vector $$ , the corresponding expression $- + ½ V^2$ . Suppose the vector is in the $x$ direction with $ = V$ , where $(, , )$ will be used here to denote the unit vectors, respectively, for the $x$ , $y$ and $z$ directions. Then $$- + \\frac 1 2 V^2 = -V^2 + \\frac 1 2 (++)V^2 = \\frac{V^2}{2} (-++).$$ The dyad has the following principal components: $-½ V^2$ in the direction parallel to $$ and $+½ V^2$ in all directions perpendicular to $$ . Now, apply this separately to the $$ and $$ parts, above, to get the desired results. The eigenvalues for the $$ part will be $±B^2/(2μ_0)$ or, in your notation $±B^2/(8π)$ , while the eigenvalues for the $$ part will be $±ε_0E^2/2$ , or in your notation, $±E^2/(8πc^2)$ . In other words, they were taking eigenvectors/eigenvalues ... but only for the $$ and $$ parts separately. Instead, you should actually be looking at the eigenvalues and eigenvectors for the whole thing, not just for the parts. You should be perform the eigenvector/eigenvalue problem on either the 3×3 matrix of the spatial-coordinate components of the stress tensor, if you're looking for the decomposition of that, or else for the full 4×4 matrix of the stress tensor, itself. For convenience, we'll write $ = /μ_0$ . Consider instead the following transform: $$\\sqrt{ε_0} = \\cos θ - \\sin θ, \\hspace 1em \\sqrt{μ_0} = \\cos θ + \\sin θ$$ for some vectors $$ , $$ and angle $θ$ later to be determined. Then: $$ \\frac{ε_0 E^2 + μ_0 H^2}2 = \\frac{m^2 + n^2}2, \\\\ \\frac{ε_0 E^2 - μ_0 H^2}2 = \\frac{m^2 - n^2}2 \\cos{2θ} - · \\sin{2θ}, \\\\ \\sqrt{ε_0 μ_0} · = · \\cos{2θ} + \\frac{m^2 - n^2}2 \\sin{2θ}, \\\\ \\sqrt{ε_0 μ_0} × = ×. $$ Now impose the requirement that $$ and $$ be orthogonal: $· = 0$ . Then $$\\sqrt{ε_0μ_0} · \\cos{2θ} = \\frac{ε_0 E^2 - μ_0 H^2}2 \\sin{2θ}.$$ Therefore, $$ \\cos{2θ} = \\frac{ε_0 E^2 - μ_0 H^2}2, \\hspace 1em \\sin{2θ} = \\sqrt{ε_0 μ_0} ·,$$ for some factor $$ that's determined by the condition that ${\\cos}^2{2θ} + {\\sin}^2{2θ} = 1$ . From this condition, we find: $$^2 = {\\left(\\frac{ε_0 E^2 - μ_0 H^2}2\\right)}^2 + {\\left(\\sqrt{ε_0μ_0} ·\\right)}^2 = {\\left(\\frac{ε_0 E^2 + μ_0 H^2}2\\right)}^2 - {\\left|\\sqrt{ε_0μ_0} ×\\right|}^2.$$ Write $$ = \\frac{ε_0 E^2 + μ_0 H^2}2 = \\frac{m^2 + n^2}2, \\hspace 1em = \\sqrt{ε_0μ_0} × = ×.$$ Then $$ = \\sqrt{^2 - ^2}.$$ Substituting into the equations for $m^2$ and $n^2$ , we have: $$\\frac{m^2 + n^2}2 = , \\hspace 1em \\frac{m^2 - n^2}2 = .$$ Thus $$m = \\sqrt{ + }, \\hspace 1em n = \\sqrt{ - }.$$ Denote the unit vectors aligned with $$ and $$ , respectively, $$ and $$ , and define $ = ×$ . Then, we have $$ = \\sqrt{ + } , \\hspace 1em = \\sqrt{ - } , \\hspace 1em = × = .$$ In tensor-dyad form, we can write the components $\\left(T^{ij}: i, j = 1, 2, 3\\right)$ as: $$-ε_0 - μ_0 + \\frac{ε_0E^2 + μ_0H^2}2 = - - + ( + + ) ,$$ or, noting that $$ + = ( + ) + ( - ) ,$$ as: $$-ε_0 - μ_0 + \\frac{ε_0E^2 + μ_0H^2}2 = - + - .$$ For the other components, we have: $$T^{00} = -, \\hspace 1em \\left(T^{10}, T^{20}, T^{30}\\right) = \\left(T^{01},T^{02},T^{03}\\right) = × = .$$ For the null field (i.e. where $ε_0E^2 = μ_0H^2$ and $· = 0$ ), one has $ = 0$ and $ = $ , this reduces further to: $$T^{00} = -, \\hspace 1em \\left(T^{10}, T^{20}, T^{30}\\right) = \\left(T^{01}, T^{02}, T^{03}\\right) = , \\hspace 1em \\left(T^{ij}: i, j = 1, 2, 3\\right) = - .$$ It has not passed my notice that this looks a lot like what you encounter in the quantized version of the field theory. Essentially, this is a directed continuum of photons, with energy density $$ oriented in the direction $$ - the classical version of a photon stream. The stress tensor has eigenvalues (meaning: zero pressure) for the transverse directions $$ and $$ and an $-$ eigenvalue for the longitudinal direction $$ . In your units $μ_0 = 4π$ , $ε_0 = 1/{4πc^2}$ , $ = μ_0 = 4π$ , and: $$ = \\frac{ε_0 E^2 + μ_0 H^2}2 = \\frac{E^2/c^2 + B^2}{8π}.$$ So, the eigenvalues are 0, for eigenvectors $$ and $$ (i.e. the directions along the plane formed by $$ and $$ ); and $-(E^2/c^2 + B^2)/{8π}$ for $$ (directions perpendicular to $$ and $$ ). For non-null fields, the zero eigenvalues split into $-$ and $+$ along the directions of $$ and $$ respectively. These are axes rotated from the $(/c, )$ frame, with the relations: $$ = \\sqrt{ε_0} \\cos θ + \\sqrt{μ_0} \\sin θ, \\hspace 1em = \\sqrt{μ_0} \\cos θ - \\sqrt{ε_0} \\sin θ$$ with $$\\cos{2θ} = \\frac{(ε_0 E^2 - μ_0 H^2)/2}{}, \\hspace 1em \\sin{2θ} = \\frac{\\sqrt{ε_0μ_0} ·}{}.$$ We can solve for $\\cos θ$ and $\\sin θ$ : $$\\cos θ = ±\\sqrt{\\frac{1 + \\cos{2θ}}2} = ±\\sqrt{\\frac{ + (ε_0 E^2 - μ_0 H^2)/2}{2}}, \\\\ \\sin θ = \\text{sgn}(±·) \\sqrt{\\frac{1 - \\cos{2θ}}2} = \\text{sgn}(±·) \\sqrt{\\frac{ - (ε_0 E^2 - μ_0 H^2)/2}{2}}.$$ One can also carry out the eigenvalue/eigenvector analysis for the full 4×4 tensor, rather than just the space-like 3×3 sub-matrix, posing the problem as: $$T^{μν} u_ν = λ g^{μν} u_ν$$ Then, the eigenvectors $$ and $$ become $(0,)$ and $(0,)$ , again with respective eigenvalues $-$ and $+$ . In place of the eigenvector $$ are two eigenvectors of the form $\\left(u_0, \\right)$ , with: $$- u_0 + = λ g^{00} u_0 = -λ, \\hspace 1em u_0 - = λ g^{11} = λ ,$$ leading to the equations: $$( - λ) u_0 = , \\hspace 1em u_0 = + λ,$$ or $$^2 - λ^2 = ^2 ⇒ λ = ±, \\hspace 1em u_0 = \\frac{ ± }{}.$$ The $$ eigenvector and $-$ eigenvalue is now $(( - )/, )$ with eigenvalue $-$ , while we also have the fourth eigenvector $(( ± )/, )$ with eigenvalue $$ . Up to powers of $c$ , the quantities $$ , $$ and $$ are energy-density, momentum-density and mass-density. The null field, which is effectively a directed continuum of photons, therefore has zero mass density, in this sense of the term. Parts of this analysis predates Einstein. He wasn't the first to pose the formula $E = mc^2$ . Nothing in the above is actually specific to relativity, nor even to a vacuum. It can equally well be carried out in a non-relativistic setting, or for isotropic media other than a vacuum (either non-relativistic media, or even relativistic media). In that case, $ε_0$ and $μ_0$ generalize, respectively to $ε$ and $μ$ , while $c = 1/\\sqrt{ε_0μ_0}$ generalizes to the wave speed, denoted here by $V = 1/\\sqrt{εμ}$ . Making the various densities dimensionally consistent, $$ would be replaced by $V$ , while $$ would be replaced by $V^2$ , and their relation with the energy density would become $^2 - ^2 V^2 = ^2 V^4$ . The references of note are the following: Poincaré's analysis (1900) \"La théorie de Lorentz la physique expérimentale et la physique mathématique”, Revue générale des sciences pures et appliquées 11 : 1163-1175. which I suspect might dovetail into the analysis just done here, and is almost certainly couched firmly in the non-relativistic paradigm (though I haven't looked at it closely yet), versus Einstein's analysis (1905) “Ist die Trägheit eines Körpers von seinem Energieinhalt abhängig?”, Annalen der Physik , 18 , 639-641, September 1905. which is more fundamental and bona fide relativistic.",
      "question_latex": [
        "T^{\\mu\\nu}=\\frac1{4\\pi}(F^{\\mu\\alpha}F^{\\nu}{}_\\alpha-\\frac14 g^{\\mu\\nu} F_{\\alpha\\beta}F^{\\alpha\\beta}).",
        "</span></p>\n<p>My textbook (unpublished textbook by Blandford and Kip Thorne) says that the stress tensor consists of:</p>\n<ul>\n<li><p>A pressure <span class=\"math-container\">",
        "</span> orthogonal to <span class=\"math-container\">",
        "</span> and a pressure <span class=\"math-container\">",
        "</span>.</p>\n</li>\n<li><p>A tension <span class=\"math-container\">",
        "</span> <em>along</em> <span class=\"math-container\">",
        "</span> and a tension <span class=\"math-container\">",
        "</span> along <span class=\"math-container\">",
        "</span>.</p>\n</li>\n</ul>\n<p>I had written down the full expression of the components of the stress-energy tensor <span class=\"math-container\">",
        "</span> and i haven't got a clue of what these statements mean, both qualitatively and quantatively. Can someone clarify this issue by explaining me the meaning of the aforementioned \"pressure orthogonal to <span class=\"math-container\">",
        "</span> and <span class=\"math-container\">",
        "</span>\" and \"stress along <span class=\"math-container\">"
      ],
      "answer_latex": [
        "T^{μν} = \\frac{1}{μ_0}\\left(F^{μα}{F^ν}_α - \\frac{1}{4} g^{μν} F_{αβ}F^{αβ}\\right),",
        "\\left(x^0, x^1, x^2, x^3\\right) = (ct, x, y, z), \\hspace 1em F_{0i} = \\frac {E_i} c, \\hspace 1em F_{ij} = B^k, \\\\\nη_{00} = -1, \\hspace 1em η_{0i} = 0 = η_{i0}, \\hspace 1em η_{ij} = δ_{ij}",
        "T^{00} = \\frac 1 2 \\left(ε_0 E^2 + \\frac{B^2}{μ_0}\\right) = \\frac {ε_0} 2 \\left(E^2 + B^2 c^2\\right), \\\\\n\\left(T^{10}, T^{20}, T^{30}\\right) = \\frac{×}{μ_0 c} = ε_0c× = \\left(T^{01}, T^{02}, T^{03}\\right), \\\\\nT^{ij} = -ε_0 E_i E_j - \\frac{B_i B_j}{μ_0} + δ^{ij} \\left(ε_0 E^2 + \\frac{B^2}{μ_0}\\right) = -ε_0 \\left(E_iE_j + B^iB^jc^2 - \\frac 1 2 δ^{ij} \\left(E^2 + B^2 c^2\\right)\\right),",
        "-ε_0 \\left( + c^2 - \\frac 1 2  \\left(E^2 + B^2 c^2\\right)\\right).",
        "</span> and <span class=\"math-container\">",
        "-ε_0 \\left( - \\frac 1 2  E^2\\right), \\hspace 1em -\\frac{1}{μ_0} \\left( - \\frac 1 2  B^2\\right).",
        "</span>, the corresponding expression <span class=\"math-container\">$- + ½  V^2$</span>. Suppose the vector is in the <span class=\"math-container\">$x$</span> direction with <span class=\"math-container\">$ = V$</span>, where <span class=\"math-container\">$(, , )$</span> will be used here to denote the unit vectors, respectively, for the <span class=\"math-container\">$x$</span>, <span class=\"math-container\">$y$</span> and <span class=\"math-container\">$z$</span> directions. Then\n<span class=\"math-container\">",
        "</span>\nThe dyad has the following principal components: <span class=\"math-container\">$-½ V^2$</span> in the direction parallel to <span class=\"math-container\">",
        "</span>. Now, apply this separately to the <span class=\"math-container\">",
        "</span> parts, above, to get the desired results. The eigenvalues for the <span class=\"math-container\">",
        "</span> part will be <span class=\"math-container\">$±ε_0E^2/2$</span>, or in your notation, <span class=\"math-container\">$±E^2/(8πc^2)$</span>.</p>\n<p>In other words, they were taking eigenvectors/eigenvalues ... but only for the <span class=\"math-container\">",
        "</span> parts separately. Instead, you should <i>actually</i> be looking at the eigenvalues and eigenvectors for the whole thing, not just for the parts. You should be perform the eigenvector/eigenvalue problem on either the 3×3 matrix of the spatial-coordinate components of the stress tensor, if you're looking for the decomposition of that, or else for the full 4×4 matrix of the stress tensor, itself.</p>\n<p>For convenience, we'll write <span class=\"math-container\">$ = /μ_0$</span>. Consider instead the following transform:\n<span class=\"math-container\">",
        "</span>\nfor some vectors <span class=\"math-container\">",
        "</span> and angle <span class=\"math-container\">$θ$</span> later to be determined. Then:\n<span class=\"math-container\">",
        "</span></p>\n<p>Now impose the requirement that <span class=\"math-container\">",
        "</span> be orthogonal: <span class=\"math-container\">$· = 0$</span>. Then\n<span class=\"math-container\">",
        "</span>\nTherefore,\n<span class=\"math-container\">",
        "</span>\nfor some factor <span class=\"math-container\">",
        "^2 = {\\left(\\frac{ε_0 E^2 - μ_0 H^2}2\\right)}^2 + {\\left(\\sqrt{ε_0μ_0} ·\\right)}^2 = {\\left(\\frac{ε_0 E^2 + μ_0 H^2}2\\right)}^2 - {\\left|\\sqrt{ε_0μ_0} ×\\right|}^2.",
        "= \\frac{ε_0 E^2 + μ_0 H^2}2 = \\frac{m^2 + n^2}2, \\hspace 1em  = \\sqrt{ε_0μ_0} × = ×.",
        "= \\sqrt{^2 - ^2}.",
        "\\frac{m^2 + n^2}2 = , \\hspace 1em \\frac{m^2 - n^2}2 = .",
        "m = \\sqrt{ + }, \\hspace 1em n = \\sqrt{ - }.",
        "</span> and <span class=\"math-container\">",
        "</span> and <span class=\"math-container\">",
        "= \\sqrt{ + } , \\hspace 1em  = \\sqrt{ - } , \\hspace 1em  = × =  .",
        "-ε_0 - μ_0 +  \\frac{ε_0E^2 + μ_0H^2}2 = - -  + ( +  + ) ,",
        "+  = ( + )  + ( - ) ,",
        "-ε_0 - μ_0 +  \\frac{ε_0E^2 + μ_0H^2}2 = -  +   -  .",
        "T^{00} = -, \\hspace 1em\n\\left(T^{10}, T^{20}, T^{30}\\right) = \\left(T^{01},T^{02},T^{03}\\right) = × = .",
        "T^{00} = -, \\hspace 1em\n\\left(T^{10}, T^{20}, T^{30}\\right) = \\left(T^{01}, T^{02}, T^{03}\\right) =  , \\hspace 1em \\left(T^{ij}: i, j = 1, 2, 3\\right) = - .",
        "</span> oriented in the direction <span class=\"math-container\">",
        "</span> and <span class=\"math-container\">",
        "</span>.</p>\n<p>In your units <span class=\"math-container\">$μ_0 = 4π$</span>, <span class=\"math-container\">$ε_0 = 1/{4πc^2}$</span>, <span class=\"math-container\">$ = μ_0  = 4π$</span>, and:\n<span class=\"math-container\">",
        "</span>\nSo, the eigenvalues are 0, for eigenvectors <span class=\"math-container\">",
        "</span> (i.e. the directions along the plane formed by <span class=\"math-container\">",
        "</span>); and <span class=\"math-container\">$-(E^2/c^2 + B^2)/{8π}$</span> for <span class=\"math-container\">",
        "</span> and <span class=\"math-container\">",
        "</span> and <span class=\"math-container\">",
        "= \\sqrt{ε_0}  \\cos θ + \\sqrt{μ_0}  \\sin θ, \\hspace 1em  = \\sqrt{μ_0}  \\cos θ - \\sqrt{ε_0}  \\sin θ",
        "\\cos{2θ} = \\frac{(ε_0 E^2 - μ_0 H^2)/2}{}, \\hspace 1em \\sin{2θ} = \\frac{\\sqrt{ε_0μ_0} ·}{}.",
        "\\cos θ = ±\\sqrt{\\frac{1 + \\cos{2θ}}2} = ±\\sqrt{\\frac{ + (ε_0 E^2 - μ_0 H^2)/2}{2}}, \\\\\n\\sin θ = \\text{sgn}(±·) \\sqrt{\\frac{1 - \\cos{2θ}}2} = \\text{sgn}(±·)\n\\sqrt{\\frac{ - (ε_0 E^2 - μ_0 H^2)/2}{2}}.",
        "T^{μν} u_ν = λ g^{μν} u_ν",
        "</span> and <span class=\"math-container\">",
        "</span> are two eigenvectors of the form <span class=\"math-container\">$\\left(u_0, \\right)$</span>, with:\n<span class=\"math-container\">",
        "</span>\nleading to the equations:\n<span class=\"math-container\">",
        "</span>\nor\n<span class=\"math-container\">",
        "</span>\nThe <span class=\"math-container\">",
        "</span>.</p>\n<p>Up to powers of <span class=\"math-container\">$c$</span>, the quantities <span class=\"math-container\">",
        "</span> and <span class=\"math-container\">",
        "</span> would be replaced by <span class=\"math-container\">$V$</span>, while <span class=\"math-container\">",
        "μ_0",
        "4π",
        "</span>\nand with your metric <span class=\"math-container\">",
        "</span> being written there as the Minkowski metric <span class=\"math-container\">",
        "</span>. A close examination of the context indicates that it is using the conventions (which it didn't spell out, explicitly):\n<span class=\"math-container\">",
        "</span>\nas <span class=\"math-container\">",
        "</span> range cyclically over <span class=\"math-container\">",
        "</span>, <span class=\"math-container\">",
        "</span>; and with <span class=\"math-container\">",
        "</span>. This yields the components listed in the article:\n<span class=\"math-container\">",
        "</span>\nwhere <span class=\"math-container\">",
        "</span>. Under your convention, <span class=\"math-container\">",
        "</span>. The last term can be written in tensor-dyad form as:\n<span class=\"math-container\">",
        "</span></p>\n<p>The authors' analysis is, at best, sloppy, if that's what they really wrote. They're saying, just look at the <span class=\"math-container\">",
        "</span> parts separately:\n<span class=\"math-container\">",
        "</span>\nMore genrally, consider for <i>any</i> vector <span class=\"math-container\">",
        "</span>, the corresponding expression <span class=\"math-container\">",
        "</span>. Suppose the vector is in the <span class=\"math-container\">",
        "</span> direction with <span class=\"math-container\">",
        "</span>, where <span class=\"math-container\">",
        "</span> will be used here to denote the unit vectors, respectively, for the <span class=\"math-container\">",
        "</span> directions. Then\n<span class=\"math-container\">",
        "- + \\frac 1 2  V^2 = -V^2 + \\frac 1 2 (++)V^2 = \\frac{V^2}{2} (-++).",
        "</span>\nThe dyad has the following principal components: <span class=\"math-container\">",
        "</span> in the direction parallel to <span class=\"math-container\">",
        "</span> in all directions perpendicular to <span class=\"math-container\">",
        "</span> part will be <span class=\"math-container\">",
        "</span> or, in your notation <span class=\"math-container\">",
        "</span>, while the eigenvalues for the <span class=\"math-container\">",
        "</span>, or in your notation, <span class=\"math-container\">",
        "</span>.</p>\n<p>In other words, they were taking eigenvectors/eigenvalues ... but only for the <span class=\"math-container\">",
        "</span> parts separately. Instead, you should <i>actually</i> be looking at the eigenvalues and eigenvectors for the whole thing, not just for the parts. You should be perform the eigenvector/eigenvalue problem on either the 3×3 matrix of the spatial-coordinate components of the stress tensor, if you're looking for the decomposition of that, or else for the full 4×4 matrix of the stress tensor, itself.</p>\n<p>For convenience, we'll write <span class=\"math-container\">",
        "</span>. Consider instead the following transform:\n<span class=\"math-container\">",
        "\\sqrt{ε_0}  =  \\cos θ -  \\sin θ, \\hspace 1em \\sqrt{μ_0}  =  \\cos θ +  \\sin θ",
        "</span> and angle <span class=\"math-container\">",
        "</span> later to be determined. Then:\n<span class=\"math-container\">",
        "\\frac{ε_0 E^2 + μ_0 H^2}2 = \\frac{m^2 + n^2}2, \\\\\n\\frac{ε_0 E^2 - μ_0 H^2}2 = \\frac{m^2 - n^2}2 \\cos{2θ} - · \\sin{2θ}, \\\\\n\\sqrt{ε_0 μ_0} · = · \\cos{2θ} + \\frac{m^2 - n^2}2 \\sin{2θ}, \\\\ \n\\sqrt{ε_0 μ_0} × = ×.",
        "</span> be orthogonal: <span class=\"math-container\">",
        "</span>. Then\n<span class=\"math-container\">",
        "\\sqrt{ε_0μ_0} · \\cos{2θ} = \\frac{ε_0 E^2 - μ_0 H^2}2 \\sin{2θ}.",
        "\\cos{2θ} = \\frac{ε_0 E^2 - μ_0 H^2}2, \\hspace 1em  \\sin{2θ} = \\sqrt{ε_0 μ_0} ·,",
        "</span> that's determined by the condition that <span class=\"math-container\">",
        "</span>. From this condition, we find:\n<span class=\"math-container\">",
        "</span></p>\n<p>Write\n<span class=\"math-container\">",
        "</span>\nThen\n<span class=\"math-container\">",
        "</span>\nSubstituting into the equations for <span class=\"math-container\">",
        "</span>, we have:\n<span class=\"math-container\">",
        "</span>\nThus\n<span class=\"math-container\">",
        "</span>\nDenote the unit vectors aligned with <span class=\"math-container\">",
        "</span>, respectively, <span class=\"math-container\">",
        "</span>, and define <span class=\"math-container\">",
        "</span>. Then, we have\n<span class=\"math-container\">",
        "</span></p>\n<p>In tensor-dyad form, we can write the components <span class=\"math-container\">",
        "</span> as:\n<span class=\"math-container\">",
        "</span>\nor, noting that\n<span class=\"math-container\">",
        "</span>\nas:\n<span class=\"math-container\">",
        "</span>\nFor the other components, we have:\n<span class=\"math-container\">",
        "</span></p>\n<p>For the <em>null field</em> (i.e. where <span class=\"math-container\">",
        "</span>), one has <span class=\"math-container\">",
        "</span>, this reduces further to:\n<span class=\"math-container\">",
        "</span></p>\n<p>It has not passed my notice that this looks a lot like what you encounter in the quantized version of the field theory. Essentially, this is a directed continuum of photons, with energy density <span class=\"math-container\">",
        "</span> - the classical version of a photon stream. The stress tensor has eigenvalues (meaning: zero pressure) for the transverse directions <span class=\"math-container\">",
        "</span> and an <span class=\"math-container\">",
        "</span> eigenvalue for the longitudinal direction <span class=\"math-container\">",
        "</span>.</p>\n<p>In your units <span class=\"math-container\">",
        "</span>, and:\n<span class=\"math-container\">",
        "= \\frac{ε_0 E^2 + μ_0 H^2}2 = \\frac{E^2/c^2 + B^2}{8π}.",
        "</span>); and <span class=\"math-container\">",
        "</span> for <span class=\"math-container\">",
        "</span> (directions perpendicular to <span class=\"math-container\">",
        "</span>).</p>\n<p>For non-null fields, the zero eigenvalues split into <span class=\"math-container\">",
        "</span> along the directions of <span class=\"math-container\">",
        "</span> respectively. These are axes rotated from the <span class=\"math-container\">",
        "</span> frame, with the relations:\n<span class=\"math-container\">",
        "</span>\nwith\n<span class=\"math-container\">",
        "</span>\nWe can solve for <span class=\"math-container\">",
        "</span>:\n<span class=\"math-container\">",
        "</span></p>\n<p>One can also carry out the eigenvalue/eigenvector analysis for the full 4×4 tensor, rather than just the space-like 3×3 sub-matrix, posing the problem as:\n<span class=\"math-container\">",
        "</span>\nThen, the eigenvectors <span class=\"math-container\">",
        "</span> become <span class=\"math-container\">",
        "</span>, again with respective eigenvalues <span class=\"math-container\">",
        "</span>. In place of the eigenvector <span class=\"math-container\">",
        "</span> are two eigenvectors of the form <span class=\"math-container\">",
        "</span>, with:\n<span class=\"math-container\">",
        "- u_0 +  = λ g^{00} u_0 = -λ, \\hspace 1em   u_0 -   = λ g^{11}  = λ ,",
        "( - λ) u_0 = , \\hspace 1em  u_0 =  + λ,",
        "^2 - λ^2 = ^2 ⇒ λ = ±, \\hspace 1em u_0 = \\frac{ ± }{}.",
        "</span> eigenvector and <span class=\"math-container\">",
        "</span> eigenvalue is now <span class=\"math-container\">",
        "</span> with eigenvalue <span class=\"math-container\">",
        "</span>, while we also have the fourth eigenvector <span class=\"math-container\">",
        "</span>.</p>\n<p>Up to powers of <span class=\"math-container\">",
        "</span>, the quantities <span class=\"math-container\">",
        "</span> are energy-density, momentum-density and mass-density. The null field, which is effectively a directed continuum of photons, therefore has zero mass density, in this sense of the term.</p>\n<p>Parts of this analysis <b>predates</b> Einstein. He wasn't the first to pose the formula <span class=\"math-container\">",
        "</span>. Nothing in the above is actually <i>specific</i> to relativity, nor even to a vacuum. It can equally well be carried out in a non-relativistic setting, or for isotropic media other than a vacuum (either non-relativistic media, or even relativistic media). In that case, <span class=\"math-container\">",
        "</span> generalize, respectively to <span class=\"math-container\">",
        "</span>, while <span class=\"math-container\">",
        "</span> generalizes to the wave speed, denoted here by <span class=\"math-container\">",
        "</span>. Making the various densities dimensionally consistent, <span class=\"math-container\">",
        "</span> would be replaced by <span class=\"math-container\">",
        "</span>, and their relation with the energy density would become <span class=\"math-container\">"
      ],
      "created": "2014-11-10T18:58:49.283",
      "golden_ner_terms": [
        "analysis",
        "angle",
        "components",
        "consistent",
        "context",
        "continuum",
        "decomposition",
        "density",
        "dyad",
        "eigenvalue",
        "eigenvalue problem",
        "eigenvalues",
        "eigenvector",
        "eigenvectors",
        "energy",
        "even",
        "expression",
        "factor",
        "field",
        "field theory",
        "formula",
        "frame",
        "link",
        "mass",
        "matrix",
        "mean",
        "metric",
        "nor",
        "null",
        "oriented",
        "orthogonal",
        "parallel",
        "perpendicular",
        "photons",
        "place",
        "plane",
        "pressure",
        "range",
        "relation",
        "relativity",
        "speed",
        "stream",
        "sub-matrix",
        "tensor",
        "term",
        "theory",
        "transform",
        "transverse",
        "unit",
        "unit vector",
        "vacuum",
        "vector",
        "vectors",
        "wikipedia",
        "zero"
      ],
      "golden_ner_count": 55,
      "golden_patterns": [
        {
          "pattern": "find-the-right-abstraction",
          "score": 2.0,
          "hotwords": [
            "generalize"
          ]
        },
        {
          "pattern": "quotient-by-irrelevance",
          "score": 2.0,
          "hotwords": [
            "up to"
          ]
        },
        {
          "pattern": "check-the-extreme-cases",
          "score": 2.0,
          "hotwords": [
            "zero"
          ]
        },
        {
          "pattern": "construct-an-explicit-witness",
          "score": 2.0,
          "hotwords": [
            "explicit"
          ]
        }
      ],
      "golden_pattern_names": [
        "find-the-right-abstraction",
        "quotient-by-irrelevance",
        "check-the-extreme-cases",
        "construct-an-explicit-witness"
      ],
      "golden_scopes": [
        {
          "type": "consider",
          "match": "Consider instead the following transform: "
        }
      ],
      "golden_scope_count": 1
    },
    {
      "id": "se-physics-203986",
      "stratum": "hard",
      "title": "Why would an alpha-beryllium neutron source generating a radioisotope not produce controllable net power?",
      "tags": [
        "nuclear-engineering",
        "accelerator-physics",
        "particle-accelerators"
      ],
      "score": 4,
      "answer_score": 2,
      "question_body": "I have a question. I was looking through the table of nuclides and noticed that the energy needed to produce certain radioisotopes was less than the energy given off by the radioactive decay of that isotope. Looking at the isotopes, the energy balance came off positively for the following reaction chain: He4 + Be-9 + 2.7 MeV -> C-12 + neutron neutron + B-11 -> B-12 B-12 -> C-12 + e- + 13.3 MeV 13.3 MeV - 2.7 MeV = 10.6 MeV net energy per alpha put in (assuming no leakages) The reactor could be powered using a 2.7 MeV electrostatic accelerator and helium ion source, with the reactor running itself and giving net power by extracting energy from the moving beta radiation using the 2.7 MeV potential difference. So my question is why would this not work? From my point of view it seems the main stumbling point is ensuring the helium ions that are accelerated a) get completely ionized with minimal wall losses and b) attain enough energy to hit the beryllium nuclei before scattering to the walls. And it might be the efficiency of the energy extractor, the beta radiation would also need to have minimal wall losses to ensure enough energy is extracted to keep the reactor running. Are these the main stumbling blocks or are there others that I am missing? What would be some ways of solving these problems?",
      "answer_body": "In short : It is reasonably feasible this reactor, at least in theory; so maybe there are technical difficulties out of this scope which prevent its usage it in practice. The analysis The reactions you present give an energy balance of the overall reaction for the reactor to work. This is a good start, but then to go further in responding what the real possibilities are, you need to balance the probabilities of each step in the reaction (cross sections, energy and geometry dependent), the time each of them takes to complete (or the rate at which they occur) and the sustainability of this reaction. All of this can be very complex (and is only the theoretical part in assessing the possibilities of the reaction) so in the following we are going to continue the naive approach: geometry and composition will be assumed optimal, without considering their practical feasibility . Elements to be considered Neutron production in the first reaction $^4He+^9Be$, in different energies Neutron thermalization for $^{11}B$ capture and its cross section $^{12}B$ decay spectrum and time it takes Neutron production in the first reaction $^4He+^9Be$ for different energies First, a note on the idea of using accelerators for driving reactors: this was proposed already by Nobel Prize Carlo Rubbia ( see wikipedia ) and is still in active research, currently using Thorium as the strongest candidate for fissile material, there are a number of publications and international conferences ( iThEO ) investigating this possibility. Ion accelerators exist and are rather abundant, so accelerating $^4He$ would not be a problem, as long as we can extract more energy than spent. $^9Be$ has a 100% natural abundance, so this already works for us: no extremely complex chemical separation needed in order to increase the rate of our first stage reaction. However is not so abundant compared to other materials on Earth, and we won't discuss the details on how could it be extracted from the compounds found in nature. The figure above (taken from IAEA ) shows the cross sections of the first reaction which appear to be very good: in the order of magnitude of $0.1 barn$. Now based on this figure let us assume the initial energy of the alphas is $10 MeV$ for convenience and because we would like to get the highest peak observed. This means that the neutrons released from this reaction will have energies up to $15 MeV$, because the difference in mass from the reacting nuclei to the resulting ones is about $5 MeV$. These neutrons will be in the so called fast range, or will be fast neutrons . On another subject, this is a direct reaction which means it should occur for times in the order of $10^{-21} s$ so is very fast. The same holds for the next reaction involved in the process which also works good for the total process, because the slowest is the last one where the $T_{1/2}$ of $^{12}B$ for beta decay (100% by the way) is $20ms$ (see Nuclear Data tables ). Let us average the total cross section of all these possible processes to $\\sigma_1 = 0.3 barn$. Neutron thermalization for $^{11}B$ capture and its cross section $^{11}B$ has an 80% abundance in nature which is also good. As with $^9Be$ is easy to find and there is probably no difficulty in extracting. In the figure below (taken from IAEA ) we see the cross section of neutron capture for $^{11}B$, and is rather low, especially since our neutrons have the energies mentioned above. This means they would need to reduce their energy in order to increase the probability of this capture reactions. Luckily all the elements we are considered are very good moderators, which means neutrons will moderate fast, and we can consider most of them will be captured. Let us assume here that the average cross section for $^{11}B$ neutron capture is $\\sigma_2=10^{-5} barn$. On the other hand because $^{10}B$ covers the other 20% abundance, 20% of the neutrons encountering a boron nucleus, won't produce $^{12}B$ because they found $^{10}B$. But will they produce $^{11}B$? How much? From the figure below (taken from IAEA ) the cross section for neutron capture of $^{10}B$ is shown. Is approximately $10^{-4} barn$ which is an order of magnitude higher than the previous one. Although we won't consider here the addition of new $^{11}B$ from this reaction, we note that it might balance the previous one since the reaction cross section is 10 times larger, while the abundance is only 4 times smaller. Following the previous interactions the neutrons need to thermalize from fast range to thermal range which requires them to undergo maybe several centimeters of a material made up of a mix of Be and B like the one considered here. We will assume that the geometry here is such that this process is maximized. However this process takes some time which is not easy to calculate, but since thermal neutrons have an average velocity of $2.2 km/s$, we can assume this process takes in average less time than the $T_{1/2}$ of the beta decay for $^{12}B$ which is $20ms$. $^{12}B$ decay spectrum and time it takes As mentioned, $^{12}B$ will decay 100% of the time via beta decay, and the $T_{1/2}$ for this decay is about $20ms$. Also for the beta decay, most of the energy escapes with the neutrino because of its low interaction, and only the energy fraction of the beta particle will effectively be used most of the time. But in this case the beta decay can occur leaving the $^{12}C$ in some of its excited states, which means that the gamma from the subsequent decay can also be used as produced energy, but this is 3% of the cases so we will consider that only the electron energy counts as produced energy. Now if we see the figure for a generic beta spectrum above (taken from here ), we can see that in average the beta particle takes about a third of the total energy, which in this case would be about $4MeV$, so this is the value we will be using for our considerations. Energy balance Now that we have cleared our considerations, we can estimate the actual energy balance accounting for cross sections and time relations. We are going to consider a cylindrical volume of cross section equal to the beam's and of unitary length $d$. We can write the Power balance or energetic balance as follows: $$P_{net} = \\sum P_k - P_{acc}$$ Where $P_acc = \\eta * E_\\alpha * I$ is the power used by the accelerator which depends on the intensity of the beam $I=2I_N$ where $I_N$ is the number of $\\alpha$ particles per second coming from the accelerator beam. Here $\\eta=P_{acc}/P_{beam}$ gives relation between the power used by the reactor to power of the beam, and which we can estimate to be 10 based on this . The power released on the first reaction can be calculated as $P_1 = E_1 * R_1$ where $R_1$ is the reaction rate for stage one which can be calculated with the macroscopic cross section $R_1 = \\phi_1 * d * \\Sigma_1 = I_N * d * \\sigma_1 \\rho_{(^{9}Be)}$ being $\\phi_1$ the flux of alpha particles coming from the beam $I_N$, and $\\rho_{(^{9}Be)}$ is the density of $^{9}Be$ atoms in the material. Now $E_1$ is the total energy produced per reaction in this stage, which mainly comes from the energy lost by expelled neutrons by moderation and the gammas of the $^{12}C$ produced here in excited state. This is not clear because while most neutrons will be absorbed after they have lost around $1-10 MeV$ of energy to the medium, some neutrons can be absorbed without losing that much energy, but will create excited $^{12}B$ excited nuclei which will eventually radiate gammas to the medium. So let us assume here $E_1 = 14 MeV$ and not consider in the next stage any energy contribution from these neutrons. Equally for the stage 2 we can estimate the power in the same way. Now $R_2 = \\phi_2 d \\sigma_2 \\rho_{(^{11}B)}$ and the energy release here we will consider to only be the difference in mass between the reacting nuclei and the product which is approximately $E_2=2.5MeV$. We will consider here $\\phi_2=\\phi_1$ which implies in ideal geometrical arrangement of the reactor as we mentioned and is coherent with the considerations done below. This is a fast portion of the process as well as the previous one, and we considered the moderation of neutrons fast enough as well, so we will consider these previous stages to occur immediately following one another and in a time scale only dominated by the delay of neutron moderation process, which let's assume that takes less than $10^{-4} s$. So this means that initially the number of $^{12}Be$ nuclei $N_{(^{12}Be)}$ starts to build up linearly and since their decay is much slower, we can assume that no energy is coming from them, so the reactor would not be sustainable initially. But at some point $N_{(^{12}Be)}$ becomes high enough so that the amount of them decaying $\\lambda N_{(^{12}Be)}$ equals the amount of them created, which is the same as the amount of reaction 2 occurring $R_2$. So putting all the above terms together we get: $P_{net} = E_1 * I_N * d * \\sigma_1 \\rho_{(^{9}Be)} + E_2 * I_N * d * \\sigma_2 \\rho_{(^{11}B)} + E_3 * I_N * d \\sigma_2 \\rho_{(^{11}B)} - \\eta E_\\alpha * 2I_N$ this relation can be normalized to $I_N$, and it depends on the balance of Be atoms and B atoms we have on the material, which is a matter of design. Let us choose them according to the cross sections in the way: $\\frac{\\sigma_1}{\\sigma_2} = \\frac{\\rho_{(^{11}B)}}{\\rho_{(^{9}Be)}}$ and that we have a rather reasonable concentration of $^{9}Be$ atoms of $\\rho_{(^{9}Be)}=10^{18}$ atoms, which put in the formula gives us: $P_{net}/I_N = \\sigma_1 \\rho_{(^{9}Be)} * \\left( E_1 + E_2 + E_3 \\right) - \\eta E_\\alpha * 2$ Using the mentioned values above, and remembering that $1eV = 1.6*10^{-19}J$ we get here a positive balance of about $400kJ$ per $\\alpha$ particle, which here means that almost twice the energy used is being produced. Of course there is some start up energy which has not being considered, but for a reactor working for sufficient time this energy can be recovered. Of course this situation would never be the case in reality, here several ideal considerations have been made. In my opinion the strongest considerations are related to the geometrical arrangement and that $\\phi_1 = \\phi_2$, which is actually very dependent on how the geometry will be. Also the fact that all neutrons and gammas deposit all their energy inside the reactor is a strong consideration. Still I think that they do not rule out the possibility for building this type of reactor, so either the system has not been studied and there is a possibility, or it has been studied and other technical difficulties have been found, like for example how factible is obtaining beryllium in nature, which is not that common compared to other materials.",
      "question_latex": [],
      "answer_latex": [
        "P_{net} = \\sum P_k - P_{acc}",
        "^4He+^9Be",
        "^{11}B",
        "^{12}B",
        "^4He",
        "^9Be",
        "0.1 barn",
        "10 MeV",
        "15 MeV",
        "5 MeV",
        "10^{-21} s",
        "T_{1/2}",
        "20ms",
        "\\sigma_1 = 0.3 barn",
        "\\sigma_2=10^{-5} barn",
        "^{10}B",
        "10^{-4} barn",
        "2.2 km/s",
        "^{12}C",
        "4MeV",
        "d",
        "Where",
        "is the power used by the accelerator which depends on the intensity of the beam",
        "where",
        "is the number of",
        "particles per second coming from the accelerator beam. </p>\n\n<p>Here",
        "gives relation between the power used by the reactor to power of the beam, and which we can estimate to be 10 based on <a href=\"https://jacowfs.jlab.org/conf/y15/ipac15/prepress/WEXC3.PDF\" rel=\"nofollow noreferrer\">this</a>.</p>\n\n<p>The power released on the first reaction can be calculated as",
        "is the reaction rate for stage one which can be calculated with the macroscopic cross section",
        "being",
        "the flux of alpha particles coming from the beam",
        ", and",
        "is the density of",
        "atoms in the material. Now",
        "is the total energy produced per reaction in this stage, which mainly comes from the energy lost by expelled neutrons by moderation and the gammas of the",
        "produced here in excited state. This is not clear because while most neutrons will be absorbed after they have lost around",
        "of energy to the medium, some neutrons can be absorbed without losing that much energy, but will create excited",
        "excited nuclei which will eventually radiate gammas to the medium. So let us assume here",
        "and not consider in the next stage any energy contribution from these neutrons.</p>\n\n<p>Equally for the stage 2 we can estimate the power in the same way. Now",
        "and the energy release here we will consider to only be the difference in mass between the reacting nuclei and the product which is approximately",
        ". We will consider here",
        "which implies in ideal geometrical arrangement of the reactor as we mentioned and is coherent with the considerations done below. </p>\n\n<p>This is a fast portion of the process as well as the previous one, and we considered the moderation of neutrons fast enough as well, so we will consider these previous stages to occur immediately following one another and in a time scale only dominated by the delay of neutron moderation process, which let's assume that takes less than",
        ".</p>\n\n<p>So this means that initially the number of",
        "nuclei",
        "starts to build up linearly and since their decay is much slower, we can assume that no energy is coming from them, so the reactor would not be sustainable initially. But at some point",
        "becomes high enough so that the amount of them decaying",
        "equals the amount of them created, which is the same as the amount of reaction 2 occurring",
        ".</p>\n\n<p>So putting all the above terms together we get:</p>\n\n<p>",
        "</p>\n\n<p>this relation can be normalized to",
        ", and it depends on the balance of Be atoms and B atoms we have on the material, which is a matter of design. Let us choose them according to the cross sections in the way:</p>\n\n<p>",
        "</p>\n\n<p>and that we have a rather reasonable concentration of",
        "atoms of",
        "atoms, which put in the formula gives us:</p>\n\n<p>",
        "</p>\n\n<p>Using the mentioned values above, and remembering that",
        "we get here a positive balance of about",
        "per",
        "particle, which here means that almost twice the energy used is being produced. Of course there is some start up energy which has not being considered, but for a reactor working for sufficient time this energy can be recovered.</p>\n\n<p>Of course this situation would never be the case in reality, here several ideal considerations have been made. In my opinion the strongest considerations are related to the geometrical arrangement and that"
      ],
      "created": "2015-09-03T06:22:47.023",
      "golden_ner_terms": [
        "abundance",
        "addition",
        "analysis",
        "atoms",
        "average",
        "balance",
        "calculate",
        "chain",
        "choose",
        "clear",
        "complete",
        "complex",
        "composition",
        "cross",
        "cross section",
        "data",
        "density",
        "design",
        "difference",
        "earth",
        "efficiency",
        "elements",
        "energy",
        "estimate",
        "eventually",
        "feasible",
        "flux",
        "formula",
        "fraction",
        "generic",
        "geometry",
        "ideal",
        "implies",
        "intensity",
        "interactions",
        "involved in",
        "ions",
        "isotope",
        "isotopes",
        "length",
        "mass",
        "matter",
        "minimal",
        "nature",
        "net",
        "neutrons",
        "nuclear",
        "nucleus",
        "number",
        "order",
        "order of magnitude",
        "point",
        "positive",
        "potential",
        "power",
        "probability",
        "product",
        "production",
        "radiation",
        "range",
        "real",
        "relation",
        "running",
        "scattering",
        "scope",
        "section",
        "separation",
        "source",
        "spectrum",
        "state",
        "step",
        "strong",
        "sufficient",
        "theory",
        "time",
        "type",
        "unitary",
        "velocity",
        "volume",
        "way",
        "wikipedia",
        "work"
      ],
      "golden_ner_count": 82,
      "golden_patterns": [
        {
          "pattern": "estimate-by-bounding",
          "score": 4.0,
          "hotwords": [
            "estimate",
            "at least"
          ]
        },
        {
          "pattern": "optimise-a-free-parameter",
          "score": 4.0,
          "hotwords": [
            "choose",
            "maximize"
          ]
        },
        {
          "pattern": "work-examples-first",
          "score": 2.0,
          "hotwords": [
            "example"
          ]
        },
        {
          "pattern": "quotient-by-irrelevance",
          "score": 2.0,
          "hotwords": [
            "up to"
          ]
        },
        {
          "pattern": "check-the-extreme-cases",
          "score": 2.0,
          "hotwords": [
            "extreme"
          ]
        },
        {
          "pattern": "construct-an-explicit-witness",
          "score": 2.0,
          "hotwords": [
            "build"
          ]
        },
        {
          "pattern": "local-to-global",
          "score": 2.0,
          "hotwords": [
            "cover"
          ]
        },
        {
          "pattern": "encode-as-algebra",
          "score": 2.0,
          "hotwords": [
            "ring"
          ]
        },
        {
          "pattern": "use-probabilistic-method",
          "score": 2.0,
          "hotwords": [
            "probability"
          ]
        },
        {
          "pattern": "unfold-the-definition",
          "score": 2.0,
          "hotwords": [
            "means that"
          ]
        },
        {
          "pattern": "monotone-approximation",
          "score": 2.0,
          "hotwords": [
            "approximate"
          ]
        }
      ],
      "golden_pattern_names": [
        "estimate-by-bounding",
        "optimise-a-free-parameter",
        "work-examples-first",
        "quotient-by-irrelevance",
        "check-the-extreme-cases",
        "construct-an-explicit-witness",
        "local-to-global",
        "encode-as-algebra",
        "use-probabilistic-method",
        "unfold-the-definition",
        "monotone-approximation"
      ],
      "golden_scopes": [
        {
          "type": "where-binding",
          "match": "where $I_N$ is"
        },
        {
          "type": "where-binding",
          "match": "where $R_1$ is"
        }
      ],
      "golden_scope_count": 2
    },
    {
      "id": "se-physics-201718",
      "stratum": "hard",
      "title": "What really happens with the charges on the surface of the conductor that let them to create equipotential surface?",
      "tags": [
        "electrostatics"
      ],
      "score": 1,
      "answer_score": 4,
      "question_body": "Everyone does know that the surface of a conductor is at equipotential during equilibrium. I was reading Feynman's lectures where I found this (bold)line: Suppose that we have a situation in which a total charge Q is placed on an arbitrary conductor. Now we will not be able to say exactly where the charges are. They will spread out in some way on the surface. How can we know how the charges have distributed themselves on the surface? They must distribute themselves so that the potential of the surface is constant. If the surface were not an equipotential, there would be an electric field inside the conductor, and the charges would keep moving until it became zero. This is a much good reasoning for the surface to be an equipotential one; if there were any region to be in higher potential, charges would flow towards them to neutralize and again make the surface equipotential. To understand his explanation, I thought of a positively charged surface that is not in equipotential status; so there would be an electric field which would prompt the free electrons inside the conductor to go there & nullify the field to make the surface equipotential, right? But what about the positive charges that are now inside the conductor? Okay, they would by repulsion move towards the surface. But what is the GUARANTEE that they would form the equipotential surface? What really happens when they go on the surface that compels them to make an equipotential surface?? [After all, you can't say:\" since you are studying electrostatics, there must be equipotential region on the surface no matter what happens; that's it \"-this is what my school-teacher said when I asked him.]",
      "answer_body": "Think of potential as like potential energy. If the mobile charges are electrons and they are mostly at rest they will mostly move towards lower potential energy which is higher potential. So think of it as like a hill with electrons free to roll down hill (higher potential) until they get to a surface of the conductor at which point they are not free to move. So, you can imagine a fence built on the surface of the conductor that keeps things from leaving. And you can imagine some hills inside. If the hills inside were flat, then indeed you could have some electrons at the edge and everything could sit there in statics. And if the hill inside was actually flat, then things can stay there. There is a mountain called Mount Saint Helens. It looks like a normal mountain except the top looks like it was just chopped off (it actually exploded in a huge eruption in 1980). The edge of the top is all at the same height; things on the edge would fall (and move away) if they were free to move. This is exactly what is going on in a conductor. Now imagine you poured water on the top. The water is free to move if the pile of water gets higher than the region around. Real water has some surface tension but the point is that it can even itself come out because a region of higher water is free to move. The same thing happens in the conductor. If you had an isolated positive charge, it would create a positive potential about itself which is a negative potential energy around itself. This would attract electrons. So if electrons moved away from a positive charge leaving a positive charge all by itself, that would not be creating equilibrium because it would attract more electrons to it. Charges can move. If a charge moved from the surface to another point inside and the charge there moved to a other point inside and so for until eventually a charge near another part of the surface moves to the surface, that is all fine. So do that. Draw a bunch of surfaces of constant potentiality in red. Then draw a bunch of curves in red that are ways orthogonal to the surfaces, these are the field lines of the electric field. Now there is no force pushing in the direction of the equipotential and the force is all in the direction of the field. Just because there is a force in a direction doesn't mean that charges go that way; that depends on the initial velocity of the charge. But imagine a region large enough to have many charges in it, the average velocity of the mobile charges could be zero because they bump into the non mobile charges except in the direction of the electric field. There they could have a non-zero average because they give and get from the non mobile charges but they consistently gain momentum in the direction of the electric field. So now imagine a current everywhere pointing in the direction of the electric field. If you are in electrostatics then the field lines don't make any loops. So they can start at one part of the surface and end on another part of the surface. That is exactly how charge can flow. This is important. This is saying that if all the charges were placed on the surface in a way that wasn't an equipotential and all the charges were kept at rest and were kept there for some reason so they are there in place long enough for say the speed of light to traverse across the whole object then there would then be electrostatic fields throughout the object. If you now let go and let all the mobile charges move inside the conductor then the field at that moment is electrostatic and pointed so that current will flow through the body so that charge on the surface flows to other parts of the surface. And it never stops anywhere inside the object. This is so frustrating. I've basically described how if you add charge to a conductor that has no net charge on the inside then the charge moves around in such a way that it continues to have no net charge on the inside. I'm saying that your concern about the positive charge never happens in the first place. How rude. But there is some truth. If conductors start with the property of having no net charge in the body of the conductor then they can continue to have that property. That is essential for electrostatics and all statics in general. I talked about their being no loops that really just means we can keep magnetism out of it (loops of electric fields is associated with changing magnetic fields). But we really had the electric field line go from one part of the surface to the other because there was no non-zero charge density to terminate on. So what if for some reason you just made a conductor or just started getting to statics. Then there might be charge in the body. Now if you still have the current be proportional to the electric field you get for instance $\\vec J=\\sigma \\vec E$ then you can take the divergence of both sides and get $$\\sigma\\frac{\\rho}{\\epsilon_0}=\\sigma\\vec \\nabla \\cdot \\vec E = \\vec \\nabla \\cdot \\vec J$$ Where we used the Maxwell equation $\\dfrac{\\rho}{\\epsilon_0}=\\vec \\nabla \\cdot \\vec E$ and we can also take the divergence of $$\\vec \\nabla \\times \\vec B=\\mu_0\\vec J+\\mu_0\\epsilon_0\\frac{\\partial \\vec E}{\\partial t}$$ to get the continuity equation $$\\vec \\nabla \\cdot \\vec J=-\\epsilon_0\\vec \\nabla \\cdot\\frac{\\partial \\vec E}{\\partial t}=-\\frac{\\partial \\rho}{\\partial t}.$$ This means we have $$\\frac{\\partial \\rho}{\\partial t}=-\\vec \\nabla \\cdot \\vec J=-\\frac{\\sigma}{\\epsilon_0}\\rho.$$ Now we have stepped out of electrostatics but that is because we are talking about how it got that way. But now we see the charge density decreases when it is positive and increases when it is negative and in fact it approaches zero exponentially. So a conductor is never perfect you can think of it as something with a super huge $\\sigma$ and so it gets super close to no charge in the body in almost no time. Now, there is another reason I didn't go straight to this. There is no law of physics that says $\\vec J=\\sigma \\vec E$ and in fact even materials that behave approximately like that have the value of $\\sigma$ depend on the frequency of the change if it changes or even just depending on the temperature (and current changes the temperature). If you placed a huge amount of charge deep inside a conductor by say shooting electrons going at almost the speed of light at it. Then yes the conductor might heat and buckle and strain and generally take some time to get that charge to the outside if it was a tremendous amount of charge. No real material is perfect. But to the degree that the charges follow the electric field then when there is a charge density the field lines will be pointing the direction as to attract a countering charge. Which is what we started with. If you had an imbalanced positive charge, it will attract mobile electrons to it and reduce the charge imbalance. If you have an excess of electrons in a spot they will push the neighbour mobile charges away from it and thus reduce the density in that region that contained them all. And the continuous charge density is always for a region that contains many charges it isn't a thing that jumps up at every proton and down at every electron. So it approaches zero by attracting more mobile charges into it or pushing more mobile charges out of it depending on whether the field diverges there in a positive or negative way and it does that based on whether there is a positive or negative charge density there. What did Feynman want to tell in the bold lines? If the equipotential surface wasn't everywhere tangent to the surface then it would dip inside. Sort of like if you divided your room in two and told your sibling not to cross to your half (TV sitcom style) and your brother or sister blew a bubble. If the bubble was tangent to the imaginary surface it wasn't supposed to cross; but if it isn't tangent to that surface then you can follow the surface into your side. So the equipotential would cross inside the conductor. Which means you can follow it inside to the body of the conductor and from there you can look at the normal to the equipotential and you'll see a field line going there. That field line either terminates on a charge density (where the electric field lines terminate) or goes to the surface of the conductor. So it has two ends either both in the surface, both on densities inside or from a density inside to the surface. If current moves in the direction of electric fields then current can flow between the two ends. If they are both on the surface then one part of the surface will have less charge at the spot that field line hit it and the the other place the field line hit the surface gains it. If one of the ends is inside then that charge density will approach the zero exponentially fast and the current will be causing it so an opposite charge will head to the other end of the field line. So two opposite charge densities inside could be weakening each other or a charge imbalance inside can be sending its excess to the surface. And both. A place where field lines terminate can have field lines from multiple directions coming out of it and they could end on different places. It might help if you drew the fields for two oppositely charged point charges then draw an arbitrarily surface around then so it contains them both. Where those field lines go to each other they are discharging each other. Where those field lines hit the surface you drew is where current is liking up charge on the surface. Eventually all the charge is on the surface. That's a good example to look at. But the example in the last paragraph doesn't explain what happens when you place charge on the surface. So draw the two charges again. Draw the field lines again. And draw a new surface, but this time draw a new surface but make sure it passes through one of them and contains the other one inside. Again, the current can follow the field lines so the charge that starts out on the surface travels through the body towards the opposite charge and some travels through the conductor from that charge to where the field line terminates on the surface and some actually travels along the surface as a surface current where the surface is tangent to the field. The two concentrations of charge discharge exponentially and you end up with current spread out on the surface. OK. Now draw a surface where it intersects both charges on its surface. Here is an example where all the charge is in the surface. It moves through the conductor and sometimes along the surface but here is the key. The current doesn't have a nonzero divergence inside the conductor so even though current flows through the body the body never acquires a nonzero charge density. So the charge stays in the surface always even though current flows through the conductor. Make sure you can see that. Current can flow even if there is no net charge density. So when Feynman says the charge moves around the surface he doesn't mean that there is can't be current inside. But the charge imbalance stays on the outside if there was no change imbalance inside. Charge can flow inside as long as equal amounts flow in and out of every region in the body of the conductor then no imbalance will form. And as we saw earlier if there originally was a charge imbalance inside then it decays away exponentially. This is how an ohmic material ($\\vec J=\\sigma \\vec E$) behaves. A different material might behaves slightly differently but if it is a good conductor it will behave similarly and you can think of a perfect conductor as an ohmic material with $\\sigma=\\infty$ so the charge on the surface just flash gets to that final configuration that an ohmic material would go to. And that's how to think of a perfect conductor. Just imagine a material with $\\vec J =\\sigma \\vec E$ and find out the state it approaches after an infinite amount of time. Then imagine your object gets super close to that in a very short time. So basically imagine that $\\vec J =\\sigma \\vec E$ for a huge $\\sigma$ even if your material isn't as simple as having $\\vec J =\\sigma \\vec E.$",
      "question_latex": [],
      "answer_latex": [
        "\\sigma\\frac{\\rho}{\\epsilon_0}=\\sigma\\vec \\nabla \\cdot \\vec E = \\vec \\nabla \\cdot \\vec J",
        "\\vec \\nabla \\times \\vec B=\\mu_0\\vec J+\\mu_0\\epsilon_0\\frac{\\partial \\vec E}{\\partial t}",
        "\\vec \\nabla \\cdot \\vec J=-\\epsilon_0\\vec \\nabla \\cdot\\frac{\\partial \\vec E}{\\partial t}=-\\frac{\\partial \\rho}{\\partial t}.",
        "\\frac{\\partial \\rho}{\\partial t}=-\\vec \\nabla \\cdot \\vec J=-\\frac{\\sigma}{\\epsilon_0}\\rho.",
        "\\vec J=\\sigma \\vec E",
        "</p>\n\n<p>Where we used the Maxwell equation",
        "and we can also take the divergence of",
        "to get the continuity equation</p>\n\n<p>",
        "</p>\n\n<p>This means we have",
        "</p>\n\n<p>Now we have stepped out of electrostatics but that is because we are talking about how it got that way. But now we see the charge density decreases when it is positive and increases when it is negative and in fact it approaches zero exponentially. So a conductor is never perfect you can think of it as something with a super huge",
        "and so it gets super close to no charge in the body in almost no time.</p>\n\n<p>Now, there is another reason I didn't go straight to this. There is no law of physics that says",
        "and in fact even materials that behave approximately like that have the value of",
        "depend on the frequency of the change if it changes or even just depending on the temperature (and current changes the temperature).</p>\n\n<p>If you placed a huge amount of charge deep inside a conductor by say shooting electrons going at almost the speed of light at it. Then yes the conductor might heat and buckle and strain and generally take some time to get that charge to the outside if it was a tremendous amount of charge. No real material is perfect.</p>\n\n<p>But to the degree that the charges follow the electric field then when there is a charge density the field lines will be pointing the direction as to attract a countering charge.</p>\n\n<p>Which is what we started with. If you had an imbalanced positive charge, it will attract mobile electrons to it and reduce the charge imbalance. If you have an excess of electrons in a spot they will push the neighbour mobile charges away from it and thus reduce the density in that region that contained them all.</p>\n\n<p>And the continuous charge density is always for a region that contains many charges it isn't a thing that jumps up at every proton and down at every electron.</p>\n\n<p>So it approaches zero by attracting more mobile charges into it or pushing more mobile charges out of it depending on whether the field diverges there in a positive or negative way and it does that based on whether there is a positive or negative charge density there.</p>\n\n<blockquote>\n  <p>What did Feynman want to tell in the bold lines?</p>\n</blockquote>\n\n<p>If the equipotential surface wasn't everywhere tangent to the surface then it would dip inside. Sort of like if you divided your room in two and told your sibling not to cross to your half (TV sitcom style) and your brother or sister blew a bubble. If the bubble was tangent to the imaginary surface it wasn't supposed to cross; but if it isn't tangent to that surface then you can follow the surface into your side.</p>\n\n<p>So the equipotential would cross inside the conductor. Which means you can follow it inside to the body of the conductor and from there you can look at the normal to the equipotential and you'll see a field line going there.  That field line either terminates on a charge density (where the electric field lines terminate) or goes to the surface of the conductor. So it has two ends either both in the surface, both on densities inside or from a density inside to the surface.</p>\n\n<p>If current moves in the direction of electric fields then current can flow between the two ends. If they are both on the surface then one part of the surface will have less charge at the spot that field line hit it and the the other place the field line hit the surface gains it.</p>\n\n<p>If one of the ends is inside then that charge density will approach the zero exponentially fast and the current will be causing it so an opposite charge will head to the other end of the field line. So two opposite charge densities inside could be weakening each other or a charge imbalance inside can be sending its excess to the surface. And both. A place where field lines terminate can have field lines from multiple directions coming out of it and they could end on different places.</p>\n\n<p>It might help if you drew the fields for two oppositely charged point charges then draw an arbitrarily surface around then so it contains them both. Where those field lines go to each other they are discharging each other. Where those field lines hit the surface you drew is where current is liking up charge on the surface. Eventually all the charge is on the surface. That's a good example to look at.</p>\n\n<p>But the example in the last paragraph doesn't explain what happens when you place charge on the surface. So draw the two charges again. Draw the field lines again. And draw a new surface, but this time draw a new surface but make sure it passes through one of them and contains the other one inside.  Again, the current can follow the field lines so the charge that starts out on the surface travels through the body towards the opposite charge and some travels through the conductor from that charge to where the field line terminates on the surface and some actually travels along the surface as a surface current where the surface is tangent to the field. The two concentrations of charge discharge exponentially and you end up with current spread out on the surface.</p>\n\n<p>OK. Now draw a surface where it intersects both charges on its surface. Here is an example where all the charge is in the surface. It moves through the conductor and sometimes along the surface but here is the key. The current doesn't have a nonzero divergence inside the conductor so even though current flows through the body the body never acquires a nonzero charge density. So the charge stays in the surface always even though current flows through the conductor.</p>\n\n<p>Make sure you can see that. Current can flow even if there is no net charge density. So when Feynman says the charge moves around the surface he doesn't mean that there is can't be current inside. But the charge imbalance stays on the outside if there was no change imbalance inside. Charge can flow inside as long as equal amounts flow in and out of every region in the body of the conductor then no imbalance will form.</p>\n\n<p>And as we saw earlier if there originally was a charge imbalance inside then it decays away exponentially. This is how an ohmic material (",
        ") behaves. A different material might behaves slightly differently but if it is a good conductor it will behave similarly and you can think of a perfect conductor as an ohmic material with",
        "so the charge on the surface just flash gets to that final configuration that an ohmic material would go to.</p>\n\n<p>And that's how to think of a perfect conductor. Just imagine a material with",
        "and find out the state it approaches after an infinite amount of time. Then imagine your object gets super close to that in a very short time. So basically imagine that",
        "for a huge",
        "even if your material isn't as simple as having"
      ],
      "created": "2015-08-21T12:27:33.007",
      "golden_ner_terms": [
        "average",
        "body",
        "charge",
        "conductor",
        "conductors",
        "configuration",
        "constant",
        "contained",
        "contains",
        "continuity equation",
        "continuous",
        "cross",
        "current",
        "degree",
        "density",
        "divergence",
        "edge",
        "electric fields",
        "electrons",
        "electrostatics",
        "energy",
        "equation",
        "equilibrium",
        "even",
        "eventually",
        "fence",
        "field",
        "flat",
        "flow",
        "frequency",
        "heat",
        "height",
        "imaginary",
        "infinite",
        "instance",
        "intersects",
        "isolated",
        "key",
        "line",
        "magnetic fields",
        "matter",
        "mean",
        "moment",
        "momentum",
        "multiple",
        "near",
        "negative",
        "net",
        "normal",
        "object",
        "opposite",
        "orthogonal",
        "passes through",
        "perfect",
        "physics",
        "place",
        "point",
        "positive",
        "potential",
        "potential energy",
        "property",
        "proportional",
        "push",
        "real",
        "region",
        "right",
        "side",
        "simple",
        "sort",
        "speed",
        "speed of light",
        "state",
        "statics",
        "straight",
        "strain",
        "surface",
        "surface tension",
        "tangent",
        "temperature",
        "time",
        "top",
        "velocity",
        "water",
        "way",
        "zero"
      ],
      "golden_ner_count": 85,
      "golden_patterns": [
        {
          "pattern": "work-examples-first",
          "score": 4.0,
          "hotwords": [
            "example",
            "for instance"
          ]
        },
        {
          "pattern": "check-the-extreme-cases",
          "score": 2.0,
          "hotwords": [
            "zero"
          ]
        },
        {
          "pattern": "encode-as-algebra",
          "score": 2.0,
          "hotwords": [
            "ring"
          ]
        },
        {
          "pattern": "monotone-approximation",
          "score": 2.0,
          "hotwords": [
            "approximate"
          ]
        }
      ],
      "golden_pattern_names": [
        "work-examples-first",
        "check-the-extreme-cases",
        "encode-as-algebra",
        "monotone-approximation"
      ],
      "golden_scopes": [],
      "golden_scope_count": 0
    },
    {
      "id": "se-physics-758492",
      "stratum": "hard",
      "title": "Why does Fock operator in Hartree-Fock method have the molecule symmetry?",
      "tags": [
        "many-body",
        "quantum-chemistry"
      ],
      "score": 1,
      "answer_score": 3,
      "question_body": "I am reading Szabo - 1967 - Modern Quantum Chemistry. In Chapter 3 P122, it mentions that the spin orbitals $\\left|\\chi_a\\right\\rangle$ obtained by the Fock operator $f\\left|\\chi_a\\right\\rangle=\\varepsilon_a\\left|\\chi_a\\right\\rangle$ can have the symmetry of the molecule. (form a basis for an irreducible representation of the point group of the molecule) It confuses me a lot because it is not obvious. I guess it could be done by proving $[f,P_R]=0$ (R is any element of the molecular point group). However Fock operator is not a linear operator and has a really complicated form by $$f(1)=h(1)+\\sum_b \\int d \\mathbf{x}_2 \\chi_b^*(2) r_{12}^{-1}\\left(1-\\mathscr{P}_{12}\\right) \\chi_b(2)$$ where $h(1)=h(\\mathbf x_1)$ and I do know it commutes with $P_R$ , ${P}_{12}$ is exchange label 1<-->2. I don't know how to do it next.",
      "answer_body": "This is an important question. What you need to realize is that the symmetry of the HF orbitals $\\{\\phi_p\\}$ and the Fockian $\\hat{F}$ are not guarenteed properties of the model $-$ we actually make $\\{\\phi_p\\}$ and $\\hat{F}$ obey certain symmetries for computational and interpretational convenience. The correct symmetries of $\\{\\phi_p\\}$ and $\\hat{F}$ depend on each other, as will be shown below. To keep things simple, I will talk about closed shell systems only (that is, all orbitals belonging to certain irreducible representations are doubly populated); the exact wave function is then a singlet spin eigenfunction ( $S=0$ ), and belongs to the totally symmetric irreducible representation of point group $G$ . If you did not care that much about the wave function, you could do HF calculations without imposing any kind of symmetry restraints on the orbitals! This approach is called Unrestricted HF (UHF), and can often lead to lower energies compared to a properly symmetry-adapted HF calculation, since you solve a less constrained optimization problem, as discussed in Chapter 3.8 of Szabo & Ostlund. Their UHF spin orbitals for H $_2$ $$ |\\psi\\rangle=\\cos(\\theta)|\\sigma^+_g\\rangle+\\sin(\\theta)|\\sigma^+_u\\rangle \\ \\ \\ , \\ \\ \\ |\\bar{\\psi}\\rangle=\\cos(\\theta)|\\bar{\\sigma}^+_g\\rangle-\\sin(\\theta)|\\bar{\\sigma}^+_u\\rangle $$ break both spin and spatial symmetry for $\\theta\\neq0$ , and neither the UHF determinant, nor $\\hat{F}$ have the correct $^1\\Sigma_g^+$ symmetry. This example shows that point group symmetry is not cemented in the HF formalism, but we have to enforce it. One way to do this would be the projection of the UHF Ansatz onto the subspace of the desired symmetry: $$ |\\Psi\\rangle=\\hat{P}_S\\hat{P}_G|\\Phi_{\\text{UHF}}\\rangle \\ , $$ which, however has its own problems (appearance of non-differentiable points on potential energy curves, violation of size consistency, etc.). A much better way to ensure the global symmetry is to force the set of HF orbitals to be symmetry orbitals (forming the basis of the irreducible representations of $G$ ). In LCAO calculations for molecules, this is achieved by taking the appropriate symmetry-adapted linear combinations of fixed atomic orbitals, while in atomic calculations ( $G=O(3)$ ), we write the orbitals in the form $$ \\phi_{nlm}(r,\\Omega)=\\chi_{nl}(r)Y^m_l(\\Omega) \\ , $$ and solve the HF equations for the radial parts. This is where we can actually turn to your question about the symmetry of the Fockian. If $\\hat{F}$ (built from symmetry orbitals) was not totally symmetric under the action of $G$ , then the orbitals could not have been made symmetry orbitals in the first place! We have to check the inner consistency of our choice; we simply have to see if $\\phi_p$ and $\\hat{F}\\phi_p$ belong to the same symmetry species. If $\\phi_p$ is the basis function of a $D$ -dimensional irreducible representation of $G$ , then the action of $\\hat{R}\\in G$ reads $$ \\hat{R}\\phi_p(\\boldsymbol{r})=\\sum_{q=1}^DR_{pq}\\phi_q(\\boldsymbol{r}) \\ , $$ where $\\boldsymbol{R}$ is a unitary matrix representation of $\\hat{R}$ . The action of $\\hat{F}$ reads $$ \\hat{F}\\phi_p(\\boldsymbol{r})=\\hat{h}\\phi_p(\\boldsymbol{r})+ \\int\\mathrm{d}^3r' \\frac{1}{|\\boldsymbol{r}-\\boldsymbol{r}'|} \\left[ \\gamma(\\boldsymbol{r}',\\boldsymbol{r}')\\phi_p(\\boldsymbol{r}) -\\frac{1}{2}\\gamma(\\boldsymbol{r},\\boldsymbol{r}')\\phi_p(\\boldsymbol{r}') \\right] \\ , $$ $\\gamma(\\boldsymbol{r},\\boldsymbol{r}')$ denoting the one-body reduced density matrix: $$ \\gamma(\\boldsymbol{r},\\boldsymbol{r}')= 2\\sum_{k\\in \\, \\text{occ}} \\phi^{*}_k(\\boldsymbol{r}')\\phi_k(\\boldsymbol{r}) \\ . $$ The two terms of $\\hat{F}\\phi_p$ correspond to the direct (Coulomb) and exchange interactions, respectively (the factor of 2 stemming from the spin summation). Operators $\\hat{h}$ and $1/|\\boldsymbol{r}_1-\\boldsymbol{r}_2|$ are totally symmetric under $G$ (pretty much by construction), so the action of $\\hat{R}$ on $\\hat{F}\\phi_p$ looks like $$ \\begin{aligned} \\hat{R}\\hat{F}\\phi_p(\\boldsymbol{r}) &= \\hat{h}\\hat{R}\\phi_p(\\boldsymbol{r})+ \\int\\mathrm{d}^3r' \\frac{1}{|\\boldsymbol{r}-\\boldsymbol{r}'|}\\hat{R} \\left[ 2\\gamma(\\boldsymbol{r}',\\boldsymbol{r}')\\phi_p(\\boldsymbol{r}) -\\gamma(\\boldsymbol{r},\\boldsymbol{r}')\\phi_p(\\boldsymbol{r}') \\right] \\\\ &= \\hat{h}\\hat{R}\\phi_p(\\boldsymbol{r})+ \\int\\mathrm{d}^3r' \\frac{1}{|\\boldsymbol{r}-\\boldsymbol{r}'|} \\left[ 2[\\hat{R}\\gamma(\\boldsymbol{r}',\\boldsymbol{r}')]\\hat{R}\\phi_p(\\boldsymbol{r}) -[\\hat{R}\\gamma(\\boldsymbol{r},\\boldsymbol{r}')]\\hat{R}\\phi_p(\\boldsymbol{r}') \\right] \\ . \\end{aligned} $$ To see the effect of $\\hat{R}$ on $\\gamma(\\boldsymbol{r},\\boldsymbol{r}')$ , we split the $k$ -sum between the $\\cal{N}$ irreducible representations the orbitals occupy (for e.g. a Ne atom, the sum runs over two one-dimensional and a three-dimensional representation corresponding to $1s$ , $2s$ and $2p$ ): $$ \\begin{aligned} \\hat{R}\\gamma(\\boldsymbol{r},\\boldsymbol{r}') &= 2\\hat{R}\\sum_{a=1}^{\\cal{N}}\\sum_{d=1}^{D_a} \\phi^{*}_{ad}(\\boldsymbol{r}')\\phi_{ad}(\\boldsymbol{r}) \\\\ &= 2\\sum_{a=1}^{\\cal{N}}\\sum_{d=1}^{D_a} \\sum_{d_1,d_2=1}^{D_a} R^{(a)*}_{dd_1}R^{(a)}_{dd_2} \\phi^{*}_{ad_1}(\\boldsymbol{r}')\\phi_{ad_2}(\\boldsymbol{r}) \\\\ &= 2\\sum_{a=1}^{\\cal{N}}\\sum_{d_1,d_2=1}^{D_a} \\delta_{d_1d_2} \\phi^{*}_{ad_1}(\\boldsymbol{r}')\\phi_{ad_2}(\\boldsymbol{r}) \\\\ &= 2\\sum_{a=1}^{\\cal{N}}\\sum_{d_1=1}^{D_a} \\phi^{*}_{ad_1}(\\boldsymbol{r}')\\phi_{ad_1}(\\boldsymbol{r}) \\\\ &= \\gamma(\\boldsymbol{r},\\boldsymbol{r}') \\ , \\end{aligned} $$ where the unitarity of $\\boldsymbol{R}$ was used. So $\\gamma(\\boldsymbol{r},\\boldsymbol{r}')$ is symmetric under $G$ , and then we have $$ \\begin{aligned} \\hat{R}\\hat{F}\\phi_p(\\boldsymbol{r}) &= \\hat{h}\\hat{R}\\phi_p(\\boldsymbol{r})+ \\int\\mathrm{d}^3r' \\frac{1}{|\\boldsymbol{r}-\\boldsymbol{r}'|} \\left[ 2\\gamma(\\boldsymbol{r}',\\boldsymbol{r}')\\hat{R}\\phi_p(\\boldsymbol{r}) -\\gamma(\\boldsymbol{r},\\boldsymbol{r}')\\hat{R}\\phi_p(\\boldsymbol{r}') \\right] \\\\ &= \\hat{F}\\hat{R}\\phi_p(\\boldsymbol{r}) \\ , \\end{aligned} $$ explicitly showing the correct symmetry of $\\hat{F}$ . But if $\\hat{F}$ is totally symmetric, then its eigenfunctions can be chosen to be symmetry functions, and thus we have come full circle. So, the main message is: $\\hat{F}$ is not necessarily totally symmetric under $G$ , but it can be made symmetric with the proper parametrization of orbitals in a perfectly consistent way. Note, however, that the above considerations are generally only true for closed-shell systems. If some of the orbitals in a given irreducible representation are only partially filled, then the product of the unitary matrices cannot be completed, hence $\\hat{R}\\gamma(\\boldsymbol{r},\\boldsymbol{r}')\\neq\\gamma(\\boldsymbol{r},\\boldsymbol{r}')$ in general, and $\\hat{F}$ is not totally symmetric. For example, the HF potential typically does not have spherical symmetry for open-shell atoms. An extra detail It might be useful to know that for closed shell atoms, the $O(3)$ invariance of $\\gamma(\\boldsymbol{r},\\boldsymbol{r}')$ immediately follows from the addition theorem of spherical harmonics as well: $$ \\begin{aligned} \\gamma(\\boldsymbol{r},\\boldsymbol{r}') &= \\sum_{n,l} \\chi_{nl}(r)\\chi_{nl}(r') \\sum_{m=-l}^{+l}Y_l^m(\\Omega)Y_l^{m*}(\\Omega') \\\\ &= \\sum_{n,l} \\chi_{nl}(r)\\chi_{nl}(r') \\frac{2l+1}{4\\pi}P_l(\\cos(\\alpha)) \\ . \\end{aligned} $$ The final result depends only on $r$ , $r'$ and $\\alpha$ (the angle enclosed by $\\boldsymbol{r}$ and $\\boldsymbol{r}'$ ), making the rotational symmetry manifest. Update (2023.08.13.) All the derivations above were carried out in coordinate representation. While this representation is more intuitive, the action of the symmetry operators on different coordinates is not always clear enough. For this reason, I will now redo the derivations in a representation-free fashion. Let us have two Hilbert spaces $H_1$ and $H_2$ each having the same set of symmetry adapted orbitals $|\\phi_p^{(1)}\\rangle\\in H_1$ , $|\\phi_p^{(2)}\\rangle\\in H_2$ . The action of the Fockian on say $|\\phi_p^{(1)}\\rangle$ then reads $$ \\hat{F}|\\phi_p^{(1)}\\rangle=\\hat{h}|\\phi_p^{(1)}\\rangle+ \\sum_{k \\in \\, \\text{occ}} \\left[2\\langle\\phi_k^{(2)}|\\hat{V}|\\phi_k^{(2)}\\rangle|\\phi_p^{(1)}\\rangle-\\langle\\phi_k^{(2)}|\\hat{V}|\\phi_p^{(2)}\\rangle|\\phi_k^{(1)}\\rangle\\right] \\ , $$ where $\\hat{V}$ is the electron-electron interaction operator acting on $H_1\\otimes H_2$ . Note that $\\langle\\phi_k^{(2)}|\\hat{V}|\\phi_k^{(2)}\\rangle$ and $\\langle\\phi_k^{(2)}|\\hat{V}|\\phi_p^{(2)}\\rangle$ are not numbers but one-body operators acting on $H_1$ . As a sanity check, you may verify that the above equation turns into the coordinate-representation version (given above) after projecting with $\\langle\\boldsymbol{r}_1|$ , inserting $$ 1=\\int\\mathrm{d}^3r'_1|\\boldsymbol{r}'_1\\rangle\\langle\\boldsymbol{r}'_1| \\ \\ \\ , \\ \\ \\ 1=\\int\\mathrm{d}^3r''_2|\\boldsymbol{r}''_2\\rangle\\langle\\boldsymbol{r}''_2| \\ , $$ and using $$ \\langle\\boldsymbol{r}_1|\\hat{h}|\\boldsymbol{r}'_1\\rangle= \\hat{h}(\\boldsymbol{r}_1)\\delta(\\boldsymbol{r}_1-\\boldsymbol{r}'_1) \\ \\ \\ , \\ \\ \\ \\langle\\boldsymbol{r}_1\\boldsymbol{r}_2|\\hat{V}|\\boldsymbol{r}'_1\\boldsymbol{r}'_2\\rangle= \\frac{1}{|\\boldsymbol{r}_1-\\boldsymbol{r}_2|} \\delta(\\boldsymbol{r}_1-\\boldsymbol{r}'_1)\\delta(\\boldsymbol{r}_2-\\boldsymbol{r}'_2) \\ . $$ The symmetry operator $\\hat{R}_1$ acts on $|\\phi_p^{(1)}\\rangle$ as $$ \\hat{R}_1|\\phi_p^{(1)}\\rangle=\\sum_{q=1}^DR_{pq}|\\phi_q^{(1)}\\rangle \\ , $$ and the analogous equation holds for $\\hat{R}_2$ and $|\\phi_p^{(2)}\\rangle$ (note that the $R_{pq}$ matrix elements do not carry any Hilbert space index). Now, let us hit $\\hat{F}|\\phi_p^{(1)}\\rangle$ with $\\hat{R}_1$ : $$ \\begin{aligned} \\hat{R}_1\\hat{F}|\\phi_p^{(1)}\\rangle &= \\hat{R}_1\\hat{h}|\\phi_p^{(1)}\\rangle + \\sum_{k \\in \\, \\text{occ}} \\left[2\\hat{R}_1\\langle\\phi_k^{(2)}|\\hat{V}|\\phi_k^{(2)}\\rangle|\\phi_p^{(1)}\\rangle-\\hat{R}_1\\langle\\phi_k^{(2)}|\\hat{V}|\\phi_p^{(2)}\\rangle|\\phi_k^{(1)}\\rangle\\right] \\\\ &= \\hat{h}\\hat{R}_1|\\phi_p^{(1)}\\rangle + \\sum_{k \\in \\, \\text{occ}} \\left[2\\hat{R}_1\\langle\\phi_k^{(2)}|\\hat{V}|\\phi_k^{(2)}\\rangle\\hat{R}_1^{\\dagger}\\hat{R}_1|\\phi_p^{(1)}\\rangle-\\hat{R}_1\\langle\\phi_k^{(2)}|\\hat{V}|\\phi_p^{(2)}\\rangle\\hat{R}_1^{\\dagger}\\hat{R}_1|\\phi_k^{(1)}\\rangle\\right] \\ . \\end{aligned} $$ We used $\\hat{R}_1^{\\dagger}\\hat{R}_1=\\hat{I}_1$ and $\\hat{R}_1^{\\dagger}\\hat{h}\\hat{R}_1=\\hat{h}$ . We need to work out the transformation of the two remaining terms. The direct part reads $$ \\begin{aligned} \\sum_{k \\in \\, \\text{occ}} \\hat{R}_1\\langle\\phi_k^{(2)}|\\hat{V}|\\phi_k^{(2)}\\rangle\\hat{R}_1^{\\dagger} &= \\sum_{k \\in \\, \\text{occ}} \\langle\\phi_k^{(2)}|\\hat{R}_1\\hat{V}\\hat{R}_1^{\\dagger}|\\phi_k^{(2)}\\rangle \\\\ &= \\sum_{k \\in \\, \\text{occ}} \\langle\\phi_k^{(2)}|\\hat{R}_2^{\\dagger}\\hat{R}_2\\hat{R}_1\\hat{V}\\hat{R}_1^{\\dagger}\\hat{R}_2^{\\dagger}\\hat{R}_2|\\phi_k^{(2)}\\rangle \\\\ &= \\sum_{k \\in \\, \\text{occ}} \\langle\\phi_k^{(2)}|\\hat{R}_2^{\\dagger}\\hat{V}\\hat{R}_2|\\phi_k^{(2)}\\rangle \\\\ &= \\sum_{a=1}^{\\cal{N}} \\sum_{d=1}^{D_a} \\langle\\phi_{ad}^{(2)}|\\hat{R}_2^{\\dagger}\\hat{V}\\hat{R}_2|\\phi_{ad}^{(2)}\\rangle \\\\ &= \\sum_{a=1}^{\\cal{N}} \\sum_{d=1}^{D_a} \\sum_{d_1,d_2=1}^{D_a} R_{dd_1}^{(a)*}R_{dd_2}^{(a)} \\langle\\phi_{ad_1}^{(2)}|\\hat{V}|\\phi_{ad_2}^{(2)}\\rangle \\\\ &= \\sum_{a=1}^{\\cal{N}} \\sum_{d_1=1}^{D_a} \\langle\\phi_{ad_1}^{(2)}|\\hat{V}|\\phi_{ad_1}^{(2)}\\rangle \\\\ &= \\sum_{k \\in \\, \\text{occ}} \\langle\\phi_k^{(2)}|\\hat{V}|\\phi_k^{(2)}\\rangle \\ . \\end{aligned} $$ We used $\\hat{R}_1\\hat{R}_2=\\hat{R}_2\\hat{R}_1$ and $(\\hat{R}_1\\hat{R}_2)\\hat{V}(\\hat{R}_1\\hat{R}_2)^{\\dagger}=\\hat{V}$ . This means that $$ \\sum_{k \\in \\, \\text{occ}} \\hat{R}_1\\langle\\phi_k^{(2)}|\\hat{V}|\\phi_k^{(2)}\\rangle\\hat{R}_1^{\\dagger}\\hat{R}_1|\\phi_p^{(1)}\\rangle= \\sum_{k \\in \\, \\text{occ}} \\langle\\phi_k^{(2)}|\\hat{V}|\\phi_k^{(2)}\\rangle\\hat{R}_1|\\phi_p^{(1)}\\rangle \\ . $$ As for the exchange term, you can verify $$ \\sum_{k \\in \\, \\text{occ}} \\hat{R}_1\\langle\\phi_k^{(2)}|\\hat{V}|\\phi_p^{(2)}\\rangle\\hat{R}_1^{\\dagger}\\hat{R}_1|\\phi_k^{(1)}\\rangle= \\sum_{k \\in \\, \\text{occ}} \\langle\\phi_k^{(2)}|\\hat{V}\\hat{R}_2|\\phi_p^{(2)}\\rangle|\\phi_k^{(1)}\\rangle $$ in a similar manner. Now, remember that the Hilbert space indices are only for bookkeeping which operators act on which states, so we can finally write $$ \\begin{aligned} \\hat{R}_1\\hat{F}|\\phi_p^{(1)}\\rangle &= \\hat{h}\\hat{R}_1|\\phi_p^{(1)}\\rangle + \\sum_{k \\in \\, \\text{occ}} \\left[2\\langle\\phi_k^{(2)}|\\hat{V}|\\phi_k^{(2)}\\rangle\\hat{R}_1|\\phi_p^{(1)}\\rangle- \\langle\\phi_k^{(2)}|\\hat{V}\\hat{R}_2|\\phi_p^{(2)}\\rangle|\\phi_k^{(1)}\\rangle \\right] \\\\ &= \\hat{F}\\hat{R}_1|\\phi_p^{(1)}\\rangle \\ , \\end{aligned} $$ or just (leaving the now unnecessary indices) $$ \\hat{R}\\hat{F}|\\phi_p\\rangle=\\hat{F}\\hat{R}|\\phi_p\\rangle \\ . $$",
      "question_latex": [
        "f(1)=h(1)+\\sum_b \\int d \\mathbf{x}_2 \\chi_b^*(2) r_{12}^{-1}\\left(1-\\mathscr{P}_{12}\\right) \\chi_b(2)",
        "\\left|\\chi_a\\right\\rangle",
        "f\\left|\\chi_a\\right\\rangle=\\varepsilon_a\\left|\\chi_a\\right\\rangle",
        "[f,P_R]=0",
        "</span></p>\n<p>where <span class=\"math-container\">",
        "</span> and I do know it commutes with <span class=\"math-container\">",
        "</span>, <span class=\"math-container\">"
      ],
      "answer_latex": [
        "|\\psi\\rangle=\\cos(\\theta)|\\sigma^+_g\\rangle+\\sin(\\theta)|\\sigma^+_u\\rangle \n\\ \\ \\ , \\ \\ \\\n|\\bar{\\psi}\\rangle=\\cos(\\theta)|\\bar{\\sigma}^+_g\\rangle-\\sin(\\theta)|\\bar{\\sigma}^+_u\\rangle",
        "|\\Psi\\rangle=\\hat{P}_S\\hat{P}_G|\\Phi_{\\text{UHF}}\\rangle \\ ,",
        "\\phi_{nlm}(r,\\Omega)=\\chi_{nl}(r)Y^m_l(\\Omega) \\ ,",
        "\\hat{R}\\phi_p(\\boldsymbol{r})=\\sum_{q=1}^DR_{pq}\\phi_q(\\boldsymbol{r}) \\ ,",
        "\\hat{F}\\phi_p(\\boldsymbol{r})=\\hat{h}\\phi_p(\\boldsymbol{r})+\n\\int\\mathrm{d}^3r'\n\\frac{1}{|\\boldsymbol{r}-\\boldsymbol{r}'|}\n\\left[\n\\gamma(\\boldsymbol{r}',\\boldsymbol{r}')\\phi_p(\\boldsymbol{r})\n-\\frac{1}{2}\\gamma(\\boldsymbol{r},\\boldsymbol{r}')\\phi_p(\\boldsymbol{r}') \n\\right] \\ ,",
        "\\gamma(\\boldsymbol{r},\\boldsymbol{r}')=\n2\\sum_{k\\in \\, \\text{occ}}\n\\phi^{*}_k(\\boldsymbol{r}')\\phi_k(\\boldsymbol{r}) \\ .",
        "\\begin{aligned}\n\\hat{R}\\hat{F}\\phi_p(\\boldsymbol{r})\n&=\n\\hat{h}\\hat{R}\\phi_p(\\boldsymbol{r})+\n\\int\\mathrm{d}^3r'\n\\frac{1}{|\\boldsymbol{r}-\\boldsymbol{r}'|}\\hat{R}\n\\left[\n2\\gamma(\\boldsymbol{r}',\\boldsymbol{r}')\\phi_p(\\boldsymbol{r})\n-\\gamma(\\boldsymbol{r},\\boldsymbol{r}')\\phi_p(\\boldsymbol{r}') \n\\right] \\\\\n&=\n\\hat{h}\\hat{R}\\phi_p(\\boldsymbol{r})+\n\\int\\mathrm{d}^3r'\n\\frac{1}{|\\boldsymbol{r}-\\boldsymbol{r}'|}\n\\left[\n2[\\hat{R}\\gamma(\\boldsymbol{r}',\\boldsymbol{r}')]\\hat{R}\\phi_p(\\boldsymbol{r})\n-[\\hat{R}\\gamma(\\boldsymbol{r},\\boldsymbol{r}')]\\hat{R}\\phi_p(\\boldsymbol{r}') \n\\right] \\ .\n\\end{aligned}",
        "\\begin{aligned}\n\\hat{R}\\gamma(\\boldsymbol{r},\\boldsymbol{r}')\n&=\n2\\hat{R}\\sum_{a=1}^{\\cal{N}}\\sum_{d=1}^{D_a}\n\\phi^{*}_{ad}(\\boldsymbol{r}')\\phi_{ad}(\\boldsymbol{r}) \\\\\n&=\n2\\sum_{a=1}^{\\cal{N}}\\sum_{d=1}^{D_a}\n\\sum_{d_1,d_2=1}^{D_a}\nR^{(a)*}_{dd_1}R^{(a)}_{dd_2}\n\\phi^{*}_{ad_1}(\\boldsymbol{r}')\\phi_{ad_2}(\\boldsymbol{r}) \\\\\n&=\n2\\sum_{a=1}^{\\cal{N}}\\sum_{d_1,d_2=1}^{D_a}\n\\delta_{d_1d_2}\n\\phi^{*}_{ad_1}(\\boldsymbol{r}')\\phi_{ad_2}(\\boldsymbol{r}) \\\\\n&=\n2\\sum_{a=1}^{\\cal{N}}\\sum_{d_1=1}^{D_a}\n\\phi^{*}_{ad_1}(\\boldsymbol{r}')\\phi_{ad_1}(\\boldsymbol{r}) \\\\\n&=\n\\gamma(\\boldsymbol{r},\\boldsymbol{r}') \\ ,\n\\end{aligned}",
        "\\begin{aligned}\n\\hat{R}\\hat{F}\\phi_p(\\boldsymbol{r})\n&=\n\\hat{h}\\hat{R}\\phi_p(\\boldsymbol{r})+\n\\int\\mathrm{d}^3r'\n\\frac{1}{|\\boldsymbol{r}-\\boldsymbol{r}'|}\n\\left[\n2\\gamma(\\boldsymbol{r}',\\boldsymbol{r}')\\hat{R}\\phi_p(\\boldsymbol{r})\n-\\gamma(\\boldsymbol{r},\\boldsymbol{r}')\\hat{R}\\phi_p(\\boldsymbol{r}') \n\\right] \\\\\n&=\n\\hat{F}\\hat{R}\\phi_p(\\boldsymbol{r}) \\ ,\n\\end{aligned}",
        "\\begin{aligned}\n\\gamma(\\boldsymbol{r},\\boldsymbol{r}')\n&=\n\\sum_{n,l}\n \\chi_{nl}(r)\\chi_{nl}(r')\n\\sum_{m=-l}^{+l}Y_l^m(\\Omega)Y_l^{m*}(\\Omega') \\\\\n&=\n\\sum_{n,l}\n \\chi_{nl}(r)\\chi_{nl}(r')\n\\frac{2l+1}{4\\pi}P_l(\\cos(\\alpha)) \\ .\n\\end{aligned}",
        "\\hat{F}|\\phi_p^{(1)}\\rangle=\\hat{h}|\\phi_p^{(1)}\\rangle+\n\\sum_{k \\in \\, \\text{occ}}\n\\left[2\\langle\\phi_k^{(2)}|\\hat{V}|\\phi_k^{(2)}\\rangle|\\phi_p^{(1)}\\rangle-\\langle\\phi_k^{(2)}|\\hat{V}|\\phi_p^{(2)}\\rangle|\\phi_k^{(1)}\\rangle\\right] \\ ,",
        "1=\\int\\mathrm{d}^3r'_1|\\boldsymbol{r}'_1\\rangle\\langle\\boldsymbol{r}'_1| \\ \\ \\ , \\ \\ \\\n1=\\int\\mathrm{d}^3r''_2|\\boldsymbol{r}''_2\\rangle\\langle\\boldsymbol{r}''_2| \\ ,",
        "\\langle\\boldsymbol{r}_1|\\hat{h}|\\boldsymbol{r}'_1\\rangle=\n\\hat{h}(\\boldsymbol{r}_1)\\delta(\\boldsymbol{r}_1-\\boldsymbol{r}'_1)\n\\ \\ \\ , \\ \\ \\ \n\\langle\\boldsymbol{r}_1\\boldsymbol{r}_2|\\hat{V}|\\boldsymbol{r}'_1\\boldsymbol{r}'_2\\rangle=\n\\frac{1}{|\\boldsymbol{r}_1-\\boldsymbol{r}_2|}\n\\delta(\\boldsymbol{r}_1-\\boldsymbol{r}'_1)\\delta(\\boldsymbol{r}_2-\\boldsymbol{r}'_2) \\ .",
        "\\hat{R}_1|\\phi_p^{(1)}\\rangle=\\sum_{q=1}^DR_{pq}|\\phi_q^{(1)}\\rangle \\ ,",
        "\\begin{aligned}\n\\hat{R}_1\\hat{F}|\\phi_p^{(1)}\\rangle\n&=\n\\hat{R}_1\\hat{h}|\\phi_p^{(1)}\\rangle\n+\n\\sum_{k \\in \\, \\text{occ}}\n\\left[2\\hat{R}_1\\langle\\phi_k^{(2)}|\\hat{V}|\\phi_k^{(2)}\\rangle|\\phi_p^{(1)}\\rangle-\\hat{R}_1\\langle\\phi_k^{(2)}|\\hat{V}|\\phi_p^{(2)}\\rangle|\\phi_k^{(1)}\\rangle\\right] \\\\\n&=\n\\hat{h}\\hat{R}_1|\\phi_p^{(1)}\\rangle\n+\n\\sum_{k \\in \\, \\text{occ}}\n\\left[2\\hat{R}_1\\langle\\phi_k^{(2)}|\\hat{V}|\\phi_k^{(2)}\\rangle\\hat{R}_1^{\\dagger}\\hat{R}_1|\\phi_p^{(1)}\\rangle-\\hat{R}_1\\langle\\phi_k^{(2)}|\\hat{V}|\\phi_p^{(2)}\\rangle\\hat{R}_1^{\\dagger}\\hat{R}_1|\\phi_k^{(1)}\\rangle\\right] \\ .\n\\end{aligned}",
        "\\begin{aligned}\n\\sum_{k \\in \\, \\text{occ}}\n\\hat{R}_1\\langle\\phi_k^{(2)}|\\hat{V}|\\phi_k^{(2)}\\rangle\\hat{R}_1^{\\dagger}\n&=\n\\sum_{k \\in \\, \\text{occ}}\n\\langle\\phi_k^{(2)}|\\hat{R}_1\\hat{V}\\hat{R}_1^{\\dagger}|\\phi_k^{(2)}\\rangle \\\\\n&=\n\\sum_{k \\in \\, \\text{occ}}\n\\langle\\phi_k^{(2)}|\\hat{R}_2^{\\dagger}\\hat{R}_2\\hat{R}_1\\hat{V}\\hat{R}_1^{\\dagger}\\hat{R}_2^{\\dagger}\\hat{R}_2|\\phi_k^{(2)}\\rangle \\\\\n&=\n\\sum_{k \\in \\, \\text{occ}}\n\\langle\\phi_k^{(2)}|\\hat{R}_2^{\\dagger}\\hat{V}\\hat{R}_2|\\phi_k^{(2)}\\rangle \\\\\n&=\n\\sum_{a=1}^{\\cal{N}}\n\\sum_{d=1}^{D_a}\n\\langle\\phi_{ad}^{(2)}|\\hat{R}_2^{\\dagger}\\hat{V}\\hat{R}_2|\\phi_{ad}^{(2)}\\rangle \\\\\n&=\n\\sum_{a=1}^{\\cal{N}}\n\\sum_{d=1}^{D_a}\n\\sum_{d_1,d_2=1}^{D_a}\nR_{dd_1}^{(a)*}R_{dd_2}^{(a)}\n\\langle\\phi_{ad_1}^{(2)}|\\hat{V}|\\phi_{ad_2}^{(2)}\\rangle \\\\\n&=\n\\sum_{a=1}^{\\cal{N}}\n\\sum_{d_1=1}^{D_a}\n\\langle\\phi_{ad_1}^{(2)}|\\hat{V}|\\phi_{ad_1}^{(2)}\\rangle \\\\\n&=\n\\sum_{k \\in \\, \\text{occ}}\n\\langle\\phi_k^{(2)}|\\hat{V}|\\phi_k^{(2)}\\rangle \\ .\n\\end{aligned}",
        "\\sum_{k \\in \\, \\text{occ}}\n\\hat{R}_1\\langle\\phi_k^{(2)}|\\hat{V}|\\phi_k^{(2)}\\rangle\\hat{R}_1^{\\dagger}\\hat{R}_1|\\phi_p^{(1)}\\rangle=\n\\sum_{k \\in \\, \\text{occ}}\n\\langle\\phi_k^{(2)}|\\hat{V}|\\phi_k^{(2)}\\rangle\\hat{R}_1|\\phi_p^{(1)}\\rangle \\ .",
        "\\sum_{k \\in \\, \\text{occ}}\n\\hat{R}_1\\langle\\phi_k^{(2)}|\\hat{V}|\\phi_p^{(2)}\\rangle\\hat{R}_1^{\\dagger}\\hat{R}_1|\\phi_k^{(1)}\\rangle=\n\\sum_{k \\in \\, \\text{occ}}\n\\langle\\phi_k^{(2)}|\\hat{V}\\hat{R}_2|\\phi_p^{(2)}\\rangle|\\phi_k^{(1)}\\rangle",
        "\\begin{aligned}\n\\hat{R}_1\\hat{F}|\\phi_p^{(1)}\\rangle\n&=\n\\hat{h}\\hat{R}_1|\\phi_p^{(1)}\\rangle\n+\n\\sum_{k \\in \\, \\text{occ}}\n\\left[2\\langle\\phi_k^{(2)}|\\hat{V}|\\phi_k^{(2)}\\rangle\\hat{R}_1|\\phi_p^{(1)}\\rangle-\n\\langle\\phi_k^{(2)}|\\hat{V}\\hat{R}_2|\\phi_p^{(2)}\\rangle|\\phi_k^{(1)}\\rangle\n\\right] \\\\\n&=\n\\hat{F}\\hat{R}_1|\\phi_p^{(1)}\\rangle \\ ,\n\\end{aligned}",
        "\\hat{R}\\hat{F}|\\phi_p\\rangle=\\hat{F}\\hat{R}|\\phi_p\\rangle \\ .",
        "\\{\\phi_p\\}",
        "\\hat{F}",
        "-",
        "S=0",
        "G",
        "_2",
        "</span>\nbreak both spin and spatial symmetry for <span class=\"math-container\">",
        "</span>, and neither the UHF determinant, nor <span class=\"math-container\">",
        "</span> have the correct <span class=\"math-container\">",
        "</span> symmetry.</p>\n<p>This example shows that point group symmetry is not cemented in the HF formalism, but we have to enforce it. One way to do this would be the projection of the UHF Ansatz onto the subspace of the desired symmetry: <span class=\"math-container\">",
        "</span>\nwhich, however has its own problems (appearance of non-differentiable points on potential energy curves, violation of size consistency, etc.).\nA much better way to ensure the global symmetry is to force the set of HF orbitals to be symmetry orbitals (forming the basis of the irreducible representations of <span class=\"math-container\">",
        "</span>). In LCAO calculations for molecules, this is achieved by taking the appropriate symmetry-adapted linear combinations of fixed atomic orbitals, while in atomic calculations (<span class=\"math-container\">",
        "</span>), we write the orbitals in the form\n<span class=\"math-container\">",
        "</span>\nand solve the HF equations for the radial parts.</p>\n<p>This is where we can actually turn to your question about the symmetry of the Fockian. If <span class=\"math-container\">",
        "</span> (built from symmetry orbitals) was not totally symmetric under the action of <span class=\"math-container\">",
        "</span>, then the orbitals could not have been made symmetry orbitals in the first place! We have to check the inner consistency of our choice; we simply have to see if <span class=\"math-container\">",
        "</span> and <span class=\"math-container\">",
        "</span> belong to the same symmetry species. If <span class=\"math-container\">",
        "</span> is the basis function of a <span class=\"math-container\">",
        "</span>-dimensional irreducible representation of <span class=\"math-container\">",
        "</span>, then the action of <span class=\"math-container\">",
        "</span> reads\n<span class=\"math-container\">",
        "</span>\nwhere <span class=\"math-container\">",
        "</span> is a unitary matrix representation of <span class=\"math-container\">",
        "</span>. The action of <span class=\"math-container\">",
        "</span>\n<span class=\"math-container\">",
        "</span> denoting the one-body reduced density matrix:\n<span class=\"math-container\">",
        "</span>\nThe two terms of <span class=\"math-container\">",
        "</span> correspond to the direct (Coulomb) and exchange interactions, respectively (the factor of 2 stemming from the spin summation).\nOperators <span class=\"math-container\">",
        "</span> are totally symmetric under <span class=\"math-container\">",
        "</span> (pretty much by construction), so the action of <span class=\"math-container\">",
        "</span>\non <span class=\"math-container\">",
        "</span> looks like\n<span class=\"math-container\">",
        "</span>\nTo see the effect of <span class=\"math-container\">",
        "</span> on <span class=\"math-container\">",
        "</span>,\nwe split the <span class=\"math-container\">",
        "</span>-sum between the <span class=\"math-container\">",
        "</span> irreducible representations the orbitals occupy (for e.g. a Ne atom, the sum runs over two one-dimensional and a three-dimensional representation corresponding to <span class=\"math-container\">",
        "</span>, <span class=\"math-container\">",
        "</span>):\n<span class=\"math-container\">",
        "</span>\nwhere the unitarity of <span class=\"math-container\">",
        "</span> was used. So <span class=\"math-container\">",
        "</span> is symmetric under <span class=\"math-container\">",
        "</span>, and then we have\n<span class=\"math-container\">",
        "</span>\nexplicitly showing the correct symmetry of <span class=\"math-container\">",
        "</span>.\nBut if <span class=\"math-container\">",
        "</span> is totally symmetric, then its eigenfunctions can be chosen to be symmetry functions, and thus we have come full circle.</p>\n<p>So, the main message is: <span class=\"math-container\">",
        "</span> is not necessarily totally symmetric under <span class=\"math-container\">",
        "</span>, but it can be made symmetric with the proper parametrization of orbitals in a perfectly consistent way.</p>\n<p>Note, however, that the above considerations are generally only true for closed-shell systems. If some of the orbitals in a given irreducible representation are only partially filled, then the product of the unitary matrices cannot be completed, hence <span class=\"math-container\">",
        "</span> in general, and <span class=\"math-container\">",
        "</span> is not totally symmetric. For example, the HF potential typically does not have spherical symmetry for open-shell atoms.</p>\n<p><strong>An extra detail</strong></p>\n<p>It might be useful to know that for closed shell atoms, the <span class=\"math-container\">",
        "</span> invariance of <span class=\"math-container\">",
        "</span> immediately follows from the addition theorem of spherical harmonics as well:\n<span class=\"math-container\">",
        "</span>\nThe final result depends only on <span class=\"math-container\">",
        "</span> (the angle enclosed by <span class=\"math-container\">",
        "</span>), making the rotational symmetry manifest.</p>\n<p><strong>Update (2023.08.13.)</strong></p>\n<p>All the derivations above were carried out in coordinate representation. While this representation is more intuitive, the action of the symmetry operators on different coordinates is not always clear enough. For this reason, I will now redo the derivations in a representation-free fashion.</p>\n<p>Let us have two Hilbert spaces <span class=\"math-container\">",
        "</span> each having the same set of symmetry adapted orbitals <span class=\"math-container\">",
        "</span>. The action of the Fockian on say <span class=\"math-container\">",
        "</span> then reads\n<span class=\"math-container\">",
        "</span> is the electron-electron interaction operator acting on <span class=\"math-container\">",
        "</span>. Note that <span class=\"math-container\">",
        "</span> are not numbers but one-body operators acting on <span class=\"math-container\">",
        "</span>. As a sanity check, you may verify that the above equation turns into the coordinate-representation version (given above) after projecting with <span class=\"math-container\">",
        "</span>, inserting\n<span class=\"math-container\">",
        "</span>\nand using\n<span class=\"math-container\">",
        "</span>\nThe symmetry operator <span class=\"math-container\">",
        "</span> acts on <span class=\"math-container\">",
        "</span> as\n<span class=\"math-container\">",
        "</span>\nand the analogous equation holds for <span class=\"math-container\">",
        "</span> (note that the <span class=\"math-container\">",
        "</span> matrix elements do not carry any Hilbert space index).\nNow, let us hit <span class=\"math-container\">",
        "</span> with <span class=\"math-container\">",
        "</span>:\n<span class=\"math-container\">",
        "</span>\nWe used <span class=\"math-container\">",
        "</span> and\n<span class=\"math-container\">",
        "</span>. We need to work out the transformation of the two remaining terms. The direct part reads\n<span class=\"math-container\">",
        "</span>.\nThis means that\n<span class=\"math-container\">",
        "</span>\nAs for the exchange term, you can verify\n<span class=\"math-container\">",
        "</span>\nin a similar manner. Now, remember that the Hilbert space indices are only for bookkeeping which operators act on which states, so we can finally write\n<span class=\"math-container\">",
        "</span>\nor just (leaving the now unnecessary indices)\n<span class=\"math-container\">"
      ],
      "created": "2023-04-06T22:11:12.350",
      "golden_ner_terms": [
        "act on",
        "action",
        "acts on",
        "adapted",
        "addition",
        "addition theorem",
        "angle",
        "ansatz",
        "atom",
        "atoms",
        "basis",
        "circle",
        "clear",
        "closed",
        "combinations",
        "consistent",
        "coordinate",
        "coordinates",
        "density",
        "determinant",
        "eigenfunction",
        "elements",
        "energy",
        "equation",
        "exchange interaction",
        "factor",
        "fixed",
        "formalism",
        "function",
        "group",
        "harmonics",
        "hilbert space",
        "index",
        "indices",
        "inner",
        "interactions",
        "irreducible",
        "label",
        "linear combination",
        "linear operator",
        "matrix",
        "matrix elements",
        "matrix representation",
        "model",
        "molecules",
        "nor",
        "numbers",
        "obvious",
        "one way",
        "onto",
        "operator",
        "operators",
        "optimization",
        "orbitals",
        "place",
        "point",
        "potential",
        "potential energy",
        "product",
        "projection",
        "quantum chemistry",
        "radial",
        "reduced",
        "representation",
        "rotational symmetry",
        "similar",
        "simple",
        "size",
        "space",
        "spherical harmonics",
        "spin orbit",
        "subspace",
        "sum",
        "summation",
        "symmetric",
        "symmetry",
        "term",
        "theorem",
        "transformation",
        "unitarity",
        "unitary",
        "unitary matrix",
        "useful",
        "wave function",
        "way",
        "work"
      ],
      "golden_ner_count": 86,
      "golden_patterns": [
        {
          "pattern": "work-examples-first",
          "score": 4.0,
          "hotwords": [
            "example",
            "e.g."
          ]
        },
        {
          "pattern": "exploit-symmetry",
          "score": 4.0,
          "hotwords": [
            "symmetry",
            "symmetric"
          ]
        },
        {
          "pattern": "construct-an-explicit-witness",
          "score": 4.0,
          "hotwords": [
            "construct",
            "explicit"
          ]
        },
        {
          "pattern": "local-to-global",
          "score": 2.0,
          "hotwords": [
            "global"
          ]
        },
        {
          "pattern": "unfold-the-definition",
          "score": 2.0,
          "hotwords": [
            "means that"
          ]
        }
      ],
      "golden_pattern_names": [
        "work-examples-first",
        "exploit-symmetry",
        "construct-an-explicit-witness",
        "local-to-global",
        "unfold-the-definition"
      ],
      "golden_scopes": [
        {
          "type": "where-binding",
          "match": "where $\\boldsymbol{R}$ is"
        },
        {
          "type": "where-binding",
          "match": "where $\\hat{V}$ is"
        },
        {
          "type": "set-notation",
          "match": "$\\hat{R}\\in G$"
        },
        {
          "type": "set-notation",
          "match": "$ \\gamma(\\boldsymbol{r},\\boldsymbol{r}')= 2\\sum_{k\\in \\, \\text{occ}} \\phi^{*}_k("
        },
        {
          "type": "set-notation",
          "match": "$|\\phi_p^{(1)}\\rangle\\in H_1$"
        },
        {
          "type": "set-notation",
          "match": "$|\\phi_p^{(2)}\\rangle\\in H_2$"
        },
        {
          "type": "set-notation",
          "match": "$ \\hat{F}|\\phi_p^{(1)}\\rangle=\\hat{h}|\\phi_p^{(1)}\\rangle+ \\sum_{k \\in \\, \\text{"
        },
        {
          "type": "set-notation",
          "match": "$ \\begin{aligned} \\hat{R}_1\\hat{F}|\\phi_p^{(1)}\\rangle &= \\hat{R}_1\\hat{h}|\\phi_"
        },
        {
          "type": "set-notation",
          "match": "$ \\begin{aligned} \\sum_{k \\in \\, \\text{occ}} \\hat{R}_1\\langle\\phi_k^{(2)}|\\hat{V"
        },
        {
          "type": "set-notation",
          "match": "$ \\sum_{k \\in \\, \\text{occ}} \\hat{R}_1\\langle\\phi_k^{(2)}|\\hat{V}|\\phi_k^{(2)}\\r"
        },
        {
          "type": "set-notation",
          "match": "$ \\sum_{k \\in \\, \\text{occ}} \\hat{R}_1\\langle\\phi_k^{(2)}|\\hat{V}|\\phi_p^{(2)}\\r"
        },
        {
          "type": "set-notation",
          "match": "$ \\begin{aligned} \\hat{R}_1\\hat{F}|\\phi_p^{(1)}\\rangle &= \\hat{h}\\hat{R}_1|\\phi_"
        }
      ],
      "golden_scope_count": 12
    },
    {
      "id": "se-physics-356536",
      "stratum": "hard",
      "title": "Electric potential vs induced emf",
      "tags": [
        "electromagnetism",
        "classical-electrodynamics"
      ],
      "score": 2,
      "answer_score": 6,
      "question_body": "Suppose we have a changing magnetic field in the $z$ direction and a conductive ring of radius r inside the magnetic field lying in the $xy$ plane. Then we have an induced emf: $$ ε= -{{dΦ} \\over {dt} }$$ This emf causes a current along the conductor-ring: $ I = \\frac{ε}{R} $ , R its resistance. (1) However we know that all points inside a conductor have the same electric potential. (2) How can both (1) and (2) be true? Some thoughts:This is a pure Faraday field and the induced electric field is of course not conservative. So how can there even be a potential? If $E$ is not conservative then we can't write $E=- \\nabla V$ , right? However we do have some sort of potential from which we get E: the emf. What's the difference between the induced emf and the electric potential? Also: what would happen if I cut the ring at one point? Would I have potential difference between the 2 ends?",
      "answer_body": "You're right. When you have a conservative $E$ field, you can define an electric potential $V$. And when you don't, you can't define such a potential (just as you can't define a potential energy $U$ for a nonconservative force $F$ - nonconservative forces still do work - but there is no associated potential energy function for such forces). General Remarks Electric potential is measured in volts and defined by $$V(x) = -\\int_{\\mathcal{O}}^x \\vec{E}\\cdot d\\vec{l} \\tag{1}$$ A voltage is defined by (in the textbooks) as a difference in potential $$\\Delta V = V_b - V_a = -\\int_a^b \\vec{E}\\cdot d\\vec{l} \\tag{2}$$ However this is too restrictive. In general, anything that takes the form $$\\int \\frac{\\vec{F}}{q}\\cdot d\\vec{l} \\tag{3}$$ can be called voltage and is measured in volts. So yes $\\text{(2)}$ above is a voltage and even $\\text{(1)}$ is a voltage if you like (as $V(x)$ is secretly $V(x) - V(\\mathcal{O}) = V(x) - 0$ and therefore a difference in potential). But note that $\\vec{F}$ can be anything. It doesn't have to be a conservative electric force. Again, anything that takes the form of $\\text{(3)}$ is measured in volts and can be called a voltage. EMF takes the form $\\text{(3)}$ [I explain this further down]. So too does potential difference. This is one reason why EMF and potential difference get mixed up: they take similar forms and hence both are measured in volts and can be called voltage (or induced voltage or whatever). And actually, this is great. If you are doing anything in the lab or talking to engineers, it doesn't really matter whether voltage means potential difference or EMF. It does, but all we really care about is energy. Voltage (equation $\\text{(3)}$)is energy. EMF and potential difference/potential are energy. Energy is energy, whether it be EMF or potential difference. Voltage is just a general term for energy. If you are ever in a situation where you don't know whether to say \"the EMF is 5 volts\" or the \"potential difference is 5 volts\", just say \"5 volts\" or \"the voltage/induced voltage/whatever is 5 volts\" and you are safe. What is EMF In order to get current to flow around a circuit, you need some force pushing charges around the wire. Let's call this the driving force $\\vec{F}$. EMF $\\mathcal{E}$ is defined as $$ \\mathcal{E} = \\oint \\frac{\\vec{F}}{q}\\cdot d\\vec{l} \\tag{4}$$ where the integration is taken around the loop. There are two main forces that drive current around a circuit: a \"source\" force from say a battery and a conservative electric field which pushes charges around the wire. $\\vec{F} = \\vec{F}_s + q\\vec{E}$. Therefore, $\\text{(4)}$ can be written $$\\mathcal{E} = \\int_a^b \\frac{\\vec{F}_s}{q}\\cdot d\\vec{l} $$ as $\\vec{F}_s$ is usually confined to a section of the loop and $E$ is conservative so it integrates to 0 (started where we left off - once around the loop). $\\vec{F}_s$ can be anything. It can be a chemical force, some temperature gradient thing, pressure on a crystal, a magnetic force, a nonconservative $E$ field, etc. So consider a battery. A conservative electric field goes from the positive terminal, around the loop, to the negative terminal, as well as from the positive terminal to the negative terminal inside the battery. Using the last equation, assuming the battery is ideal so that the chemical force is equal and opposite to the electric force, $$ \\mathcal{E} = \\int_a^b \\frac{\\vec{F}_{\\text{chemical}}}{q}\\cdot d\\vec{l} = -\\int_a^b \\vec{E}\\cdot d\\vec{l} = V$$ The EMF of the battery is equal to the potential difference across its terminals. But this does not mean that EMF is potential difference. It just happens to be so in this case. Most simple circuits turn out to be this way but realize again that EMF and potential difference are totally different. In the first place, you can't have a potential difference without an EMF generating that separation of charge in the battery. EMF generates a potential difference that happens to match the numerical value of the EMF (which you can think of as energy conservation). Then if you have a resistor connected to this battery, current $I = V/R$. I could also say $I = \\mathcal{E}/R$ as they are numerically equivalent. But it's more appropriate in my opinion to use $I = V/R$ as the energy drop is coming as electric potential energy in a conservative $E$ field (which exists throughout the wire doing the pushing). Here we begin to see, as in the next section, that All circuits require an EMF to function . Let's Look at your example There is no such thing as an electric potential in your example. There's an $E$ field, but it's not conservative. Therefore, don't say potential. There is an EMF however. You can say there's a voltage or an induced voltage if you like (from the above discussion). But there's definitely not a potential difference/potential present. For this specific example, the EMF is given by $$ \\mathcal{E} = -\\frac{d\\phi}{dt} = \\oint \\vec{E} \\cdot d\\vec{l}$$ where the driving force is that nonconservative $E$ field. The current as you say is $I = \\mathcal{E}/R$. Here again we see a true instance of the following: all circuits require an EMF . The idea that all points in a conductor are at the same potential is equivalent to saying that there is no $E$ field in a conductor. Note that this idea of there being no $E$ field in a conductor only holds for electrostatics + no time varying external magnetic fields (having a time varying $B$ prevents electrostatics anyways so saying \"+ no time varying $B$\" was redundant). Having an $E$ field in a conductor is completely fine . Turn on an $E$ field in a conductor. There is definitely an $E$ field in the conductor until electrostatics is reached. This is why conducting wires in simple circuits can have $E$ fields in them. This $E$ field is essential for driving current around even though it's through a conductor. It's just that the conductor can never reach electrostatics when it's part of a circuit. It desperately tries, but the battery prevents the wire from coming to a static situation. And with time-varying $B$ fields, nothing wrong with having an $E$ field in a conductor. When you stop varying $B$, you'll stop changing the $E$ field and things will reach statics. While you are changing the $B$ field, $E$ is changing with time. The conductor is trying to reach statics, but can never do so. So there will always be an $E$ present and hence the conductor won't be an equipotential (albeit, in simple circuits, you can take the wires to be equipotentials because $E$ is so so tiny). Too Much Theory, What to know about EMF From equation $\\text{(4)}$, because of the closed line integral, EMF does not care about conservative forces while electric potential crucially depends on a conservative E field . EMF and potential are both instances of equation $\\text{(3)}$, and therefore both tell you about energy . Potential is energy in a conservative E field . EMF is energy added to your circuit through \"nonconservative\" driving forces . In order for circuits to work, you need to pump energy into them so that charges will flow back down to low energy, making a circuit. EMF tells you how much energy driving forces give to a unit charge in one trip around the loop. Conservative forces don't give any net energy to a charge after one complete loop (started where you stopped). \"Nonconservative\" forces will give you some nonzero value to equation $\\text{(4)}$. EMF tells you how much energy was added by driving forces and hence how much energy must be dropped by dissipative/\"friction\" forces in one trip around the loop. An EMF of 5 volts means 5 volts must be dropped by every unit of charge. If you have a battery providing 2 volts of EMF and a changing magnetic field providing 6 volts of EMF, 8 volts must be dropped [by the way, If you ever see an example circuit out there with both a battery and a changing flux enclosed by the loop, more than likely their derived equations have wrong explanations. Right equation. Wrong explanation (which is basically just as bad as not knowing what you are doing). What they do is say $-d\\phi/dt = \\oint \\vec{E}\\cdot d\\vec{l}$. This is Faraday's Law, true in anycase. But what actually does that $\\vec{E}$ mean? It's the net $E$ field on your loop. In the case of a battery and a changing flux, that $E$ has both a conservative and a nonconservative component. The conservative component integrates to zero, leaving only the nonconservative $E$ providing the $-d\\phi/dt$. Therefore, if you have this simple battery + changing flux + resistor circuit, $-d\\phi/dt = I_{\\phi}R$ where $I_{\\phi}R$ is the integral of nonconservative $E$ around the loop. Now we can add a constant to each side of the equation. Since EMF battery $V_0 = I_0R$, we can say $V_0 - d\\phi/dt = (I_{\\phi} + I_0)R = IR$. Or if you want, you can write out $-d\\phi/dt = I_{\\phi}R + \\oint \\vec{E}_{\\text{conserv}} \\cdot d\\vec{l} = I_{\\phi}R - V_0 + I_0R$].",
      "question_latex": [
        "ε= -{{dΦ} \\over {dt} }",
        "z",
        "xy",
        "</p>\n\n<p>This emf causes a current along the conductor-ring:",
        ", R its resistance. <strong>(1)</strong></p>\n\n<p>However we know that all points inside a conductor have the same electric potential. <strong>(2)</strong></p>\n\n<p>How can both (1) and (2) be true?</p>\n\n<p>Some thoughts:This is a pure Faraday field and the induced electric field is of course not conservative. So how can there even be a potential? If",
        "is not conservative then we can't write"
      ],
      "answer_latex": [
        "V(x) = -\\int_{\\mathcal{O}}^x \\vec{E}\\cdot d\\vec{l} \\tag{1}",
        "\\Delta V = V_b - V_a = -\\int_a^b \\vec{E}\\cdot d\\vec{l} \\tag{2}",
        "\\int \\frac{\\vec{F}}{q}\\cdot d\\vec{l} \\tag{3}",
        "\\mathcal{E} = \\oint \\frac{\\vec{F}}{q}\\cdot d\\vec{l} \\tag{4}",
        "\\mathcal{E} = \\int_a^b \\frac{\\vec{F}_s}{q}\\cdot d\\vec{l}",
        "\\mathcal{E} = \\int_a^b \\frac{\\vec{F}_{\\text{chemical}}}{q}\\cdot d\\vec{l} = -\\int_a^b \\vec{E}\\cdot d\\vec{l} = V",
        "\\mathcal{E} = -\\frac{d\\phi}{dt} = \\oint \\vec{E} \\cdot d\\vec{l}",
        "E",
        "V",
        "U",
        "F",
        "A voltage is defined by (in the textbooks) as a difference in potential",
        "</p>\n\n<p>However this is too restrictive. In general, anything that takes the form",
        "can be called voltage and is measured in volts. So yes",
        "above is a voltage and even",
        "is a voltage if you like (as",
        "is secretly",
        "and therefore a difference in potential). But note that",
        "can be anything. It doesn't have to be a conservative electric force. Again, anything that takes the form of",
        "is measured in volts and can be called a voltage. EMF takes the form",
        "[I explain this further down]. So too does potential difference. This is one reason why EMF and potential difference get mixed up: they take similar forms and hence both are measured in volts and can be called voltage (or induced voltage or whatever). And actually, this is great. If you are doing anything in the lab or talking to engineers, it doesn't really matter whether voltage means potential difference or EMF. It does, but all we really care about is energy. Voltage (equation",
        ")is energy. EMF and potential difference/potential are energy. Energy is energy, whether it be EMF or potential difference. Voltage is just a general term for energy. If you are ever in a situation where you don't know whether to say \"the EMF is 5 volts\" or the \"potential difference is 5 volts\", just say \"5 volts\" or \"the voltage/induced voltage/whatever is 5 volts\" and you are safe. </p>\n\n<h2>What is EMF</h2>\n\n<p>In order to get current to flow around a circuit, you need some force pushing charges around the wire. Let's call this the driving force",
        ". EMF",
        "is defined as</p>\n\n<p>",
        "where the integration is taken around the loop. There are two main forces that drive current around a circuit: a \"source\" force from say a battery and a conservative electric field which pushes charges around the wire.",
        ". Therefore,",
        "can be written</p>\n\n<p>",
        "as",
        "is usually confined to a section of the loop and",
        "is conservative so it integrates to 0 (started where we left off - once around the loop).",
        "can be anything. It can be a chemical force, some temperature gradient thing, pressure on a crystal, a magnetic force, a nonconservative",
        "field, etc. So consider a battery. A conservative electric field goes from the positive terminal, around the loop, to the negative terminal, as well as from the positive terminal to the negative terminal inside the battery. Using the last equation, assuming the battery is ideal so that the chemical force is equal and opposite to the electric force, </p>\n\n<p>",
        "The EMF of the battery is equal to the potential difference across its terminals. But this does not mean that EMF is potential difference. It just happens to be so in this case. Most simple circuits turn out to be this way but realize again that EMF and potential difference are totally different. In the first place, you can't have a potential difference without an EMF generating that separation of charge in the battery. <strong>EMF generates a potential difference</strong> that happens to match the numerical value of the EMF (which you can think of as energy conservation). Then if you have a resistor connected to this battery, current",
        ". I could also say",
        "as they are numerically equivalent. But it's more <em>appropriate</em> in my opinion to use",
        "as the energy drop is coming as electric potential energy in a conservative",
        "field (which exists throughout the wire doing the pushing). Here we begin to see, as in the next section, that <strong>All circuits require an EMF to function</strong>. </p>\n\n<h2>Let's Look at your example</h2>\n\n<p>There is no such thing as an electric potential in your example. There's an",
        "field, but it's not conservative. Therefore, don't say potential. There is an EMF however. You can say there's a voltage or an induced voltage if you like (from the above discussion). But there's definitely not a potential difference/potential present. For this specific example, the EMF is given by</p>\n\n<p>",
        "where the driving force is that nonconservative",
        "field. The current as you say is",
        ". Here again we see a true instance of the following: <strong>all circuits require an EMF</strong>. The idea that all points in a conductor are at the same potential is equivalent to saying that there is no",
        "field in a conductor. Note that this idea of there being no",
        "field in a conductor only holds for electrostatics + no time varying external magnetic fields (having a time varying",
        "prevents electrostatics anyways so saying \"+ no time varying",
        "\" was redundant). Having an",
        "field in a conductor is <em>completely fine</em>. Turn on an",
        "field in a conductor. <em>There is definitely an",
        "field in the conductor</em> until electrostatics is reached. This is why conducting wires in simple circuits can have",
        "fields in them. This",
        "field is essential for driving current around even though it's through a conductor. It's just that the conductor can never reach electrostatics when it's part of a circuit. It desperately tries, but the battery prevents the wire from coming to a static situation. And with time-varying",
        "fields, nothing wrong with having an",
        "field in a conductor. When you stop varying",
        ", you'll stop changing the",
        "field and things will reach statics. While you are changing the",
        "field,",
        "is changing with time. The conductor is trying to reach statics, but can never do so. So there will always be an",
        "present and hence the conductor won't be an equipotential (albeit, in simple circuits, you can <em>take</em> the wires to be equipotentials because",
        "is so so tiny). </p>\n\n<h2>Too Much Theory, What to know about EMF</h2>\n\n<p>From equation",
        ", because of the closed line integral, <strong>EMF does not care about conservative forces while electric potential crucially depends on a conservative E field</strong>. EMF and potential are both instances of equation",
        ", and therefore both tell you about <strong>energy</strong>. Potential is <strong>energy in a conservative E field</strong>. EMF is <strong>energy added to your circuit through \"nonconservative\" driving forces</strong>. In order for circuits to work, you need to pump energy into them so that charges will flow back down to low energy, making a circuit. EMF tells you how much energy driving forces give to a unit charge in one trip around the loop. Conservative forces don't give any net energy to a charge after one complete loop (started where you stopped). \"Nonconservative\" forces will give you some nonzero value to equation",
        ". EMF tells you how much energy was added by driving forces and hence how much energy must be dropped by dissipative/\"friction\" forces in one trip around the loop. An EMF of 5 volts means 5 volts must be dropped by every unit of charge. If you have a battery providing 2 volts of EMF and a changing magnetic field providing 6 volts of EMF, 8 volts must be dropped </p>\n\n<p>[by the way, If you ever see an example circuit out there with both a battery <em>and</em> a changing flux enclosed by the loop, more than likely their derived equations have wrong explanations. Right equation. Wrong explanation (which is basically just as bad as not knowing what you are doing). What they do is say",
        ". This is Faraday's Law, true in anycase. But what actually does that",
        "mean? It's the net",
        "field on your loop. In the case of a battery and a changing flux, that",
        "has both a conservative and a nonconservative component. The conservative component integrates to zero, leaving only the nonconservative",
        "providing the",
        ". Therefore, if you have this simple battery + changing flux + resistor circuit,",
        "where",
        "is the integral of nonconservative",
        "around the loop. Now we can add a constant to each side of the equation. Since EMF battery",
        ", we can say",
        ". Or if you want, you can write out"
      ],
      "created": "2017-09-10T22:01:10.230",
      "golden_ner_terms": [
        "charge",
        "circuit",
        "closed",
        "complete",
        "component",
        "conductor",
        "connected",
        "conservative",
        "constant",
        "current",
        "cut",
        "difference",
        "electrostatics",
        "energy",
        "energy conservation",
        "equation",
        "equivalent",
        "even",
        "faraday's law",
        "field",
        "flow",
        "flux",
        "forces",
        "function",
        "generates",
        "generating",
        "gradient",
        "ideal",
        "induced",
        "instance",
        "integral",
        "integration",
        "line",
        "line integral",
        "loop",
        "magnetic fields",
        "matter",
        "mean",
        "negative",
        "net",
        "opposite",
        "order",
        "place",
        "plane",
        "point",
        "positive",
        "potential",
        "potential energy",
        "pressure",
        "radius",
        "redundant",
        "right",
        "ring",
        "section",
        "separation",
        "side",
        "similar",
        "simple",
        "sort",
        "source",
        "statics",
        "temperature",
        "term",
        "terminal",
        "theory",
        "time",
        "unit",
        "voltage",
        "way",
        "work",
        "zero"
      ],
      "golden_ner_count": 71,
      "golden_patterns": [
        {
          "pattern": "work-examples-first",
          "score": 2.0,
          "hotwords": [
            "example"
          ]
        },
        {
          "pattern": "check-the-extreme-cases",
          "score": 2.0,
          "hotwords": [
            "zero"
          ]
        },
        {
          "pattern": "transport-across-isomorphism",
          "score": 2.0,
          "hotwords": [
            "equivalent"
          ]
        }
      ],
      "golden_pattern_names": [
        "work-examples-first",
        "check-the-extreme-cases",
        "transport-across-isomorphism"
      ],
      "golden_scopes": [
        {
          "type": "where-binding",
          "match": "where $I_{\\phi}R$ is"
        }
      ],
      "golden_scope_count": 1
    },
    {
      "id": "se-physics-400877",
      "stratum": "hard",
      "title": "Separability of the outer product of a vector with itself, where the vector is a tensor product of two other vectors",
      "tags": [
        "quantum-mechanics",
        "density-operator"
      ],
      "score": 1,
      "answer_score": 4,
      "question_body": "Is it true that $$(|{\\psi}\\rangle \\otimes |{\\phi}\\rangle)(|{\\psi}\\rangle \\otimes |{\\phi}\\rangle)^{\\dagger} = (|\\psi\\rangle\\langle\\psi|)\\otimes (|\\phi\\rangle\\langle\\phi|)$$ where $^{\\dagger}$ is the conjugate transpose symbol $\\otimes$ is the tensor product symbol $|{\\psi}\\rangle \\otimes |{\\phi}\\rangle$ is the tensor product of two vectors and the left hand side is the outer product of $(|{\\psi}\\rangle \\otimes |{\\phi}\\rangle)$ with itself I am trying to determine if the left hand side is separable. If it is, and my equation is wrong, what is the correct equation? If my equation is correct, please tell me how to prove it; I cannot find that identity anywhere.",
      "answer_body": "Suppose that $\\:|\\psi\\rangle \\in \\mathrm F\\:$, where $\\:\\mathrm F\\:$ a $\\:m$-dimensional Hilbert space with basic state vectors $\\:|f_{i}\\rangle, \\quad i=1,2\\cdots,m\\:$ and $\\:|\\phi\\rangle \\in \\mathrm H\\:$, where $\\:\\mathrm H\\:$ a $\\:n$-dimensional Hilbert space with basic state vectors $\\:|h_{j}\\rangle, \\quad j=1,2\\cdots,n$. To prove the identity \\begin{equation} \\left(|\\psi\\rangle \\otimes |\\phi\\rangle\\right)\\left(|\\psi\\rangle \\otimes |\\phi\\rangle\\right)^{\\dagger} = \\left(|\\psi\\rangle\\langle\\psi|\\right)\\otimes \\left(|\\phi\\rangle\\langle\\phi|\\right) \\tag{01} \\end{equation} it's sufficient to prove the identity for the basic state vectors \\begin{equation} \\left(|f_{i}\\rangle \\otimes |h_{j}\\right)\\left(|f_{k}\\rangle \\otimes |h_{\\ell}\\right)^{\\dagger} = \\left(|f_{i}\\rangle\\langle f_{k}|\\right)\\otimes \\left(|h_{j}\\rangle\\langle h_{\\ell}|\\right) \\tag{02} \\end{equation} since if (using Einstein's summation convention) \\begin{align} |\\psi\\rangle & = a_{i}|f_{i}\\rangle \\qquad i=1,2\\cdots,m \\tag{03a}\\\\ |\\phi\\rangle & = b_{j}|h_{j}\\rangle \\qquad j=1,2\\cdots,n \\tag{03b} \\end{align} then \\begin{align} \\left(|\\psi\\rangle \\otimes |\\phi\\rangle\\right) & = a_{i}b_{j}\\left(|f_{i}\\rangle \\otimes |h_{j}\\rangle\\right) \\tag{04a}\\\\ \\left(|\\psi\\rangle \\otimes |\\phi\\rangle\\right)^{\\dagger} & = a^*_{k}b^*_{\\ell}\\left(|f_{k}\\rangle \\otimes |h_{\\ell}\\rangle\\right)^{\\dagger} \\tag{04b} \\end{align} and \\begin{align} \\left(|\\psi\\rangle \\otimes |\\phi\\rangle\\right)\\left(|\\psi\\rangle \\otimes |\\phi\\rangle\\right)^{\\dagger} & = a_{i}a^*_{k}b_{j}b^*_{\\ell}\\left(|f_{i}\\rangle \\otimes |h_{j}\\right)\\left(|f_{k}\\rangle \\otimes |h_{\\ell}\\right)^{\\dagger} \\tag{05a}\\\\ \\left(|\\psi\\rangle\\langle\\psi|\\right)\\otimes \\left(|\\phi\\rangle\\langle\\phi|\\right) & = a_{i}a^*_{k}b_{j}b^*_{\\ell}\\left(|f_{i}\\rangle\\langle f_{k}|\\right)\\otimes \\left(|h_{j}\\rangle\\langle h_{\\ell}|\\right) \\tag{05b} \\end{align} Note that \\begin{align} & \\left(|\\psi\\rangle \\otimes |\\phi\\rangle\\right)\\left(|\\psi\\rangle \\otimes |\\phi\\rangle\\right)^{\\dagger} \\tag{06a}\\\\ & \\left(|\\psi\\rangle\\langle\\psi|\\right)\\otimes \\left(|\\phi\\rangle\\langle\\phi|\\right) \\tag{06b}\\\\ & \\left(|f_{i}\\rangle \\otimes |h_{j}\\rangle\\right)\\left(|f_{k}\\rangle \\otimes |h_{\\ell}\\rangle\\right)^{\\dagger} \\tag{06c}\\\\ & \\left(|f_{i}\\rangle\\langle f_{k}|\\right)\\otimes \\left(|h_{j}\\rangle\\langle h_{\\ell}|\\right) \\tag{06d} \\end{align} are all linear transformations in the product $\\:(m\\!\\cdot\\! n)$-dimensional space $\\: \\mathrm F \\otimes \\mathrm H$. To prove identity (02) we may take without loss of generality as basic state vectors those having a component equal to 1 and all other components equal to 0 \\begin{align} \\left(|f_{i}\\rangle \\right)_{\\mu} & = \\delta_{i\\mu} , \\quad \\left(|f_{k}\\rangle \\right)_{\\rho} = \\delta_{k\\rho} \\qquad i,k,\\mu,\\rho =1,2,\\cdots, m \\tag{07a}\\\\ \\left(|h_{j}\\rangle \\right)_{\\nu} & = \\delta_{j\\nu} , \\quad \\left(|h_{\\ell}\\rangle \\right)_{\\sigma} = \\delta_{\\ell\\sigma} \\qquad j,\\ell,\\nu,\\sigma =1,2,\\cdots, n \\tag{07b} \\end{align} Then the transformations (06c) and (06d) are equal and are represented by a $\\:(m\\!\\cdot\\! n)\\times(m\\!\\cdot\\! n)$ matrix with one element equal to 1 and all other elements equal to 0. By example : Let $\\:m=3,n=2\\:$ and $\\:i=3,k=2, j=2, \\ell=1\\:$. Then \\begin{equation} \\left(|f_{i}\\rangle \\otimes |h_{j}\\rangle\\right) =\\left(|f_{3}\\rangle \\otimes |h_{2}\\rangle\\right) = \\begin{bmatrix} 0\\\\ 0\\\\ 1 \\end{bmatrix} \\otimes \\begin{bmatrix} 0\\\\ 1 \\end{bmatrix} = \\begin{bmatrix} 0\\\\ 0\\\\ 0\\\\ 0\\\\ 0\\\\ 1 \\end{bmatrix} \\tag{08} \\end{equation} \\begin{equation} \\left(|f_{k}\\rangle \\otimes |h_{\\ell}\\rangle\\right) =\\left(|f_{2}\\rangle \\otimes |h_{1}\\rangle\\right) = \\begin{bmatrix} 0\\\\ 1\\\\ 0 \\end{bmatrix} \\otimes \\begin{bmatrix} 1\\\\ 0 \\end{bmatrix} = \\begin{bmatrix} 0\\\\ 0\\\\ 1\\\\ 0\\\\ 0\\\\ 0 \\end{bmatrix} \\tag{09} \\end{equation} so \\begin{equation} \\left(|f_{3}\\rangle \\otimes |h_{2}\\rangle\\right)\\left(|f_{2}\\rangle \\otimes |h_{1}\\rangle\\right)^{\\dagger} = \\begin{bmatrix} 0\\\\ 0\\\\ 0\\\\ 0\\\\ 0\\\\ 1 \\end{bmatrix} \\cdot \\begin{bmatrix} 0\\\\ 0\\\\ 1\\\\ 0\\\\ 0\\\\ 0 \\end{bmatrix}^{\\boldsymbol{\\dagger}} = \\begin{bmatrix} 0&0&0&0&0&0\\\\ 0&0&0&0&0&0\\\\ 0&0&0&0&0&0\\\\ 0&0&0&0&0&0\\\\ 0&0&0&0&0&0\\\\ 0&0&1&0&0&0\\\\ \\end{bmatrix} \\tag{10} \\end{equation} Now \\begin{equation} \\left(|f_{i}\\rangle\\langle f_{k}|\\right) = \\left(|f_{3}\\rangle\\langle f_{2}|\\right)= \\begin{bmatrix} 0\\\\ 0\\\\ 1 \\end{bmatrix} \\begin{bmatrix} 0&1&0 \\end{bmatrix} = \\begin{bmatrix} 0&0&0\\\\ 0&0&0\\\\ 0&1&0 \\end{bmatrix} \\tag{11} \\end{equation} and \\begin{equation} \\left(|h_{j}\\rangle\\langle h_{\\ell}|\\right) = \\left(|h_{1}\\rangle\\langle h_{2}|\\right)= \\begin{bmatrix} 0\\\\ 1 \\end{bmatrix} \\begin{bmatrix} 1&0 \\end{bmatrix} = \\begin{bmatrix} 0&0\\\\ 1&0 \\end{bmatrix} \\tag{12} \\end{equation} so \\begin{equation} \\left(|f_{i}\\rangle\\langle f_{k}|\\right)\\otimes \\left(|h_{j}\\rangle\\langle h_{\\ell}|\\right) = \\left(|f_{3}\\rangle\\langle f_{2}|\\right)\\otimes \\left(|h_{1}\\rangle\\langle h_{2}|\\right) = \\begin{bmatrix} 0&0&0\\\\ 0&0&0\\\\ 0&1&0 \\end{bmatrix} \\otimes \\begin{bmatrix} 0&0\\\\ 1&0 \\end{bmatrix} \\tag{13} \\end{equation} that is \\begin{equation} \\left(|f_{3}\\rangle\\langle f_{2}|\\right)\\otimes \\left(|h_{1}\\rangle\\langle h_{2}|\\right) = \\begin{bmatrix} 0&0&0&0&0&0\\\\ 0&0&0&0&0&0\\\\ 0&0&0&0&0&0\\\\ 0&0&0&0&0&0\\\\ 0&0&0&0&0&0\\\\ 0&0&1&0&0&0\\\\ \\end{bmatrix} \\tag{14} \\end{equation} From ((10) and (14) \\begin{equation} \\left(|f_{3}\\rangle \\otimes |h_{2}\\rangle\\right)\\left(|f_{2}\\rangle \\otimes |h_{1}\\rangle\\right)^{\\dagger} = \\begin{bmatrix} 0&0&0&0&0&0\\\\ 0&0&0&0&0&0\\\\ 0&0&0&0&0&0\\\\ 0&0&0&0&0&0\\\\ 0&0&0&0&0&0\\\\ 0&0&1&0&0&0\\\\ \\end{bmatrix} = \\left(|f_{3}\\rangle\\langle f_{2}|\\right)\\otimes \\left(|h_{1}\\rangle\\langle h_{2}|\\right) \\tag{15} \\end{equation} Note : I would prefer the following symbols that give an elegant expression of identity (01). More exactly, since $\\:\\left(|\\psi\\rangle \\otimes |\\phi\\rangle\\right)\\:$ is a state in the product space $\\: \\mathrm F \\otimes \\mathrm H\\:$ we define it as a ket \\begin{equation} |\\psi\\otimes \\phi\\rangle \\stackrel{\\text{def}}{\\equiv} \\left(|\\psi\\rangle \\otimes |\\phi\\rangle\\right) \\tag{N-01} \\end{equation} Then \\begin{equation} \\left(|\\psi\\rangle \\otimes |\\phi\\rangle\\right)^{\\boldsymbol{\\dagger}}= \\langle \\psi\\otimes \\phi| \\tag{N-02} \\end{equation} and identity (01) is expressed as \\begin{equation} \\color{blue}{|\\psi\\otimes \\phi\\rangle\\langle \\psi\\otimes \\phi| = \\left(|\\psi\\rangle\\langle\\psi|\\right)\\otimes \\left(|\\phi\\rangle\\langle\\phi|\\right)} \\color{black}{} \\tag{N-03} \\end{equation} Now, if each of $\\:|\\psi\\rangle \\in \\mathrm F,|\\phi\\rangle \\in \\mathrm H\\:$ is a unit ket in its own Hilbert space \\begin{equation} \\langle\\psi|\\psi\\rangle = 1 = \\langle\\phi|\\phi\\rangle \\tag{N-04} \\end{equation} then in $\\: \\mathrm F \\otimes \\mathrm H\\:$ \\begin{equation} \\langle \\psi\\otimes \\phi|\\psi\\otimes \\phi\\rangle = 1 \\tag{N-05} \\end{equation} In this case we have the following interpretation of (N-03) : The projection on the unit ket $\\:|\\psi\\otimes \\phi\\rangle\\:$ in the product space $\\: \\mathrm F \\otimes \\mathrm H\\:$ is the product of the projection on the unit ket $\\:|\\psi\\rangle\\:$ in $\\:\\mathrm F\\:$ by the projection on the unit ket $\\:|\\phi\\rangle\\:$ in $\\:\\mathrm H$. From this example above, we could give a proof of identity (02) in general under the choice (07) for the basic state vectors. So 1. $\\left(|f_{i}\\rangle \\otimes |h_{j}\\rangle\\right)$ is a $(m\\!\\cdot\\! n)$-column vector with its $(i\\!-\\!1)n\\!+\\!j$ component equal to 1 and all the other components equal to 0 \\begin{equation} \\left(|f_{i}\\rangle \\otimes |h_{j}\\rangle\\right)= \\begin{bmatrix} 0\\\\ 0\\\\ \\vdots\\\\ 0\\\\ \\vdots\\\\ \\color{blue}{\\bf 1}\\\\ \\vdots\\\\ 0 \\end{bmatrix} \\begin{matrix} \\longleftarrow \\\\ \\longleftarrow \\\\ \\vdots \\\\ \\longleftarrow \\\\ \\vdots \\\\ \\longleftarrow \\\\ \\vdots \\\\ \\longleftarrow \\end{matrix} \\begin{matrix} 1\\hphantom{(i\\!-\\!1)n\\!+\\!j}\\\\ 2\\hphantom{(i\\!-\\!1)n\\!+\\!j}\\\\ \\vdots\\hphantom{(i\\!-\\!1)n\\!+\\!j} \\\\ \\kappa\\hphantom{(i\\!-\\!1)n\\!+\\!j}\\\\ \\vdots\\hphantom{(i\\!-\\!1)n\\!+\\!j}\\\\ \\color{red}{(i\\!-\\!1)n\\!+\\!j}\\\\ \\vdots\\hphantom{(i\\!-\\!1)n\\!+\\!j}\\\\ \\:m\\cdot n\\hphantom{(i\\!-\\!1)n} \\end{matrix} \\tag{16} \\end{equation} 2. $\\left(|f_{k}\\rangle \\otimes |h_{\\ell}\\rangle\\right)^{\\boldsymbol{\\dagger}}\\:$ is a $(m\\!\\cdot\\! n)$-row vector with its $(k\\!-\\!1)n\\!+\\!\\ell$ component equal to 1 and all the other components equal to 0 \\begin{align} \\left(|f_{k}\\rangle \\otimes |h_{\\ell}\\rangle\\right)^{\\boldsymbol{\\dagger}} & = \\begin{bmatrix} 0&0&\\cdots&0&\\cdots&\\hphantom{(k\\!-\\!1}\\color{blue}{\\bf 1}\\hphantom{)n\\!+\\!\\ell}&\\cdots&0\\\\ \\end{bmatrix} \\tag{17}\\\\ & \\hphantom{=\\!=} \\begin{matrix} \\:\\:\\uparrow &\\uparrow &\\cdots&\\uparrow&\\cdots&\\hphantom{k\\!-\\!1}\\uparrow\\hphantom{n\\!+\\!\\ell}&\\cdots&\\uparrow \\end{matrix} \\nonumber\\\\ & \\hphantom{=\\!=} \\begin{matrix} \\:\\:1 & 2 & \\cdots & \\lambda &\\cdots&\\color{red}{(k\\!-\\!1)n\\!+\\!\\ell}&\\,\\cdots&\\!\\!m\\cdot n \\end{matrix} \\nonumber \\end{align} 3. $\\left(|f_{i}\\rangle \\otimes |h_{j}\\rangle\\right)\\left(|f_{k}\\rangle \\otimes |h_{\\ell}\\rangle\\right)^{\\boldsymbol{\\dagger}}$, product of equations (16) and(17), is a $(m\\!\\cdot\\! n)\\times(m\\!\\cdot\\! n)$-square matrix with its element in the $(i\\!-\\!1)n\\!+\\!j$ row and $(k\\!-\\!1)n\\!+\\!\\ell$ column equal to 1 and all the other elements equal to 0 \\begin{align} \\left(|f_{i}\\rangle \\otimes |h_{j}\\rangle\\right)&\\left(|f_{k}\\rangle \\otimes |h_{\\ell}\\rangle\\right)^{\\boldsymbol{\\dagger}} = \\tag{18}\\\\ & \\begin{bmatrix} 0&0&\\cdots&0&\\cdots&\\hphantom{(k\\!-\\!1}0\\hphantom{)n\\!+\\!\\ell}&\\cdots&0\\\\ 0&0&\\cdots&0&\\cdots&\\hphantom{(k\\!-\\!1}0\\hphantom{)n\\!+\\!\\ell}&\\cdots&0\\\\ \\vdots&\\vdots&\\cdots&\\vdots&\\cdots&\\hphantom{(k\\!-\\!1}\\vdots\\hphantom{)n\\!+\\!\\ell}&\\cdots&\\vdots\\\\ 0&0&\\cdots&0&\\cdots&\\hphantom{(k\\!-\\!1}0\\hphantom{)n\\!+\\!\\ell}&\\cdots&0\\\\ \\vdots&\\vdots&\\cdots&\\vdots&\\cdots&\\hphantom{(k\\!-\\!1}\\vdots\\hphantom{)n\\!+\\!\\ell}&\\cdots&\\vdots\\\\ 0&0&\\cdots&0&\\cdots&\\hphantom{(k\\!-\\!1}\\color{blue}{\\bf 1}\\hphantom{)n\\!+\\!\\ell}&\\cdots&0\\\\ \\vdots&\\vdots&\\cdots&\\vdots&\\cdots&\\hphantom{(k\\!-\\!1}\\vdots\\hphantom{)n\\!+\\!\\ell}&\\cdots&\\vdots\\\\ 0&0&\\cdots&0&\\cdots&\\hphantom{(k\\!-\\!1}0\\hphantom{)n\\!+\\!\\ell}&\\cdots&0 \\end{bmatrix} \\begin{matrix} \\longleftarrow \\\\ \\longleftarrow \\\\ \\vdots \\\\ \\longleftarrow \\\\ \\vdots \\\\ \\longleftarrow \\\\ \\vdots \\\\ \\longleftarrow \\end{matrix} \\begin{matrix} 1\\hphantom{(i\\!-\\!1)n\\!+\\!j}\\\\ 2\\hphantom{(i\\!-\\!1)n\\!+\\!j}\\\\ \\vdots\\hphantom{(i\\!-\\!1)n\\!+\\!j} \\\\ \\kappa\\hphantom{(i\\!-\\!1)n\\!+\\!j}\\\\ \\vdots\\hphantom{(i\\!-\\!1)n\\!+\\!j}\\\\ \\color{red}{(i\\!-\\!1)n\\!+\\!j}\\\\ \\vdots\\hphantom{(i\\!-\\!1)n\\!+\\!j}\\\\ \\:m\\cdot n\\hphantom{(i\\!-\\!1)n} \\end{matrix} \\nonumber\\\\ & \\hphantom{.\\,} \\begin{matrix} \\:\\:\\uparrow &\\uparrow &\\cdots&\\uparrow&\\cdots&\\hphantom{k\\!-\\!1}\\uparrow\\hphantom{n\\!+\\!\\ell}&\\cdots&\\uparrow \\end{matrix} \\nonumber\\\\ & \\hphantom{.\\,} \\begin{matrix} \\:\\:1 & 2 & \\cdots & \\lambda &\\cdots&\\color{red}{(k\\!-\\!1)n\\!+\\!\\ell}&\\,\\cdots&\\!\\!m\\cdot n \\end{matrix} \\nonumber \\end{align} 4. $\\left(|f_{i}\\rangle\\langle f_{k}|\\right)$ is a $m\\!\\times\\! m$-square matrix with its element in the $i$-row and $k$-column equal to 1 and all the other elements equal to 0 \\begin{align} \\left(|f_{i}\\rangle\\langle f_{k}|\\right) & = \\tag{19}\\\\ & \\begin{bmatrix} 0&0&\\cdots&0&\\cdots&0&\\cdots&0\\\\ 0&0&\\cdots&0&\\cdots&0&\\cdots&0\\\\ \\vdots&\\vdots&\\cdots&\\vdots&\\cdots&\\vdots&\\cdots&\\vdots\\\\ 0&0&\\cdots&0&\\cdots&0&\\cdots&0\\\\ \\vdots&\\vdots&\\cdots&\\vdots&\\cdots&\\vdots&\\cdots&\\vdots\\\\ 0&0&\\cdots&0&\\cdots&\\color{blue}{\\bf 1}&\\cdots&0\\\\ \\vdots&\\vdots&\\cdots&\\vdots&\\cdots&\\vdots&\\cdots&\\vdots\\\\ 0&0&\\cdots&0&\\cdots&0&\\cdots&0 \\end{bmatrix} \\begin{matrix} \\longleftarrow \\\\ \\longleftarrow \\\\ \\vdots \\\\ \\longleftarrow \\\\ \\vdots \\\\ \\longleftarrow \\\\ \\vdots \\\\ \\longleftarrow \\end{matrix} \\begin{matrix} 1\\hphantom{(i\\!-\\!1)n\\!+\\!j}\\\\ 2\\hphantom{(i\\!-\\!1)n\\!+\\!j}\\\\ \\vdots\\hphantom{(i\\!-\\!1)n\\!+\\!j} \\\\ \\kappa\\hphantom{(i\\!-\\!1)n\\!+\\!j}\\\\ \\vdots\\hphantom{(i\\!-\\!1)n\\!+\\!j}\\\\ \\color{red}{i}\\hphantom{(i\\!-\\!1)n\\!+\\!j}\\\\ \\vdots\\hphantom{(i\\!-\\!1)n\\!+\\!j}\\\\ \\:m\\hphantom{(i\\!-\\!1)n\\!+\\!j} \\end{matrix} \\nonumber\\\\ & \\hphantom{.\\,} \\begin{matrix} \\:\\:\\uparrow &\\uparrow &\\cdots&\\uparrow&\\cdots&\\uparrow&\\cdots&\\uparrow \\end{matrix} \\nonumber\\\\ & \\hphantom{.\\,} \\begin{matrix} \\:\\:1 & 2 & \\cdots & \\lambda &\\cdots&\\color{red}{k}&\\,\\cdots&\\!\\!m \\end{matrix} \\nonumber \\end{align} 5. $\\left(|h_{j}\\rangle\\langle h_{\\ell}|\\right)$ is a $n\\!\\times\\!n$-square matrix with its element in the $j$-row and $\\ell$-column equal to 1 and all the other elements equal to 0 \\begin{align} \\left(|h_{j}\\rangle\\langle h_{\\ell}|\\right) & = \\tag{20}\\\\ & \\begin{bmatrix} 0&0&\\cdots&0&\\cdots&0&\\cdots&0\\\\ 0&0&\\cdots&0&\\cdots&0&\\cdots&0\\\\ \\vdots&\\vdots&\\cdots&\\vdots&\\cdots&\\vdots&\\cdots&\\vdots\\\\ 0&0&\\cdots&0&\\cdots&0&\\cdots&0\\\\ \\vdots&\\vdots&\\cdots&\\vdots&\\cdots&\\vdots&\\cdots&\\vdots\\\\ 0&0&\\cdots&0&\\cdots&\\color{blue}{\\bf 1}&\\cdots&0\\\\ \\vdots&\\vdots&\\cdots&\\vdots&\\cdots&\\vdots&\\cdots&\\vdots\\\\ 0&0&\\cdots&0&\\cdots&0&\\cdots&0 \\end{bmatrix} \\begin{matrix} \\longleftarrow \\\\ \\longleftarrow \\\\ \\vdots \\\\ \\longleftarrow \\\\ \\vdots \\\\ \\longleftarrow \\\\ \\vdots \\\\ \\longleftarrow \\end{matrix} \\begin{matrix} 1\\hphantom{(i\\!-\\!1)n\\!+\\!j}\\\\ 2\\hphantom{(i\\!-\\!1)n\\!+\\!j}\\\\ \\vdots\\hphantom{(i\\!-\\!1)n\\!+\\!j} \\\\ \\kappa\\hphantom{(i\\!-\\!1)n\\!+\\!j}\\\\ \\vdots\\hphantom{(i\\!-\\!1)n\\!+\\!j}\\\\ \\color{red}{j}\\hphantom{(i\\!-\\!1)n\\!+\\!j}\\\\ \\vdots\\hphantom{(i\\!-\\!1)n\\!+\\!j}\\\\ \\:n\\hphantom{(i\\!-\\!1)n\\!+\\!j} \\end{matrix} \\nonumber\\\\ & \\hphantom{.\\,} \\begin{matrix} \\:\\:\\uparrow &\\uparrow &\\cdots&\\uparrow&\\cdots&\\uparrow&\\cdots&\\uparrow \\end{matrix} \\nonumber\\\\ & \\hphantom{.\\,} \\begin{matrix} \\:\\:1 & 2 & \\cdots & \\lambda &\\cdots&\\color{red}{\\ell}&\\,\\cdots&\\!\\!n \\end{matrix} \\nonumber \\end{align} 6. $\\left(|f_{i}\\rangle\\langle f_{k}|\\right)\\otimes \\left(|h_{j}\\rangle\\langle h_{\\ell}|\\right)$ is a $m\\!\\times\\!m$-square matrix in block form. All its block elements are $n\\!\\times\\!n$-square null matrices (symbol $\\mathcal{O}_{n}$) except the block in the $i$-row and $k$-column which is equal to the $n\\!\\times\\!n$-square matrix $\\left(|h_{j}\\rangle\\langle h_{\\ell}|\\right)$. Of course $\\left(|f_{i}\\rangle\\langle f_{k}|\\right)\\otimes \\left(|h_{j}\\rangle\\langle h_{\\ell}|\\right)$ is a $(m\\!\\cdot\\! n)\\times(m\\!\\cdot\\! n)$-square matrix. In block form \\begin{align} \\left(|f_{i}\\rangle\\langle f_{k}|\\right)\\otimes \\left(|h_{j}\\rangle\\langle h_{\\ell}|\\right) & = \\tag{21}\\\\ & \\begin{bmatrix} \\mathcal{O}_{n}&\\mathcal{O}_{n}&\\cdots&\\mathcal{O}_{n}&\\cdots&\\mathcal{O}_{n}&\\cdots&\\mathcal{O}_{n}\\\\ \\mathcal{O}_{n}&\\mathcal{O}_{n}&\\cdots&\\mathcal{O}_{n}&\\cdots&\\mathcal{O}_{n}&\\cdots&\\mathcal{O}_{n}\\\\ \\vdots&\\vdots&\\cdots&\\vdots&\\cdots&\\vdots&\\cdots&\\vdots\\\\ \\mathcal{O}_{n}&\\mathcal{O}_{n}&\\cdots&\\mathcal{O}_{n}&\\cdots&\\mathcal{O}_{n}&\\cdots&\\mathcal{O}_{n}\\\\ \\vdots&\\vdots&\\cdots&\\vdots&\\cdots&\\vdots&\\cdots&\\vdots\\\\ \\mathcal{O}_{n}&\\mathcal{O}_{n}&\\cdots&\\mathcal{O}_{n}&\\cdots& \\color{blue}{\\left(|h_{j}\\rangle\\langle h_{\\ell}|\\right)}&\\cdots&\\mathcal{O}_{n}\\\\ \\vdots&\\vdots&\\cdots&\\vdots&\\cdots&\\vdots&\\cdots&\\vdots\\\\ \\mathcal{O}_{n}&\\mathcal{O}_{n}&\\cdots&\\mathcal{O}_{n}&\\cdots&\\mathcal{O}_{n}&\\cdots&\\mathcal{O}_{n}\\\\ \\end{bmatrix} \\begin{matrix} \\longleftarrow \\\\ \\longleftarrow \\\\ \\vdots \\\\ \\longleftarrow \\\\ \\vdots \\\\ \\longleftarrow \\\\ \\vdots \\\\ \\longleftarrow \\end{matrix} \\begin{matrix} 1\\hphantom{(i\\!-\\!1)n\\!+\\!j}\\\\ 2\\hphantom{(i\\!-\\!1)n\\!+\\!j}\\\\ \\vdots\\hphantom{(i\\!-\\!1)n\\!+\\!j} \\\\ \\kappa\\hphantom{(i\\!-\\!1)n\\!+\\!j}\\\\ \\vdots\\hphantom{(i\\!-\\!1)n\\!+\\!j}\\\\ \\color{red}{i}\\hphantom{(i\\!-\\!1)n\\!+\\!j}\\\\ \\vdots\\hphantom{(i\\!-\\!1)n\\!+\\!j}\\\\ \\:m\\hphantom{(i\\!-\\!1)n\\!+\\!j} \\end{matrix} \\nonumber\\\\ & \\hphantom{.\\,} \\begin{matrix} \\:\\:\\:\\,\\uparrow & \\:\\:\\:\\,\\uparrow &\\,\\,\\cdots &\\:\\,\\uparrow &\\,\\,\\cdots &\\hphantom{==}\\uparrow &\\hphantom{==}\\cdots &\\:\\:\\uparrow \\end{matrix} \\nonumber\\\\ & \\hphantom{.\\,} \\begin{matrix} \\:\\:\\:\\,1 & \\:\\:\\:\\,2 & \\,\\,\\cdots &\\:\\, \\lambda &\\,\\,\\cdots &\\hphantom{==}\\,\\color{red}{k} &\\hphantom{==}\\cdots &\\:\\: m \\end{matrix} \\nonumber \\end{align} Now, in this block form we note that above the matrix $\\left(|h_{j}\\rangle\\langle h_{\\ell}|\\right)$ there are $(i\\!-\\!1)$ rows of $n\\!\\times\\!n$-square null matrices and to the left of the matrix $\\left(|h_{j}\\rangle\\langle h_{\\ell}|\\right)$ there are $(k\\!-\\!1)$ columns of $n\\!\\times\\!n$-square null matrices. This means that the matrix $\\left(|h_{j}\\rangle\\langle h_{\\ell}|\\right)$ is located after the element in the $(i-1)n$ row and the $(k-1)n$ column. But inside this matrix $\\left(|h_{j}\\rangle\\langle h_{\\ell}|\\right)$ the unique nonzero element (=1) is located in the $j$-row and $\\ell$-column and so in the $(i\\!-\\!1)n\\!+\\!j$ row and $(k\\!-\\!1)n\\!+\\!\\ell$ column of the matrix $\\left(|f_{i}\\rangle\\langle f_{k}|\\right)\\otimes \\left(|h_{j}\\rangle\\langle h_{\\ell}|\\right)$. So \\begin{align} \\left(|f_{i}\\rangle\\langle f_{k}|\\right)\\otimes & \\left(|h_{j}\\rangle\\langle h_{\\ell}|\\right) = \\tag{22}\\\\ & \\begin{bmatrix} 0&0&\\cdots&0&\\cdots&\\hphantom{(k\\!-\\!1}0\\hphantom{)n\\!+\\!\\ell}&\\cdots&0\\\\ 0&0&\\cdots&0&\\cdots&\\hphantom{(k\\!-\\!1}0\\hphantom{)n\\!+\\!\\ell}&\\cdots&0\\\\ \\vdots&\\vdots&\\cdots&\\vdots&\\cdots&\\hphantom{(k\\!-\\!1}\\vdots\\hphantom{)n\\!+\\!\\ell}&\\cdots&\\vdots\\\\ 0&0&\\cdots&0&\\cdots&\\hphantom{(k\\!-\\!1}0\\hphantom{)n\\!+\\!\\ell}&\\cdots&0\\\\ \\vdots&\\vdots&\\cdots&\\vdots&\\cdots&\\hphantom{(k\\!-\\!1}\\vdots\\hphantom{)n\\!+\\!\\ell}&\\cdots&\\vdots\\\\ 0&0&\\cdots&0&\\cdots&\\hphantom{(k\\!-\\!1}\\color{blue}{\\bf 1}\\hphantom{)n\\!+\\!\\ell}&\\cdots&0\\\\ \\vdots&\\vdots&\\cdots&\\vdots&\\cdots&\\hphantom{(k\\!-\\!1}\\vdots\\hphantom{)n\\!+\\!\\ell}&\\cdots&\\vdots\\\\ 0&0&\\cdots&0&\\cdots&\\hphantom{(k\\!-\\!1}0\\hphantom{)n\\!+\\!\\ell}&\\cdots&0 \\end{bmatrix} \\begin{matrix} \\longleftarrow \\\\ \\longleftarrow \\\\ \\vdots \\\\ \\longleftarrow \\\\ \\vdots \\\\ \\longleftarrow \\\\ \\vdots \\\\ \\longleftarrow \\end{matrix} \\begin{matrix} 1\\hphantom{(i\\!-\\!1)n\\!+\\!j}\\\\ 2\\hphantom{(i\\!-\\!1)n\\!+\\!j}\\\\ \\vdots\\hphantom{(i\\!-\\!1)n\\!+\\!j} \\\\ \\kappa\\hphantom{(i\\!-\\!1)n\\!+\\!j}\\\\ \\vdots\\hphantom{(i\\!-\\!1)n\\!+\\!j}\\\\ \\color{red}{(i\\!-\\!1)n\\!+\\!j}\\\\ \\vdots\\hphantom{(i\\!-\\!1)n\\!+\\!j}\\\\ \\:m\\cdot n\\hphantom{(i\\!-\\!1)n} \\end{matrix} \\nonumber\\\\ & \\hphantom{.\\,} \\begin{matrix} \\:\\:\\uparrow &\\uparrow &\\cdots&\\uparrow&\\cdots&\\hphantom{k\\!-\\!1}\\uparrow\\hphantom{n\\!+\\!\\ell}&\\cdots&\\uparrow \\end{matrix} \\nonumber\\\\ & \\hphantom{.\\,} \\begin{matrix} \\:\\:1 & 2 & \\cdots & \\lambda &\\cdots&\\color{red}{(k\\!-\\!1)n\\!+\\!\\ell}&\\,\\cdots&\\!\\!m\\cdot n \\end{matrix} \\nonumber \\end{align} Equations (18) and (22) give a proof of the identity (02) and so a proof of the identity (01).",
      "question_latex": [
        "(|{\\psi}\\rangle \\otimes |{\\phi}\\rangle)(|{\\psi}\\rangle \\otimes |{\\phi}\\rangle)^{\\dagger} = (|\\psi\\rangle\\langle\\psi|)\\otimes (|\\phi\\rangle\\langle\\phi|)",
        "</p>\n\n<p>where</p>\n\n<ul>\n<li><p>",
        "is the conjugate transpose symbol </p></li>\n<li><p>",
        "is the tensor product symbol </p></li>\n<li><p>",
        "is the tensor product of two vectors </p></li>\n<li><p>and the left hand side is the outer product of"
      ],
      "answer_latex": [
        "\\:|\\psi\\rangle \\in \\mathrm F\\:",
        "\\:\\mathrm F\\:",
        "\\:m",
        "\\:|f_{i}\\rangle, \\quad i=1,2\\cdots,m\\:",
        "\\:|\\phi\\rangle \\in \\mathrm H\\:",
        "\\:\\mathrm H\\:",
        "\\:n",
        "\\:|h_{j}\\rangle, \\quad j=1,2\\cdots,n",
        "\\:(m\\!\\cdot\\! n)",
        "\\: \\mathrm F \\otimes \\mathrm H",
        "\\:(m\\!\\cdot\\! n)\\times(m\\!\\cdot\\! n)",
        "\\:m=3,n=2\\:",
        "\\:i=3,k=2, j=2, \\ell=1\\:",
        "\\:\\left(|\\psi\\rangle \\otimes |\\phi\\rangle\\right)\\:",
        "\\: \\mathrm F \\otimes \\mathrm H\\:",
        "\\:|\\psi\\rangle \\in \\mathrm F,|\\phi\\rangle \\in \\mathrm H\\:",
        "\\:|\\psi\\otimes \\phi\\rangle\\:",
        "\\:|\\psi\\rangle\\:",
        "\\:|\\phi\\rangle\\:",
        "\\:\\mathrm H",
        "\\left(|f_{i}\\rangle \\otimes |h_{j}\\rangle\\right)",
        "(m\\!\\cdot\\! n)",
        "(i\\!-\\!1)n\\!+\\!j",
        "\\left(|f_{k}\\rangle \\otimes |h_{\\ell}\\rangle\\right)^{\\boldsymbol{\\dagger}}\\:",
        "(k\\!-\\!1)n\\!+\\!\\ell",
        "\\left(|f_{i}\\rangle \\otimes |h_{j}\\rangle\\right)\\left(|f_{k}\\rangle \\otimes |h_{\\ell}\\rangle\\right)^{\\boldsymbol{\\dagger}}",
        "(m\\!\\cdot\\! n)\\times(m\\!\\cdot\\! n)",
        "\\left(|f_{i}\\rangle\\langle f_{k}|\\right)",
        "m\\!\\times\\! m",
        "i",
        "k",
        "\\left(|h_{j}\\rangle\\langle h_{\\ell}|\\right)",
        "n\\!\\times\\!n",
        "j",
        "\\ell",
        "\\left(|f_{i}\\rangle\\langle f_{k}|\\right)\\otimes \\left(|h_{j}\\rangle\\langle h_{\\ell}|\\right)",
        "m\\!\\times\\!m",
        "\\mathcal{O}_{n}",
        "(i\\!-\\!1)",
        "(k\\!-\\!1)",
        "(i-1)n",
        "(k-1)n"
      ],
      "created": "2018-04-19T18:45:50.630",
      "golden_ner_terms": [
        "basic",
        "block",
        "column",
        "column vector",
        "component",
        "components",
        "conjugate",
        "conjugate transpose",
        "elements",
        "equation",
        "expression",
        "hilbert space",
        "identity",
        "interpretation",
        "left hand side",
        "linear transformation",
        "matrix",
        "null",
        "outer",
        "outer product",
        "product",
        "projection",
        "row",
        "row vector",
        "separable",
        "side",
        "space",
        "state",
        "sufficient",
        "summation",
        "tensor",
        "tensor product",
        "transformations",
        "transpose",
        "unit",
        "vector",
        "vectors",
        "without loss of generality"
      ],
      "golden_ner_count": 38,
      "golden_patterns": [
        {
          "pattern": "work-examples-first",
          "score": 2.0,
          "hotwords": [
            "example"
          ]
        },
        {
          "pattern": "check-the-extreme-cases",
          "score": 2.0,
          "hotwords": [
            "zero"
          ]
        },
        {
          "pattern": "exploit-symmetry",
          "score": 2.0,
          "hotwords": [
            "without loss of generality"
          ]
        },
        {
          "pattern": "unfold-the-definition",
          "score": 2.0,
          "hotwords": [
            "means that"
          ]
        }
      ],
      "golden_pattern_names": [
        "work-examples-first",
        "check-the-extreme-cases",
        "exploit-symmetry",
        "unfold-the-definition"
      ],
      "golden_scopes": [
        {
          "type": "assume",
          "match": "Suppose that $"
        },
        {
          "type": "set-notation",
          "match": "$\\:|\\psi\\rangle \\in \\mathrm F\\:$"
        },
        {
          "type": "set-notation",
          "match": "$\\:|\\phi\\rangle \\in \\mathrm H\\:$"
        },
        {
          "type": "set-notation",
          "match": "$\\:|\\psi\\rangle \\in \\mathrm F,|\\phi\\rangle \\in \\mathrm H\\:$"
        }
      ],
      "golden_scope_count": 4
    },
    {
      "id": "se-physics-19075",
      "stratum": "hard",
      "title": "Does the potential energy of fluid rising on a string change?",
      "tags": [
        "classical-mechanics",
        "surface-tension",
        "potential-energy"
      ],
      "score": 2,
      "answer_score": 1,
      "question_body": "Lets say I have a glass of water at rest. Then I go and hang a string above the water (vertically), such as the end of the string is immersed in the water. Over time some of the water is going to migrate upwards and climb the rope. The water level is going to drop slightly, as the rope gets \"wet\". Does the total potential energy of the water change, and if yes what provides the energy for the water to do work against gravity? If the length of the string a really long, does the water stop climbing at some point? Is all this a result of surface tension on the water?",
      "answer_body": "tl;dr: yes, the potential energy increases. Surface effects (surface tension leading to capillary action) provide the requisite energy, but the water will eventually stop rising as the weight of the water in the thread balances the surface tension. Rather than discussing threads, I'm going to do as Lubos suggests and consider a capillary tube ---a very small tube up which water will rise. This preserves the essential physics (contact between water and a surface) while simplifying matters enough that we can perform some explicit calculations. Suppose, for the sake of concreteness, that the glass is a cylinder of radius $r$ and that the water has an initial height (before you put the capillary tube in) of $h$. Then the center of mass of the water is at $h_{cm} = h/2$, and the initial gravitational potential energy is $$ U_{grav} = \\rho V g h_{cm} = \\frac{1}{2}\\rho \\pi r^2 h^2 $$ (where I've written the $\\rho$ for the density of water and put the zero of gravitational potential energy at the bottom of the glass). If, then, you put a cylindrical capillary tube of radius $\\delta r \\ll r$ into the glass of water, and the water rises to a level $l$ above the new surface of the water in the glass, then we have a volume $$ V_{tube}' = \\pi \\delta r^2 l $$ of water in the tube, and $$ V_{glass}' = \\pi r^2 h - V_{tube}' = \\pi r^2 h \\left(1 - \\frac{\\delta r^2}{r^2} \\frac{l}{h}\\right) $$ in the glass proper. The surface of the water in the glass is now at $$ h' = V_{glass}'/(\\pi r^2) = h \\left(1 - \\frac{\\delta r^2}{r^2} \\frac{l}{h}\\right) $$ so the center of mass of the water in the glass is at $$ h_{cm}' = h'/2 = h_{cm}\\left(1 - \\frac{\\delta r^2}{r^2} \\frac{l}{h}\\right) $$ and the gravitational potential energy of the water in the glass is $$ U_{grav, glass}' = \\rho gV_{glass}'h_{cm}' = \\rho g\\pi r^2 h^2/2 \\left(1 - \\frac{\\delta r^2}{r^2}\\frac{l}{h}\\right)^2 $$ To next-to-leading order in $\\delta r$ (since $\\delta r \\ll r$, $\\frac{\\delta r^2}{r^2}$ is small and $\\frac{\\delta r^4}{r^4}$ is very tiny indeed, so I can ignore it), this is $$ U_{grav, glass}' \\approx g\\rho \\pi r^2 h^2/2 \\left(1 - 2\\frac{\\delta r^2}{r^2}\\frac{l}{h}\\right) = U_{grav} - g\\rho\\pi r^2 h^2 \\frac{\\delta r^2 l}{r^2 h} $$ The height of the center of mass of the water in the capillary tube is $$ l/2 + h' = l/2 + h \\left(1 - \\frac{\\delta r^2}{r^2} \\frac{l}{h}\\right), $$ so the potential energy of that water is $$ U_{grav, tube}' = \\rho V_{tube}' g (l/2 + h') = \\rho g \\pi \\delta r^2l \\left[l/2 + h \\left(1 - \\frac{\\delta r^2}{r^2} \\frac{l}{h}\\right)\\right]; $$ to next-to-leading order in $\\frac{\\delta r}{r}$, this is $$ U_{grav, tube}' \\approx \\rho V_{tube}' g (l/2 + h') = \\rho g \\pi\\delta r^2l(l/2 + h). $$ At last, we can write down the total gravitational potential energy of all the water: $$ U_{grav}' \\approx U_{grav} + g\\rho\\pi \\delta r^2l^2/2 $$ To answer the first part of your question, the gravitational potential energy of the water does change. As you mentioned in your reply to Lubos, the drop in the level of the water in the glass does offset the increase in (gravitational) potential energy, but not totally. So far, so good: I've been able to make a reasonably straightforward calculation of the gravitational potential energy of the two configurations, and compare them. The concepts are from freshman physics, and the techniques aren't hard to follow. The thing is, the gravitational potential energy is not the only energy in this situation, and that's where the question gets complicated. There's also an energy associated with every interface---every surface of contact between two different materials. If you ponder a little while, that statement becomes pretty obvious. Think about a kid blowing a soap-bubble: he has to do some amount of work (call it $\\delta W$) by blowing on that little thing shaped like a Quidditch goal in order to increase the surface area (inside and out) of the bubble. We define the surface tension $\\gamma$ as $$ \\gamma = \\frac{\\delta W}{\\delta A} $$ (more or less---in actuality, we have to pass to the derivative: consider very small $\\delta W$ and $\\delta A$. For a slightly less intuitive example in which the surface tension comes out a bit more nicely, without having to pass to a derivative, see https://en.wikipedia.org/wiki/Surface_tension#Two_definitions ). I don't actually understand what causes surface tension; I assume it's inter-molecular forces (e.g. van der Waals or polar forces), but I'd like to see some calculations, or better yet experiments, bearing that out. So that's one part of the answer: the surface tension, and ultimately the forces between the water molecules, provide the work required to draw the water up into the tube. But that's not the whole story. Naively, even when we take surface tension into account, the minimum energy condition would seem to be a perfectly flat surface. Keeping the level of the water thus at $h$, inside the tube or outside, would certainly minimize the surface area of the water. Nor does saying ``surface tension!!one!'' really constitute an explanation. In order to come up with a satisfactory explanation, we have to shift from thinking about energies to thinking about forces. One can think of surface tension as a sort of analogue to pressure: it tells you how much force is exerted across a line of given length drawn along an interface between two substances. Surface tension is to the length of a line along an interface as pressure is to the area of the interface. This has a surprising result: that at an ideal three-way interface, for example at (an idealized version of) the edge of a droplet sitting on a table or the place where the water meets the capillary tube, the liquid makes a specific angle with the solid. In this case, there are three relevant sets of interfaces, each with its own surface tension: liquid-gas ($\\gamma_{LG}$, water and air), liquid-solid ($\\gamma_{LS}$, water and the glass of the capillary tube), and gas-solid ($\\gamma_{GS}$, air and glass). If the whole situation is in equilibrium, we can do a force balance: the horizontal component of the force due to the solid-gas surface tension has to equal the sum of those due to solid-liquid and liquid-gas surface tensions. Otherwise, the interface would be moving. Writing $\\theta_e$ for the contact angle , the angle between the solid and the liquid measured through the liquid (that is to say, on the liquid side of the interface), and dividing through by the length of the three-way interface, $$ \\gamma_{GS} = \\gamma_{LS} + \\gamma_{LG} \\cos\\theta_e. $$ This is Young's Equation (https://en.wikipedia.org/wiki/Wetting#Simplification_to_planar_geometry.2C_Young.27s_relation); solving for the contact $\\theta_e$, $$ \\theta_e = \\cos^{-1}\\frac{\\gamma_{GS} - \\gamma_{LS}}{\\gamma_{LG}} $$ This immediately rules out the all-horizontal case, which would require $\\theta_e = 0$, or $$ \\frac{\\gamma_{GS} - \\gamma_{LS}}{\\gamma_{LG}} = \\frac{\\pi}{2} $$ which is not the case for water. It also give us a clue as to how to proceed: the vertical component of the force due to surface tension very near the three-way interface between water, air, and tube is $\\gamma (2\\pi r) \\cos \\theta_e$; this force must balance the weight of the water in the tube, which is $r (\\pi r^2) l \\rho$ in the notation from the first part of this answer. At last, we see that surface tension draws the water up to a height $$ l = \\frac{2\\cos\\theta_e}{rg \\rho} $$ So: yes, the potential energy increases. Surface effects (surface tension leading to capillary action) provide the requisite energy, but the water will eventually stop rising as the weight of the water in the thread balances the surface tension. For more information: There's Wikipedia, of course: look at the articles on surface tension, capillary action, and contact angle. I also used Landau, L.D. and E.M. Lifshitz. Fluid Mechanics. 2nd English edition, revised. Trans. J.B. Sykes and W.H. Reid. Course of Theoretical Physics Vol. 6. Oxford: Pergamon Press, 1986. \\S 61, pp. 238-244. Note in particular problems 2 and 3.This book leaves a great deal for the reader to figure out on his own. I'm coming to believe that that is usual in the Course of Theoretical Physics. Much more explicit is Kundu, Pijush K. and Ira M. Cohen. Fluid Mechanics . 2nd edition. San Diego: Academic Press, 2002. pp. 9-12. from which I drew (with modification) my calculation of the height to which capillary action draws the water. Note in particular example 1. This appears to be an undergrad engineers' textbook. For more information on wetting---how liquids interact with solid surfaces in general---you might try Wikipedia, of course, but also de Gennes, P.G. (1985). \"Wetting: statics and dynamics\". Reviews of Modern Physics 57 (3): 827–863. http://dx.doi.org/10.1103/RevModPhys.57.827 . Though I've only read a little bit and can't vouch for its quality myself, it appears to be frequently cited, so that's something.",
      "question_latex": [],
      "answer_latex": [
        "U_{grav} = \\rho V g h_{cm} = \\frac{1}{2}\\rho \\pi r^2 h^2",
        "V_{tube}' = \\pi \\delta r^2 l",
        "V_{glass}' = \\pi r^2 h - V_{tube}' = \\pi r^2 h \\left(1 - \\frac{\\delta r^2}{r^2} \\frac{l}{h}\\right)",
        "h' = V_{glass}'/(\\pi r^2) = h \\left(1 - \\frac{\\delta r^2}{r^2} \\frac{l}{h}\\right)",
        "h_{cm}' = h'/2 =  h_{cm}\\left(1 - \\frac{\\delta r^2}{r^2} \\frac{l}{h}\\right)",
        "U_{grav, glass}' = \\rho gV_{glass}'h_{cm}' = \\rho g\\pi r^2 h^2/2 \\left(1 - \\frac{\\delta r^2}{r^2}\\frac{l}{h}\\right)^2",
        "U_{grav, glass}' \\approx g\\rho \\pi r^2 h^2/2 \\left(1 - 2\\frac{\\delta r^2}{r^2}\\frac{l}{h}\\right) \r\n= U_{grav} - g\\rho\\pi r^2 h^2 \\frac{\\delta r^2 l}{r^2 h}",
        "l/2 + h' = l/2 + h \\left(1 - \\frac{\\delta r^2}{r^2} \\frac{l}{h}\\right),",
        "U_{grav, tube}' = \\rho V_{tube}' g (l/2 + h') = \\rho g \\pi \\delta r^2l \r\n\\left[l/2 + h \\left(1 - \\frac{\\delta r^2}{r^2} \\frac{l}{h}\\right)\\right];",
        "U_{grav, tube}' \\approx \\rho V_{tube}' g (l/2 + h') = \\rho g \\pi\\delta r^2l(l/2 + h).",
        "U_{grav}' \\approx U_{grav} + g\\rho\\pi \\delta r^2l^2/2",
        "\\gamma = \\frac{\\delta W}{\\delta A}",
        "\\gamma_{GS} = \\gamma_{LS} + \\gamma_{LG} \\cos\\theta_e.",
        "\\theta_e = \\cos^{-1}\\frac{\\gamma_{GS} - \\gamma_{LS}}{\\gamma_{LG}}",
        "\\frac{\\gamma_{GS} - \\gamma_{LS}}{\\gamma_{LG}} = \\frac{\\pi}{2}",
        "l =  \\frac{2\\cos\\theta_e}{rg \\rho}",
        "r",
        "h",
        "h_{cm} = h/2",
        "(where I've written the",
        "for the density of water and put the\nzero of gravitational potential energy at the bottom of the glass).</p>\n\n<p>If, then, you put a cylindrical capillary tube of radius",
        "into the glass of water, and the water rises to a level",
        "above\nthe new surface of the water in the glass, then we have a volume",
        "of water in the tube, and",
        "in the glass proper. The surface of the water in the glass is now at",
        "so the center of mass of the water in the glass is at",
        "and the gravitational potential energy of the water in the glass is",
        "To next-to-leading order in",
        "(since",
        ",",
        "is small and",
        "is very\ntiny indeed, so I can ignore it), this is",
        "The height of the center of mass of the water in the capillary tube is",
        "so the potential energy of that water is",
        "to next-to-leading order in",
        ", this is",
        "At last, we can write down the total gravitational potential energy of\nall the water:",
        "</p>\n\n<p>To answer the first part of your question, the gravitational\npotential energy of the water does change. As you mentioned in your\nreply to Lubos, the drop in the level of the water in the glass does\noffset the increase in (gravitational) potential energy, but not\ntotally.</p>\n\n<p>So far, so good: I've been able to make a reasonably straightforward\ncalculation of the gravitational potential energy of the two\nconfigurations, and compare them. The concepts are from freshman\nphysics, and the techniques aren't hard to follow.</p>\n\n<p>The thing is, the gravitational potential energy is not the only\nenergy in this situation, and that's where the question gets\ncomplicated. There's also an energy associated with every\ninterface---every surface of contact between two different materials.</p>\n\n<p>If you ponder a little while, that statement becomes pretty\nobvious. Think about a kid blowing a soap-bubble: he has to do some\namount of work (call it",
        ") by blowing on that little thing\nshaped like a Quidditch goal in order to increase the surface area\n(inside and out) of the bubble. We define the surface tension",
        "as",
        "(more or less---in actuality, we have to pass to the derivative:\nconsider very small",
        "and",
        ". For a slightly less\nintuitive example in which the surface tension comes out a bit more\nnicely, without having to pass to a derivative, see\n<a href=\"https://en.wikipedia.org/wiki/Surface_tension#Two_definitions\" rel=\"nofollow\">https://en.wikipedia.org/wiki/Surface_tension#Two_definitions</a>). I\ndon't actually understand what causes surface tension; I assume it's\ninter-molecular forces (e.g. van der Waals or polar forces), but I'd\nlike to see some calculations, or better yet experiments, bearing that\nout. So that's one part of the answer: the surface tension, and\nultimately the forces between the water molecules, provide the work\nrequired to draw the water up into the tube. </p>\n\n<p>But that's not the whole story. Naively, even when we take surface\ntension into account, the minimum energy condition would seem to be a\nperfectly flat surface. Keeping the level of the water thus at",
        ",\ninside the tube or outside, would certainly minimize the surface area\nof the water. Nor does saying ``surface tension!!one!'' really\nconstitute an explanation.</p>\n\n<p>In order to come up with a satisfactory explanation, we have to shift\nfrom thinking about energies to thinking about forces. One can think\nof surface tension as a sort of analogue to pressure: it tells you how\nmuch force is exerted across a line of given length drawn along an\ninterface between two substances. Surface tension is to the length of\na line along an interface as pressure is to the area of the\ninterface. </p>\n\n<p>This has a surprising result: that at an ideal three-way interface,\nfor example at (an idealized version of) the edge of a droplet sitting\non a table or the place where the water meets the capillary tube, the\nliquid makes a specific angle with the solid. In this case, there are\nthree relevant sets of interfaces, each with its own surface tension:\nliquid-gas (",
        ", water and air), liquid-solid\n(",
        ", water and the glass of the capillary tube), and\ngas-solid (",
        ", air and glass).</p>\n\n<p>If the whole situation is in equilibrium, we can do a force balance:\nthe horizontal component of the force due to the solid-gas surface\ntension has to equal the sum of those due to solid-liquid and\nliquid-gas surface tensions. Otherwise, the interface would be\nmoving. Writing",
        "for the <em>contact angle</em>, the angle\nbetween the solid and the liquid measured through the liquid (that is\nto say, on the liquid side of the interface), and dividing through by\nthe length of the three-way interface,",
        "This is Young's Equation\n(https://en.wikipedia.org/wiki/Wetting#Simplification_to_planar_geometry.2C_Young.27s_relation);\nsolving for the contact",
        "This immediately rules out the all-horizontal case, which would\nrequire",
        ", or",
        "which is not the case for water.</p>\n\n<p>It also give us a clue as to how to proceed: the vertical component of\nthe force due to surface tension very near the three-way interface\nbetween water, air, and tube is",
        "; this\nforce must balance the weight of the water in the tube, which is",
        "in the notation from the first part of this\nanswer. At last, we see that surface tension draws the water up to a\nheight"
      ],
      "created": "2012-01-03T15:38:41.313",
      "golden_ner_terms": [
        "action",
        "air",
        "angle",
        "area",
        "balance",
        "bottom",
        "capillary action",
        "center",
        "center of mass",
        "component",
        "cylinder",
        "density",
        "derivative",
        "edge",
        "energy",
        "equation",
        "equilibrium",
        "even",
        "eventually",
        "flat",
        "forces",
        "glass",
        "gravity",
        "height",
        "ideal",
        "information",
        "length",
        "level",
        "line",
        "mass",
        "meets",
        "modification",
        "molecules",
        "near",
        "nor",
        "obvious",
        "order",
        "physics",
        "place",
        "point",
        "polar",
        "potential",
        "potential energy",
        "pressure",
        "radius",
        "side",
        "solid",
        "sort",
        "statics",
        "string",
        "sum",
        "surface",
        "surface area",
        "surface tension",
        "time",
        "volume",
        "water",
        "weight",
        "wetting",
        "wikipedia",
        "work",
        "zero"
      ],
      "golden_ner_count": 62,
      "golden_patterns": [
        {
          "pattern": "work-examples-first",
          "score": 4.0,
          "hotwords": [
            "example",
            "e.g."
          ]
        },
        {
          "pattern": "quotient-by-irrelevance",
          "score": 2.0,
          "hotwords": [
            "up to"
          ]
        },
        {
          "pattern": "check-the-extreme-cases",
          "score": 2.0,
          "hotwords": [
            "zero"
          ]
        },
        {
          "pattern": "construct-an-explicit-witness",
          "score": 2.0,
          "hotwords": [
            "explicit"
          ]
        },
        {
          "pattern": "encode-as-algebra",
          "score": 2.0,
          "hotwords": [
            "ring"
          ]
        }
      ],
      "golden_pattern_names": [
        "work-examples-first",
        "quotient-by-irrelevance",
        "check-the-extreme-cases",
        "construct-an-explicit-witness",
        "encode-as-algebra"
      ],
      "golden_scopes": [],
      "golden_scope_count": 0
    },
    {
      "id": "se-physics-64551",
      "stratum": "hard",
      "title": "Table of matrix elements of powers of $r$ for radial functions in $H$ atom",
      "tags": [
        "atomic-physics",
        "orbital-motion",
        "resource-recommendations",
        "hydrogen",
        "matrix-elements"
      ],
      "score": 3,
      "answer_score": 4,
      "question_body": "Im looking for some references here. I hope it is the right place to ask. I need to find a table of (or a formula from which to extrapolate) the matrix elements of the radial functions of the hydrogen atom evaluated in powers of $r$ for both diagonal and off-diagonal elements. That is, I need this: $$\\langle R_{nl}|r^k|R_{n'l'}\\rangle$$ I found this article http://dx.doi.org/10.1088/0953-4075/28/3/007 which is great, but I'm looking for something more textbook-like, say suited for undergrad. I especially need positive powers until $k=4$ . Any help is very appreciated.",
      "answer_body": "I haven't been able to find either tables of these matrix elements or a formula online, so I decided to provide them here myself. The following tables give the radial matrix elements of $r^k$ for $k$ from -2 to 4, and for the six lowest-energy states (having $n$ from 1 to 3). The rows and columns are labeled by $nl$ . The units are such that the Bohr radius $a_0$ is 1. $$\\begin{array}{c|cccccc} r^{-2} & 10 & 20 & 21 & 30 & 31 & 32 \\\\ \\hline 10 & 2 & \\frac{4 \\sqrt{2}}{9} & \\frac{2 \\sqrt{\\frac{2}{3}}}{9} & \\frac{7}{12 \\sqrt{3}} & \\frac{1}{4 \\sqrt{6}} & \\frac{1}{12 \\sqrt{30}} \\\\ 20 & \\frac{4 \\sqrt{2}}{9} & \\frac{1}{4} & 0 & \\frac{292 \\sqrt{\\frac{2}{3}}}{1875} & \\frac{16}{625 \\sqrt{3}} & -\\frac{128}{1875 \\sqrt{15}} \\\\ 21 & \\frac{2 \\sqrt{\\frac{2}{3}}}{9} & 0 & \\frac{1}{12} & \\frac{2 \\sqrt{2}}{625} & \\frac{64}{1875} & \\frac{32}{625 \\sqrt{5}} \\\\ 30 & \\frac{7}{12 \\sqrt{3}} & \\frac{292 \\sqrt{\\frac{2}{3}}}{1875} & \\frac{2 \\sqrt{2}}{625} & \\frac{2}{27} & 0 & 0 \\\\ 31 & \\frac{1}{4 \\sqrt{6}} & \\frac{16}{625 \\sqrt{3}} & \\frac{64}{1875} & 0 & \\frac{2}{81} & 0 \\\\ 32 & \\frac{1}{12 \\sqrt{30}} & -\\frac{128}{1875 \\sqrt{15}} & \\frac{32}{625 \\sqrt{5}} & 0 & 0 & \\frac{2}{135} \\\\ \\end{array}$$ $$\\begin{array}{c|cccccc} r^{-1} & 10 & 20 & 21 & 30 & 31 & 32 \\\\ \\hline 10 & 1 & \\frac{4 \\sqrt{2}}{27} & \\frac{8 \\sqrt{\\frac{2}{3}}}{27} & \\frac{\\sqrt{3}}{16} & \\frac{5}{16 \\sqrt{6}} & \\frac{\\sqrt{\\frac{3}{10}}}{16} \\\\ 20 & \\frac{4 \\sqrt{2}}{27} & \\frac{1}{4} & -\\frac{1}{4 \\sqrt{3}} & \\frac{92 \\sqrt{6}}{3125} & \\frac{64}{3125 \\sqrt{3}} & -\\frac{448 \\sqrt{\\frac{3}{5}}}{3125} \\\\ 21 & \\frac{8 \\sqrt{\\frac{2}{3}}}{27} & -\\frac{1}{4 \\sqrt{3}} & \\frac{1}{4} & -\\frac{72 \\sqrt{2}}{3125} & \\frac{192}{3125} & \\frac{768}{3125 \\sqrt{5}} \\\\ 30 & \\frac{\\sqrt{3}}{16} & \\frac{92 \\sqrt{6}}{3125} & -\\frac{72 \\sqrt{2}}{3125} & \\frac{1}{9} & -\\frac{1}{9 \\sqrt{2}} & \\frac{1}{9 \\sqrt{10}} \\\\ 31 & \\frac{5}{16 \\sqrt{6}} & \\frac{64}{3125 \\sqrt{3}} & \\frac{192}{3125} & -\\frac{1}{9 \\sqrt{2}} & \\frac{1}{9} & -\\frac{1}{9 \\sqrt{5}} \\\\ 32 & \\frac{\\sqrt{\\frac{3}{10}}}{16} & -\\frac{448 \\sqrt{\\frac{3}{5}}}{3125} & \\frac{768}{3125 \\sqrt{5}} & \\frac{1}{9 \\sqrt{10}} & -\\frac{1}{9 \\sqrt{5}} & \\frac{1}{9} \\\\ \\end{array}$$ $$\\begin{array}{c|cccccc} r^0 & 10 & 20 & 21 & 30 & 31 & 32 \\\\ \\hline 10 & 1 & 0 & \\frac{16 \\sqrt{\\frac{2}{3}}}{27} & 0 & \\frac{3 \\sqrt{\\frac{3}{2}}}{16} & \\frac{3 \\sqrt{\\frac{3}{10}}}{16} \\\\ 20 & 0 & 1 & -\\frac{\\sqrt{3}}{2} & 0 & \\frac{384 \\sqrt{3}}{3125} & -\\frac{3072 \\sqrt{\\frac{3}{5}}}{3125} \\\\ 21 & \\frac{16 \\sqrt{\\frac{2}{3}}}{27} & -\\frac{\\sqrt{3}}{2} & 1 & -\\frac{144 \\sqrt{2}}{3125} & 0 & \\frac{4608}{3125 \\sqrt{5}} \\\\ 30 & 0 & 0 & -\\frac{144 \\sqrt{2}}{3125} & 1 & -\\frac{2 \\sqrt{2}}{3} & \\sqrt{\\frac{2}{5}} \\\\ 31 & \\frac{3 \\sqrt{\\frac{3}{2}}}{16} & \\frac{384 \\sqrt{3}}{3125} & 0 & -\\frac{2 \\sqrt{2}}{3} & 1 & -\\frac{\\sqrt{5}}{3} \\\\ 32 & \\frac{3 \\sqrt{\\frac{3}{10}}}{16} & -\\frac{3072 \\sqrt{\\frac{3}{5}}}{3125} & \\frac{4608}{3125 \\sqrt{5}} & \\sqrt{\\frac{2}{5}} & -\\frac{\\sqrt{5}}{3} & 1 \\\\ \\end{array}$$ $$\\begin{array}{c|cccccc} r & 10 & 20 & 21 & 30 & 31 & 32 \\\\ \\hline 10 & \\frac{3}{2} & -\\frac{32 \\sqrt{2}}{81} & \\frac{128 \\sqrt{\\frac{2}{3}}}{81} & -\\frac{9 \\sqrt{3}}{64} & \\frac{27 \\sqrt{\\frac{3}{2}}}{64} & \\frac{9 \\sqrt{\\frac{15}{2}}}{64} \\\\ 20 & -\\frac{32 \\sqrt{2}}{81} & 6 & -3 \\sqrt{3} & -\\frac{11808 \\sqrt{6}}{15625} & \\frac{27648 \\sqrt{3}}{15625} & -\\frac{119808 \\sqrt{\\frac{3}{5}}}{15625} \\\\ 21 & \\frac{128 \\sqrt{\\frac{2}{3}}}{81} & -3 \\sqrt{3} & 5 & \\frac{10368 \\sqrt{2}}{15625} & -\\frac{27648}{15625} & \\frac{165888}{15625 \\sqrt{5}} \\\\ 30 & -\\frac{9 \\sqrt{3}}{64} & -\\frac{11808 \\sqrt{6}}{15625} & \\frac{10368 \\sqrt{2}}{15625} & \\frac{27}{2} & -9 \\sqrt{2} & 3 \\sqrt{10} \\\\ 31 & \\frac{27 \\sqrt{\\frac{3}{2}}}{64} & \\frac{27648 \\sqrt{3}}{15625} & -\\frac{27648}{15625} & -9 \\sqrt{2} & \\frac{25}{2} & -\\frac{9 \\sqrt{5}}{2} \\\\ 32 & \\frac{9 \\sqrt{\\frac{15}{2}}}{64} & -\\frac{119808 \\sqrt{\\frac{3}{5}}}{15625} & \\frac{165888}{15625 \\sqrt{5}} & 3 \\sqrt{10} & -\\frac{9 \\sqrt{5}}{2} & \\frac{21}{2} \\\\ \\end{array}$$ $$\\begin{array}{c|cccccc} r^2 & 10 & 20 & 21 & 30 & 31 & 32 \\\\ \\hline 10 & 3 & -\\frac{512 \\sqrt{2}}{243} & \\frac{1280 \\sqrt{\\frac{2}{3}}}{243} & -\\frac{81 \\sqrt{3}}{128} & \\frac{135 \\sqrt{\\frac{3}{2}}}{128} & \\frac{81 \\sqrt{\\frac{15}{2}}}{128} \\\\ 20 & -\\frac{512 \\sqrt{2}}{243} & 42 & -20 \\sqrt{3} & -\\frac{953856 \\sqrt{6}}{78125} & \\frac{1714176 \\sqrt{3}}{78125} & -\\frac{5308416 \\sqrt{\\frac{3}{5}}}{78125} \\\\ 21 & \\frac{1280 \\sqrt{\\frac{2}{3}}}{243} & -20 \\sqrt{3} & 30 & \\frac{1057536 \\sqrt{2}}{78125} & -\\frac{1990656}{78125} & \\frac{6967296}{78125 \\sqrt{5}} \\\\ 30 & -\\frac{81 \\sqrt{3}}{128} & -\\frac{953856 \\sqrt{6}}{78125} & \\frac{1057536 \\sqrt{2}}{78125} & 207 & -135 \\sqrt{2} & 45 \\sqrt{10} \\\\ 31 & \\frac{135 \\sqrt{\\frac{3}{2}}}{128} & \\frac{1714176 \\sqrt{3}}{78125} & -\\frac{1990656}{78125} & -135 \\sqrt{2} & 180 & -63 \\sqrt{5} \\\\ 32 & \\frac{81 \\sqrt{\\frac{15}{2}}}{128} & -\\frac{5308416 \\sqrt{\\frac{3}{5}}}{78125} & \\frac{6967296}{78125 \\sqrt{5}} & 45 \\sqrt{10} & -63 \\sqrt{5} & 126 \\\\ \\end{array}$$ $$\\begin{array}{c|cccccc} r^3 & 10 & 20 & 21 & 30 & 31 & 32 \\\\ \\hline 10 & \\frac{15}{2} & -\\frac{2560 \\sqrt{2}}{243} & \\frac{5120 \\sqrt{\\frac{2}{3}}}{243} & -\\frac{1215 \\sqrt{3}}{512} & \\frac{1215 \\sqrt{\\frac{3}{2}}}{512} & \\frac{1701 \\sqrt{\\frac{15}{2}}}{512} \\\\ 20 & -\\frac{2560 \\sqrt{2}}{243} & 330 & -150 \\sqrt{3} & -\\frac{66313728 \\sqrt{6}}{390625} & \\frac{105504768 \\sqrt{3}}{390625} & -\\frac{264757248 \\sqrt{\\frac{3}{5}}}{390625} \\\\ 21 & \\frac{5120 \\sqrt{\\frac{2}{3}}}{243} & -150 \\sqrt{3} & 210 & \\frac{76889088 \\sqrt{2}}{390625} & -\\frac{125411328}{390625} & \\frac{334430208}{390625 \\sqrt{5}} \\\\ 30 & -\\frac{1215 \\sqrt{3}}{512} & -\\frac{66313728 \\sqrt{6}}{390625} & \\frac{76889088 \\sqrt{2}}{390625} & \\frac{6885}{2} & -\\frac{8775}{2 \\sqrt{2}} & \\frac{2835 \\sqrt{\\frac{5}{2}}}{2} \\\\ 31 & \\frac{1215 \\sqrt{\\frac{3}{2}}}{512} & \\frac{105504768 \\sqrt{3}}{390625} & -\\frac{125411328}{390625} & -\\frac{8775}{2 \\sqrt{2}} & 2835 & -945 \\sqrt{5} \\\\ 32 & \\frac{1701 \\sqrt{\\frac{15}{2}}}{512} & -\\frac{264757248 \\sqrt{\\frac{3}{5}}}{390625} & \\frac{334430208}{390625 \\sqrt{5}} & \\frac{2835 \\sqrt{\\frac{5}{2}}}{2} & -945 \\sqrt{5} & 1701 \\\\ \\end{array}$$ $$\\begin{array}{c|cccccc} r^4 & 10 & 20 & 21 & 30 & 31 & 32 \\\\ \\hline 10 & \\frac{45}{2} & -\\frac{40960 \\sqrt{2}}{729} & \\frac{71680 \\sqrt{\\frac{2}{3}}}{729} & -\\frac{3645 \\sqrt{3}}{512} & 0 & \\frac{5103 \\sqrt{\\frac{15}{2}}}{256} \\\\ 20 & -\\frac{40960 \\sqrt{2}}{729} & 2880 & -1260 \\sqrt{3} & -\\frac{4592443392 \\sqrt{6}}{1953125} & \\frac{6772211712 \\sqrt{3}}{1953125} & -\\frac{14714929152 \\sqrt{\\frac{3}{5}}}{1953125} \\\\ 21 & \\frac{71680 \\sqrt{\\frac{2}{3}}}{729} & -1260 \\sqrt{3} & 1680 & \\frac{5361334272 \\sqrt{2}}{1953125} & -\\frac{8026324992}{1953125} & \\frac{18059231232}{1953125 \\sqrt{5}} \\\\ 30 & -\\frac{3645 \\sqrt{3}}{512} & -\\frac{4592443392 \\sqrt{6}}{1953125} & \\frac{5361334272 \\sqrt{2}}{1953125} & \\frac{122715}{2} & -\\frac{76545}{\\sqrt{2}} & 11907 \\sqrt{10} \\\\ 31 & 0 & \\frac{6772211712 \\sqrt{3}}{1953125} & -\\frac{8026324992}{1953125} & -\\frac{76545}{\\sqrt{2}} & 48195 & -15309 \\sqrt{5} \\\\ 32 & \\frac{5103 \\sqrt{\\frac{15}{2}}}{256} & -\\frac{14714929152 \\sqrt{\\frac{3}{5}}}{1953125} & \\frac{18059231232}{1953125 \\sqrt{5}} & 11907 \\sqrt{10} & -15309 \\sqrt{5} & 25515 \\\\ \\end{array}$$ To clarify, these radial matrix elements are $$\\langle n' l' | \\, r^k \\, | n l \\rangle \\equiv \\int_0^\\infty r^2 dr \\, R_{n'l'}(r) \\, r^k \\, R_{nl}(r)$$ where $$R_{nl}(r) = N_{nl} \\, e^{-\\rho/2} \\rho^l L_{n-l+1}^{2l+1}(\\rho)$$ is the radial part of the hydrogen wavefunction $$\\psi_{nlm}(r, \\theta, \\phi) = R_{nl}(r) Y_l^m(\\theta, \\phi),$$ $\\rho$ is related to $r$ by $$\\rho \\equiv \\frac{2r}{n},$$ and the normalization factor $N_{nl}$ is $$N_{nl} \\equiv \\sqrt{\\left(\\frac{2}{n}\\right)^3 \\frac{(n-l-1)!}{2n(n+l)!}}.$$ Remember, I am using units where $a_0=1$ . The integral for a particular matrix element can be done in any computer algebra system; it’s just a polynomial times an exponential. The TeX output for the tables above was generated by Mathematica . A general formula for an arbitrary matrix element can be obtained by using the following integral formula from functions.wolfram.com : $$I(\\alpha, p; m, \\lambda, a; n, \\beta, b) \\equiv \\int_0^\\infty t^{\\alpha-1} e^{-pt} L_m^\\lambda(at) L_n^\\beta(bt)\\,dt \\\\ =\\frac{\\Gamma(\\alpha)}{p^\\alpha} \\frac{(\\lambda+1)_m}{m!} \\frac{(\\beta+1)_n}{n!} \\sum_{j=0}^m \\frac{(-m)_j(\\alpha)_j}{(\\lambda+1)_j j!} \\left(\\frac{a}{p}\\right)^j \\sum_{k=0}^n \\frac{(-n)_k(j+\\alpha)_k}{(\\beta+1)_k k!} \\left(\\frac{b}{p}\\right)^k$$ where $(x)_n$ is the Pochhammer symbol $$(x)_n \\equiv x(x+1)\\dots(x+n-1).$$ The result for the radial matrix element is $$\\langle n' l' | \\, r^k \\, | n l \\rangle = N_{n'l'} N_{nl} \\left(\\frac{2}{n'}\\right)^{l'} \\left(\\frac{2}{n}\\right)^l \\\\ \\times I\\left(3+k+l'+l, \\frac{1}{n'}+\\frac{1}{n}; n'-l'-1, 2l'+1, \\frac{2}{n’}; n-l-1, 2l+1,\\frac{2}{n}\\right).$$ Note that the double-sum $I$ has $(n'-l')(n-l)$ terms, each of which is rational if the $k$ in $r^k$ is. The square roots in the matrix elements come from the normalization factors. This formula is considerably quicker (at least in Mathematica ) than evaluating the individual integrals, especially when $n$ or $n’$ get large. This formula works for non-integral $k$ as well.",
      "question_latex": [
        "\\langle R_{nl}|r^k|R_{n'l'}\\rangle",
        "r",
        "</span></p>\n<p>I found this article <a href=\"http://dx.doi.org/10.1088/0953-4075/28/3/007\" rel=\"nofollow noreferrer\">http://dx.doi.org/10.1088/0953-4075/28/3/007</a> which is great, but I'm looking for something more textbook-like, say suited for undergrad. I especially need positive powers until <span class=\"math-container\">"
      ],
      "answer_latex": [
        "\\begin{array}{c|cccccc}\nr^{-2}  & 10 & 20 & 21 & 30 & 31 & 32 \\\\\n\\hline\n 10 & 2 & \\frac{4 \\sqrt{2}}{9} & \\frac{2 \\sqrt{\\frac{2}{3}}}{9} & \\frac{7}{12 \\sqrt{3}} & \\frac{1}{4 \\sqrt{6}} & \\frac{1}{12 \\sqrt{30}} \\\\\n 20 & \\frac{4 \\sqrt{2}}{9} & \\frac{1}{4} & 0 & \\frac{292 \\sqrt{\\frac{2}{3}}}{1875} & \\frac{16}{625 \\sqrt{3}} & -\\frac{128}{1875 \\sqrt{15}} \\\\\n 21 & \\frac{2 \\sqrt{\\frac{2}{3}}}{9} & 0 & \\frac{1}{12} & \\frac{2 \\sqrt{2}}{625} & \\frac{64}{1875} & \\frac{32}{625 \\sqrt{5}} \\\\\n 30 & \\frac{7}{12 \\sqrt{3}} & \\frac{292 \\sqrt{\\frac{2}{3}}}{1875} & \\frac{2 \\sqrt{2}}{625} & \\frac{2}{27} & 0 & 0 \\\\\n 31 & \\frac{1}{4 \\sqrt{6}} & \\frac{16}{625 \\sqrt{3}} & \\frac{64}{1875} & 0 & \\frac{2}{81} & 0 \\\\\n 32 & \\frac{1}{12 \\sqrt{30}} & -\\frac{128}{1875 \\sqrt{15}} & \\frac{32}{625 \\sqrt{5}} & 0 & 0 & \\frac{2}{135} \\\\\n\\end{array}",
        "\\begin{array}{c|cccccc}\nr^{-1}  & 10 & 20 & 21 & 30 & 31 & 32 \\\\\n\\hline\n 10 & 1 & \\frac{4 \\sqrt{2}}{27} & \\frac{8 \\sqrt{\\frac{2}{3}}}{27} & \\frac{\\sqrt{3}}{16} & \\frac{5}{16 \\sqrt{6}} & \\frac{\\sqrt{\\frac{3}{10}}}{16} \\\\\n 20 & \\frac{4 \\sqrt{2}}{27} & \\frac{1}{4} & -\\frac{1}{4 \\sqrt{3}} & \\frac{92 \\sqrt{6}}{3125} & \\frac{64}{3125 \\sqrt{3}} & -\\frac{448 \\sqrt{\\frac{3}{5}}}{3125} \\\\\n 21 & \\frac{8 \\sqrt{\\frac{2}{3}}}{27} & -\\frac{1}{4 \\sqrt{3}} & \\frac{1}{4} & -\\frac{72 \\sqrt{2}}{3125} & \\frac{192}{3125} & \\frac{768}{3125 \\sqrt{5}} \\\\\n 30 & \\frac{\\sqrt{3}}{16} & \\frac{92 \\sqrt{6}}{3125} & -\\frac{72 \\sqrt{2}}{3125} & \\frac{1}{9} & -\\frac{1}{9 \\sqrt{2}} & \\frac{1}{9 \\sqrt{10}} \\\\\n 31 & \\frac{5}{16 \\sqrt{6}} & \\frac{64}{3125 \\sqrt{3}} & \\frac{192}{3125} & -\\frac{1}{9 \\sqrt{2}} & \\frac{1}{9} & -\\frac{1}{9 \\sqrt{5}} \\\\\n 32 & \\frac{\\sqrt{\\frac{3}{10}}}{16} & -\\frac{448 \\sqrt{\\frac{3}{5}}}{3125} & \\frac{768}{3125 \\sqrt{5}} & \\frac{1}{9 \\sqrt{10}} & -\\frac{1}{9 \\sqrt{5}} & \\frac{1}{9} \\\\\n\\end{array}",
        "\\begin{array}{c|cccccc}\nr^0  & 10 & 20 & 21 & 30 & 31 & 32 \\\\\n\\hline\n 10 & 1 & 0 & \\frac{16 \\sqrt{\\frac{2}{3}}}{27} & 0 & \\frac{3 \\sqrt{\\frac{3}{2}}}{16} & \\frac{3 \\sqrt{\\frac{3}{10}}}{16} \\\\\n 20 & 0 & 1 & -\\frac{\\sqrt{3}}{2} & 0 & \\frac{384 \\sqrt{3}}{3125} & -\\frac{3072 \\sqrt{\\frac{3}{5}}}{3125} \\\\\n 21 & \\frac{16 \\sqrt{\\frac{2}{3}}}{27} & -\\frac{\\sqrt{3}}{2} & 1 & -\\frac{144 \\sqrt{2}}{3125} & 0 & \\frac{4608}{3125 \\sqrt{5}} \\\\\n 30 & 0 & 0 & -\\frac{144 \\sqrt{2}}{3125} & 1 & -\\frac{2 \\sqrt{2}}{3} & \\sqrt{\\frac{2}{5}} \\\\\n 31 & \\frac{3 \\sqrt{\\frac{3}{2}}}{16} & \\frac{384 \\sqrt{3}}{3125} & 0 & -\\frac{2 \\sqrt{2}}{3} & 1 & -\\frac{\\sqrt{5}}{3} \\\\\n 32 & \\frac{3 \\sqrt{\\frac{3}{10}}}{16} & -\\frac{3072 \\sqrt{\\frac{3}{5}}}{3125} & \\frac{4608}{3125 \\sqrt{5}} & \\sqrt{\\frac{2}{5}} & -\\frac{\\sqrt{5}}{3} & 1 \\\\\n\\end{array}",
        "\\begin{array}{c|cccccc}\nr  & 10 & 20 & 21 & 30 & 31 & 32 \\\\\n\\hline\n 10 & \\frac{3}{2} & -\\frac{32 \\sqrt{2}}{81} & \\frac{128 \\sqrt{\\frac{2}{3}}}{81} & -\\frac{9 \\sqrt{3}}{64} & \\frac{27 \\sqrt{\\frac{3}{2}}}{64} & \\frac{9 \\sqrt{\\frac{15}{2}}}{64} \\\\\n 20 & -\\frac{32 \\sqrt{2}}{81} & 6 & -3 \\sqrt{3} & -\\frac{11808 \\sqrt{6}}{15625} & \\frac{27648 \\sqrt{3}}{15625} & -\\frac{119808 \\sqrt{\\frac{3}{5}}}{15625} \\\\\n 21 & \\frac{128 \\sqrt{\\frac{2}{3}}}{81} & -3 \\sqrt{3} & 5 & \\frac{10368 \\sqrt{2}}{15625} & -\\frac{27648}{15625} & \\frac{165888}{15625 \\sqrt{5}} \\\\\n 30 & -\\frac{9 \\sqrt{3}}{64} & -\\frac{11808 \\sqrt{6}}{15625} & \\frac{10368 \\sqrt{2}}{15625} & \\frac{27}{2} & -9 \\sqrt{2} & 3 \\sqrt{10} \\\\\n 31 & \\frac{27 \\sqrt{\\frac{3}{2}}}{64} & \\frac{27648 \\sqrt{3}}{15625} & -\\frac{27648}{15625} & -9 \\sqrt{2} & \\frac{25}{2} & -\\frac{9 \\sqrt{5}}{2} \\\\\n 32 & \\frac{9 \\sqrt{\\frac{15}{2}}}{64} & -\\frac{119808 \\sqrt{\\frac{3}{5}}}{15625} & \\frac{165888}{15625 \\sqrt{5}} & 3 \\sqrt{10} & -\\frac{9 \\sqrt{5}}{2} & \\frac{21}{2} \\\\\n\\end{array}",
        "\\begin{array}{c|cccccc}\nr^2  & 10 & 20 & 21 & 30 & 31 & 32 \\\\\n\\hline\n 10 & 3 & -\\frac{512 \\sqrt{2}}{243} & \\frac{1280 \\sqrt{\\frac{2}{3}}}{243} & -\\frac{81 \\sqrt{3}}{128} & \\frac{135 \\sqrt{\\frac{3}{2}}}{128} & \\frac{81 \\sqrt{\\frac{15}{2}}}{128} \\\\\n 20 & -\\frac{512 \\sqrt{2}}{243} & 42 & -20 \\sqrt{3} & -\\frac{953856 \\sqrt{6}}{78125} & \\frac{1714176 \\sqrt{3}}{78125} & -\\frac{5308416 \\sqrt{\\frac{3}{5}}}{78125} \\\\\n 21 & \\frac{1280 \\sqrt{\\frac{2}{3}}}{243} & -20 \\sqrt{3} & 30 & \\frac{1057536 \\sqrt{2}}{78125} & -\\frac{1990656}{78125} & \\frac{6967296}{78125 \\sqrt{5}} \\\\\n 30 & -\\frac{81 \\sqrt{3}}{128} & -\\frac{953856 \\sqrt{6}}{78125} & \\frac{1057536 \\sqrt{2}}{78125} & 207 & -135 \\sqrt{2} & 45 \\sqrt{10} \\\\\n 31 & \\frac{135 \\sqrt{\\frac{3}{2}}}{128} & \\frac{1714176 \\sqrt{3}}{78125} & -\\frac{1990656}{78125} & -135 \\sqrt{2} & 180 & -63 \\sqrt{5} \\\\\n 32 & \\frac{81 \\sqrt{\\frac{15}{2}}}{128} & -\\frac{5308416 \\sqrt{\\frac{3}{5}}}{78125} & \\frac{6967296}{78125 \\sqrt{5}} & 45 \\sqrt{10} & -63 \\sqrt{5} & 126 \\\\\n\\end{array}",
        "\\begin{array}{c|cccccc}\nr^3  & 10 & 20 & 21 & 30 & 31 & 32 \\\\\n\\hline\n 10 & \\frac{15}{2} & -\\frac{2560 \\sqrt{2}}{243} & \\frac{5120 \\sqrt{\\frac{2}{3}}}{243} & -\\frac{1215 \\sqrt{3}}{512} & \\frac{1215 \\sqrt{\\frac{3}{2}}}{512} & \\frac{1701 \\sqrt{\\frac{15}{2}}}{512} \\\\\n 20 & -\\frac{2560 \\sqrt{2}}{243} & 330 & -150 \\sqrt{3} & -\\frac{66313728 \\sqrt{6}}{390625} & \\frac{105504768 \\sqrt{3}}{390625} & -\\frac{264757248 \\sqrt{\\frac{3}{5}}}{390625} \\\\\n 21 & \\frac{5120 \\sqrt{\\frac{2}{3}}}{243} & -150 \\sqrt{3} & 210 & \\frac{76889088 \\sqrt{2}}{390625} & -\\frac{125411328}{390625} & \\frac{334430208}{390625 \\sqrt{5}} \\\\\n 30 & -\\frac{1215 \\sqrt{3}}{512} & -\\frac{66313728 \\sqrt{6}}{390625} & \\frac{76889088 \\sqrt{2}}{390625} & \\frac{6885}{2} & -\\frac{8775}{2 \\sqrt{2}} & \\frac{2835 \\sqrt{\\frac{5}{2}}}{2} \\\\\n 31 & \\frac{1215 \\sqrt{\\frac{3}{2}}}{512} & \\frac{105504768 \\sqrt{3}}{390625} & -\\frac{125411328}{390625} & -\\frac{8775}{2 \\sqrt{2}} & 2835 & -945 \\sqrt{5} \\\\\n 32 & \\frac{1701 \\sqrt{\\frac{15}{2}}}{512} & -\\frac{264757248 \\sqrt{\\frac{3}{5}}}{390625} & \\frac{334430208}{390625 \\sqrt{5}} & \\frac{2835 \\sqrt{\\frac{5}{2}}}{2} & -945 \\sqrt{5} & 1701 \\\\\n\\end{array}",
        "\\begin{array}{c|cccccc}\nr^4  & 10 & 20 & 21 & 30 & 31 & 32 \\\\\n\\hline\n 10 & \\frac{45}{2} & -\\frac{40960 \\sqrt{2}}{729} & \\frac{71680 \\sqrt{\\frac{2}{3}}}{729} & -\\frac{3645 \\sqrt{3}}{512} & 0 & \\frac{5103 \\sqrt{\\frac{15}{2}}}{256} \\\\\n 20 & -\\frac{40960 \\sqrt{2}}{729} & 2880 & -1260 \\sqrt{3} & -\\frac{4592443392 \\sqrt{6}}{1953125} & \\frac{6772211712 \\sqrt{3}}{1953125} & -\\frac{14714929152 \\sqrt{\\frac{3}{5}}}{1953125} \\\\\n 21 & \\frac{71680 \\sqrt{\\frac{2}{3}}}{729} & -1260 \\sqrt{3} & 1680 & \\frac{5361334272 \\sqrt{2}}{1953125} & -\\frac{8026324992}{1953125} & \\frac{18059231232}{1953125 \\sqrt{5}} \\\\\n 30 & -\\frac{3645 \\sqrt{3}}{512} & -\\frac{4592443392 \\sqrt{6}}{1953125} & \\frac{5361334272 \\sqrt{2}}{1953125} & \\frac{122715}{2} & -\\frac{76545}{\\sqrt{2}} & 11907 \\sqrt{10} \\\\\n 31 & 0 & \\frac{6772211712 \\sqrt{3}}{1953125} & -\\frac{8026324992}{1953125} & -\\frac{76545}{\\sqrt{2}} & 48195 & -15309 \\sqrt{5} \\\\\n 32 & \\frac{5103 \\sqrt{\\frac{15}{2}}}{256} & -\\frac{14714929152 \\sqrt{\\frac{3}{5}}}{1953125} & \\frac{18059231232}{1953125 \\sqrt{5}} & 11907 \\sqrt{10} & -15309 \\sqrt{5} & 25515 \\\\\n\\end{array}",
        "\\langle n' l' | \\, r^k \\, | n l \\rangle \\equiv \\int_0^\\infty r^2 dr \\, R_{n'l'}(r) \\, r^k \\, R_{nl}(r)",
        "R_{nl}(r) = N_{nl} \\, e^{-\\rho/2} \\rho^l L_{n-l+1}^{2l+1}(\\rho)",
        "\\psi_{nlm}(r, \\theta, \\phi) = R_{nl}(r) Y_l^m(\\theta, \\phi),",
        "\\rho \\equiv \\frac{2r}{n},",
        "N_{nl} \\equiv \\sqrt{\\left(\\frac{2}{n}\\right)^3 \\frac{(n-l-1)!}{2n(n+l)!}}.",
        "I(\\alpha, p; m, \\lambda, a; n, \\beta, b) \\equiv\n\\int_0^\\infty t^{\\alpha-1} e^{-pt} L_m^\\lambda(at) L_n^\\beta(bt)\\,dt \\\\\n=\\frac{\\Gamma(\\alpha)}{p^\\alpha} \\frac{(\\lambda+1)_m}{m!} \\frac{(\\beta+1)_n}{n!} \\sum_{j=0}^m \\frac{(-m)_j(\\alpha)_j}{(\\lambda+1)_j j!} \\left(\\frac{a}{p}\\right)^j \\sum_{k=0}^n \\frac{(-n)_k(j+\\alpha)_k}{(\\beta+1)_k k!} \\left(\\frac{b}{p}\\right)^k",
        "(x)_n \\equiv x(x+1)\\dots(x+n-1).",
        "\\langle n' l' | \\, r^k \\, | n l \\rangle =  N_{n'l'} N_{nl} \\left(\\frac{2}{n'}\\right)^{l'} \\left(\\frac{2}{n}\\right)^l \\\\\n\\times I\\left(3+k+l'+l, \\frac{1}{n'}+\\frac{1}{n}; n'-l'-1, 2l'+1, \\frac{2}{n’}; n-l-1, 2l+1,\\frac{2}{n}\\right).",
        "r^k",
        "k",
        "n",
        "nl",
        "a_0",
        "</span></p>\n<p><span class=\"math-container\">",
        "</span></p>\n<p>To clarify, these radial matrix elements are</p>\n<p><span class=\"math-container\">",
        "</span></p>\n<p>where</p>\n<p><span class=\"math-container\">",
        "</span></p>\n<p>is the radial part of the <a href=\"https://en.wikipedia.org/wiki/Hydrogen_atom#Wavefunction\" rel=\"nofollow noreferrer\">hydrogen wavefunction</a></p>\n<p><span class=\"math-container\">",
        "</span> is related to <span class=\"math-container\">",
        "</span> by</p>\n<p><span class=\"math-container\">",
        "</span></p>\n<p>and the normalization factor <span class=\"math-container\">",
        "</span> is</p>\n<p><span class=\"math-container\">",
        "</span></p>\n<p>Remember, I am using units where <span class=\"math-container\">",
        "</span>.</p>\n<p>The integral for a particular matrix element can be done in any computer algebra system; it’s just a polynomial times an exponential. The TeX output for the tables above was generated by <em>Mathematica</em>.</p>\n<p>A general formula for an <em>arbitrary</em> matrix element can be obtained by using the following integral formula from <a href=\"https://functions.wolfram.com/Polynomials/LaguerreL3/21/ShowAll.html\" rel=\"nofollow noreferrer\">functions.wolfram.com</a>:</p>\n<p><span class=\"math-container\">",
        "</span></p>\n<p>where <span class=\"math-container\">",
        "</span> is the <a href=\"https://mathworld.wolfram.com/PochhammerSymbol.html\" rel=\"nofollow noreferrer\">Pochhammer symbol</a></p>\n<p><span class=\"math-container\">",
        "</span></p>\n<p>The result for the radial matrix element is</p>\n<p><span class=\"math-container\">",
        "</span></p>\n<p>Note that the double-sum <span class=\"math-container\">",
        "</span> has <span class=\"math-container\">",
        "</span> terms, each of which is rational if the <span class=\"math-container\">",
        "</span> in <span class=\"math-container\">",
        "</span> is. The square roots in the matrix elements come from the normalization factors.</p>\n<p>This formula is considerably quicker (at least in <em>Mathematica</em>) than evaluating the individual integrals, especially when <span class=\"math-container\">",
        "</span> or <span class=\"math-container\">",
        "</span> get large.</p>\n<p>This formula works for non-integral <span class=\"math-container\">"
      ],
      "created": "2013-05-14T12:19:52.187",
      "golden_ner_terms": [
        "algebra",
        "atom",
        "computer",
        "diagonal",
        "elements",
        "exponential",
        "factor",
        "formula",
        "generated by",
        "hydrogen",
        "integral",
        "mathematica",
        "matrix",
        "matrix elements",
        "normalization",
        "place",
        "pochhammer symbol",
        "polynomial",
        "polynomial time",
        "positive",
        "radial",
        "radius",
        "rational",
        "right",
        "square",
        "square root",
        "wavefunction"
      ],
      "golden_ner_count": 27,
      "golden_patterns": [
        {
          "pattern": "encode-as-algebra",
          "score": 4.0,
          "hotwords": [
            "algebra",
            "polynomial"
          ]
        },
        {
          "pattern": "dualise-the-problem",
          "score": 2.0,
          "hotwords": [
            "dual"
          ]
        },
        {
          "pattern": "estimate-by-bounding",
          "score": 2.0,
          "hotwords": [
            "at least"
          ]
        }
      ],
      "golden_pattern_names": [
        "encode-as-algebra",
        "dualise-the-problem",
        "estimate-by-bounding"
      ],
      "golden_scopes": [
        {
          "type": "where-binding",
          "match": "where $(x)_n$ is"
        }
      ],
      "golden_scope_count": 1
    },
    {
      "id": "se-physics-671337",
      "stratum": "hard",
      "title": "How is this Hamiltonian diagonalized?",
      "tags": [
        "condensed-matter",
        "hamiltonian",
        "parity"
      ],
      "score": 4,
      "answer_score": 7,
      "question_body": "In this paper , they have a Hamiltonian of the form \\begin{equation} H=\\begin{pmatrix} z\\frac{\\mathrm{d}}{\\mathrm{d}z}+g(z+\\frac{\\mathrm{d}}{\\mathrm{d}z}) & \\gamma z\\frac{\\mathrm{d}}{\\mathrm{d}z}+\\Delta\\\\ \\gamma z\\frac{\\mathrm{d}}{\\mathrm{d}z}+\\Delta & z\\frac{\\mathrm{d}}{\\mathrm{d}z}-g(z+\\frac{\\mathrm{d}}{\\mathrm{d}z}) \\end{pmatrix} \\end{equation} And they claim that a unitary transformation given by $$\\mathcal U=\\frac{1}{\\sqrt 2}\\begin{pmatrix} 1 & 1\\\\ \\mathcal T & -\\mathcal T \\end{pmatrix} \\quad\\quad \\text{where } \\mathcal Tu(z)=u(-z) \\text{ is the Parity operator}$$ diagonalizes the Hamiltonian. Here is my attempt to see this. $$\\mathcal U^{-1}=\\frac {1 }{\\sqrt 2}\\left( \\begin{array}{cc} 1 & \\frac{1}{\\mathcal T} \\\\ 1 & -\\frac{1}{\\mathcal T} \\\\ \\end{array} \\right)=\\frac{1}{\\sqrt 2}\\left( \\begin{array}{cc} 1 & \\mathcal T \\\\ 1 & -\\mathcal T \\\\ \\end{array} \\right) \\quad\\quad \\text{where I used } \\mathcal T^2=\\mathbb{1}$$ Now, \\begin{align} \\mathcal U^{-1}H\\mathcal U&=\\frac12 \\left( \\begin{array}{cc} 1 & \\mathcal T \\\\ 1 & -\\mathcal T \\\\ \\end{array} \\right)\\begin{pmatrix} z\\frac{\\mathrm{d}}{\\mathrm{d}z}+g(z+\\frac{\\mathrm{d}}{\\mathrm{d}z}) & \\gamma z\\frac{\\mathrm{d}}{\\mathrm{d}z}+\\Delta\\\\ \\gamma z\\frac{\\mathrm{d}}{\\mathrm{d}z}+\\Delta & z\\frac{\\mathrm{d}}{\\mathrm{d}z}-g(z+\\frac{\\mathrm{d}}{\\mathrm{d}z}) \\end{pmatrix}\\begin{pmatrix} 1 & 1\\\\ \\mathcal T & -\\mathcal T \\end{pmatrix}\\\\ &=\\frac12 \\left( \\begin{array}{cc} 1 & \\mathcal T \\\\ 1 & -\\mathcal T \\\\ \\end{array} \\right) \\left( \\begin{array}{cc} \\frac{d}{\\text{dz}} g+\\mathcal T(\\Delta +\\gamma z\\frac{d}{\\text{dz}})+z\\frac{d}{\\text{dz}}+g z & \\frac{d}{\\text{dz}} g-\\mathcal T(\\Delta +\\gamma z\\frac{d}{\\text{dz}})+z\\frac{d}{\\text{dz}}+g z \\\\ \\Delta +\\mathcal T(-\\frac{d}{\\text{dz}} g+z\\frac{d}{\\text{dz}}-g z)+\\gamma z\\frac{d}{\\text{dz}} & \\Delta -\\mathcal T(-\\frac{d}{\\text{dz}} g+z\\frac{d}{\\text{dz}}-g z)+\\gamma z\\frac{d}{\\text{dz}} \\\\ \\end{array} \\right)\\\\ &=\\frac12 \\left( \\begin{array}{cc} \\mathcal T(\\Delta +\\mathcal T(-\\frac{d}{\\text{dz}} g+z\\frac{d}{\\text{dz}}-g z)+\\gamma z\\frac{d}{\\text{dz}})+\\frac{d}{\\text{dz}} g+\\mathcal T(\\Delta +\\gamma z\\frac{d}{\\text{dz}})+z\\frac{d}{\\text{dz}}+g z & \\mathcal T(\\Delta -\\mathcal T(-\\frac{d}{\\text{dz}} g+z\\frac{d}{\\text{dz}}-g z)+\\gamma z\\frac{d}{\\text{dz}})+\\frac{d}{\\text{dz}} g-\\mathcal T(\\Delta +\\gamma z\\frac{d}{\\text{dz}})+z\\frac{d}{\\text{dz}}+g z \\\\ -\\mathcal T(\\Delta +\\mathcal T(-\\frac{d}{\\text{dz}} g+z\\frac{d}{\\text{dz}}-g z)+\\gamma z\\frac{d}{\\text{dz}})+\\frac{d}{\\text{dz}} g+\\mathcal T(\\Delta +\\gamma z\\frac{d}{\\text{dz}})+z\\frac{d}{\\text{dz}}+g z & -\\mathcal T(\\Delta -\\mathcal T(-\\frac{d}{\\text{dz}} g+z\\frac{d}{\\text{dz}}-g z)+\\gamma \\frac{d}{f\\text{dz}} z)+\\frac{d}{\\text{dz}} g-\\mathcal T(\\Delta +\\gamma z\\frac{d}{\\text{dz}})+z\\frac{d}{\\text{dz}}+g z \\\\ \\end{array} \\right) \\end{align} The off-diagonal term on the top right is \\begin{align} &\\mathcal T(\\Delta -\\mathcal T(-\\frac{d}{\\text{dz}} g+z\\frac{d}{\\text{dz}}-g z)+\\gamma z\\frac{d}{\\text{dz}})+\\frac{d}{\\text{dz}} g-\\mathcal T(\\Delta +\\gamma z\\frac{d}{\\text{dz}})+z\\frac{d}{\\text{dz}}+g z\\\\ &=\\mathcal T(\\Delta +\\gamma z \\frac{d}{dz})+\\frac{d}{\\text{dz}} g-z\\frac{d}{\\text{dz}}+g z+\\frac{d}{\\text{dz}} g-\\mathcal T(\\Delta +\\gamma z\\frac{d}{\\text{dz}})+z\\frac{d}{\\text{dz}}+g z\\\\ &=2g\\left(z+\\frac{\\mathrm d}{\\mathrm d z} \\right) \\end{align} I can't see how the non-diagonal terms are zero. Am I missing something here?",
      "answer_body": "$\\newcommand{\\du}{\\mathrm{1\\!\\!1}}$ $\\newcommand{\\e}{\\boldsymbol=}$ $\\newcommand{\\m}{\\boldsymbol-}$ $\\newcommand{\\p}{\\boldsymbol+}$ $\\newcommand{\\qqlaqq}{\\qquad\\boldsymbol{-\\!\\!\\!-\\!\\!\\!-\\!\\!\\!\\longrightarrow}\\qquad}$ We have the Hamiltonian \\begin{equation} \\mathcal H\\e \\begin{bmatrix} z\\dfrac{\\mathrm d }{\\mathrm d z}\\p g\\left(z\\p\\dfrac{\\mathrm d }{\\mathrm d z}\\right) & \\gamma z\\dfrac{\\mathrm d }{\\mathrm d z}\\p\\Delta\\vphantom{\\tfrac{\\tfrac{a}{b}}{\\tfrac{a}{b}}}\\\\ \\gamma z\\dfrac{\\mathrm d }{\\mathrm d z}\\p\\Delta & z\\dfrac{\\mathrm d }{\\mathrm d z}\\m g\\left(z\\p\\dfrac{\\mathrm d }{\\mathrm d z}\\right)\\vphantom{\\tfrac{\\dfrac{a}{b}}{\\tfrac{a}{b}}} \\end{bmatrix} \\tag{01}\\label{01} \\end{equation} For convenience define the operators \\begin{equation} z\\dfrac{\\mathrm d }{\\mathrm d z}\\boldsymbol\\equiv \\mathcal A\\,,\\qquad g\\left(z\\p\\dfrac{\\mathrm d }{\\mathrm d z}\\right)\\boldsymbol\\equiv\\mathcal B \\tag{02}\\label{02} \\end{equation} Then our Hamiltonian is simplified to \\begin{equation} \\mathcal H\\e \\begin{bmatrix} \\mathcal A\\p\\mathcal B & \\gamma \\mathcal A\\p\\Delta\\vphantom{\\tfrac{\\tfrac{a}{b}}{\\tfrac{a}{b}}}\\\\ \\gamma \\mathcal A\\p\\Delta & \\mathcal A\\m\\mathcal B\\vphantom{\\tfrac{\\dfrac{a}{b}}{\\tfrac{a}{b}}} \\end{bmatrix} \\tag{03}\\label{03} \\end{equation} We must prove that the canonical Fulton–Gouterman transformation \\begin{equation} \\mathcal U\\boldsymbol=\\frac{\\sqrt{2}}{2} \\begin{bmatrix} \\hphantom{\\boldsymbol-}\\mathrm I & \\hphantom{\\boldsymbol-}\\mathrm I\\vphantom{\\tfrac{\\tfrac{a}{b}}{\\tfrac{a}{b}}}\\hphantom{-}\\\\ \\hphantom{\\boldsymbol-}\\mathcal T & \\boldsymbol-\\mathcal T\\vphantom{\\tfrac{\\dfrac{a}{b}}{\\tfrac{a}{b}}}\\hphantom{\\boldsymbol-} \\end{bmatrix}\\,,\\quad \\mathcal U^{\\boldsymbol-1}\\boldsymbol=\\frac{\\sqrt{2}}{2} \\begin{bmatrix} \\hphantom{\\boldsymbol-}\\mathrm I & \\hphantom{\\boldsymbol-}\\mathcal T\\vphantom{\\tfrac{\\tfrac{a}{b}}{\\tfrac{a}{b}}}\\hphantom{-}\\\\ \\hphantom{\\boldsymbol-}\\mathrm I & \\boldsymbol-\\mathcal T\\vphantom{\\tfrac{\\dfrac{a}{b}}{\\tfrac{a}{b}}}\\hphantom{\\boldsymbol-} \\end{bmatrix}\\,,\\quad \\mathcal T f\\left(z\\right)\\boldsymbol=f\\left(\\boldsymbol-z\\right) \\tag{04}\\label{04} \\end{equation} where $\\:\\mathcal T\\:$ the parity transformation, transforms the Hamiltonian \\eqref{03} onto diagonal form. We have \\begin{equation} \\begin{split} \\mathcal U^{\\boldsymbol-1}\\mathcal H\\,\\mathcal U & \\boldsymbol=\\frac{1}{2} \\begin{bmatrix} \\hphantom{\\boldsymbol-}\\mathrm I & \\hphantom{\\boldsymbol-}\\mathcal T\\vphantom{\\left(z\\boldsymbol+\\dfrac{\\mathrm d }{\\mathrm d z}\\right)\\tfrac{\\tfrac{a}{b}}{\\tfrac{a}{b}}}\\hphantom{-}\\\\ \\hphantom{\\boldsymbol-}\\mathrm I & \\boldsymbol-\\mathcal T\\vphantom{\\tfrac{\\dfrac{a}{b}}{\\tfrac{a}{b}}}\\hphantom{\\boldsymbol-} \\end{bmatrix} \\begin{bmatrix} \\mathcal A\\boldsymbol+\\mathcal B & \\gamma \\mathcal A\\boldsymbol+\\Delta\\vphantom{\\left(z\\boldsymbol+\\dfrac{\\mathrm d }{\\mathrm d z}\\right)\\tfrac{\\tfrac{a}{b}}{\\tfrac{a}{b}}}\\\\ \\gamma \\mathcal A\\boldsymbol+\\Delta & \\mathcal A\\boldsymbol-\\mathcal B\\vphantom{\\tfrac{\\dfrac{a}{b}}{\\tfrac{a}{b}}} \\end{bmatrix} \\begin{bmatrix} \\hphantom{\\boldsymbol-}\\mathrm I & \\hphantom{\\boldsymbol-}\\mathrm I\\vphantom{\\left(z\\boldsymbol+\\dfrac{\\mathrm d }{\\mathrm d z}\\right)\\tfrac{\\tfrac{a}{b}}{\\tfrac{a}{b}}}\\hphantom{-}\\\\ \\hphantom{\\boldsymbol-}\\mathcal T & \\boldsymbol-\\mathcal T\\vphantom{\\tfrac{\\dfrac{a}{b}}{\\tfrac{a}{b}}}\\hphantom{\\boldsymbol-} \\end{bmatrix}\\\\ &\\boldsymbol=\\frac{1}{2} \\begin{bmatrix} \\mathcal A\\boldsymbol+\\mathcal B\\boldsymbol+\\mathcal T \\left(\\gamma \\mathcal A\\boldsymbol+\\Delta\\right) & \\gamma \\mathcal A\\boldsymbol+\\Delta\\boldsymbol+\\mathcal T \\left(\\mathcal A\\boldsymbol-\\mathcal B\\right)\\vphantom{\\left(z\\boldsymbol+\\dfrac{\\mathrm d }{\\mathrm d z}\\right)\\tfrac{\\tfrac{a}{b}}{\\tfrac{a}{b}}}\\\\ \\mathcal A\\boldsymbol+\\mathcal B\\boldsymbol-\\mathcal T \\left(\\gamma \\mathcal A\\boldsymbol+\\Delta\\right) & \\gamma \\mathcal A\\boldsymbol+\\Delta\\boldsymbol-\\mathcal T \\left(\\mathcal A\\boldsymbol-\\mathcal B\\right)\\vphantom{\\tfrac{\\dfrac{a}{b}}{\\tfrac{a}{b}}} \\end{bmatrix} \\begin{bmatrix} \\hphantom{\\boldsymbol-}\\mathrm I & \\hphantom{\\boldsymbol-}\\mathrm I\\vphantom{\\left(z\\boldsymbol+\\dfrac{\\mathrm d }{\\mathrm d z}\\right)\\tfrac{\\tfrac{a}{b}}{\\tfrac{a}{b}}}\\hphantom{-}\\\\ \\hphantom{\\boldsymbol-}\\mathcal T & \\boldsymbol-\\mathcal T\\vphantom{\\tfrac{\\dfrac{a}{b}}{\\tfrac{a}{b}}}\\hphantom{\\boldsymbol-} \\end{bmatrix}\\\\ &\\boldsymbol=\\frac{1}{2} \\begin{bmatrix} \\hphantom{\\boldsymbol-}\\mathcal H_{11} & \\hphantom{\\boldsymbol-}\\mathcal H_{12}\\vphantom{\\left(z\\boldsymbol+\\dfrac{\\mathrm d }{\\mathrm d z}\\right)\\tfrac{\\tfrac{a}{b}}{\\tfrac{a}{b}}}\\hphantom{-}\\\\ \\hphantom{\\boldsymbol-}\\mathcal H_{21} & \\hphantom{\\boldsymbol-}\\mathcal H_{22}\\vphantom{\\tfrac{\\dfrac{a}{b}}{\\tfrac{a}{b}}}\\hphantom{\\boldsymbol-} \\end{bmatrix} \\end{split} \\tag{05}\\label{05} \\end{equation} where \\begin{align} \\mathcal H_{11} & \\boldsymbol= \\mathcal A\\boldsymbol+\\mathcal B\\boldsymbol+\\mathcal T \\left(\\gamma \\mathcal A\\boldsymbol+\\Delta\\right)\\boldsymbol+\\left(\\gamma \\mathcal A\\boldsymbol+\\Delta\\right)\\mathcal T \\boldsymbol+\\mathcal T \\left(\\mathcal A\\boldsymbol-\\mathcal B\\right)\\mathcal T \\tag{06a}\\label{06a}\\\\ \\mathcal H_{22} & \\boldsymbol= \\mathcal A\\boldsymbol+\\mathcal B\\boldsymbol-\\mathcal T \\left(\\gamma \\mathcal A\\boldsymbol+\\Delta\\right)\\boldsymbol-\\left(\\gamma \\mathcal A\\boldsymbol+\\Delta\\right)\\mathcal T \\boldsymbol+\\mathcal T \\left(\\mathcal A\\boldsymbol-\\mathcal B\\right)\\mathcal T \\tag{06b}\\label{06b}\\\\ \\mathcal H_{12}&\\boldsymbol= \\mathcal A\\boldsymbol+\\mathcal B\\boldsymbol+\\mathcal T \\left(\\gamma \\mathcal A\\boldsymbol+\\Delta\\right)\\boldsymbol-\\left(\\gamma \\mathcal A\\boldsymbol+\\Delta\\right)\\mathcal T \\boldsymbol-\\mathcal T \\left(\\mathcal A\\boldsymbol-\\mathcal B\\right)\\mathcal T \\tag{06c}\\label{06c}\\\\ \\mathcal H_{21}&\\boldsymbol= \\mathcal A\\boldsymbol+\\mathcal B\\boldsymbol-\\mathcal T \\left(\\gamma \\mathcal A\\boldsymbol+\\Delta\\right)\\boldsymbol+\\left(\\gamma \\mathcal A\\boldsymbol+\\Delta\\right)\\mathcal T \\boldsymbol-\\mathcal T \\left(\\mathcal A\\boldsymbol-\\mathcal B\\right)\\mathcal T \\tag{06d}\\label{06d} \\end{align} It could be proved that $\\:\\mathcal T\\:$ commutes with $\\:\\mathcal A\\:$ and anti-commutes with $\\:\\mathcal B\\:$ \\begin{equation} \\mathcal T \\mathcal A\\boldsymbol= \\mathcal A\\,\\mathcal T\\,,\\qquad \\mathcal T \\mathcal B\\boldsymbol{=-} \\mathcal B\\,\\mathcal T \\tag{07}\\label{07} \\end{equation} so \\begin{align} \\tfrac{1}{2}\\mathcal H_{11} & \\boldsymbol= \\mathcal A\\boldsymbol+\\mathcal B\\boldsymbol+\\left(\\gamma \\mathcal A\\boldsymbol+\\Delta\\right)\\mathcal T\\boldsymbol=z\\dfrac{\\mathrm d }{\\mathrm d z}\\boldsymbol+g\\left(z\\boldsymbol+\\dfrac{\\mathrm d }{\\mathrm d z}\\right)\\boldsymbol + \\left(\\gamma z\\dfrac{\\mathrm d }{\\mathrm d z}\\boldsymbol+\\Delta\\right)\\mathcal T \\boldsymbol\\equiv \\mathcal H_{\\boldsymbol+} \\tag{08a}\\label{08a}\\\\ \\tfrac{1}{2}\\mathcal H_{22} & \\boldsymbol= \\mathcal A\\boldsymbol+\\mathcal B\\boldsymbol-\\left(\\gamma \\mathcal A\\boldsymbol+\\Delta\\right)\\mathcal T\\boldsymbol=z\\dfrac{\\mathrm d }{\\mathrm d z}\\boldsymbol+g\\left(z\\boldsymbol+\\dfrac{\\mathrm d }{\\mathrm d z}\\right)\\boldsymbol- \\left(\\gamma z\\dfrac{\\mathrm d }{\\mathrm d z}\\boldsymbol+\\Delta\\right)\\mathcal T \\boldsymbol\\equiv \\mathcal H_{\\boldsymbol-} \\tag{08b}\\label{08b}\\\\ \\tfrac{1}{2}\\mathcal H_{12}&\\boldsymbol= \\mathcal O \\tag{08c}\\label{08c}\\\\ \\tfrac{1}{2}\\mathcal H_{21} & \\boldsymbol= \\mathcal O \\tag{08d}\\label{08d} \\end{align} that is \\begin{align} \\mathcal U^{\\boldsymbol-1}\\mathcal H\\,\\mathcal U & \\boldsymbol= \\begin{bmatrix} \\mathcal H_{\\boldsymbol+} & \\mathcal O\\vphantom{\\tfrac{\\tfrac{a}{b}}{\\tfrac{a}{b}}}\\\\ \\mathcal O & \\mathcal H_{\\boldsymbol-}\\vphantom{\\tfrac{\\dfrac{a}{b}}{\\tfrac{a}{b}}} \\end{bmatrix} \\tag{09}\\label{09}\\\\ & \\boldsymbol= \\begin{bmatrix} z\\dfrac{\\mathrm d }{\\mathrm d z}\\boldsymbol+g\\left(z\\boldsymbol+\\dfrac{\\mathrm d }{\\mathrm d z}\\right)\\boldsymbol + \\left(\\gamma z\\dfrac{\\mathrm d }{\\mathrm d z}\\boldsymbol+\\Delta\\right)\\mathcal T & \\mathcal O\\vphantom{\\tfrac{\\tfrac{a}{b}}{\\tfrac{a}{b}}}\\\\ \\mathcal O & z\\dfrac{\\mathrm d }{\\mathrm d z}\\boldsymbol+g\\left(z\\boldsymbol+\\dfrac{\\mathrm d }{\\mathrm d z}\\right)\\boldsymbol- \\left(\\gamma z\\dfrac{\\mathrm d }{\\mathrm d z}\\boldsymbol+\\Delta\\right)\\mathcal T \\vphantom{\\tfrac{\\dfrac{a}{b}}{\\tfrac{a}{b}}} \\end{bmatrix} \\end{align} $=\\!=\\!=\\!=\\!=\\!=\\!=\\!=\\!=\\!=\\!=\\!=\\!=\\!=\\!=\\!=\\!=\\!=\\!=\\!=\\!=\\!=\\!=\\!=\\!=\\!=\\!=\\!=\\!=\\!=\\!=\\!=\\!=\\!=\\!=\\!=\\!=\\!=\\!=\\!=\\!=\\!=\\!=\\!=\\!$ ADDENDUM The Hamiltonian $\\:\\mathcal H\\:$ of equation \\eqref{01} is of the Fulton-Gouterman type according to the following definition Definition A 2×2 hermitian operator $\\:\\hat H\\:$ is said to be of the Fulton-Gouterman type , and denoted by $\\:\\hat H_{FG}\\:$ , if $\\hat H\\:$ is similar to \\begin{equation} \\hat H_{FG}\\boldsymbol=A\\du\\boldsymbol+B\\sigma_1\\boldsymbol+C\\sigma_2\\boldsymbol+D\\sigma_3 \\tag{A-01}\\label{A-01} \\end{equation} there is a hermitian operator $\\:\\hat{\\rm R}\\:$ such that \\begin{equation} \\left[\\hat{\\rm R},A\\right]\\boldsymbol=\\left[\\hat{\\rm R}, B\\right]\\boldsymbol=0\\,,\\quad \\left\\{\\hat{\\rm R},C\\right\\}\\boldsymbol=\\left\\{\\hat{\\rm R},D\\right\\}\\boldsymbol=0 \\tag{A-02}\\label{A-02} \\end{equation} where $\\:\\left[\\,,\\right]\\:$ and $\\:\\left\\{\\,,\\right\\}\\:$ denote the conventional commutator and anticommutator. Note that any 2×2 hermitian operator $\\:\\hat H\\:$ can be expressed in the form \\begin{equation} \\hat H\\boldsymbol=\\sum\\limits_{j=0}^{3}h_j\\sigma_j\\, \\quad h_j\\boldsymbol=\\tfrac12\\mathrm{Tr}\\left(\\hat H\\sigma_j\\right), \\tag{A-03}\\label{A-03} \\end{equation} where $\\:h_j$ ’s are one-dimensional operators in a suitable Hilbert space and here and elsewhere the standard representation of the Pauli matrices $\\:\\sigma_j\\:,j = 1,2,3,$ is assumed. For the sake of compactness, we set $\\:\\sigma_0:\\boldsymbol=\\du\\:$ in summation formulas, with $\\:\\du\\:$ being the unit matrix. With respect to the diagonalization of a Fulton-Gouterman type hermitian operator $\\:\\hat H_{FG}\\:$ the following theorem is valid Theorem Any $\\:\\hat H_{FG}\\:$ given by equation \\eqref{A-01} can be diagonalized by means of a unitary transformation, \\begin{equation} U_{FG}\\hat H_{FG}U_{FG}^{\\boldsymbol-1}\\boldsymbol=\\left(A\\boldsymbol+D\\right)\\du\\boldsymbol+B\\hat{\\rm R}\\,\\sigma_3\\boldsymbol-\\mathrm iC\\hat{\\rm R}\\,\\sigma_3 \\tag{A-04}\\label{A-04} \\end{equation} induced by \\begin{equation} U_{FG}\\boldsymbol=\\frac12\\left[\\left(1\\boldsymbol+\\hat{\\rm R}\\right)U_{13}\\boldsymbol+\\left(1\\boldsymbol-\\hat{\\rm R}\\right)U_{2}^{\\boldsymbol-1}\\right] \\tag{A-05}\\label{A-05} \\end{equation} where $\\:U_{13}\\boldsymbol=\\left(\\sigma_1\\boldsymbol+\\sigma_3\\right)/\\sqrt{2}\\:$ and $\\:U_{2}\\boldsymbol=\\left(\\du\\boldsymbol+\\mathrm i\\sigma_2\\right)/\\sqrt{2}$ . The diagonal elements of the diagonalized Hamiltonian are \\begin{equation} \\hat{\\rm L}_{\\boldsymbol\\pm}\\boldsymbol=A\\boldsymbol+D\\boldsymbol\\pm\\left(B\\boldsymbol-\\mathrm iC\\right)\\hat{\\rm R} \\tag{A-06}\\label{A-06} \\end{equation} $-\\!\\!\\!-\\!\\!\\!-\\!\\!\\!-\\!\\!\\!-\\!\\!\\!-\\!\\!\\!-\\!\\!\\!-\\!\\!\\!-\\!\\!\\!-\\!\\!\\!-\\!\\!\\!-\\!\\!\\!-\\!\\!\\!-\\!\\!\\!-\\!\\!\\!-\\!\\!\\!-\\!\\!\\!-\\!\\!\\!-\\!\\!\\!-\\!\\!\\!-\\!\\!\\!-\\!\\!\\!-\\!\\!\\!-\\!\\!\\!-\\!\\!\\!-\\!\\!\\!-\\!\\!\\!-\\!\\!\\!-\\!\\!\\!-\\!\\!\\!-\\!\\!\\!-\\!\\!\\!$ Note that in equation \\eqref{A-05} we have \\begin{align} U_{13} & \\e\\tfrac{\\sqrt{2}}{2}\\left(\\sigma_1 \\p \\sigma_3\\right)\\e\\tfrac{\\sqrt{2}}{2} \\begin{bmatrix} \\:1 & \\hphantom{\\m} 1 \\vphantom{\\tfrac{a}{b}}\\:\\:\\\\ \\:1 & \\m 1 \\vphantom{\\tfrac{a}{b}}\\:\\: \\end{bmatrix} \\tag{A-07a}\\label{A-07a}\\\\ U_{2} & \\e\\tfrac{\\sqrt{2}}{2}\\left(\\du\\p\\mathrm i\\sigma_2\\right)\\e\\tfrac{\\sqrt{2}}{2} \\begin{bmatrix} \\hphantom{\\m} 1 & \\:\\:1\\: \\vphantom{\\tfrac{a}{b}}\\\\ \\m 1 & \\:\\: 1 \\:\\vphantom{\\tfrac{a}{b}} \\end{bmatrix} \\tag{A-07b}\\label{A-07b}\\\\ \\texttt{so} \\qquad U_{2}^{\\m 1} & \\e\\tfrac{\\sqrt{2}}{2}\\left(\\du\\m\\mathrm i\\sigma_2\\right)\\e\\tfrac{\\sqrt{2}}{2} \\begin{bmatrix} \\:\\:1 & \\m 1 \\:\\vphantom{\\tfrac{a}{b}}\\\\ \\:\\:1 & \\hphantom{\\m} 1 \\:\\vphantom{\\tfrac{a}{b}} \\end{bmatrix} \\tag{A-07c}\\label{A-07c} \\end{align} Using above equations the unitary transformation $\\:U_{FG}\\:$ is \\begin{align} U_{FG} & \\e\\tfrac12\\left[\\left(1\\p\\hat{\\rm R}\\right)U_{13}\\p\\left(1\\m\\hat{\\rm R}\\right)U_{2}^{\\m 1}\\right] \\nonumber\\\\ & \\e\\tfrac{\\sqrt{2}}{4}\\left[\\left(1\\p\\hat{\\rm R}\\right) \\begin{bmatrix} \\:1 & \\hphantom{\\m} 1 \\vphantom{\\tfrac{a}{b}}\\:\\:\\\\ \\:1 & \\m 1 \\vphantom{\\tfrac{a}{b}}\\:\\: \\end{bmatrix} \\p \\left(1\\m\\hat{\\rm R}\\right) \\begin{bmatrix} \\:\\:1 & \\m 1 \\:\\vphantom{\\tfrac{a}{b}}\\\\ \\:\\:1 & \\hphantom{\\m} 1 \\:\\vphantom{\\tfrac{a}{b}} \\end{bmatrix}\\right] \\nonumber \\end{align} that is \\begin{equation} U_{FG}\\e\\tfrac{\\sqrt{2}}{2} \\begin{bmatrix} \\hphantom{\\m}\\mathrm I & \\hphantom{\\m}\\hat{\\rm R}\\vphantom{\\dfrac{a}{b}}\\hphantom{-}\\\\ \\hphantom{\\m}\\mathrm I & \\m\\hat{\\rm R}\\vphantom{\\dfrac{a}{b}}\\hphantom{\\m} \\end{bmatrix} \\tag{A-08}\\label{A-08} \\end{equation} The general case of Fulton-Gouterman type hermitian operators and transformations in this ADDENDUM yields the special case of the question under the following replacements \\begin{align} \\hat H_{FG} & \\qqlaqq \\mathcal H \\tag{A-09.1}\\label{A-09.1}\\\\ U_{FG} & \\qqlaqq \\mathcal U^{\\m 1} \\tag{A-09.2}\\label{A-09.2}\\\\ \\hat{\\rm R} & \\qqlaqq \\mathcal T \\tag{A-09.3}\\label{A-09.3}\\\\ A & \\qqlaqq z\\dfrac{\\mathrm d }{\\mathrm d z} \\tag{A-09.4}\\label{A-09.4}\\\\ B & \\qqlaqq \\gamma z\\dfrac{\\mathrm d }{\\mathrm d z}\\p\\Delta \\tag{A-09.5}\\label{A-09.5}\\\\ C & \\qqlaqq 0 \\tag{A-09.6}\\label{A-09.6}\\\\ D & \\qqlaqq g\\left(z\\p\\dfrac{\\mathrm d }{\\mathrm d z}\\right) \\tag{A-09.7}\\label{A-09.7}\\\\ \\hat{\\rm L}_{\\boldsymbol\\pm} & \\qqlaqq \\mathcal H_{\\boldsymbol\\pm} \\tag{A-09.8}\\label{A-09.8} \\end{align}",
      "question_latex": [
        "\\mathcal U=\\frac{1}{\\sqrt 2}\\begin{pmatrix}\n1 & 1\\\\\n\\mathcal T & -\\mathcal T\n\\end{pmatrix} \\quad\\quad \\text{where } \\mathcal Tu(z)=u(-z) \\text{ is the Parity operator}",
        "\\mathcal U^{-1}=\\frac {1 }{\\sqrt 2}\\left(\n\\begin{array}{cc}\n 1 & \\frac{1}{\\mathcal T} \\\\\n 1 & -\\frac{1}{\\mathcal T} \\\\\n\\end{array}\n\\right)=\\frac{1}{\\sqrt 2}\\left(\n\\begin{array}{cc}\n 1 & \\mathcal T \\\\\n 1 & -\\mathcal T \\\\\n\\end{array}\n\\right) \\quad\\quad \\text{where I used } \\mathcal T^2=\\mathbb{1}",
        "</span>\ndiagonalizes the Hamiltonian.</p>\n<p>Here is my attempt to see this.</p>\n<p><span class=\"math-container\">"
      ],
      "answer_latex": [
        "\\newcommand{\\du}{\\mathrm{1\\!\\!1}}",
        "\\newcommand{\\e}{\\boldsymbol=}",
        "\\newcommand{\\m}{\\boldsymbol-}",
        "\\newcommand{\\p}{\\boldsymbol+}",
        "\\newcommand{\\qqlaqq}{\\qquad\\boldsymbol{-\\!\\!\\!-\\!\\!\\!-\\!\\!\\!\\longrightarrow}\\qquad}",
        "\\:\\mathcal T\\:",
        "\\:\\mathcal A\\:",
        "\\:\\mathcal B\\:",
        "=\\!=\\!=\\!=\\!=\\!=\\!=\\!=\\!=\\!=\\!=\\!=\\!=\\!=\\!=\\!=\\!=\\!=\\!=\\!=\\!=\\!=\\!=\\!=\\!=\\!=\\!=\\!=\\!=\\!=\\!=\\!=\\!=\\!=\\!=\\!=\\!=\\!=\\!=\\!=\\!=\\!=\\!=\\!=\\!",
        "\\:\\mathcal H\\:",
        "\\:\\hat H\\:",
        "\\:\\hat H_{FG}\\:",
        "\\hat H\\:",
        "\\:\\hat{\\rm R}\\:",
        "\\:\\left[\\,,\\right]\\:",
        "\\:\\left\\{\\,,\\right\\}\\:",
        "\\:h_j",
        "\\:\\sigma_j\\:,j = 1,2,3,",
        "\\:\\sigma_0:\\boldsymbol=\\du\\:",
        "\\:\\du\\:",
        "\\:U_{13}\\boldsymbol=\\left(\\sigma_1\\boldsymbol+\\sigma_3\\right)/\\sqrt{2}\\:",
        "\\:U_{2}\\boldsymbol=\\left(\\du\\boldsymbol+\\mathrm i\\sigma_2\\right)/\\sqrt{2}",
        "-\\!\\!\\!-\\!\\!\\!-\\!\\!\\!-\\!\\!\\!-\\!\\!\\!-\\!\\!\\!-\\!\\!\\!-\\!\\!\\!-\\!\\!\\!-\\!\\!\\!-\\!\\!\\!-\\!\\!\\!-\\!\\!\\!-\\!\\!\\!-\\!\\!\\!-\\!\\!\\!-\\!\\!\\!-\\!\\!\\!-\\!\\!\\!-\\!\\!\\!-\\!\\!\\!-\\!\\!\\!-\\!\\!\\!-\\!\\!\\!-\\!\\!\\!-\\!\\!\\!-\\!\\!\\!-\\!\\!\\!-\\!\\!\\!-\\!\\!\\!-\\!\\!\\!-\\!\\!\\!",
        "\\:U_{FG}\\:"
      ],
      "created": "2021-10-12T21:13:55.017",
      "golden_ner_terms": [
        "anticommutator",
        "canonical",
        "commutator",
        "compactness",
        "diagonal",
        "diagonalization",
        "elements",
        "equation",
        "hamiltonian",
        "hermitian",
        "hermitian operator",
        "hilbert space",
        "induced",
        "matrix",
        "onto",
        "operator",
        "operators",
        "parity",
        "pauli matrices",
        "representation",
        "right",
        "similar",
        "space",
        "summation",
        "term",
        "theorem",
        "top",
        "transformation",
        "transformations",
        "type",
        "unit",
        "unitary",
        "unitary transformation",
        "valid",
        "zero"
      ],
      "golden_ner_count": 35,
      "golden_patterns": [
        {
          "pattern": "pass-to-a-subsequence",
          "score": 2.0,
          "hotwords": [
            "compactness"
          ]
        },
        {
          "pattern": "the-diagonal-argument",
          "score": 2.0,
          "hotwords": [
            "diagonal"
          ]
        },
        {
          "pattern": "monotone-approximation",
          "score": 2.0,
          "hotwords": [
            "limit"
          ]
        }
      ],
      "golden_pattern_names": [
        "pass-to-a-subsequence",
        "the-diagonal-argument",
        "monotone-approximation"
      ],
      "golden_scopes": [],
      "golden_scope_count": 0
    },
    {
      "id": "se-physics-649959",
      "stratum": "hard",
      "title": "Discretization of Newton's Equation with velocity perpendicular to gradient",
      "tags": [
        "newtonian-mechanics",
        "forces",
        "computational-physics",
        "discrete"
      ],
      "score": 2,
      "answer_score": 2,
      "question_body": "Suppose I have a function $f:\\mathbb{R}^n\\to\\mathbb{R}$ and $g(x)=\\nabla_xf(x)$ and $H(x) = \\nabla^2 f(x)$ are its gradient and Hessian matrix. I have the following ODE system $$ \\begin{align} \\dot x(t) &= v(t) \\\\ \\dot v(t) &= - \\frac{v(t)^\\top H(x_t) v(t)}{||g(x(t))||^2} g(x(t)) \\end{align} $$ with initial condition $(x_0, v_0)$ with $v_0\\perp g(x_0)$ . Clearly $\\frac{d}{dt} v(t)^\\top g(x(t)) = 0$ so that the velocity stays perpendicular to the gradient $v_t\\perp g(x(t))$ . I would like to discretize this ODE. I have read about Euler, Verlet and Leapfrog methods. Usually, when you have the system $$ \\dot x(t) = v(t) \\\\ \\dot v(t) = a(t) $$ the Leapfrog method workds well. However everyone seems to say this is not a good method to use when the force/acceleration depends on the velocity, as in this case. The lecture notes say that this system of equations could be discretized as $$ \\begin{align} x_{t + \\delta/2} &= x_t + \\frac{\\delta}{2}v_t \\\\ v_{t+\\delta} &=v_t - \\delta \\frac{v(t)^\\top H(x_{t + \\delta/2}) v(t)}{||g(x_{t + \\delta/2})||^2} g(x_{t + \\delta/2}) \\\\ x_{t+\\delta} &= x_{t + \\delta/2} + \\frac{\\delta}{2} v_{t + \\delta} \\end{align} $$ but doesn't explain why this is the correct scheme and what scheme this even is. To me it looks like a wrong Leapfrog method where they are doing things in reverse: doing half a position step, a full velocity step and half a position step. Can someone help me understand this?",
      "answer_body": "As I mentioned, your system is the system of differential equations, written in the ambient space $\\mathbb{R}^m$ , of the geodesics on the equipotential surface $f(x) = c$ . So let us have a smooth function $$f \\, : \\, \\mathbb{R}^m \\, \\longrightarrow \\, \\mathbb{R}$$ which we call 'the potential', and let us fix a real number (a value) $c \\,\\in \\, \\mathbb{R}$ which gives us the smooth level hyper-surface $$M_c \\, = \\, \\{ \\, x \\, \\in \\, \\mathbb{R}^m \\,\\, | \\,\\,\\, f(x) = c\\,\\,\\}$$ If you restrict the standard flat Euclidean metric on $M_c$ you get a Riemannian metric on $M_c$ . Now, the geodesics of $M_c$ are not geodesics of $\\mathbb{R}^m$ , the latter are just straight lines, but the geodesics of $M_c$ try to follow the curved surface of $M_c$ with as little deviation from the ambient geodesics as possible. Which means that their acceleration, that causes them to curve and follow the surface instead of staying straight, when orthogonally projected onto the tangent space of $M_c$ at each point on the geodesic, should be zero (so no acceleration should be visible for creatures on the surface, that do not look into the ambient space). What is that mean. Let $x = x(t)$ be a geodesic on $M_c$ . Then the acceleration should be its second derivative, i.e. $$\\text{acceleration} \\, = \\, \\frac{d^2x}{dt^2}$$ Furthermore, since the orthogonal projection of $\\frac{d^2x}{dt^2}$ onto the tangent space of $M_c$ at the point $x$ should be zero, that means that $$\\frac{d^2x}{dt^2} \\,\\,\\text{ should be aligned with the normal vector to $M_c$ at $x$ }$$ But a normal vector to $M_c$ at $x$ is the gradient $g(x) \\, = \\, \\nabla f(x)$ so $$\\frac{d^2x}{dt^2} \\,\\,\\text{ should be aligned with the normal vector $\\nabla f(x)$ }$$ And there you have the first hint, that the equations of the geodesic should look like $$\\frac{d^2x}{dt^2} \\, = \\, \\lambda \\, \\nabla f(x)$$ where you can expect $$\\lambda \\, = \\, \\lambda\\Big(x, \\, \\frac{dx}{dt}\\Big)$$ Combine the latter system of equation with the restriction $$f(x) \\, = \\, c$$ When you differentiate the latter restriction with respect to $t$ , you get the first new restriction $$\\nabla f(x)^T \\, \\frac{dx}{dt} \\, = \\, 0$$ i.e. the velocity $\\frac{dx}{dt}(t)$ of $x(t)$ is always perpendicular to $\\nabla f(x)$ , which means the velocity is tangent to the hyper-surface $M_c$ (which is not surprising, it is expected and required). Now, we go even further and differentiate the latter dot product identity once more with respect to $t$ and get $$\\nabla f(x)^T \\, \\frac{d^2x}{dt^2} \\, + \\, \\frac{dx}{dt}^T H_f(x) \\, \\frac{dx}{dt} \\, = \\, 0$$ Plug $\\frac{d^2x}{dt^2} \\, = \\, \\lambda \\, \\nabla f(x)$ into the latter identity and you get $$\\nabla f(x)^T \\, \\big(\\lambda \\, \\nabla f(x)\\,\\big) \\, + \\, \\frac{dx}{dt}^T H_f(x) \\, \\frac{dx}{dt} \\, = \\, 0$$ $$\\lambda \\, \\nabla f(x)^T \\, \\, \\nabla f(x) \\, + \\, \\frac{dx}{dt}^T H_f(x) \\, \\frac{dx}{dt} \\, = \\, 0$$ $$\\lambda \\, \\big| \\nabla f(x) \\big|^2 \\, + \\, \\frac{dx}{dt}^T H_f(x) \\, \\frac{dx}{dt} \\, = \\, 0$$ so when you solve for $\\lambda$ you get $$\\lambda \\, = \\, - \\, \\frac{\\, \\frac{dx}{dt}^T H_f(x) \\, \\frac{dx}{dt} \\,}{\\big| \\nabla f(x) \\big|^2}$$ And now you see that the system of differential equations for the geodesics on $M_c$ , written with variables from the ambient space $\\mathbb{R}^m$ , is \\begin{align} &\\frac{dx}{dt} \\, = \\, v\\\\ &\\frac{dv}{dt} \\, = \\, -\\, \\frac{\\, v^T H_f(x) \\, v \\,}{\\big| \\nabla f(x) \\big|^2}\\,\\,\\nabla f(x) \\end{align} with initial conditions and restrictions \\begin{align} &x(0) \\, = \\, x_0\\\\ &v(0) \\, = \\, v_0\\\\ &f(x_0) \\, = \\, c\\\\ &\\nabla f(x_0)^T\\,v_0 \\, = \\, 0 \\end{align} Then, any solution to the system of equations that satisfies the initial restrictions will keep satisfying them for all times $t$ , i.e. for all $t$ \\begin{align} &f\\big(x(t)\\big) \\, = \\, c\\\\ &\\nabla f\\big(\\,x(t)\\,\\big)^T\\,v(t) \\, = \\, 0 \\end{align} That's because the equations have been constructed to satisfy these conditions in the first place. By differentiating these identities once and twice and combine with the initial restrictions, you can verify the identities for all $t$ , which we actually already did in order to determine the exact formula for $\\lambda$ . One more observation. The magnitude of the velocity $v(t)$ is constant for all $t$ . Indeed \\begin{align} \\frac{d}{dt}\\, \\big(\\,\\, |v|^2 \\,\\big) \\, =& \\, 2 \\, v^T \\, \\frac{dv}{dt} \\, = \\, 2\\, v^T \\left( \\, -\\, \\frac{\\, v^T H_f(x) \\, v \\,}{\\big| \\nabla f(x) \\big|^2}\\,\\,\\nabla f(x) \\, \\right)\\\\ =& \\, 2\\, \\left( \\, -\\, \\frac{\\, v^T H_f(x) \\, v \\,}{\\big| \\nabla f(x) \\big|^2} \\right) \\Big(\\, v^T\\, \\nabla f(x) \\, \\Big)\\\\ =& \\, 2\\, \\left( \\, -\\, \\frac{\\, v^T H_f(x) \\, v \\,}{\\big| \\nabla f(x) \\big|^2} \\right) \\Big(\\, \\nabla f(x)^T v \\, \\Big)\\\\ =& \\, 2\\, \\left( \\, -\\, \\frac{\\, v^T H_f(x) \\, v \\,}{\\big| \\nabla f(x) \\big|^2} \\right)\\cdot 0\\\\ =& \\, 0 \\end{align} so $|v(t)| = |v_0| = v_0$ is constant for all $t$ (another feature of the geodesic flow). In the light of this analysis, I would try to develop and implement a time-discrete version of the geodesic flow on $M_c$ , replicating as many of the properties above as possible, instead of trying direct blind numerics for the differential equations. Here is my suggestion: A discrete geodesic flow on the hyper-equipotential surface $M_c$ of the function $f(x)$ with fixed small step $h$ . Assume you have generated the following pair of position and velocity $$\\big(\\,x_n,\\, v_n\\,\\big)$$ satisfying the restrictions \\begin{align} &f(x_n) \\, = \\, c\\\\ &\\nabla f(x_n)^T\\,v_n =\\, 0\\\\ &|v_n| \\, = \\, v_0 \\end{align} Step 1. Generate the new intermediate position $$\\tilde{x}_n \\, = \\, x_n \\, + \\, h\\,v_n$$ Step 2. Solve the non-linear system of $n+1$ equations for the unknown $n+1$ variables $\\big(\\,x_{n+1}, \\,\\, \\lambda_{n+1}\\,\\big)$ \\begin{align} &\\lambda_{n+1}\\nabla f(x_{n+1}) \\, + \\, x_{n+1} \\, = \\, \\tilde{x}_n\\\\ &f(x_{n+1}) \\, = \\, c \\end{align} Geometrically, this system tells you that $x_{n+1}$ is the orthogonal projection of $\\tilde{x}_{n}$ onto the hyper-surface $M_c$ along the normal vector $\\nabla f(x_{n+1})$ of $M_c$ , calculated at the projected point $x_{n+1}$ . In other words, $x_{n+1}$ is chosen on the hyper-surface $M_c$ so that the $\\lambda-$ parametrized line $$x_{n+1} \\, + \\, \\lambda \\nabla f(x_{n+1})\\, , $$ which is orthogonal to $M_c$ , passes through the point $\\tilde{x}_n$ . As you can see, here we have $\\lambda_{n+1}\\nabla f(x_{n+1}) $ which is the discrete analogue of the orthogonal force that keeps the geodesic on the hyper-surface $M_c$ preventing it from escaping into the ambient space $\\mathbb{R}^m$ . You can try to solve this system of non-linear equation by say Newton's method, for which you will need the hessian $H_f(x)$ and that's where the hessian appears in this discrete scheme. So, start with initial conditions $x^0_{n+1} \\, = \\,\\tilde{x}_n,\\,\\, \\lambda^0_{n+1} = 0$ . Then keep iterating over $k$ , with $n$ fixed, and keep solving the linear system for the new variables $\\big(\\,x_{n+1}^{k+1}, \\,\\, v_{n+1}^{k+1}\\, \\big)$ form the previous already known variables $\\big(\\,x_{n+1}^{k}, \\,\\, v_{n+1}^{k}\\, \\big)$ calculate the Jacobian $$ \\Big[\\,J_{n+1}^k\\,\\Big] \\, = \\,\\begin{bmatrix} \\lambda_{n+1}^k H_f({x}_{n+1}^k) + I_{m\\times m} & \\nabla f({x}_{n+1}^k)\\\\ \\nabla f({x}_{n+1}^k)^T & 0 \\end{bmatrix} $$ and then use it as the matrix for the system of linear equations for the unknown variables $\\big(\\,x_{n+1}^{k+1}, \\,\\, v_{n+1}^{k+1}\\, \\big)$ $$ \\Big[\\,J_{n+1}^k\\,\\Big] \\begin{bmatrix} x_{n+1}^{k+1}\\\\ \\lambda_{n+1}^{k+1}\\end{bmatrix} \\, = \\, \\Big[\\,J_{n+1}^k\\,\\Big] \\begin{bmatrix} x_{n+1}^{k}\\\\ \\lambda_{n+1}^{k}\\end{bmatrix} \\, - \\, \\begin{bmatrix} \\lambda_{n+1}\\nabla f(x_{n+1}^{k}) \\, + \\, x_{n+1}^{k} \\, - \\, \\tilde{x}_n\\\\ f(x_{n+1}^k) \\, - \\, c\\end{bmatrix} $$ $$k = k + 1$$ until $$|f(x_{n+1}^{k}) \\, - \\, c|^2 \\,<\\, \\varepsilon$$ for some fixed $\\varepsilon$ threshold error. In attempt to save computational time and memory (hopefully it works), maybe you can try to cheat a bit and calculate the Jacobian for Newton's method only at the initial conditions $x^0_{n+1} \\, = \\,\\tilde{x}_n,\\,\\, \\lambda^0_{n+1} = \\lambda_{n}$ (or $\\lambda_{n+1}^0 = 0$ which allows you to skip the hessian altogether, but I do not know if it works) and keep reusing it. Here $\\lambda_n$ is the result from the calculation from the previous point $x_n$ . So first, calculate and invert the following Jacobian at $\\tilde{x}_n$ $$ \\Big[\\,\\text{J inv} \\, \\Big] \\, = \\,\\begin{bmatrix} \\lambda_{n+1}^0 H_f(\\tilde{x}_n) + I_{m \\times m} & \\nabla f(\\tilde{x}_{n})\\\\ \\nabla f(\\tilde{x}_{n})^T & 0 \\end{bmatrix}^{-1} $$ Second, again starting from initial conditions $x^0_{n+1} \\, = \\,\\tilde{x}_n,\\,\\, \\lambda^0_{n+1} = 0$ (or $\\lambda^0_{n+1} = \\lambda_n$ from the preceding calculation for $x_n$ ), and while $$|f(x_{n+1}^{k}) \\, - \\, c|^2 \\,\\geq \\, \\varepsilon$$ keep iterating over $k$ the discrete dynamical system $$\\begin{bmatrix} x_{n+1}^{k+1}\\\\ \\lambda_{n+1}^{k+1}\\end{bmatrix} \\, = \\, \\begin{bmatrix} x_{n+1}^{k}\\\\ \\lambda_{n+1}^{k}\\end{bmatrix} \\, - \\, \\Big[\\,\\text{J inv} \\, \\Big] \\begin{bmatrix} \\lambda_{n+1}\\nabla f(x_{n+1}^{k}) \\, + \\, x_{n+1}^{k} \\, - \\, \\tilde{x}_n\\\\ f(x_{n+1}^k) \\, - \\, c\\end{bmatrix}$$ $$k = k + 1$$ The result is the new position $x_{n+1}$ , which is on the surface $M_c$ (numerically :) ) Step 3. Finally, we calculate the new velocity $v_{n+1}$ . We project orthogonally the old velocity $v_n$ onto the tangent hyper-plane of $M_c$ at the new position $x_{n+1}$ and then we rescale it to have magnitude $v_0$ \\begin{align} &\\tilde{v}_n \\, = \\, v_n \\, - \\, \\left(\\,\\frac{\\,\\nabla f(x_{n+1})^T \\, v_n\\,}{|\\nabla f(x_{n+1})|^2}\\,\\right)\\, \\nabla f(x_{n+1})\\\\ &v_{n+1} \\, = \\, v_0 \\, \\frac{\\tilde{v}_n}{|\\tilde{v}_n|} \\end{align} As you can see here, just like in the smooth case, the velocities $v_{n+1}$ and $v_n$ are coplanar with the normal gradient vector $\\nabla f(x_{n+1})$ . And again, you can see that the velocity evolves only along the normal gradient vector $\\nabla f(x_{n+1})$ , which is the discrete analogue of the normal force redirecting the velocity in the smooth case. By executing steps 1, 2 and 3 we obtain the new position and velocity of the geodesic flow $$\\big(\\,x_{n+1},\\, v_{n+1}\\,\\big)$$ By construction, the new pair also satisfies the geodesic restrictions \\begin{align} &f(x_{n+1}) \\, = \\, c\\\\ &\\nabla f(x_{n+1})^T\\,v_{n+1} =\\, 0\\\\ &|v_{n+1}| \\, = \\, v_0 \\end{align} By iterating steps 1, 2, 3 you get a discrete analogue of the geodesic flow on $M_c$ . And I think the result will have a fairly good behaviour and will emulate many of the properties of the smooth geodesic flow. Edit. As a test, I implemented this method for the case of the geodesic flow on a 3D ellipsoid. I chose an ellipsoid whose axes are aligned with the coordinate axes. I implemented the method using a fixed Jacobian for Newton's method, when generating the orthogonal projection of the intermediate point onto the surface of the ellipsoid. It works quite well, so for nice surfaces probably there is no need to calculate a hessian, which is good news. import numpy as np import matplotlib.pyplot as plt def Jacobian_inv(z_, A): grad = A.dot(z_[0:3]) J = np.empty((4,4), dtype=float) J[0:3,0:3] = np.diag([1,1,1]) J[0:3, 3] = grad J[3, 0:3] = 2*grad J[3,3] = 0. return np.linalg.inv(J) def project_position(z_med, A, accuracy): z = z_med J_1 = Jacobian_inv(z, A) while True: Ax = A.dot(z[0:3]) xAx_1 = z[0:3].dot(Ax) - 1. if abs(xAx_1) < accuracy: return z F = np.concatenate( ( z[3]*Ax + z[0:3] - z_med[0:3], np.array([xAx_1]) ) ) z = z - J_1.dot(F) def project_velocity(x_, v_, norm_v, A): Ax = A.dot(x_) v = v_ - (Ax.dot(v_))*Ax / Ax.dot(Ax) return norm_v * v / np.sqrt(v.dot(v)) def geod_flow_step(z_, v_, norm_v, A, accuracy, step): z = np.concatenate( ( z_[0:3] + step * v_, np.array([0.]) ) ) z = project_position(z, A, accuracy) v = project_velocity(z[0:3], v_, norm_v, A) return z, v def geod_flow(x_in, v_in, A, norm_v, accuracy, step, n_steps): n = n_steps #int(arc_length / step) x = np.empty((n, 3), dtype=float) v = np.empty((n, 3), dtype=float) x[0,:] = x_in v[0,:] = v_in z = np.array([x_in[0], x_in[1], x_in[2], 0.0]) for m in range(n-1): z, v[m+1,:] = geod_flow_step(z, v[m,:], norm_v, A, accuracy, step) x[m+1,:] = z[0:3] return x, v x0 = np.array([3., 0., 0.]) v0 = np.array([0., 1., 2.]) semi_axes = np.array([3.*3., 2.*2, 1.*1.]) D = np.diag(1. / semi_axes) norm_v0 = 1. accuracy = 1e-7 step = 0.05 v0 = v0 / np.linalg.norm(v0) v0 = norm_v0 * v0 x, v = geod_flow(x0, v0, D, norm_v0, accuracy, step, 1500) fig = plt.figure() ax = fig.add_subplot(projection='3d') ax.set_xlim((-4, 4)) ax.set_ylim((-4, 4)) ax.set_zlim((-4, 4)) ax.plot(x[:,0], x[:,1], x[:,2], 'r-') ax.plot(x[0,0], x[0,1], x[0,2], 'bo') ax.plot(x[-1,0], x[-1,1], x[-1,2], 'go') plt.show() Starting point $[3, 0, 0]$ and direction vector $[0, 1, 1]$ Starting point $[3, 0, 0]$ and direction vector $[0, 1, 2]$ Starting point $[3, 0, 0]$ and direction vector $[0, 1, 3]$ The blue point is the starting point, the green one is the endpoint. Geodesic on a submanifold of a Euclidean space. A geodesic on a manifold is a curve whose covariant derivative (i.e. derivative within the framework of the manifold's geometry) is zero. This means that there is no acceleration on the manifold that makes the geodesic turn, so the geodesic is straight within the manifold's geometry. However, in our case, the manifold is a hyper-surface in some Euclidean space. So the geometry of this hyper-surface is inherited from the ambient Euclidean geometry. A geodesic on the hyper-surface doesn't curve as seen from the hyper-surface, but it definitely curves in the ambient Euclidean space, because the hyper-surface itself is not straight (unless it is hyper-plane) and the geodesic itself is not a straight line. So, the geodesic should curve in the ambient space but should not curve from the point of view of the hyper-surface. That means that the acceleration vector of the geodesic, which exists in the Euclidean space, should not be visible on the hyper-surface. This last statement simply means that the orthogonal projection of this acceleration on the tangent hyper-plane of the hyper-surface at each point on the geodesic should not exist, i.e. it should be zero. This is true exactly when the acceleration vector at each point of the geodesic in the Euclidean space is perpendicular to the tangent plane at that point. The gradient of the hype-surface is also perpendicular to the tangent hyper-plane at each point from the hyper-surface. Hence, the acceleration vector and the gradient must be colinear, i.e. aligned.",
      "question_latex": [
        "\\begin{align}\n\\dot x(t) &= v(t) \\\\\n\\dot v(t) &= - \\frac{v(t)^\\top H(x_t) v(t)}{||g(x(t))||^2} g(x(t))\n\\end{align}",
        "\\dot x(t) = v(t) \\\\\n\\dot v(t) = a(t)",
        "\\begin{align}\nx_{t + \\delta/2} &= x_t + \\frac{\\delta}{2}v_t \\\\\nv_{t+\\delta} &=v_t - \\delta \\frac{v(t)^\\top H(x_{t + \\delta/2}) v(t)}{||g(x_{t + \\delta/2})||^2} g(x_{t + \\delta/2}) \\\\\nx_{t+\\delta} &= x_{t + \\delta/2} + \\frac{\\delta}{2} v_{t + \\delta}\n\\end{align}",
        "f:\\mathbb{R}^n\\to\\mathbb{R}",
        "g(x)=\\nabla_xf(x)",
        "H(x) = \\nabla^2 f(x)",
        "</span>\nwith initial condition <span class=\"math-container\">",
        "</span> with <span class=\"math-container\">",
        "</span>. Clearly <span class=\"math-container\">",
        "</span> so that the velocity stays perpendicular to the gradient <span class=\"math-container\">",
        "</span>. I would like to discretize this ODE. I have read about Euler, Verlet and Leapfrog methods. Usually, when you have the system\n<span class=\"math-container\">",
        "</span>\nthe Leapfrog method workds well. However everyone seems to say this is not a good method to use when the force/acceleration depends on the velocity, as in this case. The lecture notes say that this system of equations  could be discretized as\n<span class=\"math-container\">"
      ],
      "answer_latex": [
        "f \\, : \\, \\mathbb{R}^m \\, \\longrightarrow \\, \\mathbb{R}",
        "M_c \\, = \\, \\{ \\, x \\, \\in \\, \\mathbb{R}^m \\,\\, | \\,\\,\\, f(x) = c\\,\\,\\}",
        "\\text{acceleration} \\, = \\,  \\frac{d^2x}{dt^2}",
        "\\frac{d^2x}{dt^2} \\,\\,\\text{ should be aligned with the normal vector to $M_c$ at $x$ }",
        "\\frac{d^2x}{dt^2} \\,\\,\\text{ should be aligned with the normal vector $\\nabla f(x)$ }",
        "\\frac{d^2x}{dt^2} \\, = \\, \\lambda \\, \\nabla f(x)",
        "\\lambda \\, = \\, \\lambda\\Big(x, \\, \\frac{dx}{dt}\\Big)",
        "f(x) \\, = \\, c",
        "\\nabla f(x)^T \\, \\frac{dx}{dt} \\, = \\, 0",
        "\\nabla f(x)^T \\, \\frac{d^2x}{dt^2} \\, + \\, \\frac{dx}{dt}^T H_f(x) \\, \\frac{dx}{dt} \\, = \\, 0",
        "\\nabla f(x)^T \\, \\big(\\lambda \\, \\nabla f(x)\\,\\big) \\, + \\, \\frac{dx}{dt}^T H_f(x) \\, \\frac{dx}{dt} \\, = \\, 0",
        "\\lambda \\, \\nabla f(x)^T \\,  \\, \\nabla f(x) \\, + \\, \\frac{dx}{dt}^T H_f(x) \\, \\frac{dx}{dt} \\, = \\, 0",
        "\\lambda \\, \\big| \\nabla f(x) \\big|^2  \\, + \\, \\frac{dx}{dt}^T H_f(x) \\, \\frac{dx}{dt} \\, = \\, 0",
        "\\lambda \\, = \\, - \\, \\frac{\\, \\frac{dx}{dt}^T H_f(x) \\, \\frac{dx}{dt} \\,}{\\big| \\nabla f(x) \\big|^2}",
        "\\big(\\,x_n,\\, v_n\\,\\big)",
        "\\tilde{x}_n \\, = \\, x_n \\, + \\, h\\,v_n",
        "x_{n+1} \\, + \\, \\lambda \\nabla f(x_{n+1})\\, ,",
        "\\Big[\\,J_{n+1}^k\\,\\Big] \\, = \n\\,\\begin{bmatrix}\n\\lambda_{n+1}^k H_f({x}_{n+1}^k) + I_{m\\times m} & \\nabla f({x}_{n+1}^k)\\\\\n\\nabla f({x}_{n+1}^k)^T & 0\n\\end{bmatrix}",
        "\\Big[\\,J_{n+1}^k\\,\\Big]  \\begin{bmatrix} x_{n+1}^{k+1}\\\\ \\lambda_{n+1}^{k+1}\\end{bmatrix} \\, = \\, \\Big[\\,J_{n+1}^k\\,\\Big]  \\begin{bmatrix} x_{n+1}^{k}\\\\ \\lambda_{n+1}^{k}\\end{bmatrix} \\, - \\, \\begin{bmatrix} \\lambda_{n+1}\\nabla f(x_{n+1}^{k}) \\, + \\, x_{n+1}^{k} \\, - \\, \\tilde{x}_n\\\\  f(x_{n+1}^k) \\, - \\, c\\end{bmatrix}",
        "k =  k + 1",
        "|f(x_{n+1}^{k}) \\, - \\, c|^2 \\,<\\, \\varepsilon",
        "\\Big[\\,\\text{J inv} \\, \\Big] \\, = \n\\,\\begin{bmatrix}\n\\lambda_{n+1}^0 H_f(\\tilde{x}_n) + I_{m \\times m} & \\nabla f(\\tilde{x}_{n})\\\\\n\\nabla f(\\tilde{x}_{n})^T & 0\n\\end{bmatrix}^{-1}",
        "|f(x_{n+1}^{k}) \\, - \\, c|^2 \\,\\geq \\, \\varepsilon",
        "\\begin{bmatrix} x_{n+1}^{k+1}\\\\ \\lambda_{n+1}^{k+1}\\end{bmatrix} \\, = \\, \\begin{bmatrix} x_{n+1}^{k}\\\\ \\lambda_{n+1}^{k}\\end{bmatrix} \\, - \\, \\Big[\\,\\text{J inv} \\, \\Big] \\begin{bmatrix} \\lambda_{n+1}\\nabla f(x_{n+1}^{k}) \\, + \\, x_{n+1}^{k} \\, - \\, \\tilde{x}_n\\\\  f(x_{n+1}^k) \\, - \\, c\\end{bmatrix}",
        "k =  k + 1",
        "\\big(\\,x_{n+1},\\, v_{n+1}\\,\\big)",
        "\\mathbb{R}^m",
        "f(x) = c",
        "</span> which we call 'the potential', and let us fix a real number (a value) <span class=\"math-container\">",
        "</span> which gives us the smooth level hyper-surface\n<span class=\"math-container\">",
        "</span>\nIf you restrict the standard flat Euclidean metric on <span class=\"math-container\">",
        "</span> you get a Riemannian metric on <span class=\"math-container\">",
        "</span>. Now, the geodesics of <span class=\"math-container\">",
        "</span> are not geodesics of <span class=\"math-container\">",
        "</span>, the latter are just straight lines, but the geodesics of <span class=\"math-container\">",
        "</span> try to follow the curved surface of <span class=\"math-container\">",
        "</span> with as little deviation from the ambient geodesics as possible. Which means that their acceleration, that causes them to curve and follow the surface instead of staying straight, when orthogonally projected onto the tangent space of <span class=\"math-container\">",
        "</span> at each point on the geodesic, should be zero (so no acceleration should be visible for creatures on the surface, that do not look into the ambient space). What is that mean. Let <span class=\"math-container\">",
        "</span> be a geodesic on <span class=\"math-container\">",
        "</span>. Then the acceleration should be its second derivative, i.e.\n<span class=\"math-container\">",
        "</span><br />\nFurthermore, since the orthogonal projection of <span class=\"math-container\">",
        "</span> onto the tangent space of <span class=\"math-container\">",
        "</span> at the point <span class=\"math-container\">",
        "</span> should be zero, that means that\n<span class=\"math-container\">",
        "\\frac{d^2x}{dt^2} \\,\\,\\text{ should be aligned with the normal vector to",
        "at",
        "}",
        "</span>\nBut a normal vector to <span class=\"math-container\">",
        "</span> at <span class=\"math-container\">",
        "</span> is the gradient <span class=\"math-container\">",
        "</span> so\n<span class=\"math-container\">",
        "\\frac{d^2x}{dt^2} \\,\\,\\text{ should be aligned with the normal vector",
        "</span>\nAnd there you have the first hint, that the equations of the geodesic should look like\n<span class=\"math-container\">",
        "</span> where you can expect\n<span class=\"math-container\">",
        "</span>\nCombine the latter system of equation with the restriction <span class=\"math-container\">",
        "</span>\nWhen you differentiate the latter restriction with respect to <span class=\"math-container\">",
        "</span>, you get the first new restriction\n<span class=\"math-container\">",
        "</span>\ni.e. the velocity <span class=\"math-container\">",
        "</span> of <span class=\"math-container\">",
        "</span> is always perpendicular to <span class=\"math-container\">",
        "</span>, which means the velocity is tangent to the hyper-surface <span class=\"math-container\">",
        "</span> (which is not surprising, it is expected and required). Now, we go even further and differentiate the latter dot product identity once more with respect to <span class=\"math-container\">",
        "</span> and get\n<span class=\"math-container\">",
        "</span>\nPlug <span class=\"math-container\">",
        "</span> into the latter identity and you get</p>\n<p><span class=\"math-container\">",
        "</span></p>\n<p><span class=\"math-container\">",
        "</span> so when you solve for <span class=\"math-container\">",
        "</span> you get</p>\n<p><span class=\"math-container\">",
        "</span></p>\n<p>And now you see that the system of differential equations for the geodesics on <span class=\"math-container\">",
        "</span>, written with variables from the ambient space <span class=\"math-container\">",
        "</span>, is</p>\n<p><span class=\"math-container\">\\begin{align}\n&\\frac{dx}{dt} \\, = \\, v\\\\\n&\\frac{dv}{dt} \\, = \\, -\\, \\frac{\\, v^T H_f(x) \\, v \\,}{\\big| \\nabla f(x) \\big|^2}\\,\\,\\nabla f(x)\n\\end{align}</span>\nwith initial conditions and restrictions\n<span class=\"math-container\">\\begin{align}\n&x(0) \\, = \\, x_0\\\\\n&v(0) \\, = \\, v_0\\\\\n&f(x_0) \\, = \\, c\\\\\n&\\nabla f(x_0)^T\\,v_0 \\, = \\, 0\n\\end{align}</span>\nThen, any solution to the system of equations that satisfies the initial restrictions will keep satisfying them for all times <span class=\"math-container\">",
        "</span>, i.e. for all <span class=\"math-container\">",
        "</span></p>\n<p><span class=\"math-container\">\\begin{align}\n&f\\big(x(t)\\big) \\, = \\, c\\\\\n&\\nabla f\\big(\\,x(t)\\,\\big)^T\\,v(t) \\, = \\, 0\n\\end{align}</span></p>\n<p>That's because the equations have been constructed to satisfy these conditions in the first place. By differentiating these identities once and twice and combine with the initial restrictions, you can verify the identities for all <span class=\"math-container\">",
        "</span>, which we actually already did in order to determine the exact formula for <span class=\"math-container\">",
        "</span>.</p>\n<p>One more observation. The magnitude of the velocity <span class=\"math-container\">",
        "</span> is constant for all <span class=\"math-container\">",
        "</span>. Indeed</p>\n<p><span class=\"math-container\">\\begin{align}\n\\frac{d}{dt}\\,\n\\big(\\,\\, |v|^2 \\,\\big) \\, =& \\, 2 \\, v^T \\, \\frac{dv}{dt} \\, = \\, 2\\, v^T \\left( \\,  -\\, \\frac{\\, v^T H_f(x) \\, v \\,}{\\big| \\nabla f(x) \\big|^2}\\,\\,\\nabla f(x) \\, \\right)\\\\\n=& \\, 2\\, \\left( \\,  -\\, \\frac{\\, v^T H_f(x) \\, v \\,}{\\big| \\nabla f(x) \\big|^2}  \\right) \\Big(\\, v^T\\, \\nabla f(x) \\, \\Big)\\\\\n=& \\, 2\\, \\left( \\,  -\\, \\frac{\\, v^T H_f(x) \\, v \\,}{\\big| \\nabla f(x) \\big|^2}  \\right) \\Big(\\, \\nabla f(x)^T v \\, \\Big)\\\\\n=& \\, 2\\, \\left( \\,  -\\, \\frac{\\, v^T H_f(x) \\, v \\,}{\\big| \\nabla f(x) \\big|^2}  \\right)\\cdot 0\\\\\n=& \\, 0\n\\end{align}</span></p>\n<p>so <span class=\"math-container\">",
        "</span> (another feature of the geodesic flow).</p>\n<p>In the light of this analysis, I would try to develop and implement a time-discrete version of the geodesic flow on <span class=\"math-container\">",
        "</span>, replicating as many of the properties above as possible, instead of trying direct blind numerics for the differential equations.</p>\n<p>Here is my suggestion:</p>\n<p><strong>A discrete geodesic flow on the hyper-equipotential surface <span class=\"math-container\">",
        "</span> of the function <span class=\"math-container\">",
        "</span> with fixed small step <span class=\"math-container\">",
        "</span>.</strong></p>\n<p>Assume you have generated the following pair of position and velocity\n<span class=\"math-container\">",
        "</span>\nsatisfying the restrictions</p>\n<p><span class=\"math-container\">\\begin{align}\n&f(x_n) \\, = \\, c\\\\\n&\\nabla f(x_n)^T\\,v_n  =\\, 0\\\\\n&|v_n| \\, = \\, v_0\n\\end{align}</span></p>\n<p><strong>Step 1.</strong> Generate the new intermediate position <span class=\"math-container\">",
        "</span></p>\n<p><strong>Step 2.</strong> Solve the non-linear system of <span class=\"math-container\">",
        "</span> equations for the unknown <span class=\"math-container\">",
        "</span> variables\n<span class=\"math-container\">",
        "</span></p>\n<p><span class=\"math-container\">\\begin{align}\n&\\lambda_{n+1}\\nabla f(x_{n+1}) \\, + \\, x_{n+1} \\, = \\, \\tilde{x}_n\\\\\n&f(x_{n+1}) \\, = \\, c\n\\end{align}</span></p>\n<p>Geometrically, this system tells you that <span class=\"math-container\">",
        "</span> is the orthogonal projection of <span class=\"math-container\">",
        "</span> onto the hyper-surface <span class=\"math-container\">",
        "</span> along the normal vector <span class=\"math-container\">",
        "</span>, calculated at the projected point <span class=\"math-container\">",
        "</span>. In other words, <span class=\"math-container\">",
        "</span> is chosen on the hyper-surface <span class=\"math-container\">",
        "</span> so that the <span class=\"math-container\">",
        "</span>parametrized line <span class=\"math-container\">",
        "</span> which is orthogonal to <span class=\"math-container\">",
        "</span>, passes through the point <span class=\"math-container\">",
        "</span>. As you can see, here we have <span class=\"math-container\">",
        "</span> which is the discrete analogue of the orthogonal force that keeps the geodesic on the hyper-surface <span class=\"math-container\">",
        "</span> preventing it from escaping into the ambient space <span class=\"math-container\">",
        "</span>.</p>\n<p>You can try to solve this system of non-linear equation by say Newton's method, for which you will need the hessian <span class=\"math-container\">",
        "</span> and that's where the hessian appears in this discrete scheme. So, start with initial conditions <span class=\"math-container\">",
        "</span>. Then keep iterating over <span class=\"math-container\">",
        "</span>, with <span class=\"math-container\">",
        "</span> fixed, and keep solving the linear system for the new variables <span class=\"math-container\">",
        "</span> form the previous already known variables <span class=\"math-container\">",
        "</span> calculate the Jacobian</p>\n<p><span class=\"math-container\">",
        "</span></p>\n<p>and then use it as the matrix for the system of linear equations for the unknown variables <span class=\"math-container\">",
        "</span></p>\n<p>until <span class=\"math-container\">",
        "</span> for some fixed <span class=\"math-container\">",
        "</span> threshold error.</p>\n<p>In attempt to save computational time and memory (hopefully it works), maybe you can try to cheat a bit and calculate the Jacobian for Newton's method only at the initial conditions <span class=\"math-container\">",
        "</span> (or <span class=\"math-container\">",
        "</span> which allows you to skip the hessian altogether, but I do not know if it works) and keep reusing it. Here <span class=\"math-container\">",
        "</span> is the result from the calculation from the previous point <span class=\"math-container\">",
        "</span>.</p>\n<p>So first, calculate and invert the following Jacobian at <span class=\"math-container\">",
        "</span></p>\n<p>Second, again starting from initial conditions <span class=\"math-container\">",
        "</span> from the preceding calculation for <span class=\"math-container\">",
        "</span>), and while <span class=\"math-container\">",
        "</span> keep iterating over <span class=\"math-container\">",
        "</span> the discrete dynamical system\n<span class=\"math-container\">",
        "</span>\n<span class=\"math-container\">",
        "</span>\nThe result is the new position <span class=\"math-container\">",
        "</span>, which is on the surface <span class=\"math-container\">",
        "</span> (numerically :) )</p>\n<p><strong>Step 3.</strong> Finally, we calculate the new velocity <span class=\"math-container\">",
        "</span>. We project orthogonally the old velocity <span class=\"math-container\">",
        "</span> onto the tangent hyper-plane of <span class=\"math-container\">",
        "</span> at the new position <span class=\"math-container\">",
        "</span> and then we rescale it to have magnitude <span class=\"math-container\">",
        "</span></p>\n<p><span class=\"math-container\">\\begin{align}\n&\\tilde{v}_n \\, = \\, v_n \\, - \\, \\left(\\,\\frac{\\,\\nabla f(x_{n+1})^T \\, v_n\\,}{|\\nabla f(x_{n+1})|^2}\\,\\right)\\, \\nabla f(x_{n+1})\\\\\n&v_{n+1} \\, = \\, v_0 \\, \\frac{\\tilde{v}_n}{|\\tilde{v}_n|}\n\\end{align}</span></p>\n<p>As you can see here, just like in the smooth case, the velocities <span class=\"math-container\">",
        "</span> and <span class=\"math-container\">",
        "</span> are coplanar with the normal gradient vector <span class=\"math-container\">",
        "</span>. And again, you can see that the velocity evolves only along the normal gradient vector <span class=\"math-container\">",
        "</span>, which is the discrete analogue of the normal force redirecting the velocity in the smooth case.</p>\n<p>By executing steps 1, 2 and 3 we obtain the new position and velocity of the geodesic flow</p>\n<p><span class=\"math-container\">",
        "</span></p>\n<p>By construction, the new pair also satisfies the geodesic restrictions</p>\n<p><span class=\"math-container\">\\begin{align}\n&f(x_{n+1}) \\, = \\, c\\\\\n&\\nabla f(x_{n+1})^T\\,v_{n+1}  =\\, 0\\\\\n&|v_{n+1}| \\, = \\, v_0\n\\end{align}</span></p>\n<p>By iterating steps 1, 2, 3 you get a discrete analogue of the geodesic flow on <span class=\"math-container\">",
        "</span>. And I think the result will have a fairly good behaviour and will emulate many of the properties of the smooth geodesic flow.</p>\n<p><strong>Edit.</strong> As a test, I implemented this method for the case of the geodesic flow on a 3D ellipsoid. I chose an ellipsoid whose axes are aligned with the coordinate axes. I implemented the method using a fixed Jacobian for Newton's method, when generating the orthogonal projection of the intermediate point onto the surface of the ellipsoid. It works quite well, so for nice surfaces probably there is no need to calculate a hessian, which is good news.</p>\n<pre><code>import numpy as np\nimport matplotlib.pyplot as plt\n\ndef Jacobian_inv(z_, A):\n    grad = A.dot(z_[0:3])\n    J = np.empty((4,4), dtype=float)\n    J[0:3,0:3] = np.diag([1,1,1])\n    J[0:3, 3] = grad \n    J[3, 0:3] = 2*grad\n    J[3,3] = 0.\n    return np.linalg.inv(J)\n\ndef project_position(z_med, A, accuracy):\n    z = z_med\n    J_1 = Jacobian_inv(z, A)\n    while True:\n        Ax = A.dot(z[0:3])\n        xAx_1 = z[0:3].dot(Ax) - 1.\n        if abs(xAx_1) < accuracy:\n            return z\n        F = np.concatenate(  ( z[3]*Ax + z[0:3] - z_med[0:3],  np.array([xAx_1]) )  )\n        z = z - J_1.dot(F)\n\ndef project_velocity(x_, v_, norm_v, A):\n    Ax = A.dot(x_)\n    v = v_ - (Ax.dot(v_))*Ax / Ax.dot(Ax)\n    return norm_v * v / np.sqrt(v.dot(v))\n\ndef geod_flow_step(z_, v_, norm_v, A, accuracy, step):\n    z = np.concatenate(  ( z_[0:3] + step * v_,   np.array([0.]) )  )\n    z = project_position(z, A, accuracy)\n    v = project_velocity(z[0:3], v_, norm_v, A)\n    return z, v\n\ndef geod_flow(x_in, v_in, A, norm_v, accuracy, step, n_steps):\n    n = n_steps #int(arc_length / step)\n    x = np.empty((n, 3), dtype=float)\n    v = np.empty((n, 3), dtype=float)\n    x[0,:] = x_in\n    v[0,:] = v_in\n    z = np.array([x_in[0], x_in[1], x_in[2], 0.0])\n    for m in range(n-1):\n        z, v[m+1,:] = geod_flow_step(z, v[m,:], norm_v, A, accuracy, step)\n        x[m+1,:] = z[0:3]\n    return x, v\n    \n\nx0 = np.array([3., 0., 0.])\nv0 = np.array([0., 1., 2.])\nsemi_axes = np.array([3.*3., 2.*2, 1.*1.])\nD = np.diag(1. / semi_axes)\nnorm_v0 = 1.\naccuracy = 1e-7\nstep = 0.05\n\nv0 = v0 / np.linalg.norm(v0)\nv0 = norm_v0 * v0 \n \n\nx, v = geod_flow(x0, v0, D, norm_v0, accuracy, step, 1500)\n\nfig = plt.figure()\n\nax = fig.add_subplot(projection='3d')\n\nax.set_xlim((-4, 4))\nax.set_ylim((-4, 4))\nax.set_zlim((-4, 4))\n\nax.plot(x[:,0], x[:,1], x[:,2], 'r-')\nax.plot(x[0,0], x[0,1], x[0,2], 'bo')\nax.plot(x[-1,0], x[-1,1], x[-1,2], 'go')\nplt.show()\n</code></pre>\n<p>Starting point <span class=\"math-container\">",
        "</span> and direction vector <span class=\"math-container\">",
        "</span></p>\n<p><a href=\"https://i.stack.imgur.com/WY0hY.png\" rel=\"nofollow noreferrer\"><img src=\"https://i.stack.imgur.com/WY0hY.png\" alt=\"\" /></a></p>\n<p>Starting point <span class=\"math-container\">",
        "</span>\n<a href=\"https://i.stack.imgur.com/CIYOF.png\" rel=\"nofollow noreferrer\"><img src=\"https://i.stack.imgur.com/CIYOF.png\" alt=\"enter image description here\" /></a></p>\n<p>Starting point <span class=\"math-container\">"
      ],
      "created": "2021-07-06T22:49:04.147",
      "golden_ner_terms": [
        "acceleration",
        "analysis",
        "blue",
        "calculate",
        "constant",
        "coordinate",
        "coplanar",
        "covariant derivative",
        "curve",
        "derivative",
        "differential",
        "differential equation",
        "differential equations",
        "differentiate",
        "direction vector",
        "discrete",
        "dot product",
        "dynamical system",
        "ellipsoid",
        "endpoint",
        "equation",
        "euclidean",
        "euclidean geometry",
        "euclidean metric",
        "euclidean space",
        "euler",
        "even",
        "exact form",
        "fix",
        "fixed",
        "flat",
        "flow",
        "formula",
        "function",
        "generate",
        "generating",
        "geodesic",
        "geodesics",
        "geometry",
        "gradient",
        "hessian",
        "hessian matrix",
        "identity",
        "initial condition",
        "jacobian",
        "l system",
        "level",
        "line",
        "linear equation",
        "linear system",
        "manifold",
        "matrix",
        "mean",
        "metric",
        "newton's method",
        "normal",
        "normal vector",
        "number",
        "observation",
        "ode",
        "onto",
        "order",
        "orthogonal",
        "orthogonal projection",
        "passes through",
        "perpendicular",
        "place",
        "plane",
        "point",
        "potential",
        "product",
        "project",
        "projection",
        "real",
        "real number",
        "restriction",
        "riemannian metric",
        "satisfy",
        "scheme",
        "second derivative",
        "smooth",
        "smooth function",
        "solution",
        "space",
        "step",
        "straight",
        "submanifold",
        "surface",
        "system of linear equations",
        "tangent",
        "tangent plane",
        "tangent space",
        "time",
        "vector",
        "velocity",
        "zero"
      ],
      "golden_ner_count": 96,
      "golden_patterns": [
        {
          "pattern": "find-the-right-abstraction",
          "score": 2.0,
          "hotwords": [
            "framework"
          ]
        },
        {
          "pattern": "check-the-extreme-cases",
          "score": 2.0,
          "hotwords": [
            "zero"
          ]
        },
        {
          "pattern": "construct-an-explicit-witness",
          "score": 2.0,
          "hotwords": [
            "construct"
          ]
        },
        {
          "pattern": "unfold-the-definition",
          "score": 2.0,
          "hotwords": [
            "means that"
          ]
        }
      ],
      "golden_pattern_names": [
        "find-the-right-abstraction",
        "check-the-extreme-cases",
        "construct-an-explicit-witness",
        "unfold-the-definition"
      ],
      "golden_scopes": [
        {
          "type": "let-binding",
          "match": "Let $x = x(t)$ be"
        },
        {
          "type": "for-any",
          "match": "for all $t$"
        },
        {
          "type": "for-any",
          "match": "for all $t$"
        },
        {
          "type": "for-any",
          "match": "for all $t$"
        },
        {
          "type": "for-any",
          "match": "for all $t$"
        },
        {
          "type": "set-notation",
          "match": "$c \\,\\in \\, \\mathbb{R}$"
        },
        {
          "type": "set-notation",
          "match": "$M_c \\, = \\, \\{ \\, x \\, \\in \\, \\mathbb{R}^m \\,\\, | \\,\\,\\, f(x) = c\\,\\,\\}$"
        }
      ],
      "golden_scope_count": 7
    },
    {
      "id": "se-physics-503822",
      "stratum": "hard",
      "title": "QM Continuity Equation: many-electron in the magnetic field version?",
      "tags": [
        "quantum-mechanics",
        "many-body"
      ],
      "score": 2,
      "answer_score": 3,
      "question_body": "In 1-particle non-relativistic QM we have the continuity condition as a per definitionem property for the 1-electron probability current density for an electron in the magnetic field in a stationary state and ignoring spin $$ \\nabla\\cdot{\\bf j}=0$$ with the definition of $$ {\\bf j}=-\\frac{i\\hbar}{2m}\\left(\\psi^*\\nabla\\psi-\\psi\\nabla\\psi^*\\right) +\\frac{e}{m}{\\bf A}\\psi^*\\psi $$ My question is what happens in the case of $n$ interacting (non-separable) electrons? The definition of the $n$ -electron current is then as the diagonal of the real part of the kinematic (effective one-particle) momentum density $$ {\\bf j}=diag\\{\\Re[\\hat\\pi \\rho(\\vec{r},\\vec{r}')]\\}$$ with the $n$ -particle kinematic (mechanical) momentum operator. $$ \\hat\\pi = \\sum_{j=1}^n \\big(\\hat p_j + e{\\bf A}(\\vec{r}_j)\\big)$$ and the one-particle reduced density matrix (1P-RMD) $$ \\rho(\\vec{r},\\vec{r}') = \\int \\Psi^*(\\vec{r}',\\vec{r}_2,\\cdots,\\vec{r}_2)\\Psi(\\vec{r},\\vec{r}_2,\\cdots,\\vec{r}_n) d\\vec{r}_2 \\dots d\\vec{r}_n$$ So the question is How to derive to derive the continuity condition $$ \\nabla\\cdot{\\bf j} = 0$$ for the n-electron effective one-particle probability current density $${\\bf j}=diag\\{\\Re[\\hat\\pi \\rho(\\vec{r},\\vec{r}')]\\}?$$ This question is inspired by this very recent publication where the problem is treated by explicitly assuming a product type expansion for the $n$ -electron wave function. Then there is also this very closely related PSE question on the QM n-particle continuity condition but here without the external magnetic field. However, the highest ranked answer looks very impressive to me and is based on second quantization. So already an extension of this answer to the magnetic field case would be of interest. I would be mostly interested if it possible to derive the result on basis of a 1P-RDM basis like for example using the von-Neumann equation for a 1P-RDM dynamics. Starting an attempt to answer: In analogy to the derivation of the hydrodynamical formulation of the Schrödinger mechanics we start with a \"polar form\" of the 1P-RDM $$ \\rho(\\vec{r},\\vec{r}';t)=R(\\vec{r},\\vec{r}';t)\\exp{{i\\frac{S(\\vec r,\\vec r';t)}{\\hbar}}} $$ Now the task is to insert that into the von Neumann equation, that looks for the static case like $$ -\\frac{i}{\\hbar}[\\hat H,\\hat \\rho] = 0$$ (where the prefactor of course can be omitted). And about here I come to my limits. I have no clear idea how to insert the 1P-RDM into the commutator ... any help appreciated.",
      "answer_body": "Switching to a different formulation can make life easier. To make life easier, I'll do two things: I'll use non-relativistic quantum field theory (NRQFT), also called second quantization. This allows deriving the general continuity equation as a simple operator equation, without any need for multi-electron wavefunctions or reduced density matrices. The operator equation is derived as a direct consequence of the gauge invariance of the Hamiltonian. When it's time to introduce a state, I'll use the full state. This is easier than trying to use an explicit reduced density matrix, and for the time-independent continuity equation, the 1P-RDM result falls out automatically anyway. That's because conceptually, when we work with a reduced density matrix, all we're really doing is agreeing to consider only a limited set of operators — such as the set of \"one-particle\" operators, which includes the electric current operator. After deriving the desired result, I'll briefly explain how the derivation relates to the von Neumann equation. The model, part 1: The Hilbert space and Hamiltonian Let $\\varphi(\\mathbf{x})$ be a field operator parameterized by $\\mathbf{x}$ , which denotes the 3-tuple of coordinates for a point in space. Let $\\varphi^\\dagger(\\mathbf{x})$ denote the adjoint of $\\varphi(\\mathbf{x})$ . The OP specifies electrons, which are fermions, so these operators satisfy the anti-commutation relations $$ \\big\\{\\varphi(\\mathbf{x}),\\,\\varphi^\\dagger(\\mathbf{y})\\big\\} = \\delta^3(\\mathbf{x} - \\mathbf{y}) \\tag{1} $$ and $$ \\big\\{\\varphi(\\mathbf{x}),\\,\\varphi(\\mathbf{y})\\big\\} = 0 \\tag{2} $$ with $\\{A,B\\} := AB+BA$ . This is QFT's version of saying that the wavefunction must be completely antisymmetric. As suggested in the OP, I'll pretend that electrons have spin $0$ , so the field operators don't need a spin index. I'll treat the magnetic field as a time-independent external field, expressed in terms of a gauge field $\\mathbf{A}(\\mathbf{x})$ . For any given choice of the gauge field, the Hamiltonian is $$ H[\\mathbf{A}] = H_2[\\mathbf{A}] + H_4[\\mathbf{A}] \\tag{3} $$ \\begin{align} H_2[\\mathbf{A}] &= \\int d^3x\\ \\frac{\\big(\\mathbf{D}\\varphi(\\mathbf{x})\\big)^\\dagger \\big(\\mathbf{D}\\varphi(\\mathbf{x})\\big)}{2m} \\tag{4} \\\\ H_4[\\mathbf{A}] &= \\int d^3x\\,d^3y\\ \\big(\\varphi(\\mathbf{x})\\varphi(\\mathbf{y})\\big)^\\dagger v(\\mathbf{x}-\\mathbf{y}) \\varphi(\\mathbf{x})\\varphi(\\mathbf{y}) \\tag{5} \\end{align} where $$ \\mathbf{D} = \\nabla + ie\\mathbf{A}(\\mathbf{x}) \\tag{6} $$ and where $v(\\mathbf{x})$ is a real-valued, decreasing positive function of $|\\mathbf{x}|$ that describes the repulsive Coulomb interaction between electrons. For any given gauge field $\\mathbf{A}$ , the Hamiltonian is manifestly non-negative, and a state $|0\\rangle$ that satisfies $$ \\varphi(\\mathbf{x})|0\\rangle = 0 \\tag{7} $$ for all $\\mathbf{x}$ is clearly a state with the lowest possible energy (zero). We can interpret this as the vacuum state, which has no electrons. Applying the operator $$ \\varphi^\\dagger(f) := \\int d^3x\\ f(\\mathbf{x})\\,\\varphi^\\dagger(\\mathbf{x}) \\tag{8} $$ to any state adds an electron with wavefunction $f$ . The Pauli exclusion principle is enforced by the anti-commutation relations, which imply that applying $\\varphi^\\dagger(f)$ twice to any state gives zero. In words, no two electrons can have the same wavefunction. The model, part 2: Gauge invariance and observables We're really defining a whole family of models, one for each choice of the gauge field $\\mathbf{A}$ . In this context, an \"observable\" is really a whole family of operators, one operator $F[\\mathbf{A}]$ for each choice of the gauge field $\\mathbf{A}$ , such that the following condition holds for all functions $\\theta(\\mathbf{x})$ : $$ U[\\theta]\\,F[\\mathbf{A}+\\nabla\\theta]\\,U^{-1}[\\theta] = F[\\mathbf{A}] \\tag{9} $$ where the unitary operator $U[\\theta]$ is defined by the conditions \\begin{gather} U[\\theta]\\,\\varphi(\\mathbf{x})\\,U^{-1}[\\theta] = \\exp\\big(ie\\,\\theta(\\mathbf{x})\\big)\\,\\varphi(\\mathbf{x}). \\tag{10} \\end{gather} In words, equation (9) says that observables are gauge invariant . An example is $$ F[\\mathbf{A}] = \\varphi^\\dagger(\\mathbf{x}) \\left(ie\\int_\\mathbf{y}^\\mathbf{x} \\mathbf{A}\\cdot d\\mathbf{z} \\right) \\varphi(\\mathbf{y}) \\tag{11} $$ for a fixed pair of points $\\mathbf{x}$ and $\\mathbf{y}$ . This satisfies the condition (9), so it qualifies as an observable. Deriving the general (operator) continuity equation The Hamiltonian $H[\\mathbf{A}]$ , regarded as a functional of the gauge field, is gauge invariant in the sense defined above. The continuity equation (as an operator equation) can be derived from the gauge invariance of $H[\\mathbf{A}]$ . To do this, start with the identity $$ U[\\theta]\\,H[\\mathbf{A}+\\nabla\\theta]\\,U^{-1}[\\theta] = H[\\mathbf{A}], \\tag{12} $$ which expresses the gauge invariance of $H[\\mathbf{A}]$ . Vary both sides with respect to the arbitrary function $\\theta$ and then set $\\theta=0$ to get $$ \\left(\\frac{\\delta}{\\delta\\theta} H[\\mathbf{A}+\\nabla\\theta]\\right)_{\\theta=0} + \\left(\\frac{\\delta}{\\delta\\theta} U[\\theta]H[\\mathbf{A}]U^{-1}[\\theta]\\right)_{\\theta=0} = 0. \\tag{13} $$ Rewrite the first term using this identity: $$ \\left(\\frac{\\delta}{\\delta\\theta} H[\\mathbf{A}+\\nabla\\theta]\\right)_{\\theta=0} \\propto \\nabla\\cdot\\mathbf{J} \\tag{14} $$ with $$ \\mathbf{J}(\\mathbf{x}) := \\frac{\\delta}{\\delta \\mathbf{A}(\\mathbf{x})}H[\\mathbf{A}]. \\tag{15} $$ Equation (15) defines the current-density operator, and by working out the right-hand side, we see that it resembles the expression shown in the OP — except that here it is an operator , involving the electron field operators in place of a single-particle wavefunction. For the second term in (13), use the fact that any operator $X$ satisfies identity $$ \\left(\\frac{\\delta}{\\delta\\theta(\\mathbf{x})} U[\\theta]XU^{-1}[\\theta] \\right)_{\\theta=0} \\propto [X,J_0(\\mathbf{x})] \\tag{16} $$ where $J_0$ is the charge-density operator $$ J_0(\\mathbf{x}) := e\\varphi^\\dagger(\\mathbf{x})\\varphi(\\mathbf{x}). \\tag{17} $$ Altogether, this shows that equation (12), which expresses the gauge invariance of the Hamiltonian, implies the continuity equation $$ \\nabla\\cdot\\mathbf{J}(\\mathbf{x}) \\propto [H,J_0(\\mathbf{x})] \\tag{18} $$ for the current- and charge-density operators. Deriving $\\nabla\\cdot\\mathbf{j}=0$ The continuity equation derived above is an operator equation. It doesn't refer to any state. To answer the question in the OP, we need to introduce a multi-electron state and then consider the \"one-particle reduced density matrix.\" Let $|\\psi\\rangle$ be an arbitrary $N$ -electron state-vector: $$ |\\psi\\rangle = \\int d^3x_1\\cdots d^3x_N\\ \\Psi(\\mathbf{x}_1,...,\\mathbf{x}_N) \\varphi^\\dagger(\\mathbf{x}_1)\\cdots \\varphi^\\dagger(\\mathbf{x}_N)|0\\rangle \\tag{19} $$ where $\\Psi$ is the $N$ -electron wavefunction. We might as well use the abbreviation $$ \\psi(X) := \\frac{\\langle\\psi|X|\\psi\\rangle}{\\langle\\psi|\\psi\\rangle} \\tag{20} $$ for any operator $X$ , because all predictions that we make with quantum theory ultimately boil down to expressions of this form. I'll follow mathematicians and refer to $\\psi(\\cdots)$ as the state . It takes any operator as input and returns its expecation value as output. Inserting the operator-valued continuity equation into $\\psi(\\cdots)$ gives $$ \\nabla\\cdot\\psi(\\mathbf{J})\\propto\\psi\\big([H,J_0]\\big). \\tag{21} $$ If we further assume that $\\psi(\\cdots)$ is a stationary state — that is, if we assume that $|\\psi\\rangle$ is an eigenstate of $H$ — then the right-hand side is zero, which leaves $$ \\nabla\\cdot\\mathbf{j} = 0 \\hskip1cm\\text{with}\\hskip1cm\\mathbf{j} := \\psi(\\mathbf{J}). \\tag{22} $$ This is the desired result. Relation to the 1P-RDM Suppose that $X$ is any operator of the form $$ X = \\int d^3x\\,d^3y\\ \\varphi^\\dagger(\\mathbf{x})M(\\mathbf{x}-\\mathbf{y}) \\varphi(\\mathbf{y}) \\tag{23} $$ where $M$ is an ordinary function or differential operator. The (components of the) current operator $\\mathbf{J}$ is one example. For any such operator $X$ , the quantity $\\psi(X)$ is equal to the trace of $X$ multiplied by the \"one-particle reduced density matrix\" defined in the OP. This is clear by inspection of equations (19), (20), and (23), taking the anti-commutation relations (1)-(2) into account. We don't need to make the reduced density matrix explicit, because whenever we talk about a reduced density matrix, all we're really doing is agreeing to consider only a limited set of operators as inputs to the state $\\psi(\\cdots)$ . In the \"one-particle\" case, we're agreeing to consider only operators of the form (23), which are sometimes called \"one-particle\" operators because they annihilate-and-then-create just one particle. The operator $\\mathbf{J}$ defined above has this form, so the result (22) is automatically a 1P-RDM equation. Relation to the von Neumann equation To relate the preceding derivation to the von Neumann equation, start with $$ \\frac{d}{dt}\\psi(\\cdots) \\propto\\psi\\big([H,\\cdots]\\big), \\tag{24} $$ which may be interpreted as the time-evolution equation for the density matrix. (Remember that working with a reduced density matrix really just means that we've agreed to consider only a limited set of operators.) When we write it as in (24), we don't need to worry about how to define an effective one-particle Hamiltonian, which would be awkward because of the two-particle Coulomb interaction. All we need to do is insert the charge-density operator $J_0$ into (24) and use the identity (21) to get $$ \\frac{d}{dt}\\psi(J_0) \\propto \\nabla\\cdot\\mathbf{j}. \\tag{25} $$ For a stationary state, the left-hand side is zero, which leaves the desired result (22).",
      "question_latex": [
        "\\nabla\\cdot{\\bf j}=0",
        "{\\bf j}=-\\frac{i\\hbar}{2m}\\left(\\psi^*\\nabla\\psi-\\psi\\nabla\\psi^*\\right) +\\frac{e}{m}{\\bf A}\\psi^*\\psi",
        "{\\bf j}=diag\\{\\Re[\\hat\\pi \\rho(\\vec{r},\\vec{r}')]\\}",
        "\\hat\\pi  = \\sum_{j=1}^n \\big(\\hat p_j + e{\\bf A}(\\vec{r}_j)\\big)",
        "\\rho(\\vec{r},\\vec{r}') = \\int \\Psi^*(\\vec{r}',\\vec{r}_2,\\cdots,\\vec{r}_2)\\Psi(\\vec{r},\\vec{r}_2,\\cdots,\\vec{r}_n) d\\vec{r}_2 \\dots d\\vec{r}_n",
        "\\nabla\\cdot{\\bf j} = 0",
        "{\\bf j}=diag\\{\\Re[\\hat\\pi \\rho(\\vec{r},\\vec{r}')]\\}?",
        "\\rho(\\vec{r},\\vec{r}';t)=R(\\vec{r},\\vec{r}';t)\\exp{{i\\frac{S(\\vec r,\\vec r';t)}{\\hbar}}}",
        "-\\frac{i}{\\hbar}[\\hat H,\\hat \\rho] = 0",
        "</span></p>\n\n<p>with the definition of</p>\n\n<p><span class=\"math-container\">",
        "</span></p>\n\n<p>My question is what happens in the case of <span class=\"math-container\">",
        "</span> interacting (non-separable) electrons? </p>\n\n<p>The definition of the <span class=\"math-container\">",
        "</span>-electron current is then as the diagonal of the real part of the kinematic (effective one-particle) momentum density</p>\n\n<p><span class=\"math-container\">",
        "</span></p>\n\n<p>with the <span class=\"math-container\">",
        "</span>-particle kinematic (mechanical) momentum operator. </p>\n\n<p><span class=\"math-container\">",
        "</span>\nand the one-particle reduced density matrix (1P-RMD)\n<span class=\"math-container\">",
        "</span></p>\n\n<p>So the question is </p>\n\n<blockquote>\n  <p>How to derive to derive the continuity condition \n  <span class=\"math-container\">",
        "</span>\n  for the n-electron effective one-particle probability current density <span class=\"math-container\">",
        "</span></p>\n</blockquote>\n\n<p>This question is inspired by this <a href=\"https://aip.scitation.org/doi/pdf/10.1063/1.5124250\" rel=\"nofollow noreferrer\">very recent publication</a> where the problem is treated by explicitly assuming a product type expansion for the <span class=\"math-container\">",
        "</span>-electron wave function. </p>\n\n<p>Then there is also <a href=\"https://physics.stackexchange.com/questions/294837/qm-continuity-equation-many-body-version-for-density-operator\">this very closely related PSE question</a> on the QM n-particle continuity condition but here without the external magnetic field. However, the highest ranked answer looks very impressive to me and is based on second quantization. So already an extension of this answer to the magnetic field case would be of interest.</p>\n\n<blockquote>\n  <p>I would be mostly interested if it possible to derive the result on basis of a 1P-RDM basis like for example using the <a href=\"https://en.wikipedia.org/wiki/Density_matrix#The_von_Neumann_equation_for_time_evolution\" rel=\"nofollow noreferrer\">von-Neumann equation</a> for a 1P-RDM dynamics.</p>\n</blockquote>\n\n<hr>\n\n<p>Starting an attempt to answer:</p>\n\n<p>In analogy to the derivation of the hydrodynamical formulation of the Schrödinger mechanics we start with a \"polar form\" of the 1P-RDM \n<span class=\"math-container\">",
        "</span>\nNow the task is to insert that into the von Neumann equation, that looks for the static case like\n<span class=\"math-container\">"
      ],
      "answer_latex": [
        "\\big\\{\\varphi(\\mathbf{x}),\\,\\varphi^\\dagger(\\mathbf{y})\\big\\}\n = \\delta^3(\\mathbf{x} - \\mathbf{y})\n\\tag{1}",
        "\\big\\{\\varphi(\\mathbf{x}),\\,\\varphi(\\mathbf{y})\\big\\} = 0\n\\tag{2}",
        "H[\\mathbf{A}] =\n H_2[\\mathbf{A}] +\n H_4[\\mathbf{A}] \n\\tag{3}",
        "\\mathbf{D} = \\nabla + ie\\mathbf{A}(\\mathbf{x})\n\\tag{6}",
        "\\varphi(\\mathbf{x})|0\\rangle = 0\n\\tag{7}",
        "\\varphi^\\dagger(f) := \\int d^3x\\ f(\\mathbf{x})\\,\\varphi^\\dagger(\\mathbf{x})\n\\tag{8}",
        "U[\\theta]\\,F[\\mathbf{A}+\\nabla\\theta]\\,U^{-1}[\\theta] = F[\\mathbf{A}]\n\\tag{9}",
        "F[\\mathbf{A}] = \\varphi^\\dagger(\\mathbf{x})\n  \\left(ie\\int_\\mathbf{y}^\\mathbf{x} \\mathbf{A}\\cdot d\\mathbf{z}\n  \\right)\n  \\varphi(\\mathbf{y})\n\\tag{11}",
        "U[\\theta]\\,H[\\mathbf{A}+\\nabla\\theta]\\,U^{-1}[\\theta] = H[\\mathbf{A}],\n\\tag{12}",
        "\\left(\\frac{\\delta}{\\delta\\theta}\n H[\\mathbf{A}+\\nabla\\theta]\\right)_{\\theta=0} +\n \\left(\\frac{\\delta}{\\delta\\theta}\n U[\\theta]H[\\mathbf{A}]U^{-1}[\\theta]\\right)_{\\theta=0} = 0.\n\\tag{13}",
        "\\left(\\frac{\\delta}{\\delta\\theta}\n H[\\mathbf{A}+\\nabla\\theta]\\right)_{\\theta=0}\n  \\propto \\nabla\\cdot\\mathbf{J}\n\\tag{14}",
        "\\mathbf{J}(\\mathbf{x}) \n := \\frac{\\delta}{\\delta \\mathbf{A}(\\mathbf{x})}H[\\mathbf{A}].\n\\tag{15}",
        "\\left(\\frac{\\delta}{\\delta\\theta(\\mathbf{x})} U[\\theta]XU^{-1}[\\theta]\n \\right)_{\\theta=0}\n   \\propto [X,J_0(\\mathbf{x})]\n\\tag{16}",
        "J_0(\\mathbf{x}) := e\\varphi^\\dagger(\\mathbf{x})\\varphi(\\mathbf{x}).\n\\tag{17}",
        "\\nabla\\cdot\\mathbf{J}(\\mathbf{x})\n   \\propto [H,J_0(\\mathbf{x})]\n\\tag{18}",
        "|\\psi\\rangle = \\int d^3x_1\\cdots d^3x_N\\\n  \\Psi(\\mathbf{x}_1,...,\\mathbf{x}_N)\n  \\varphi^\\dagger(\\mathbf{x}_1)\\cdots\n  \\varphi^\\dagger(\\mathbf{x}_N)|0\\rangle\n\\tag{19}",
        "\\psi(X) := \\frac{\\langle\\psi|X|\\psi\\rangle}{\\langle\\psi|\\psi\\rangle}\n\\tag{20}",
        "\\nabla\\cdot\\psi(\\mathbf{J})\\propto\\psi\\big([H,J_0]\\big).\n\\tag{21}",
        "\\nabla\\cdot\\mathbf{j} = 0\n\\hskip1cm\\text{with}\\hskip1cm\\mathbf{j} := \\psi(\\mathbf{J}).\n\\tag{22}",
        "X = \\int d^3x\\,d^3y\\ \\varphi^\\dagger(\\mathbf{x})M(\\mathbf{x}-\\mathbf{y})\n   \\varphi(\\mathbf{y})\n\\tag{23}",
        "\\frac{d}{dt}\\psi(\\cdots) \\propto\\psi\\big([H,\\cdots]\\big),\n\\tag{24}",
        "\\frac{d}{dt}\\psi(J_0) \\propto \\nabla\\cdot\\mathbf{j}.\n\\tag{25}",
        "\\varphi(\\mathbf{x})",
        "\\mathbf{x}",
        "\\varphi^\\dagger(\\mathbf{x})",
        "</span>\nand\n<span class=\"math-container\">",
        "</span>\nwith <span class=\"math-container\">",
        "</span>. This is QFT's version of saying that the wavefunction must be completely antisymmetric. As suggested in the OP, </p>\n\n<ul>\n<li><p>I'll pretend that electrons have spin <span class=\"math-container\">",
        "</span>, so the field operators don't need a spin index. </p></li>\n<li><p>I'll treat the magnetic field as a time-independent external field, expressed in terms of a gauge field <span class=\"math-container\">",
        "</span>.</p></li>\n</ul>\n\n<p>For any given choice of the gauge field, the Hamiltonian is\n<span class=\"math-container\">",
        "</span>\n<span class=\"math-container\">\\begin{align}\n H_2[\\mathbf{A}] \n &= \\int d^3x\\ \\frac{\\big(\\mathbf{D}\\varphi(\\mathbf{x})\\big)^\\dagger\n    \\big(\\mathbf{D}\\varphi(\\mathbf{x})\\big)}{2m}\n\\tag{4}\n\\\\\n H_4[\\mathbf{A}] \n &= \\int d^3x\\,d^3y\\ \n  \\big(\\varphi(\\mathbf{x})\\varphi(\\mathbf{y})\\big)^\\dagger\n  v(\\mathbf{x}-\\mathbf{y})\n  \\varphi(\\mathbf{x})\\varphi(\\mathbf{y})\n\\tag{5}\n\\end{align}</span>\nwhere \n<span class=\"math-container\">",
        "</span>\nand where <span class=\"math-container\">",
        "</span> is a real-valued, decreasing positive function of <span class=\"math-container\">",
        "</span> that describes the repulsive Coulomb interaction between electrons. For any given gauge field <span class=\"math-container\">",
        "</span>, the Hamiltonian is manifestly non-negative, and a state <span class=\"math-container\">",
        "</span> that satisfies\n<span class=\"math-container\">",
        "</span>\nfor all <span class=\"math-container\">",
        "</span> is clearly a state with the lowest possible energy (zero). We can interpret this as the vacuum state, which has no electrons. Applying the operator\n<span class=\"math-container\">",
        "</span>\nto any state adds an electron with wavefunction <span class=\"math-container\">",
        "</span>. The Pauli exclusion principle is enforced by the anti-commutation relations, which imply that applying <span class=\"math-container\">",
        "</span> twice to any state gives zero. In words, no two electrons can have the same wavefunction.</p>\n\n<h2> The model, part 2: Gauge invariance and observables </h2>\n\n<p>We're really defining a whole family of models, one for each choice of the gauge field <span class=\"math-container\">",
        "</span>. In this context, an \"observable\" is really a whole family of operators, one operator <span class=\"math-container\">",
        "</span> for each choice of the gauge field <span class=\"math-container\">",
        "</span>, such that the following condition holds for all functions <span class=\"math-container\">",
        "</span>:\n<span class=\"math-container\">",
        "</span>\nwhere the unitary operator <span class=\"math-container\">",
        "</span> is defined by the conditions\n<span class=\"math-container\">\\begin{gather}\n U[\\theta]\\,\\varphi(\\mathbf{x})\\,U^{-1}[\\theta]\n = \\exp\\big(ie\\,\\theta(\\mathbf{x})\\big)\\,\\varphi(\\mathbf{x}).\n\\tag{10}\n\\end{gather}</span>\nIn words, equation (9) says that observables are <strong>gauge invariant</strong>. An example is\n<span class=\"math-container\">",
        "</span>\nfor a fixed pair of points <span class=\"math-container\">",
        "</span> and <span class=\"math-container\">",
        "</span>. This satisfies the condition (9), so it qualifies as an observable.</p>\n\n<h2> Deriving the general (operator) continuity equation </h2>\n\n<p>The Hamiltonian <span class=\"math-container\">",
        "</span>, regarded as a functional of the gauge field, is gauge invariant in the sense defined above. The continuity equation (as an operator equation) can be derived from the gauge invariance of <span class=\"math-container\">",
        "</span>. To do this, start with the identity\n<span class=\"math-container\">",
        "</span>\nwhich expresses the gauge invariance of <span class=\"math-container\">",
        "</span>. Vary both sides with respect to the arbitrary function <span class=\"math-container\">",
        "</span> and then set <span class=\"math-container\">",
        "</span> to get\n<span class=\"math-container\">",
        "</span>\nRewrite the first term using this identity:\n<span class=\"math-container\">",
        "</span>\nwith\n<span class=\"math-container\">",
        "</span>\nEquation (15) defines the current-density operator, and by working out the right-hand side, we see that it resembles the expression shown in the OP — except that here it is an <em>operator</em>, involving the electron field operators in place of a single-particle wavefunction. For the second term in (13), use the fact that any operator <span class=\"math-container\">",
        "</span> satisfies identity\n<span class=\"math-container\">",
        "</span>\nwhere <span class=\"math-container\">",
        "</span> is the charge-density operator\n<span class=\"math-container\">",
        "</span>\nAltogether, this shows that equation (12), which expresses the gauge invariance of the Hamiltonian, implies the continuity equation\n<span class=\"math-container\">",
        "</span>\nfor the current- and charge-density operators.</p>\n\n<h2> Deriving <span class=\"math-container\">",
        "</span> </h2>\n\n<p>The continuity equation derived above is an operator equation. It doesn't refer to any state. To answer the question in the OP, we need to introduce a multi-electron state and then consider the \"one-particle reduced density matrix.\" Let <span class=\"math-container\">",
        "</span> be an arbitrary <span class=\"math-container\">",
        "</span>-electron state-vector:\n<span class=\"math-container\">",
        "</span> is the <span class=\"math-container\">",
        "</span>-electron wavefunction. We might as well use the abbreviation\n<span class=\"math-container\">",
        "</span>\nfor any operator <span class=\"math-container\">",
        "</span>, because all predictions that we make with quantum theory ultimately boil down to expressions of this form. I'll follow mathematicians and refer to <span class=\"math-container\">",
        "</span> as the <strong>state</strong>. It takes any operator as input and returns its expecation value as output. Inserting the operator-valued continuity equation into <span class=\"math-container\">",
        "</span> gives\n<span class=\"math-container\">",
        "</span>\nIf we further assume that <span class=\"math-container\">",
        "</span> is a stationary state — that is, if we assume that <span class=\"math-container\">",
        "</span> is an eigenstate of <span class=\"math-container\">",
        "</span> — then the right-hand side is zero, which leaves\n<span class=\"math-container\">",
        "</span>\nThis is the desired result.</p>\n\n<h2> Relation to the 1P-RDM </h2>\n\n<p>Suppose that <span class=\"math-container\">",
        "</span> is any operator of the form\n<span class=\"math-container\">",
        "</span> is an ordinary function or differential operator. The (components of the) current operator <span class=\"math-container\">",
        "</span> is one example. For any such operator <span class=\"math-container\">",
        "</span>, the quantity <span class=\"math-container\">",
        "</span> is equal to the trace of <span class=\"math-container\">",
        "</span> multiplied by the \"one-particle reduced density matrix\" defined in the OP. This is clear by inspection of equations (19), (20), and (23), taking the anti-commutation relations (1)-(2) into account. </p>\n\n<p>We don't need to make the reduced density matrix explicit, because whenever we talk about a reduced density matrix, all we're <em>really</em> doing is agreeing to consider only a limited set of operators as inputs to the state <span class=\"math-container\">",
        "</span>. In the \"one-particle\" case, we're agreeing to consider only operators of the form (23), which are sometimes called \"one-particle\" operators because they annihilate-and-then-create just one particle. The operator <span class=\"math-container\">",
        "</span> defined above has this form, so the result (22) is automatically a 1P-RDM equation.</p>\n\n<h2> Relation to the von Neumann equation </h2>\n\n<p>To relate the preceding derivation to the von Neumann equation, start with\n<span class=\"math-container\">",
        "</span>\nwhich may be interpreted as the time-evolution equation for the density matrix. (Remember that working with a reduced density matrix really just means that we've agreed to consider only a limited set of operators.) When we write it as in (24), we don't need to worry about how to define an effective one-particle Hamiltonian, which would be awkward because of the two-particle Coulomb interaction. All we need to do is insert the charge-density operator <span class=\"math-container\">",
        "</span> into (24) and use the identity (21) to get\n<span class=\"math-container\">"
      ],
      "created": "2019-09-20T13:04:05.137",
      "golden_ner_terms": [
        "adjoint",
        "analogy",
        "antisymmetric",
        "basis",
        "clear",
        "commutator",
        "components",
        "consequence",
        "context",
        "continuity equation",
        "coordinates",
        "current",
        "decreasing",
        "density",
        "density operator",
        "derivation",
        "diagonal",
        "differential",
        "differential operator",
        "effective",
        "electric current",
        "electrons",
        "energy",
        "equation",
        "expansion",
        "expression",
        "extension",
        "fermions",
        "field",
        "field theory",
        "fixed",
        "function",
        "functional",
        "gauge",
        "gauge invariance",
        "hamiltonian",
        "hilbert space",
        "identity",
        "implies",
        "index",
        "interest",
        "invariant",
        "matrix",
        "model",
        "models",
        "momentum",
        "observables",
        "operator",
        "operators",
        "pauli exclusion principle",
        "place",
        "point",
        "polar",
        "polar form",
        "positive",
        "probability",
        "product",
        "property",
        "quantization",
        "quantum field theory",
        "real",
        "real part",
        "reduced",
        "relation",
        "satisfy",
        "second quantization",
        "side",
        "simple",
        "space",
        "state",
        "stationary",
        "term",
        "theory",
        "time",
        "trace",
        "type",
        "unitary",
        "unitary operator",
        "vacuum",
        "wave function",
        "wavefunction",
        "work",
        "zero"
      ],
      "golden_ner_count": 83,
      "golden_patterns": [
        {
          "pattern": "exploit-symmetry",
          "score": 4.0,
          "hotwords": [
            "symmetric",
            "invariant"
          ]
        },
        {
          "pattern": "work-examples-first",
          "score": 2.0,
          "hotwords": [
            "example"
          ]
        },
        {
          "pattern": "check-the-extreme-cases",
          "score": 2.0,
          "hotwords": [
            "zero"
          ]
        },
        {
          "pattern": "construct-an-explicit-witness",
          "score": 2.0,
          "hotwords": [
            "explicit"
          ]
        },
        {
          "pattern": "dualise-the-problem",
          "score": 2.0,
          "hotwords": [
            "adjoint"
          ]
        },
        {
          "pattern": "unfold-the-definition",
          "score": 2.0,
          "hotwords": [
            "means that"
          ]
        },
        {
          "pattern": "construct-auxiliary-object",
          "score": 2.0,
          "hotwords": [
            "introduce"
          ]
        },
        {
          "pattern": "monotone-approximation",
          "score": 2.0,
          "hotwords": [
            "limit"
          ]
        }
      ],
      "golden_pattern_names": [
        "exploit-symmetry",
        "work-examples-first",
        "check-the-extreme-cases",
        "construct-an-explicit-witness",
        "dualise-the-problem",
        "unfold-the-definition",
        "construct-auxiliary-object",
        "monotone-approximation"
      ],
      "golden_scopes": [
        {
          "type": "let-binding",
          "match": "Let $\\varphi(\\mathbf{x})$ be"
        },
        {
          "type": "let-binding",
          "match": "Let $\\varphi^\\dagger(\\mathbf{x})$ denote"
        },
        {
          "type": "let-binding",
          "match": "Let $|\\psi\\rangle$ be"
        },
        {
          "type": "assume",
          "match": "Suppose that $"
        },
        {
          "type": "for-any",
          "match": "for all $\\mathbf{x}$"
        },
        {
          "type": "where-binding",
          "match": "where $v(\\mathbf{x})$ is"
        },
        {
          "type": "where-binding",
          "match": "where $J_0$ is"
        },
        {
          "type": "where-binding",
          "match": "where $\\Psi$ is"
        },
        {
          "type": "where-binding",
          "match": "where $M$ is"
        }
      ],
      "golden_scope_count": 9
    },
    {
      "id": "se-physics-511137",
      "stratum": "hard",
      "title": "Torque and Angular Momentum for point particles",
      "tags": [
        "classical-mechanics",
        "angular-momentum",
        "rotational-dynamics",
        "torque",
        "rotational-kinematics"
      ],
      "score": 2,
      "answer_score": 1,
      "question_body": "I have first been introduced to the notion of torque in regards to a rod (or any rigid body) rotating about a fixed axle. Then I kind of got the intuition of torque as if it is the equivalent of force from linear motion but in rotational motion. I have seen how torque can be thought of as what is needed to cause angular acceleration. Then I have been introduced to angular momentum as to being defined as $\\vec{L}=\\vec{r}\\times\\vec{p}$ however this time I was introduced to this quantity not necessarily in the context of rotation. I have seen that if you do $\\dfrac{d\\vec{L}}{dt}=\\vec{\\tau_{net}}$ now this is where the confusion comes from. Lets say that a particle is moving and I apply a force in a arbitrary direction and I change its momentum, then this would mean that I change its angular momentum and thus my applied force should produce a torque. However all this time I thought torque has to do with rotation and with rigid bodies, what does torque on a particle really mean, since I am not making it rotate about any axis? I am just confused as to what does torque mean in the sense of a particle. The same question can be posed about angular momentum, what is the connection between angular momentum of a single particle, and the angular momentum of a rigid body rotating about an axis eg $\\vec{L}=I\\vec{\\omega}$ how are these two same ideas the same even though they seem so different? Thank You",
      "answer_body": "These are the fundamental definitions you must know and the rest follow from these. Let's say you are in reference frame $S$ with origin $O$ . Assume that there's a particle of mass $m$ located at position $\\vec{r}$ (called the position vector) with respect to $O$ and its velocity is $\\vec{v}$ . Then, $$\\vec{L}_{\\text{of the point particle with respect to a reference point $P$}} := (\\vec{r}-\\vec{r}_P) \\times m\\vec{v} \\tag{1}$$ $$\\vec{\\tau}_{\\text{acting on the point particle with respect to a reference point $P$}} := (\\vec{r}-\\vec{r}_P) \\times \\vec{F}_{\\text{net acting on the particle}} \\tag{2}$$ For a point particle observed in an inertial frame, the following equation is true (the equation you've seen). But keep in mind that both angular momentum and torque are defined with respect to some reference point $P$ (unlike momentum and force). $$\\frac{d\\vec{L}_P}{dt}=\\vec{\\tau}_P$$ This can now be extended to a collection of interacting point particles. And further, to rigid bodies. Classical Mechanics by Goldstein is an excellent reference. $$\\vec{L}_{\\text{of a rigid body with respect to $P$}} = \\int dm ((\\vec{r}-\\vec{r}_P) \\times \\vec{v})$$ $$\\vec{\\tau}_{\\text{of the rigid body with respect to point $P$}}=\\sum_{\\text{$i$ : running over all external forces only}} (\\vec{r}_i-\\vec{r}_P) \\times {\\vec{F}_i}^\\text{external force}$$ $$\\frac{d}{dt}(\\vec{L}_{\\text{of a rigid body with respect to $P$}}) = \\vec{\\tau}_{\\text{of the rigid body with respect to point $P$}}$$ Anyway, I instead will focus on the confusion that might be on your mind. \"If angular momentum is defined with respect to a point, why do I regularly hear the phrase 'angular momentum about the axis of rotation'?\" Consider the pure rotation of a 3-dimensional rigid body with respect to an arbitrary axis that might not pass through the center of mass. If this is the case, then every infinitesimal piece of the rigid body is executing a circular motion whose center lies somewhere in this axis. Consider a point $O$ on the axis of rotation, with respect to which we are going to find the angular momentum. It's a figure I got from the internet, so focus on only the labels $O$ , $C$ , $P$ , and the circular orbit. Forget the rest of the inscriptions. $$d\\vec{L}_O = \\vec{OP} \\times dm \\vec{v}=(\\vec{OC} + \\vec{CP}) \\times m \\vec{v} = \\vec{OC} \\times dm \\vec{v} + \\vec{CP} \\times dm \\vec{v}$$ Note that both $\\vec{CP}$ and $\\vec{OC}$ are perpendicular to $\\vec{v}$ . The component of $d\\vec{L}_O$ that is parallel to the axis of rotation comes from the cross product of $\\vec{CP}$ with $\\vec{v}$ . The other one provides the component perpendicular to the axis of rotation. Also $v= CP \\omega$ since the rigid body is under pure rotation about the axis. Let's find the component net angular momentum of the rigid body with respect to point $O$ that is parallel to the axis of rotation. Let's call $\\vec{CP}=\\vec{r}'$ . $$L_z = \\int dm CP^2 \\omega = \\int dm r'^2 \\omega = I \\omega$$ This component is the same irrespective of the location of the reference point $O$ . This is useful because if we know the component of the net external torque on the rigid body with respect to any point on the axis of rotation parallel to the axis of rotation, we can equate the components to get $$ I \\omega = \\tau^{\\text{ext}}_{z}$$ The equation $\\vec{L} = I \\vec{\\omega}$ is not true always. And you need to be careful. Let me leave you with one more interesting result that you must be familiar with. The net torque due to the gravitational force on a rigid body with respect to point $P$ is given by $$\\vec{\\tau}=\\int dm (\\vec{r}-\\vec{r}_P) \\times \\vec{g} = (\\int dm (\\vec{r}-\\vec{r}_P))\\times \\vec{g} = M (\\vec{r}_{\\text{COM of the rigid body}} - \\vec{r}_P) \\times \\vec{g}$$ It all comes from the two fundamental definitions. EDIT (Response to comments are made here) : Angular momentum is fundamentally defined with respect to a point (not an axis). Angular momentum about an axis is a derived notion that's not universally useful to talk about. You can show that the angular momentum of a rigid body of mass $M$ with respect to point $P$ under pure translation obeys $$\\vec{L}_P = M (\\vec{R}_{COM}-\\vec{r}_P) \\times \\vec{v}_{COM}$$ Now consider the rigid body to be a disc that lies in the $x$ - $y$ plane and the center of the disc moves in a straight line along the path $y=a$ with uniform velocity $\\vec{v}=v \\hat{i}$ . We're considering pure translation here. Let's say the center of the disc (which is also the position of the COM of the disc) is at $(x=x_0+vt,y=b)$ at time $t$ . Let's calculate the angular momentum of this disc at $t$ with respect to the origin $O$ , $$\\vec{L}_O (t) = M ((x_0+vt) \\hat{i} + b \\hat{j}) \\times v \\hat{i} = -Mbv \\hat{k}$$ Now you see that the disc's angular momentum is independent of the position of the disc : it's independent of $t$ . Since the entire angular momentum is along the $\\hat{k}$ direction for this particular example, one can say that the angular momentum of the disc about the $z$ axis is $-Mbv$ . But does it really add anything new to your knowledge of the angular momentum of the disc that was not previously already in $\\vec{L}_O$ ? $$\\mathbf{\\text{EDIT II}}$$ $$\\mathbf{\\text{Definition :}}\\text{ A collection of $N$ particles is called a rigid collection if and only if}$$ $$|\\mathbf{r}_i-\\mathbf{r}_j|=c_{ij} \\text{ (some constant that doesn't vary with time) } \\forall i,j$$ It means that the distance between any two particles belonging to a rigid collection is fixed and unchanging. A rigid body is a solid body that is a natural generalization of this rigid collection of $N$ particles where $N \\to \\infty$ . In a rigid body, the distance between any two points is fixed and immutable. In general, different points on the rigid body move with different velocities ( $\\mathbf{v}=\\mathbf{v}(\\mathbf{r})$ ). Consider a point labelled by its position vector $\\mathbf{r}$ on the rigid body. Now, also consider an infinitesimal volume $dV$ centered at the point which houses a mass $dm=\\rho(\\mathbf{r})dV$ (where $\\rho(\\mathbf{r})$ gives the density of the rigid body at $\\mathbf{r}$ ). And this infinitesimal mass, moving with velocity $\\mathbf{v}(\\mathbf{r})$ , behaves just like a particle. The angular momentum of any collection (can be rigid or unrigid) of $N$ particles with respect to $P$ is given by $$ \\mathbf{L}_P = \\sum_{i=1}^N m_i((\\mathbf{r}_i-\\mathbf{r}_P) \\times \\mathbf{v}_i)$$ Can we extend this to a rigid body? Let's say we have a rigid body occupying volume $V$ . Instead of a particle with mass $m_i$ located at position $\\mathbf{r}_i$ having velocity $\\mathbf{v}_i$ , we have an infinitesimal mass $dm=\\rho(\\mathbf{r})dV$ located at $\\mathbf{r}$ moving with velocity $\\mathbf{v}=\\mathbf{v}(\\mathbf{r})$ . So, instead of summation, we will have to integrate over the volume $V$ $$\\mathbf{L}_P = \\int_{V} dm ((\\mathbf{r}-\\mathbf{r}_P) \\times \\mathbf{v})$$ Goldstein introduces rigid bodies in $Pg. 11$ of chapter $1$ . I'd strongly urge you to read the chapter from the beginning as it explains how certain concepts that you may already be familiar with (like center of mass of a collection of particles) emerges. $$\\mathbf{\\text{EDIT III}}$$ Let's say we have a collection of $N$ particles. What is the angular momentum of this collection of particles with respect to a general point $P$ that moves with velocity $\\mathbf{v}_P$ ? Well, there are two ways to define that. $$\\text{Absolute angular momentum : } \\mathbf{L}_P = \\sum_{i=1}^Nm_i(\\mathbf{r}_i-\\mathbf{r}_P) \\times \\mathbf{v}_i$$ $$\\text{Relative angular momentum : } \\mathbf{L}'_P=\\sum_{i=1}^N m_i (\\mathbf{r}-\\mathbf{r}_P)\\times (\\mathbf{v}_i-\\mathbf{v}_P)$$ $$\\text{In general, }\\mathbf{L}_P \\neq \\mathbf{L}'_P$$ However, what happens when our moving point $P$ is the center of mass ( $CM$ ) of the particle-collection? $$\\mathbf{L}'_{CM}=\\sum_i m_i (\\mathbf{r}_i-\\mathbf{r}_{CM})\\times(\\mathbf{v}_i-\\mathbf{v}_{CM})=\\sum_i m_i (\\mathbf{r}_i-\\mathbf{r}_{CM})\\times \\mathbf{v}_i - \\sum_i m_i (\\mathbf{r}_i-\\mathbf{r}_{CM}) \\times \\mathbf{v}_{CM}$$ $$\\Rightarrow \\mathbf{L}'_{CM}= \\mathbf{L}_{CM} - \\left((\\sum_i m_i \\mathbf{r}_i) - (\\sum_i m_i \\mathbf{r}_{CM})\\right)\\times \\mathbf{v}_{CM}=\\mathbf{L}_{CM} - (M\\mathbf{r}_{CM} - M\\mathbf{r}_{CM})\\times \\mathbf{v}_{CM}$$ $$ \\Rightarrow \\mathbf{L}'_{CM} = \\mathbf{L}_{CM}$$ Since we normally calculate angular momentum either with respect to a stationary point or with respect to the $CM$ , this distinction between absolute and relative angular momentum is never encountered. But now you must be onto many more questions such as, $$ \\text{Which is/are correct : } \\frac{d\\mathbf{L} _P}{dt}=\\mathbf{\\tau}_P \\;\\;\\text{ | }\\;\\; \\frac{d\\mathbf{L}'_P}{dt}=\\mathbf{\\tau}_P$$ $$\\text{where }\\mathbf{\\tau}_P = \\sum_{i=1}^N (\\mathbf{r}_i-\\mathbf{r}_P) \\times {\\mathbf{F}_i}^{\\text{ext}} \\text{ irrespective of whether $P$ is in motion or not}$$ It turns out that neither is correct in general. I'd highly recommend you consult this source ( link ) to know more about this.",
      "question_latex": [
        "\\vec{L}=\\vec{r}\\times\\vec{p}",
        "\\dfrac{d\\vec{L}}{dt}=\\vec{\\tau_{net}}",
        "\\vec{L}=I\\vec{\\omega}"
      ],
      "answer_latex": [
        "\\vec{L}_{\\text{of the point particle with respect to a reference point $P$}} := (\\vec{r}-\\vec{r}_P) \\times m\\vec{v} \\tag{1}",
        "\\vec{\\tau}_{\\text{acting on the point particle with respect to a reference point $P$}} := (\\vec{r}-\\vec{r}_P) \\times \\vec{F}_{\\text{net acting on the particle}} \\tag{2}",
        "\\frac{d\\vec{L}_P}{dt}=\\vec{\\tau}_P",
        "\\vec{L}_{\\text{of a rigid body with respect to $P$}} = \\int dm ((\\vec{r}-\\vec{r}_P) \\times \\vec{v})",
        "\\vec{\\tau}_{\\text{of the rigid body with respect to point $P$}}=\\sum_{\\text{$i$ : running over all external forces only}} (\\vec{r}_i-\\vec{r}_P) \\times {\\vec{F}_i}^\\text{external force}",
        "\\frac{d}{dt}(\\vec{L}_{\\text{of a rigid body with respect to $P$}}) = \\vec{\\tau}_{\\text{of the rigid body with respect to point $P$}}",
        "d\\vec{L}_O = \\vec{OP} \\times dm \\vec{v}=(\\vec{OC} + \\vec{CP}) \\times m \\vec{v} = \\vec{OC} \\times dm \\vec{v} + \\vec{CP} \\times dm \\vec{v}",
        "L_z = \\int dm  CP^2 \\omega = \\int dm r'^2 \\omega = I \\omega",
        "I \\omega = \\tau^{\\text{ext}}_{z}",
        "\\vec{\\tau}=\\int dm (\\vec{r}-\\vec{r}_P) \\times \\vec{g} = (\\int dm (\\vec{r}-\\vec{r}_P))\\times \\vec{g} =  M (\\vec{r}_{\\text{COM of the rigid body}} - \\vec{r}_P) \\times \\vec{g}",
        "\\vec{L}_P = M (\\vec{R}_{COM}-\\vec{r}_P) \\times \\vec{v}_{COM}",
        "\\vec{L}_O (t) = M ((x_0+vt) \\hat{i} + b \\hat{j}) \\times v \\hat{i} = -Mbv \\hat{k}",
        "\\mathbf{\\text{EDIT II}}",
        "\\mathbf{\\text{Definition :}}\\text{ A collection of $N$ particles is called a rigid collection if and only if}",
        "|\\mathbf{r}_i-\\mathbf{r}_j|=c_{ij} \\text{ (some constant that doesn't vary with time) } \\forall i,j",
        "\\mathbf{L}_P = \\sum_{i=1}^N m_i((\\mathbf{r}_i-\\mathbf{r}_P) \\times \\mathbf{v}_i)",
        "\\mathbf{L}_P = \\int_{V} dm ((\\mathbf{r}-\\mathbf{r}_P) \\times \\mathbf{v})",
        "\\mathbf{\\text{EDIT III}}",
        "\\text{Absolute angular momentum : } \\mathbf{L}_P = \\sum_{i=1}^Nm_i(\\mathbf{r}_i-\\mathbf{r}_P) \\times \\mathbf{v}_i",
        "\\text{Relative angular momentum : } \\mathbf{L}'_P=\\sum_{i=1}^N m_i (\\mathbf{r}-\\mathbf{r}_P)\\times (\\mathbf{v}_i-\\mathbf{v}_P)",
        "\\text{In general, }\\mathbf{L}_P \\neq \\mathbf{L}'_P",
        "\\mathbf{L}'_{CM}=\\sum_i m_i (\\mathbf{r}_i-\\mathbf{r}_{CM})\\times(\\mathbf{v}_i-\\mathbf{v}_{CM})=\\sum_i m_i (\\mathbf{r}_i-\\mathbf{r}_{CM})\\times \\mathbf{v}_i - \\sum_i m_i (\\mathbf{r}_i-\\mathbf{r}_{CM}) \\times \\mathbf{v}_{CM}",
        "\\Rightarrow \\mathbf{L}'_{CM}= \\mathbf{L}_{CM} - \\left((\\sum_i m_i \\mathbf{r}_i) - (\\sum_i m_i \\mathbf{r}_{CM})\\right)\\times \\mathbf{v}_{CM}=\\mathbf{L}_{CM} - (M\\mathbf{r}_{CM} - M\\mathbf{r}_{CM})\\times \\mathbf{v}_{CM}",
        "\\Rightarrow \\mathbf{L}'_{CM} = \\mathbf{L}_{CM}",
        "\\text{Which is/are correct : } \\frac{d\\mathbf{L}\n_P}{dt}=\\mathbf{\\tau}_P \\;\\;\\text{  |  }\\;\\; \\frac{d\\mathbf{L}'_P}{dt}=\\mathbf{\\tau}_P",
        "\\text{where }\\mathbf{\\tau}_P = \\sum_{i=1}^N (\\mathbf{r}_i-\\mathbf{r}_P) \\times {\\mathbf{F}_i}^{\\text{ext}} \\text{ irrespective of whether $P$ is in motion or not}",
        "S",
        "O",
        "m",
        "\\vec{r}",
        "\\vec{v}",
        "\\vec{L}_{\\text{of the point particle with respect to a reference point",
        "}} := (\\vec{r}-\\vec{r}_P) \\times m\\vec{v} \\tag{1}",
        "</span> \n<span class=\"math-container\">",
        "\\vec{\\tau}_{\\text{acting on the point particle with respect to a reference point",
        "}} := (\\vec{r}-\\vec{r}_P) \\times \\vec{F}_{\\text{net acting on the particle}} \\tag{2}",
        "</span></p>\n\n<p>For a point particle observed in an inertial frame, the following equation is true (the equation you've seen). But keep in mind that both angular momentum and torque are defined with respect to some reference point <span class=\"math-container\">",
        "</span> (unlike momentum and force). </p>\n\n<p><span class=\"math-container\">",
        "</span></p>\n\n<p>This can now be extended to a collection of interacting point particles. And further, to rigid bodies. <em>Classical Mechanics</em> by Goldstein is an excellent reference.</p>\n\n<p><span class=\"math-container\">",
        "\\vec{L}_{\\text{of a rigid body with respect to",
        "}} = \\int dm ((\\vec{r}-\\vec{r}_P) \\times \\vec{v})",
        "</span>\n<span class=\"math-container\">",
        "\\vec{\\tau}_{\\text{of the rigid body with respect to point",
        "}}=\\sum_{\\text{",
        ": running over all external forces only}} (\\vec{r}_i-\\vec{r}_P) \\times {\\vec{F}_i}^\\text{external force}",
        "\\frac{d}{dt}(\\vec{L}_{\\text{of a rigid body with respect to",
        "}}) = \\vec{\\tau}_{\\text{of the rigid body with respect to point",
        "}}",
        "</span> </p>\n\n<p>Anyway, I instead will focus on the confusion that might be on your mind. \"If angular momentum is defined with respect to a point, why do I regularly hear the phrase 'angular momentum about the axis of rotation'?\"</p>\n\n<p>Consider the pure rotation of a 3-dimensional rigid body with respect to an arbitrary axis that might not pass through the center of mass. If this is the case, then every infinitesimal piece of the rigid body is executing a circular motion whose center lies somewhere in this axis.</p>\n\n<p><a href=\"https://i.stack.imgur.com/iZpBt.png\" rel=\"nofollow noreferrer\"><img src=\"https://i.stack.imgur.com/iZpBt.png\" alt=\"Diagram\"></a></p>\n\n<p>Consider a point <span class=\"math-container\">",
        "</span> on the axis of rotation, with respect to which we are going to find the angular momentum. It's a figure I got from the internet, so focus on only the labels <span class=\"math-container\">",
        "</span>, <span class=\"math-container\">",
        "</span>, and the circular orbit. Forget the rest of the inscriptions.</p>\n\n<p><span class=\"math-container\">",
        "</span></p>\n\n<p>Note that both <span class=\"math-container\">",
        "</span> and <span class=\"math-container\">",
        "</span> are perpendicular to <span class=\"math-container\">",
        "</span>. The component of <span class=\"math-container\">",
        "</span> that is parallel to the axis of rotation comes from the cross product of <span class=\"math-container\">",
        "</span> with <span class=\"math-container\">",
        "</span>. The other one provides the component perpendicular to the axis of rotation. Also <span class=\"math-container\">",
        "</span> since the rigid body is under pure rotation about the axis.</p>\n\n<p>Let's find the component net angular momentum of the rigid body with respect to point <span class=\"math-container\">",
        "</span> that is parallel to the axis of rotation. Let's call <span class=\"math-container\">",
        "</span>.\n<span class=\"math-container\">",
        "</span></p>\n\n<p>This component is the same irrespective of the location of the reference point <span class=\"math-container\">",
        "</span>.</p>\n\n<p>This is useful because if we know the component of the net external torque on the rigid body with respect to any point on the axis of rotation parallel to the axis of rotation, we can equate the components to get</p>\n\n<p><span class=\"math-container\">",
        "</span> </p>\n\n<p>The equation <span class=\"math-container\">",
        "</span> is not true always. And you need to be careful.</p>\n\n<p>Let me leave you with one more interesting result that you must be familiar with. The net torque due to the gravitational force on a rigid body with respect to point <span class=\"math-container\">",
        "</span> is given by\n<span class=\"math-container\">",
        "</span></p>\n\n<p>It all comes from the two fundamental definitions.</p>\n\n<p><strong>EDIT (Response to comments are made here) :</strong></p>\n\n<p>Angular momentum is fundamentally defined with respect to a point (not an axis). Angular momentum about an axis is a derived notion that's not universally useful to talk about. You can show that the angular momentum of a rigid body of mass <span class=\"math-container\">",
        "</span> with respect to point <span class=\"math-container\">",
        "</span> under pure translation obeys</p>\n\n<p><span class=\"math-container\">",
        "</span></p>\n\n<p>Now consider the rigid body to be a disc that lies in the <span class=\"math-container\">",
        "</span>-<span class=\"math-container\">",
        "</span> plane and the center of the disc moves in a straight line along the path <span class=\"math-container\">",
        "</span> with uniform velocity <span class=\"math-container\">",
        "</span>. We're considering pure translation here. Let's say the center of the disc (which is also the position of the COM of the disc) is at <span class=\"math-container\">",
        "</span> at time <span class=\"math-container\">",
        "</span>. Let's calculate the angular momentum of this disc at <span class=\"math-container\">",
        "</span> with respect to the origin <span class=\"math-container\">",
        "</span>,</p>\n\n<p><span class=\"math-container\">",
        "</span></p>\n\n<p>Now you see that the disc's angular momentum is independent of the position of the disc : it's independent of <span class=\"math-container\">",
        "</span>. Since the entire angular momentum is along the <span class=\"math-container\">",
        "</span> direction for this particular example, one can say that the angular momentum of the disc about the <span class=\"math-container\">",
        "</span> axis is <span class=\"math-container\">",
        "</span>. But does it really add anything new to your knowledge of the angular momentum of the disc that was not previously already in <span class=\"math-container\">",
        "</span>? </p>\n\n<hr>\n\n<p><span class=\"math-container\">",
        "\\mathbf{\\text{Definition :}}\\text{ A collection of",
        "particles is called a rigid collection if and only if}",
        "</span> </p>\n\n<p>It means that the distance between any two particles belonging to a rigid collection is fixed and unchanging. A rigid body is a solid body that is a natural generalization of this rigid collection of <span class=\"math-container\">",
        "</span> particles where <span class=\"math-container\">",
        "</span>. In a rigid body, the distance between any two points is fixed and immutable. In general, different points on the rigid body move with different velocities (<span class=\"math-container\">",
        "</span>). Consider a point labelled by its position vector <span class=\"math-container\">",
        "</span> on the rigid body. Now, also consider an infinitesimal volume <span class=\"math-container\">",
        "</span> centered at the point which houses a mass <span class=\"math-container\">",
        "</span> (where <span class=\"math-container\">",
        "</span> gives the density of the rigid body at <span class=\"math-container\">",
        "</span>). And this infinitesimal mass, moving with velocity <span class=\"math-container\">",
        "</span>, behaves just like a particle.</p>\n\n<p>The angular momentum of any collection (can be rigid or unrigid) of <span class=\"math-container\">",
        "</span> particles with respect to <span class=\"math-container\">",
        "</span> is given by</p>\n\n<p><span class=\"math-container\">",
        "</span></p>\n\n<p>Can we extend this to a rigid body? Let's say we have a rigid body occupying volume <span class=\"math-container\">",
        "</span>. Instead of a particle with mass <span class=\"math-container\">",
        "</span> located at position <span class=\"math-container\">",
        "</span> having velocity <span class=\"math-container\">",
        "</span>, we have an infinitesimal mass <span class=\"math-container\">",
        "</span> located at <span class=\"math-container\">",
        "</span> moving with velocity <span class=\"math-container\">",
        "</span>. So, instead of summation, we will have to integrate over the volume <span class=\"math-container\">",
        "</span></p>\n\n<p><span class=\"math-container\">",
        "</span></p>\n\n<p>Goldstein introduces rigid bodies in <span class=\"math-container\">",
        "</span> of chapter <span class=\"math-container\">",
        "</span>. I'd strongly urge you to read the chapter from the beginning as it explains how certain concepts that you may already be familiar with (like center of mass of a collection of particles) emerges.</p>\n\n<hr>\n\n<p><span class=\"math-container\">",
        "</span>\nLet's say we have a collection of <span class=\"math-container\">",
        "</span> particles. What is the angular momentum of this collection of particles with respect to a general point <span class=\"math-container\">",
        "</span> that moves with velocity <span class=\"math-container\">",
        "</span>? Well, there are two ways to define that. </p>\n\n<p><span class=\"math-container\">",
        "</span></p>\n\n<p>However, what happens when our moving point <span class=\"math-container\">",
        "</span> is the center of mass (<span class=\"math-container\">",
        "</span>) of the particle-collection?\n<span class=\"math-container\">",
        "</span> <span class=\"math-container\">",
        "</span></p>\n\n<p>Since we normally calculate angular momentum either with respect to a stationary point or with respect to the <span class=\"math-container\">",
        "</span>, this distinction between absolute and relative angular momentum is never encountered. But now you must be onto many more questions such as,\n<span class=\"math-container\">",
        "\\text{where }\\mathbf{\\tau}_P = \\sum_{i=1}^N (\\mathbf{r}_i-\\mathbf{r}_P) \\times {\\mathbf{F}_i}^{\\text{ext}} \\text{ irrespective of whether",
        "is in motion or not}"
      ],
      "created": "2019-10-30T23:10:56.850",
      "golden_ner_terms": [
        "acceleration",
        "angular momentum",
        "axis",
        "body",
        "calculate",
        "center",
        "center of mass",
        "circular",
        "classical mechanics",
        "collection",
        "component",
        "components",
        "connection",
        "constant",
        "context",
        "cross",
        "cross product",
        "density",
        "disc",
        "distance",
        "entire",
        "equate",
        "equation",
        "equivalent",
        "even",
        "fixed",
        "focus",
        "forces",
        "frame",
        "generalization",
        "independent",
        "infinitesimal",
        "integrate",
        "internet",
        "intuition",
        "line",
        "link",
        "mass",
        "mean",
        "momentum",
        "net",
        "onto",
        "orbit",
        "origin",
        "parallel",
        "pass through",
        "path",
        "perpendicular",
        "plane",
        "point",
        "point particles",
        "position vector",
        "product",
        "rigid",
        "rotate",
        "rotation",
        "running",
        "solid",
        "source",
        "stationary",
        "stationary point",
        "straight",
        "summation",
        "time",
        "torque",
        "translation",
        "useful",
        "vector",
        "velocity",
        "volume"
      ],
      "golden_ner_count": 70,
      "golden_patterns": [
        {
          "pattern": "work-examples-first",
          "score": 2.0,
          "hotwords": [
            "example"
          ]
        },
        {
          "pattern": "encode-as-algebra",
          "score": 2.0,
          "hotwords": [
            "ring"
          ]
        },
        {
          "pattern": "unfold-the-definition",
          "score": 2.0,
          "hotwords": [
            "means that"
          ]
        },
        {
          "pattern": "construct-auxiliary-object",
          "score": 2.0,
          "hotwords": [
            "introduce"
          ]
        }
      ],
      "golden_pattern_names": [
        "work-examples-first",
        "encode-as-algebra",
        "unfold-the-definition",
        "construct-auxiliary-object"
      ],
      "golden_scopes": [
        {
          "type": "consider",
          "match": "Consider the pure rotation of a 3-dimensional rigid body with respect to "
        },
        {
          "type": "consider",
          "match": "Consider a point "
        },
        {
          "type": "consider",
          "match": "Consider a point labelled by its position vector "
        }
      ],
      "golden_scope_count": 3
    },
    {
      "id": "se-physics-790649",
      "stratum": "hard",
      "title": "If we have a beam of photons and a beam of protons with the same energy, which one will get further inside the human body?",
      "tags": [
        "photons",
        "atomic-physics",
        "radiation",
        "biophysics",
        "protons"
      ],
      "score": 3,
      "answer_score": 1,
      "question_body": "If we have a beam of photons (high energy X-ray) and a beam of protons both with the same energy, which one will get further inside the human body and why? Can this be explaind due to LET (Linear energy transfer)?",
      "answer_body": "I suppose that the photon beam you say is the high-energy $\\gamma$ -ray produced during nuclear decay, it is ture to say that the photon beam usually travels further. However, the penetrates depth of different beam depends on their energy and medium they interact with. However, not strictly speaking, protons, due to their charge, can easily interact with atoms in the medium and lose energy, thus having a very short penetration depth. $\\gamma$ -ray interacts much less with the medium and travels further. If we further compare with electrons, we can find that electron travels further due to its small mass. The neutron travels further due to its lack of charge, thus reducing the interaction. UPDATE As I mentioned earlier, this process depends on the energy of the beam and the medium in which it interacts with. So not comparing them under a specific energy scale is actually very uncritical. The high-energy particles I am considering here commonly come from nuclear decay, nuclear reactor, and compact accelerators (such as medical equipment), beam can possesses energies of ~MeV . I will give a few examples: ${}^{60}\\mathrm{Co}$ can undergo a $\\beta$ decay to the excited state of ${}^{60}\\mathrm{Ni}$ , releasing a electron with a energy of 0.309 MeV . The excited ${}^{60}\\mathrm{Ni}$ then emits two photons, each with energies of 1.17 MeV and 1.33 MeV , before transitioning back to its ground state. Reactions occurring in nuclear fusion： \\begin{eqnarray} {}^{2}\\mathrm{H}+{}^{2}\\mathrm{H}\\rightarrow {}^{3}\\mathrm{He}+n+\\text{3.25 MeV} \\\\\\\\ {}^{2}\\mathrm{H}+{}^{2}\\mathrm{H}\\rightarrow {}^{3}\\mathrm{H}+p+\\text{4.0 MeV} \\\\\\\\ {}^{2}\\mathrm{H}+{}^{3}\\mathrm{H}\\rightarrow {}^{4}\\mathrm{He}+n+\\text{17.6 MeV} \\\\\\\\ {}^{2}\\mathrm{H}+{}^{3}\\mathrm{He}\\rightarrow {}^{4}\\mathrm{He}+p+\\text{18.3 MeV} \\end{eqnarray} In proton radiation therapy, the energy of protons is set from tens of MeV to hundreds of MeV ( Charged particles in radiation oncology ). At such an energy scale, these high-energy particles can be classified into three categories as follow, each with different mechanisms of interaction with matter： 1. Charged Particles Protons, electrons, muons and $\\alpha$ -particles ( ${}^{4}\\text{He}$ ) all belong to this category. Charged particles with high kinetic energy can interact with matter by engaging their electric fields with orbital electrons, thereby transferring energy to the electrons. This may cause ionization or excitation of the material. This process is known as Coulomb collision , and the resulting energy loss is referred to as ionization loss. An intuitive conclusion is that the longer the interaction time with the electric field of electrons, the greater the impulse obtained by charged particles, resulting in more energy loss. Hence, slower-moving particles experience substantial ionization losses. The ionization energy loss rate (also known as ionization stopping power) can be roughly written as: $$\\frac 1N \\left(\\frac{\\mathrm dE}{\\mathrm dx}\\right)_{\\text{col}}\\propto \\frac{nz^2Z}{(v/c)^2},$$ where $E$ is the energy of the particle, $Z$ is the atomic number of the material, $N$ is the number of atoms per unit volume in the medium， $z$ is the charge of the particle in untis of electron charge, $V$ and $c$ are the velocities of charged particles and the speed of light, respectively. We can conclude that: (1) The more charges particles carry, the greater the ionization loss. (2) Ionization loss is not directly related to mass, instead it is inversely proportional to the square of the velocity of motion. But if two particles have the same energy, the heavier particle deposits more dose due to its slower speed. Due to this energy correlation, the energy deposition curve of charged heavy ions usually exhibits a so-called Bragg peak. Maximum deposition dose when particles are close to stopping. The follow figure shows the distribution of the absorbed dose along the range of protons with the energy 121 MeV in water ( Phys. Med. Biol. 56 (2011) 7725–7735 ). (3) Ionization loss strongly depends on the materials interacting with it. Another mechanism for the deposition of energy by charged particles is referred to bremsstrahlung . When charged particles collide with electrons and atomic nuclei in the medium, acceleration is generated, which transfers a portion of energy to electromagnetic radiation and leads to radiation loss. The stopping power related to the bremsstrahlung is called radiation stopping power, which is proportional to $\\frac{(Zz)^2}{m^2}$ . Due to the greater the mass, the smaller the impact, the radiation loss is usually only important for electrons. To further illustrate, I cite some data and plots from newest Review of Particle Physics . This figure shows mass stopping power for positive muons in copper as a function of their energy. It can be seen that only a very high energy portion of radiation loss is important. This figure shows fractional energy loss per radiation length in lead as a function of electron or positron energy. It can be seen that ionization loss is reduced in the high-energy region, and radiation loss becomes dominant for electrons. However, in the lower energy part, ionization loss is the main factor, and in materials, the ionization loss of protons is higher than that of electrons. However, such performance heavily relies on materials. The critical energy $E_c$ is sometimes defined as the energy at which the ionization loss and the radiation loss are equal. The critical energy for lead, aluminum, water, and air are 7.43 MeV, 42.70 MeV, 78.33 MeV and 87.92 MeV respectively. If the energy is far below this critical value, the radiation loss becomes negligible, and it can be inferred that electrons travel further than protons due to lower ionization loss. You can find further data at Atomic and Nuclear Properties of Materials . 2. Neutron Above, we compared the penetration depth of protons and neutrons, and the transport process of neutrons in matter also strictly depends on their energy. Thermal neutron referred to as a free neutron that has kinetic energy of about 0.025 eV, which is approximately the kinetic energy possessed by neutron thermal motion at room temperature. On the other hand, a fast neutron has an energy range of $5\\times 10^5\\mathrm {eV} \\sim 10^7\\mathrm{eV}$ . Neutrons, having no electric charge, engage matter primarily via nuclear forces (mainly strong interactions). Due to the short-range nature of strong interaction forces and the fact that the nucleus occupies the entire volume of an atom, the interaction between neutrons and matter is weaker than that of protons. Here are several different reactions when neutrons pass through a medium: Elastic Scattering When a neutron with energy $E_1$ collides positively with a stationary particle with a mass $M$ times that of a neutron, the energy $E_2$ after the collision satisfies: $$\\frac{E_2}{E_1}=\\frac{M-1}{M+1}.$$ Therefore, hydrogen-containing substances and lighter elements have a better slowing effect on fast neutrons. Water, polyethylene, paraffin, graphite, and lithium hydride are common fast neutron moderators, with water and graphite often used in nuclear reactors. Inelastic Scattering Neutrons with relatively high energy interact with the target nucleus, causing it to excite and transfer energy to matter. Inelastic scattering can only occur when the energy of neutrons exceeds the first excitation energy of the target nucleus. The first excitation energy of heavy nuclei is usually 100 keV, while the excitation energy of light nuclei is usually several MeV. Therefore, higher energy neutrons are prone to slow down due to inelastic collisions with heavier atomic nuclei with higher atomic numbers. Radiative capture $\\propto \\frac {1}{\\sqrt E}$ , it is the main reaction that occurs when fast neutrons are absorbed by matter. 3. Photon The more low-energy electromagnetic waves should be analysed for their specific frequency bands. For instance, as an illustration, another response I provided might helpful [Why does Near Infrared (NIR) light allow deeper imaging in biological tissue in Multiphoton Microscopy?][https://physics.stackexchange.com/a/790651/350999]. For $\\gamma$ -ray and X-ray, the primary processes are Compton scattering, photoelecric effect and electron pair effect. There will also be some non-dominant contributions from coherent scattering and photonuclear reactions. Here, I cite scattering cross-sections of various processes involving the interaction between photons of different energies and matter (from Particle Physics Review), note that the vertical axis in the figure is a logarithmic coordinate system. Finally we come to the comparison of which goes further in matter (described in the new title as in the body). You can see that the answer actually depends on the energy of the beam and the substances with which it interacts. But under these narrower conditions: beams with ~MeV of energy travelling through the body (water), my answer is photons. The proton will quickly decelerate to an energy low enough that violent ionisation loss occurs, depositing almost all of the energy in the body and getting a Bragg peak. Photon beams and proton beams are like two types of bullets, one with greater penetration and one with greater stopping power, and I'll show you how to take advantage of that property. Radiotherapy is one of the most common and effective therapies for cancer, which uses high-energy beams to focus on tumors and directly kill cancer cells Radiotherapy should have low toxicity in the entrance channel (normal tissue) and be very effective in cell killing in the target region (tumour). See Physics and biomedical challenges of cancer therapy with accelerated heavy ions and charged particles in radiation oncology . People found that high‑energy charged particles could be used for cancer treatment, owing to their advantages compared with traditional X‑rays treatment (photon beam). By contrast with X-rays or $\\gamma$ -rays, which are absorbed by the body and show an exponential decrease in the radiation dose with increasing tissue depth, charged particles deposit little energy at the body’s surface, when their velocity is high, and instead deposit most of their energy just before they come to rest in tissue (corresponding to the Bragg peak, see blow figure) Figure: Depth-dose distributions showing the Bragg peak for different ions and the reduced straggling of heavier ions. The X-ray depth–dose curve is calculated for a high-energy 21-MeV linac. Energies of the ions correspond approximately to the same range (15 cm in tissue): 148 MeV ${}^1$ H beam, 170 MeV/n ${}^3$ He beam and 270 MeV/n ${}^{12}\\mathrm C$ beam ( Physics and biomedical challenges of cancer therapy with accelerated heavy ions ). I would like to reiterate that the above analysis is applicable to this energy scale. At LHC, the energy of particles can reach TeV, while cosmic rays even contain particles with EeV energy. I don't know about physics at higher energy scales and cannot draw conclusions. Essentially, $\\gamma$ -rays propagate farther than charged particles due to their smaller energy loss during their passage, but this depends on the different physical processes behind them. For protons or other charged particles, the stopping power is an energy dependent value that is almost inversely proportional to the square of velocity. Therefore, at the end of the range, a very large dose deposition occurs due to the decrease in energy. For $\\gamma$ -rays, the photoelectric effect and electron pair effect do not change the energy of photons, but only reduce the number of photons. Therefore, the depth-dose distributions curve of $\\gamma$ -rays is much smoother. In particular, $\\gamma$ -rays have no concept of range, which means that there is almost no transmission after this distance, so that photons can almost always reach even very deep places in small quantities.",
      "question_latex": [],
      "answer_latex": [
        "\\frac 1N \\left(\\frac{\\mathrm dE}{\\mathrm dx}\\right)_{\\text{col}}\\propto \\frac{nz^2Z}{(v/c)^2},",
        "\\frac{E_2}{E_1}=\\frac{M-1}{M+1}.",
        "\\gamma",
        "{}^{60}\\mathrm{Co}",
        "\\beta",
        "{}^{60}\\mathrm{Ni}",
        "\\alpha",
        "{}^{4}\\text{He}",
        "</span>\nwhere <span class=\"math-container\">",
        "</span> is the energy of the particle, <span class=\"math-container\">",
        "</span> is the atomic number of the material, <span class=\"math-container\">",
        "</span> is the number of atoms per unit volume in the medium， <span class=\"math-container\">",
        "</span> is the charge of the particle in untis of electron charge, <span class=\"math-container\">",
        "</span> and <span class=\"math-container\">",
        "</span> are the velocities of charged particles and the speed of light, respectively.</p>\n<p>We can conclude that:<br>\n(1) The more charges particles carry, the greater the ionization loss.<br>\n(2) Ionization loss is not directly related to mass, instead it is inversely proportional to the square of the velocity of motion. But if two particles have the same energy, the heavier particle deposits more dose due to its slower speed. Due to this energy correlation, the energy deposition curve of charged heavy ions usually exhibits a so-called Bragg peak. Maximum deposition dose when particles are close to stopping. The follow figure shows the distribution of the absorbed dose along the range of protons with the energy 121 MeV in water (<a href=\"https://doi.org/10.1088/0031-9155/56/24/003\" rel=\"nofollow noreferrer\">Phys. Med. Biol. 56 (2011) 7725–7735</a>). <br>\n(3) Ionization loss strongly depends on the materials interacting with it.</p>\n<p><a href=\"https://i.stack.imgur.com/21572.png\" rel=\"nofollow noreferrer\"><img src=\"https://i.stack.imgur.com/21572.png\" alt=\" Distribution of the absorbed dose along the range of protons with the energy 121 MeV in water (Bragg's curve) Phys. Med. Biol. 56 (2011) 7725–7735\" /></a></p>\n<p>Another mechanism for the deposition of energy by charged particles is referred to <strong>bremsstrahlung</strong>. When charged particles collide with electrons and atomic nuclei in the medium, acceleration is generated, which transfers a portion of energy to electromagnetic radiation and leads to radiation loss. The stopping power related to the bremsstrahlung is called radiation stopping power, which is proportional to <span class=\"math-container\">",
        "</span>. Due to the greater the mass, the smaller the impact, the radiation loss is usually only important for electrons.</p>\n<p>To further illustrate, I cite some data and plots from newest <a href=\"https://doi.org/10.1093/ptep/ptac097\" rel=\"nofollow noreferrer\">Review of Particle Physics</a>. This figure shows mass stopping power for positive muons in copper as a function of their energy. It can be seen that only a very high energy portion of radiation loss is important.</p>\n<p><a href=\"https://i.stack.imgur.com/eL4tP.png\" rel=\"nofollow noreferrer\"><img src=\"https://i.stack.imgur.com/eL4tP.png\" alt=\"enter image description here\" /></a></p>\n<p>This figure shows fractional energy loss per radiation length in lead\nas a function of electron or positron energy. It can be seen that ionization loss is reduced in the high-energy region, and radiation loss becomes dominant for electrons. However, in the lower energy part, ionization loss is the main factor, and in materials, the ionization loss of protons is higher than that of electrons. However, such performance heavily relies on materials. The <em>critical energy</em>\n<span class=\"math-container\">",
        "</span> is sometimes defined as the energy at which the ionization loss and the radiation loss are equal. The critical energy for lead, aluminum, water, and air are 7.43 MeV, 42.70 MeV, 78.33 MeV and  87.92 MeV respectively. If the energy is far below this critical value, the radiation loss becomes negligible, and it can be inferred that electrons travel further than protons due to lower ionization loss. You can find further data at <a href=\"https://pdg.lbl.gov/2023/AtomicNuclearProperties/\" rel=\"nofollow noreferrer\">Atomic and Nuclear Properties of Materials</a>.</p>\n<img src=\"https://i.stack.imgur.com/3IOzn.png\" width=\"500\" >\n<h4>2. Neutron</h4>\nAbove, we compared the penetration depth of protons and neutrons, and the transport process of neutrons in matter also strictly depends on their energy. Thermal neutron referred to as a free neutron that has kinetic energy of about 0.025 eV, which is approximately the kinetic energy possessed by neutron thermal motion at room temperature. On the other hand, a fast neutron has an energy range of <span class=\"math-container\">",
        "</span>. Neutrons, having no electric charge, engage matter primarily via nuclear forces (mainly strong interactions). Due to the short-range nature of strong interaction forces and the fact that the nucleus occupies the entire volume of an atom, the interaction between neutrons and matter is weaker than that of protons. Here are several different reactions when neutrons pass through a medium:\n<h6>Elastic Scattering</h6>\n<p>When a neutron with energy <span class=\"math-container\">",
        "</span> collides positively with a stationary particle with a mass <span class=\"math-container\">",
        "</span> times that of a neutron, the energy <span class=\"math-container\">",
        "</span> after the collision satisfies:</p>\n<p><span class=\"math-container\">",
        "</span></p>\n<p>Therefore, hydrogen-containing substances and lighter elements have a better slowing effect on fast neutrons. Water, polyethylene, paraffin, graphite, and lithium hydride are common fast neutron moderators, with water and graphite often used in nuclear reactors.</p>\n<h6>Inelastic Scattering</h6>\n<p>Neutrons with relatively high energy interact with the target nucleus, causing it to excite and transfer energy to matter. Inelastic scattering can only occur when the energy of neutrons exceeds the first excitation energy of the target nucleus. The first excitation energy of heavy nuclei is usually 100 keV, while the excitation energy of light nuclei is usually several MeV. Therefore, higher energy neutrons are prone to slow down due to inelastic collisions with heavier atomic nuclei with higher atomic numbers.</p>\n<h6>Radiative capture</h6>\n<p><span class=\"math-container\">",
        "</span>, it is the main reaction that occurs when fast neutrons are absorbed by matter.</p>\n<h4>3. Photon</h4>\nThe more low-energy electromagnetic waves should be analysed for their specific frequency bands. For instance, as an illustration, another response I provided might helpful [Why does Near Infrared (NIR) light allow deeper imaging in biological tissue in Multiphoton Microscopy?][https://physics.stackexchange.com/a/790651/350999]. \n<p>For <span class=\"math-container\">",
        "</span>-ray and X-ray, the primary processes are Compton scattering, photoelecric effect and electron pair effect. There will also be some non-dominant contributions from coherent scattering and photonuclear reactions. Here, I cite scattering cross-sections of various processes involving the interaction between photons of different energies and matter (from Particle Physics Review), note that the vertical axis in the figure is a logarithmic coordinate system.</p>\n<p><a href=\"https://i.stack.imgur.com/U5YMc.png\" rel=\"nofollow noreferrer\"><img src=\"https://i.stack.imgur.com/U5YMc.png\" alt=\"enter image description here\" /></a></p>\n<p><strong>Finally</strong> we come to the comparison of which goes further in matter (described in the new title as in the body). You can see that the answer actually depends on the energy of the beam and the substances with which it interacts. But under these narrower conditions: beams with ~MeV of energy travelling through the body (water), my answer is photons. The proton will quickly decelerate to an energy low enough that violent ionisation loss occurs, depositing almost all of the energy in the body and getting a Bragg peak. Photon beams and proton beams are like two types of bullets, one with greater penetration and one with greater stopping power, and I'll show you how to take advantage of that property.</p>\n<p>Radiotherapy is one of the most common and effective therapies for cancer, which uses high-energy beams to focus on tumors and directly kill cancer cells Radiotherapy should have low toxicity in the entrance channel (normal tissue) and be very effective in cell killing in the target region (tumour). See <a href=\"https://doi.org/10.1038/s42254-021-00368-5\" rel=\"nofollow noreferrer\">Physics and biomedical challenges of cancer therapy with accelerated heavy ions</a> and <a href=\"https://doi.org/10.1038/nrclinonc.2009.183\" rel=\"nofollow noreferrer\">charged particles in radiation oncology</a>.</p>\n<p>People found that high‑energy charged particles could be used for cancer treatment, owing to their advantages compared with traditional X‑rays treatment (photon beam). By contrast with X-rays or <span class=\"math-container\">",
        "</span>-rays, which are absorbed by the body and show an exponential decrease\nin the radiation dose with increasing tissue depth, charged particles deposit little energy at the body’s surface, when their velocity is high, and instead deposit most of their energy just before they come to rest in tissue (corresponding to the Bragg peak, see blow figure)</p>\n<p><a href=\"https://i.stack.imgur.com/slZiv.png\" rel=\"nofollow noreferrer\"><img src=\"https://i.stack.imgur.com/slZiv.png\" alt=\"enter image description here\" /></a></p>\n<p>Figure: Depth-dose distributions showing the Bragg peak for different ions and the reduced straggling of heavier ions. The X-ray depth–dose curve is calculated for a high-energy 21-MeV linac. Energies of the ions correspond approximately to the same range (15 cm in tissue): 148 MeV<span class=\"math-container\">",
        "</span>H beam, 170 MeV/n<span class=\"math-container\">",
        "</span>He beam and 270 MeV/n<span class=\"math-container\">",
        "</span> beam (<a href=\"https://doi.org/10.1038/s42254-021-00368-5\" rel=\"nofollow noreferrer\">Physics and biomedical challenges of cancer therapy with accelerated heavy ions</a>).</p>\n<p>I would like to reiterate that the above analysis is applicable to this energy scale. At LHC, the energy of particles can reach TeV, while cosmic rays even contain particles with EeV energy. I don't know about physics at higher energy scales and cannot draw conclusions.</p>\n<hr />\n<p>Essentially, <span class=\"math-container\">",
        "</span>-rays propagate farther than charged particles due to their smaller energy loss during their passage, but this depends on the different physical processes behind them. For protons or other charged particles, the stopping power is an energy dependent value that is almost inversely proportional to the square of velocity. Therefore, at the end of the range, a very large dose deposition occurs due to the decrease in energy. For <span class=\"math-container\">",
        "</span>-rays, the photoelectric effect and electron pair effect do not change the energy of photons, but only reduce the number of photons. Therefore, the depth-dose distributions curve of <span class=\"math-container\">",
        "</span>-rays is much smoother. In particular, <span class=\"math-container\">"
      ],
      "created": "2023-11-30T12:41:26.550",
      "golden_ner_terms": [
        "acceleration",
        "air",
        "almost all",
        "analysis",
        "atom",
        "atoms",
        "axis",
        "body",
        "category",
        "cell",
        "charge",
        "collision",
        "compact",
        "concept",
        "conclusion",
        "coordinate",
        "correlation",
        "cosmic rays",
        "critical value",
        "curve",
        "data",
        "depth",
        "distance",
        "distribution",
        "dominant",
        "effective",
        "electric fields",
        "electromagnetic radiation",
        "electrons",
        "elements",
        "energy",
        "entire",
        "even",
        "exponential",
        "factor",
        "field",
        "focus",
        "forces",
        "frequency",
        "function",
        "ground state",
        "heavy ion",
        "hundreds",
        "imaging",
        "increasing",
        "instance",
        "interactions",
        "inversely proportional",
        "ionization energy",
        "ions",
        "length",
        "mass",
        "matter",
        "nature",
        "near",
        "neutrons",
        "normal",
        "nuclear",
        "nucleus",
        "number",
        "numbers",
        "particle physics",
        "pass through",
        "photoelectric effect",
        "photons",
        "physics",
        "positive",
        "power",
        "primary",
        "property",
        "proportional",
        "protons",
        "radiation",
        "range",
        "reduced",
        "region",
        "scales",
        "scattering",
        "speed",
        "speed of light",
        "square",
        "state",
        "stationary",
        "strictly",
        "strong",
        "surface",
        "temperature",
        "time",
        "unit",
        "velocity",
        "volume",
        "water",
        "waves",
        "weaker"
      ],
      "golden_ner_count": 94,
      "golden_patterns": [
        {
          "pattern": "work-examples-first",
          "score": 6.0,
          "hotwords": [
            "example",
            "for instance",
            "illustrate"
          ]
        },
        {
          "pattern": "construct-an-explicit-witness",
          "score": 2.0,
          "hotwords": [
            "exhibit"
          ]
        },
        {
          "pattern": "encode-as-algebra",
          "score": 2.0,
          "hotwords": [
            "ring"
          ]
        },
        {
          "pattern": "unfold-the-definition",
          "score": 2.0,
          "hotwords": [
            "means that"
          ]
        },
        {
          "pattern": "transport-across-isomorphism",
          "score": 2.0,
          "hotwords": [
            "transport"
          ]
        },
        {
          "pattern": "monotone-approximation",
          "score": 2.0,
          "hotwords": [
            "approximate"
          ]
        }
      ],
      "golden_pattern_names": [
        "work-examples-first",
        "construct-an-explicit-witness",
        "encode-as-algebra",
        "unfold-the-definition",
        "transport-across-isomorphism",
        "monotone-approximation"
      ],
      "golden_scopes": [
        {
          "type": "where-binding",
          "match": "where $E$ is"
        }
      ],
      "golden_scope_count": 1
    },
    {
      "id": "se-physics-651969",
      "stratum": "hard",
      "title": "Charges and relative motion",
      "tags": [
        "electromagnetism",
        "electric-fields",
        "charge",
        "coulombs-law",
        "relative-motion"
      ],
      "score": 4,
      "answer_score": 6,
      "question_body": "We know that coulomb charge can't be applied on charges in motion. Why? What if they are moving in same velocity? Aren't they actually at rest for each other? ( yeah, it depends upon medium, so I'm assuming vaccum for all the cases). So what's the new coming in motion? Also, if that's false, doesn't that disprove that motion is relative because we can just find out in only 2 objects who's moving by calculating if coulomb's formula gave us right answer about the force.",
      "answer_body": "$\\texttt{C O N T E N T S}$ $\\texttt{Abstract}$ $\\boldsymbol\\S\\texttt{ A. Two charges }q_1,q_2\\texttt{ at rest (system S$'$)}$ $\\boldsymbol\\S\\texttt{ B. Two charges }q_1,q_2\\texttt{ in uniform translational motion (system S)}$ $\\boldsymbol\\S\\texttt{ C. The Lorentz boost transformation ($\\texttt S$$'\\longrightarrow$ S) }$ Abstract A first case concerns two electric charges $\\:q_1\\:$ and $\\:q_2\\:$ at rest with respect to an inertial system $\\:\\texttt S'$ . The electrostatic fields and forces using Coulomb's law are given in $\\boldsymbol\\S\\,\\texttt{A}$ . A second case concerns two electric charges $\\:q_1\\:$ and $\\:q_2\\:$ in motion both with common uniform velocity $\\,\\boldsymbol{\\upsilon}\\,$ with respect to an inertial system $\\:\\texttt S$ . The electromagnetic fields and forces are given in $\\boldsymbol\\S\\,\\texttt{B}\\,$ using well-known expressions derived from the relativistic Liénard–Wiechert potentials. The two cases are studied independently of one another with no relation between them until $\\boldsymbol\\S\\,\\texttt{C}$ where it is assumed that the inertial system $\\,\\texttt S\\,$ of the latter case arises from a Lorentz boost transformation with velocity $\\,\\boldsymbol{-\\upsilon}\\,$ with respect to the inertial system $\\,\\texttt S'\\,$ of the former case. Now the electromagnetic fields and forces in system $\\,\\texttt S\\,$ are derived from the Lorentz boost transformation of the electrostatic fields and forces in the rest system $\\,\\texttt S'$ . The results from this application of the Lorentz transformation are in full agreement with those in $\\boldsymbol\\S\\,\\texttt{B}\\,$ derived from the Liénard–Wiechert potentials. Moreover they provide relations connecting the two systems. $\\boldsymbol\\S$ A. Two charges $\\,q_1,q_2\\,$ at rest Consider two electric charges $\\:q_1\\:$ and $\\:q_2\\:$ at rest with respect to an inertial system $\\:\\texttt S'$ , see Figure-01. One charge feels only the electrostatic Coulomb force of the other. Fields and forces are as follows \\begin{align} \\texttt{field of charge }q_1\\texttt{ in }\\texttt S' :\\quad\\mathbf E'_1 &\\boldsymbol{=}\\boldsymbol{+}\\dfrac{q_1}{4\\pi\\epsilon_0}\\dfrac{\\mathbf r'}{\\Vert \\mathbf r' \\Vert^3}\\,, \\quad \\mathbf B'_1\\boldsymbol{=0} \\tag{A-01a}\\label{A-01a}\\\\ \\texttt{field of charge }q_2\\texttt{ in }\\texttt S' :\\quad\\mathbf E'_2 &\\boldsymbol{=}\\boldsymbol{-}\\dfrac{q_2}{4\\pi\\epsilon_0}\\dfrac{\\mathbf r'}{\\Vert \\mathbf r' \\Vert^3}\\,, \\quad \\mathbf B'_2\\boldsymbol{=0} \\tag{A-01b}\\label{A-01b}\\\\ \\texttt{force on charge }q_1\\texttt{ in }\\texttt S' :\\quad\\mathbf F'_1 & \\boldsymbol{=}q_1\\mathbf E'_2 \\boldsymbol{=}\\boldsymbol{-}\\dfrac{q_1q_2}{4\\pi\\epsilon_0}\\dfrac{\\mathbf r'}{\\Vert \\mathbf r' \\Vert^3} \\tag{A-01c}\\label{A-01c}\\\\ \\texttt{force on charge }q_2\\texttt{ in }\\texttt S' :\\quad\\mathbf F'_2 & \\boldsymbol{=}q_2\\mathbf E'_1 \\boldsymbol{=}\\boldsymbol{+}\\dfrac{q_2q_1}{4\\pi\\epsilon_0}\\dfrac{\\mathbf r'}{\\Vert \\mathbf r' \\Vert^3} \\tag{A-01d}\\label{A-01d} \\end{align} $\\boldsymbol\\S$ B. Two charges $\\,q_1,q_2\\,$ in uniform translational motion Consider two electric charges $\\:q_1\\:$ and $\\:q_2\\:$ in motion both with common uniform velocity $\\,\\boldsymbol{\\upsilon}\\,$ with respect to an inertial system $\\:\\texttt S$ . The electromagnetic fields and forces are shown in Figure-02 and Figure-03 respectively. Derived from the relativistic Liénard–Wiechert potentials are as follows \\begin{align} &\\texttt{field of charge }q_1\\texttt{ in }\\texttt S : \\nonumber\\\\ \\mathbf E_1 & \\boldsymbol{=}\\boldsymbol{+}\\dfrac{q_1}{4\\pi \\epsilon_0}\\dfrac{\\left(1\\!\\boldsymbol{-}\\!\\beta^2\\right)}{\\left(1\\!\\boldsymbol{-}\\!\\beta^2\\sin^2\\!\\phi\\right)^{3/2}}\\dfrac{\\mathbf{{r}}}{\\:\\:\\Vert\\mathbf{r}\\Vert^3}\\,, \\quad\\mathbf B_1\\boldsymbol{=}\\dfrac{1}{c^2}\\left(\\boldsymbol{\\upsilon}\\boldsymbol{\\times}\\mathbf E_1\\right) \\tag{B-01a}\\label{B-01a}\\\\ &\\texttt{field of charge }q_2\\texttt{ in }\\texttt S : \\nonumber\\\\ \\mathbf E_2 & \\boldsymbol{=}\\boldsymbol{-}\\dfrac{q_2}{4\\pi \\epsilon_0}\\dfrac{\\left(1\\!\\boldsymbol{-}\\!\\beta^2\\right)}{\\left(1\\!\\boldsymbol{-}\\!\\beta^2\\sin^2\\!\\phi\\right)^{3/2}}\\dfrac{\\mathbf{{r}}}{\\:\\:\\Vert\\mathbf{r}\\Vert^3}\\,, \\quad \\mathbf B_2\\boldsymbol{=}\\dfrac{1}{c^2}\\left(\\boldsymbol{\\upsilon}\\boldsymbol{\\times}\\mathbf E_2\\right) \\tag{B-01b}\\label{B-01b}\\\\ &\\texttt{Lorentz force on charge }q_1\\texttt{ in }\\texttt S : \\quad\\mathbf F_1 \\boldsymbol{=}q_1\\left(\\mathbf E_2\\boldsymbol{+}\\boldsymbol{\\upsilon}\\boldsymbol{\\times}\\mathbf B_2\\right) \\tag{B-01c}\\label{B-01c}\\\\ &\\texttt{Lorentz force on charge }q_2\\texttt{ in }\\texttt S : \\quad\\mathbf F_2 \\boldsymbol{=}q_2\\left(\\mathbf E_1\\boldsymbol{+}\\boldsymbol{\\upsilon}\\boldsymbol{\\times}\\mathbf B_1\\right) \\tag{B-01d}\\label{B-01d} \\end{align} where \\begin{equation} \\beta\\boldsymbol{=}\\dfrac{\\upsilon}{c} \\tag{B-02}\\label{B-02} \\end{equation} $\\boldsymbol\\S$ C. The Lorentz boost transformation Suppose now that the system $\\,\\texttt S\\,$ of $\\boldsymbol\\S\\,\\texttt{B}\\,$ arises from the system $\\,\\texttt S'\\,$ of $\\boldsymbol\\S\\,\\texttt{A}\\,$ by a Lorentz boost with velocity $\\,\\boldsymbol{-\\upsilon}$ . Then the electromagnetic fields of the two charges and the Lorentz forces between them $\\mathbf E_\\jmath,\\mathbf B_\\jmath, \\mathbf F_\\jmath$ could result from the Lorentz transformation of those $\\mathbf E'_\\jmath,\\mathbf B'_\\jmath, \\mathbf F'_\\jmath$ in the rest system $\\,\\texttt S'$ . Our task in this paragraph is to prove that the results provided by this Lorentz transformation are in full agreement with those in $\\boldsymbol\\S\\,\\texttt{B}\\,$ provided by the Liénard–Wiechert potentials. The Lorentz boost transformation with finite differences is \\begin{align} \\Delta\\mathbf x & \\boldsymbol{=} \\Delta\\mathbf x'\\boldsymbol{+}\\left(\\gamma\\boldsymbol{-}1\\right)\\left(\\Delta\\mathbf x'\\boldsymbol{\\cdot}\\mathbf n\\right)\\mathbf n\\boldsymbol{+}\\gamma\\boldsymbol{\\upsilon} \\Delta t' \\tag{C-01a}\\label{C-01a}\\\\ \\Delta t & \\boldsymbol{=} \\gamma\\left(\\Delta t'\\boldsymbol{+}\\dfrac{\\boldsymbol{\\upsilon}\\boldsymbol{\\cdot} \\Delta\\mathbf x'}{c^{2}}\\right) \\tag{C-01b}\\label{C-01b}\\\\ \\gamma & \\boldsymbol{=} 1\\Bigg/\\sqrt{1 \\boldsymbol{-}\\dfrac{\\upsilon^2}{c^2}}\\,,\\quad \\mathbf n\\boldsymbol{=}\\dfrac{\\boldsymbol{\\upsilon}}{\\Vert\\boldsymbol{\\upsilon}\\Vert}\\boldsymbol{=}\\dfrac{\\boldsymbol{\\upsilon}}{\\upsilon} \\tag{C-01c}\\label{C-01c} \\end{align} where $\\Delta\\mathbf x', \\Delta t'$ the space and time separation of two events in the accented rest frame $\\,\\texttt S'\\,$ and $\\Delta\\mathbf x, \\Delta t$ the space and time separation of two events in the frame $\\,\\texttt S$ . Consider now that Figure-01 represents a snapshot of the charges in their rest system $\\,\\texttt S'\\,$ so that \\begin{equation} \\Delta\\mathbf x'\\boldsymbol{=}\\mathbf x'_2\\boldsymbol{-}\\mathbf x'_1\\boldsymbol{=}\\mathbf r'\\,,\\qquad \\Delta t'\\boldsymbol{=}t'_2\\boldsymbol{-}t'_1\\boldsymbol{=}0 \\tag{C-02}\\label{C-02} \\end{equation} Inserting above $\\Delta\\mathbf x', \\Delta t'$ in the Lorentz boost equations \\eqref{C-01a}-\\eqref{C-01b} the space and time separations $\\Delta\\mathbf x, \\Delta t$ of these events in frame $\\,\\texttt S\\,$ are \\begin{align} \\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\mathbf r^{\\boldsymbol{*}}& \\boldsymbol{=} \\Delta\\mathbf x \\boldsymbol{=}\\mathbf x_2\\boldsymbol{-}\\mathbf x_1 \\boldsymbol{=} \\mathbf r'\\boldsymbol{+}\\left(\\gamma\\boldsymbol{-}1\\right)\\left(\\mathbf r'\\boldsymbol{\\cdot}\\mathbf n\\right)\\mathbf n \\boldsymbol{=} \\mathbf r'\\boldsymbol{+}\\left(\\gamma\\boldsymbol{-}1\\right)\\mathbf r'_{\\boldsymbol{\\Vert}} \\boldsymbol{=}\\mathbf r'_{\\boldsymbol{\\perp}}\\boldsymbol{+}\\gamma\\mathbf r'_{\\boldsymbol{\\Vert}} \\tag{C-03a}\\label{C-03a}\\\\ \\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\! \\Delta t & \\boldsymbol{=}t_2\\boldsymbol{-}t_1\\boldsymbol{=} \\gamma\\dfrac{\\boldsymbol{\\upsilon}\\boldsymbol{\\cdot} \\mathbf r'}{c^{2}} \\tag{C-03b}\\label{C-03b} \\end{align} The two events are not simultaneous in general in system $\\,\\texttt S$ . More exactly the charge $\\,q_2\\,$ positioned at $\\,\\mathbf r^{\\boldsymbol{*}}\\,$ with respect to $\\,q_1\\,$ is appeared there at a time $\\,t_2\\,$ where \\begin{equation} \\left. \\begin{cases} t_2 \\boldsymbol{>}t_1 & \\texttt{if }\\left(\\boldsymbol{\\upsilon}\\boldsymbol{\\cdot} \\mathbf r'\\right)\\boldsymbol{>}0 \\\\ t_2 \\boldsymbol{=}t_1 & \\texttt{if }\\left(\\boldsymbol{\\upsilon}\\boldsymbol{\\cdot} \\mathbf r'\\right)\\boldsymbol{=}0 \\\\ t_2 \\boldsymbol{<}t_1 & \\texttt{if }\\left(\\boldsymbol{\\upsilon}\\boldsymbol{\\cdot} \\mathbf r'\\right)\\boldsymbol{<}0 \\end{cases}\\right\\} \\tag{C-04}\\label{C-04} \\end{equation} To see where the charge $\\,q_2\\,$ would be at the time moment $\\,t_1\\,$ in order to have a snapshot at this time moment in $\\,\\texttt S$ , we must translate it by \\begin{equation} \\boldsymbol{\\upsilon}\\Delta t \\boldsymbol{=} \\boldsymbol{\\upsilon}\\gamma\\dfrac{\\boldsymbol{\\upsilon}\\boldsymbol{\\cdot} \\mathbf r'}{c^{2}}\\boldsymbol{=}\\gamma\\dfrac{\\upsilon^2}{c^2}\\left(\\mathbf r'\\boldsymbol{\\cdot}\\mathbf n\\right)\\mathbf n \\boldsymbol{=}\\dfrac{\\gamma^2\\boldsymbol{-}1}{\\gamma}\\left(\\mathbf r'\\boldsymbol{\\cdot}\\mathbf n\\right)\\mathbf n\\boldsymbol{=}\\dfrac{\\gamma^2\\boldsymbol{-}1}{\\gamma}\\mathbf r'_{\\boldsymbol{\\Vert}} \\tag{C-05}\\label{C-05} \\end{equation} meaning that at the time moment $\\,t_1\\,$ the charge $\\,q_2\\,$ is positioned with respect to $\\,q_1\\,$ at \\begin{equation} \\mathbf r \\boldsymbol{=} \\mathbf r^{\\boldsymbol{*}}\\boldsymbol{-}\\boldsymbol{\\upsilon}\\Delta t \\boldsymbol{=} \\mathbf r'\\boldsymbol{+}\\left(\\gamma\\boldsymbol{-}1\\right)\\left(\\mathbf r'\\boldsymbol{\\cdot}\\mathbf n\\right)\\mathbf n \\boldsymbol{-}\\dfrac{\\gamma^2\\boldsymbol{-}1}{\\gamma}\\left(\\mathbf r'\\boldsymbol{\\cdot}\\mathbf n\\right)\\mathbf n \\tag{C-06}\\label{C-06} \\end{equation} so \\begin{equation} \\mathbf r \\boldsymbol{=}\\mathbf r'\\boldsymbol{-}\\dfrac{\\gamma\\boldsymbol{-}1}{\\gamma}(\\mathbf r'\\boldsymbol{\\cdot} \\mathbf n)\\mathbf n \\boldsymbol{=}\\mathbf r'_{\\boldsymbol{\\perp}}\\boldsymbol{+}\\dfrac{1}{\\gamma}\\mathbf r'_{\\boldsymbol{\\Vert}} \\tag{C-07}\\label{C-07} \\end{equation} The vectors $\\mathbf r',\\mathbf r^{\\boldsymbol{*}},\\mathbf r$ and their relations are shown in Figure-04. Each one is composed by two components, one along the velocity direction $\\,\\mathbf n\\,$ and the other normal to it \\begin{equation} \\mathbf a \\boldsymbol{=}\\mathbf a_{\\boldsymbol{\\Vert}}\\boldsymbol{+}\\mathbf a_{\\boldsymbol{\\perp}}\\boldsymbol{=}\\underbrace{\\left(\\mathbf n\\boldsymbol{\\cdot}\\mathbf a\\right)\\mathbf n}_{\\mathbf a_{\\boldsymbol{\\Vert}}}\\boldsymbol{+}\\overbrace{\\underbrace{\\mathbf a\\boldsymbol{-}\\left(\\mathbf n\\boldsymbol{\\cdot}\\mathbf a\\right)\\mathbf n}_{\\mathbf a_{\\boldsymbol{\\perp}}}}^{\\left(\\mathbf n\\boldsymbol{\\times}\\mathbf a\\right)\\boldsymbol{\\times}\\mathbf n}\\;,\\qquad \\mathbf a \\boldsymbol{=}\\mathbf r',\\mathbf r^{\\boldsymbol{*}},\\mathbf r \\tag{C-08}\\label{C-08} \\end{equation} Let examine now how the electromagnetic fields and forces produced by the charges are Lorentz transformed from the rest frame $\\,\\texttt S'\\,$ to the frame $\\,\\texttt S$ . It's sufficient to determine the field and the force from one particle, say $\\,q_1$ , to the other $\\,q_2$ . Under the Lorentz boost transformation expressed by equations \\eqref{C-01a}-\\eqref{C-01b} the electromagnetic field is transformed as follows \\begin{align} \\mathbf E_1 & \\boldsymbol{=}\\gamma \\mathbf E'_1\\boldsymbol{-}\\left(\\gamma\\boldsymbol{-}1\\right)\\left( \\mathbf E'_1\\boldsymbol{\\cdot}\\mathbf{n}\\right)\\mathbf{n}\\boldsymbol{-}\\;\\gamma\\,\\left(\\boldsymbol{\\upsilon}\\boldsymbol{\\times} \\mathbf B'_1\\right) \\tag{C-09a}\\label{C-09a}\\\\ \\mathbf B_1 & \\boldsymbol{=} \\gamma \\mathbf B'_1\\boldsymbol{-}\\left(\\gamma\\boldsymbol{-}1\\right)\\left(\\mathbf B'_1\\boldsymbol{\\cdot}\\mathbf{n}\\right)\\mathbf{n}\\boldsymbol{+}\\dfrac{\\gamma}{c^{2}}\\left(\\boldsymbol{\\upsilon}\\boldsymbol{\\times}\\mathbf E'_1\\right) \\tag{C-09b}\\label{C-09b} \\end{align} Since $\\,\\mathbf B'_1\\boldsymbol{=0}$ , see equation \\eqref{A-01a}, we have \\begin{align} \\mathbf E_1 & \\boldsymbol{=}\\gamma \\mathbf E'_1\\boldsymbol{-}\\left(\\gamma\\boldsymbol{-}1\\right)\\left( \\mathbf E'_1\\boldsymbol{\\cdot}\\mathbf{n}\\right)\\mathbf{n} \\tag{C-10a}\\label{C-10a}\\\\ \\mathbf B_1 & \\boldsymbol{=} \\dfrac{\\gamma}{c^{2}}\\left(\\boldsymbol{\\upsilon}\\boldsymbol{\\times}\\mathbf E'_1\\right) \\tag{C-10b}\\label{C-10b} \\end{align} Note that since $\\,\\boldsymbol{\\upsilon}\\boldsymbol{\\times}\\mathbf n\\boldsymbol{=}\\boldsymbol{\\upsilon}\\boldsymbol{\\times}\\boldsymbol{\\upsilon}/\\upsilon\\boldsymbol{=0}\\,$ \\begin{equation} \\mathbf B_1 \\boldsymbol{=} \\dfrac{\\gamma}{c^{2}}\\left(\\boldsymbol{\\upsilon}\\boldsymbol{\\times}\\mathbf E'_1\\right)\\boldsymbol{=} \\dfrac{1}{c^{2}}\\left(\\boldsymbol{\\upsilon}\\boldsymbol{\\times}\\gamma\\mathbf E'_1\\right) \\boldsymbol{=}\\dfrac{1}{c^{2}}\\Bigl(\\boldsymbol{\\upsilon}\\boldsymbol{\\times}\\overbrace{\\bigl[\\gamma \\mathbf E'_1\\boldsymbol{-}\\left(\\gamma\\boldsymbol{-}1\\right)\\left( \\mathbf E'_1\\boldsymbol{\\cdot}\\mathbf{n}\\right)\\mathbf{n}\\bigr]}^{\\mathbf E_1}\\Bigr)\\boldsymbol{\\implies} \\nonumber \\end{equation} so \\begin{equation} \\boxed{\\:\\:\\mathbf B_1\\boldsymbol{=} \\dfrac{1}{c^{2}}\\left(\\boldsymbol{\\upsilon}\\boldsymbol{\\times}\\mathbf E_1\\right)\\:\\:} \\tag{C-11}\\label{C-11} \\end{equation} This is identical to the expression of $\\,\\mathbf B_1\\,$ in equation \\eqref{B-01a} derived by the Liénard–Wiechert potentials. Inserting in \\eqref{C-10a} the expression $\\,\\mathbf E'_1 \\boldsymbol{=}\\left(q_1/4\\pi\\epsilon_0\\right)\\bigl(\\mathbf r'/\\Vert\\mathbf r' \\Vert^3\\bigr)$ , see equation \\eqref{A-01a}, we have \\begin{align} \\mathbf E_1 & \\boldsymbol{=} \\dfrac{q_1}{4\\pi \\epsilon_0}\\biggl[\\gamma \\dfrac{\\mathbf r'}{\\Vert \\mathbf r' \\Vert^3}\\boldsymbol{-}\\left(\\gamma\\boldsymbol{-}1\\right)\\left( \\dfrac{\\mathbf r'}{\\Vert \\mathbf r' \\Vert^3}\\boldsymbol{\\cdot}\\mathbf{n}\\right)\\mathbf{n}\\biggr] \\nonumber\\\\ & \\boldsymbol{=} \\dfrac{\\gamma}{\\Vert \\mathbf r' \\Vert^3}\\underbrace{\\left[\\mathbf r'\\boldsymbol{-}\\dfrac{\\gamma\\boldsymbol{-}1}{\\gamma}(\\mathbf r'\\boldsymbol{\\cdot} \\mathbf n)\\mathbf n \\right]}_{\\mathbf r}\\boldsymbol{=} \\dfrac{\\gamma\\,\\mathbf r}{\\Vert \\mathbf r' \\Vert^3} \\implies \\nonumber\\\\ \\mathbf E_1 & \\boldsymbol{=} \\dfrac{q_1}{4\\pi \\epsilon_0}\\dfrac{\\gamma\\,\\mathbf r}{\\Vert \\mathbf r' \\Vert^3} \\tag{C-12}\\label{C-12} \\end{align} From the relation \\eqref{C-07} between $\\,\\mathbf r,\\mathbf r'$ and the help of Figure-04 we could prove (1) that \\begin{equation} \\dfrac{\\gamma}{\\Vert \\mathbf r' \\Vert^3}\\boldsymbol{=} \\dfrac{\\left(1\\boldsymbol{-}\\beta^2\\right)}{\\left(1\\boldsymbol{-}\\beta^2\\sin^2\\!\\phi\\right)^{3/2}}\\dfrac{1}{\\Vert\\mathbf r\\Vert^3} \\tag{C-13}\\label{C-13} \\end{equation} so \\eqref{C-12} yields \\begin{equation} \\boxed{\\:\\:\\mathbf E_1 \\boldsymbol{=} \\dfrac{q_1}{4\\pi \\epsilon_0}\\dfrac{\\left(1\\boldsymbol{-}\\beta^2\\right)}{\\left(1\\boldsymbol{-}\\beta^2\\sin^2\\!\\phi\\right)^{3/2}}\\dfrac{\\mathbf r}{\\Vert\\mathbf r\\Vert^3}\\:\\:} \\tag{C-14}\\label{C-14} \\end{equation} This is identical to the expression of $\\,\\mathbf E_1\\,$ in equation \\eqref{B-01a} derived by the Liénard–Wiechert potentials. $=\\!=\\!=\\!=\\!=\\!=\\!=\\!=\\!=\\!=\\!=\\!=\\!=\\!=\\!=\\!=\\!=\\!=\\!=\\!=\\!=\\!=\\!=\\!=\\!=\\!=\\!=\\!=\\!=\\!=\\!=\\!=\\!=\\!=\\!=\\!=\\!\\!=\\!=\\!=\\!\\!=\\!=\\!=\\!\\!=\\!=\\!=\\!\\!=\\!=\\!=\\!\\!=\\!=\\!=\\!\\!=\\!=\\!=\\!\\!=\\!=\\!=\\!\\!=\\!=\\!=\\!\\!=\\!=\\!=\\!$ Reference (1) : Magnetic field due to a single moving charge . Reference (2): Electric field associated with moving charge . $=\\!=\\!=\\!=\\!=\\!=\\!=\\!=\\!=\\!=\\!=\\!=\\!=\\!=\\!=\\!=\\!=\\!=\\!=\\!=\\!=\\!=\\!=\\!=\\!=\\!=\\!=\\!=\\!=\\!=\\!=\\!=\\!=\\!=\\!=\\!=\\!\\!=\\!=\\!=\\!\\!=\\!=\\!=\\!\\!=\\!=\\!=\\!\\!=\\!=\\!=\\!\\!=\\!=\\!=\\!\\!=\\!=\\!=\\!\\!=\\!=\\!=\\!\\!=\\!=\\!=\\!\\!=\\!=\\!=\\!$ (1) Proof of equation \\eqref{C-13} From equation \\eqref{C-07} and Figure-04 we have \\begin{align} \\Vert\\mathbf r\\Vert^2 & \\boldsymbol{=}\\Vert\\mathbf r'\\Vert^2\\boldsymbol{+}\\left(\\dfrac{\\gamma\\boldsymbol{-}1}{\\gamma}\\right)^2\\underbrace{\\Vert\\left(\\mathbf r'\\boldsymbol{\\cdot} \\mathbf n\\right)\\mathbf n\\Vert^2}_{\\Vert\\mathbf r'\\Vert^2\\cos^2\\phi'} \\boldsymbol{-}2\\left(\\dfrac{\\gamma\\boldsymbol{-}1}{\\gamma}\\right)\\underbrace{\\left(\\mathbf r'\\boldsymbol{\\cdot} \\mathbf n\\right)^2}_{\\Vert\\mathbf r'\\Vert^2\\cos^2\\phi'} \\boldsymbol{\\implies} \\nonumber\\\\ \\Vert\\mathbf r\\Vert^2 & \\boldsymbol{=}\\Vert\\mathbf r'\\Vert^2\\left[1\\boldsymbol{-}\\left(1\\boldsymbol{-}\\dfrac{1}{\\gamma^2}\\right)\\cos^2\\phi'\\right] \\boldsymbol{\\implies} \\nonumber \\end{align} \\begin{equation} \\Vert\\mathbf r\\Vert^2\\boldsymbol{=} \\Vert\\mathbf r'\\Vert^2\\left(1\\boldsymbol{-}\\beta^2\\cos^2\\phi'\\right) \\tag{Pr-01}\\label{Pr-01} \\end{equation} From the normal components equality $\\:\\mathbf r_{\\boldsymbol{\\perp}}\\boldsymbol{=}\\mathbf r'_{\\boldsymbol{\\perp}}\\:$ we have \\begin{equation} \\Vert\\mathbf r\\Vert^2\\sin^2\\phi\\boldsymbol{=} \\Vert\\mathbf r'\\Vert^2\\sin^2\\phi' \\tag{Pr-02}\\label{Pr-02} \\end{equation} that is \\begin{equation} \\cos^2\\phi'\\boldsymbol{=}1\\boldsymbol{-}\\dfrac{\\Vert\\mathbf r\\Vert^2}{\\Vert\\mathbf r'\\Vert^2}\\sin^2\\phi \\tag{Pr-03}\\label{Pr-03} \\end{equation} Inserting this expression of $\\:\\cos^2\\phi'\\:$ in equation \\eqref{Pr-01} and solving with respect to $\\:\\Vert\\mathbf r\\Vert^2/\\Vert\\mathbf r'\\Vert^2\\:$ we find \\begin{equation} \\dfrac{\\Vert\\mathbf r\\Vert^2}{\\Vert\\mathbf r'\\Vert^2}\\boldsymbol{=}\\dfrac{1\\boldsymbol{-}\\beta^2}{1\\boldsymbol{-}\\beta^2\\sin^2\\phi} \\tag{Pr-04}\\label{Pr-04} \\end{equation} so \\begin{equation} \\dfrac{\\Vert\\mathbf r\\Vert^3}{\\Vert\\mathbf r'\\Vert^3}\\boldsymbol{=}\\dfrac{\\left(1\\boldsymbol{-}\\beta^2\\right)^{3/2}}{\\left(1\\boldsymbol{-}\\beta^2\\sin^2\\phi\\right)^{3/2}}\\boldsymbol{=}\\dfrac{\\sqrt{1\\boldsymbol{-}\\beta^2}\\left(1\\boldsymbol{-}\\beta^2\\right)}{\\left(1\\boldsymbol{-}\\beta^2\\sin^2\\phi\\right)^{3/2}}\\boldsymbol{=}\\dfrac{\\gamma^{\\boldsymbol{-}1}\\left(1\\boldsymbol{-}\\beta^2\\right)}{\\left(1\\boldsymbol{-}\\beta^2\\sin^2\\phi\\right)^{3/2}} \\tag{Pr-05}\\label{Pr-05} \\end{equation} or \\begin{equation} \\dfrac{\\gamma}{\\Vert \\mathbf r' \\Vert^3}\\boldsymbol{=} \\dfrac{\\left(1\\boldsymbol{-}\\beta^2\\right)}{\\left(1\\boldsymbol{-}\\beta^2\\sin^2\\!\\phi\\right)^{3/2}}\\dfrac{1}{\\Vert\\mathbf r\\Vert^3} \\tag{Pr-06}\\label{Pr-06} \\end{equation} proving equation \\eqref{C-13}.",
      "question_latex": [],
      "answer_latex": [
        "\\texttt{C O N T E N T S}",
        "\\texttt{Abstract}",
        "\\boldsymbol\\S\\texttt{ A. Two charges }q_1,q_2\\texttt{ at rest (system S",
        ")}",
        "\\boldsymbol\\S\\texttt{ B. Two charges }q_1,q_2\\texttt{ in uniform translational motion (system S)}",
        "\\boldsymbol\\S\\texttt{ C. The Lorentz boost transformation (",
        "'\\longrightarrow",
        "</span></p>\n<hr />\n<p><strong>Abstract</strong></p>\n<p>A first case concerns two electric charges <span class=\"math-container\">",
        "</span> and <span class=\"math-container\">",
        "</span> at rest with respect to an inertial system <span class=\"math-container\">",
        "</span>. The electrostatic fields and forces using Coulomb's law are given in <span class=\"math-container\">",
        "</span>. A second case concerns two electric charges <span class=\"math-container\">",
        "</span> in motion both with common uniform velocity <span class=\"math-container\">",
        "</span> with respect to an inertial system <span class=\"math-container\">",
        "</span>. The electromagnetic fields and forces are given in <span class=\"math-container\">",
        "</span> using well-known expressions derived from the relativistic Liénard–Wiechert potentials. The two cases are studied independently of one another with no relation between them until <span class=\"math-container\">",
        "</span> where it is assumed that the inertial system <span class=\"math-container\">",
        "</span> of the latter case arises from a Lorentz boost transformation with velocity <span class=\"math-container\">",
        "</span> with respect to the inertial system <span class=\"math-container\">",
        "</span> of the former case. Now the electromagnetic fields and forces in system <span class=\"math-container\">",
        "</span> are derived from the Lorentz boost transformation of the electrostatic fields and forces in the rest system <span class=\"math-container\">",
        "</span>. The results from this application of the Lorentz transformation are in full agreement with those in <span class=\"math-container\">",
        "</span> derived from the Liénard–Wiechert potentials. Moreover they provide relations connecting the two systems.</p>\n<hr />\n<p><span class=\"math-container\">",
        "</span> <strong>A. Two charges <span class=\"math-container\">",
        "</span> at rest</strong></p>\n<p>Consider two electric charges <span class=\"math-container\">",
        "</span>, see Figure-01. One charge feels only the electrostatic Coulomb force of the other. Fields and forces are as follows\n<span class=\"math-container\">\\begin{align}\n\\texttt{field of charge }q_1\\texttt{ in }\\texttt S' :\\quad\\mathbf E'_1  &\\boldsymbol{=}\\boldsymbol{+}\\dfrac{q_1}{4\\pi\\epsilon_0}\\dfrac{\\mathbf r'}{\\Vert  \\mathbf r' \\Vert^3}\\,, \\quad \\mathbf B'_1\\boldsymbol{=0} \n\\tag{A-01a}\\label{A-01a}\\\\\n\\texttt{field of charge }q_2\\texttt{ in }\\texttt S'  :\\quad\\mathbf E'_2  &\\boldsymbol{=}\\boldsymbol{-}\\dfrac{q_2}{4\\pi\\epsilon_0}\\dfrac{\\mathbf r'}{\\Vert  \\mathbf r' \\Vert^3}\\,, \\quad \\mathbf B'_2\\boldsymbol{=0} \n\\tag{A-01b}\\label{A-01b}\\\\ \n\\texttt{force on charge }q_1\\texttt{ in }\\texttt S' :\\quad\\mathbf F'_1 & \\boldsymbol{=}q_1\\mathbf E'_2  \\boldsymbol{=}\\boldsymbol{-}\\dfrac{q_1q_2}{4\\pi\\epsilon_0}\\dfrac{\\mathbf r'}{\\Vert  \\mathbf r' \\Vert^3}\n\\tag{A-01c}\\label{A-01c}\\\\\n\\texttt{force on charge }q_2\\texttt{ in }\\texttt S'  :\\quad\\mathbf F'_2 & \\boldsymbol{=}q_2\\mathbf E'_1  \\boldsymbol{=}\\boldsymbol{+}\\dfrac{q_2q_1}{4\\pi\\epsilon_0}\\dfrac{\\mathbf r'}{\\Vert  \\mathbf r' \\Vert^3}\n\\tag{A-01d}\\label{A-01d}      \n\\end{align}</span></p>\n<p><a href=\"https://i.stack.imgur.com/QqdNI.png\" rel=\"nofollow noreferrer\"><img src=\"https://i.stack.imgur.com/QqdNI.png\" alt=\"enter image description here\" /></a></p>\n<hr />\n<p><span class=\"math-container\">",
        "</span> <strong>B. Two charges <span class=\"math-container\">",
        "</span> in uniform translational motion</strong></p>\n<p>Consider two electric charges <span class=\"math-container\">",
        "</span>. The electromagnetic fields and forces are shown in Figure-02 and Figure-03 respectively. Derived from the relativistic Liénard–Wiechert potentials are as follows\n<span class=\"math-container\">\\begin{align}\n&\\texttt{field of charge }q_1\\texttt{ in }\\texttt S :\n\\nonumber\\\\\n\\mathbf E_1  & \\boldsymbol{=}\\boldsymbol{+}\\dfrac{q_1}{4\\pi \\epsilon_0}\\dfrac{\\left(1\\!\\boldsymbol{-}\\!\\beta^2\\right)}{\\left(1\\!\\boldsymbol{-}\\!\\beta^2\\sin^2\\!\\phi\\right)^{3/2}}\\dfrac{\\mathbf{{r}}}{\\:\\:\\Vert\\mathbf{r}\\Vert^3}\\,,  \\quad\\mathbf B_1\\boldsymbol{=}\\dfrac{1}{c^2}\\left(\\boldsymbol{\\upsilon}\\boldsymbol{\\times}\\mathbf E_1\\right)\n\\tag{B-01a}\\label{B-01a}\\\\\n&\\texttt{field of charge }q_2\\texttt{ in }\\texttt S :\n\\nonumber\\\\\n\\mathbf E_2 & \\boldsymbol{=}\\boldsymbol{-}\\dfrac{q_2}{4\\pi \\epsilon_0}\\dfrac{\\left(1\\!\\boldsymbol{-}\\!\\beta^2\\right)}{\\left(1\\!\\boldsymbol{-}\\!\\beta^2\\sin^2\\!\\phi\\right)^{3/2}}\\dfrac{\\mathbf{{r}}}{\\:\\:\\Vert\\mathbf{r}\\Vert^3}\\,, \\quad \\mathbf B_2\\boldsymbol{=}\\dfrac{1}{c^2}\\left(\\boldsymbol{\\upsilon}\\boldsymbol{\\times}\\mathbf E_2\\right) \n\\tag{B-01b}\\label{B-01b}\\\\\n&\\texttt{Lorentz force on charge }q_1\\texttt{ in }\\texttt S :\n\\quad\\mathbf F_1  \\boldsymbol{=}q_1\\left(\\mathbf E_2\\boldsymbol{+}\\boldsymbol{\\upsilon}\\boldsymbol{\\times}\\mathbf B_2\\right)\n\\tag{B-01c}\\label{B-01c}\\\\\n&\\texttt{Lorentz force on charge }q_2\\texttt{ in }\\texttt S :\n\\quad\\mathbf F_2   \\boldsymbol{=}q_2\\left(\\mathbf E_1\\boldsymbol{+}\\boldsymbol{\\upsilon}\\boldsymbol{\\times}\\mathbf B_1\\right)\n\\tag{B-01d}\\label{B-01d} \n\\end{align}</span><br />\nwhere\n<span class=\"math-container\">\\begin{equation}\n\\beta\\boldsymbol{=}\\dfrac{\\upsilon}{c}\n\\tag{B-02}\\label{B-02} \n\\end{equation}</span></p>\n<p><a href=\"https://i.stack.imgur.com/0SICb.png\" rel=\"nofollow noreferrer\"><img src=\"https://i.stack.imgur.com/0SICb.png\" alt=\"enter image description here\" /></a></p>\n<p><a href=\"https://i.stack.imgur.com/VSBW5.png\" rel=\"nofollow noreferrer\"><img src=\"https://i.stack.imgur.com/VSBW5.png\" alt=\"enter image description here\" /></a></p>\n<hr />\n<p><span class=\"math-container\">",
        "</span> <strong>C. The Lorentz boost transformation</strong></p>\n<p>Suppose now that the system <span class=\"math-container\">",
        "</span> of  <span class=\"math-container\">",
        "</span> arises from the system <span class=\"math-container\">",
        "</span> by a Lorentz boost with velocity <span class=\"math-container\">",
        "</span>. Then the electromagnetic fields of the two charges and the Lorentz forces between them <span class=\"math-container\">",
        "</span> could result from the Lorentz transformation of those <span class=\"math-container\">",
        "</span> in the rest system <span class=\"math-container\">",
        "</span>. Our task in this paragraph is to prove that the results provided by this Lorentz transformation are in full agreement with those in  <span class=\"math-container\">",
        "</span>  provided by the Liénard–Wiechert potentials.</p>\n<p>The Lorentz boost transformation with finite differences is\n<span class=\"math-container\">\\begin{align}                 \n\\Delta\\mathbf x & \\boldsymbol{=}  \\Delta\\mathbf x'\\boldsymbol{+}\\left(\\gamma\\boldsymbol{-}1\\right)\\left(\\Delta\\mathbf x'\\boldsymbol{\\cdot}\\mathbf n\\right)\\mathbf n\\boldsymbol{+}\\gamma\\boldsymbol{\\upsilon} \\Delta t'\n\\tag{C-01a}\\label{C-01a}\\\\\n \\Delta t & \\boldsymbol{=}  \\gamma\\left(\\Delta t'\\boldsymbol{+}\\dfrac{\\boldsymbol{\\upsilon}\\boldsymbol{\\cdot} \\Delta\\mathbf x'}{c^{2}}\\right)\n\\tag{C-01b}\\label{C-01b}\\\\\n\\gamma & \\boldsymbol{=} 1\\Bigg/\\sqrt{1 \\boldsymbol{-}\\dfrac{\\upsilon^2}{c^2}}\\,,\\quad \\mathbf n\\boldsymbol{=}\\dfrac{\\boldsymbol{\\upsilon}}{\\Vert\\boldsymbol{\\upsilon}\\Vert}\\boldsymbol{=}\\dfrac{\\boldsymbol{\\upsilon}}{\\upsilon}\n\\tag{C-01c}\\label{C-01c}\n\\end{align}</span>\nwhere <span class=\"math-container\">",
        "</span> the space and time separation of two events in the accented rest frame <span class=\"math-container\">",
        "</span> the space and time separation of two events in the  frame <span class=\"math-container\">",
        "</span>.</p>\n<p>Consider now that Figure-01 represents a snapshot of the charges in their rest system <span class=\"math-container\">",
        "</span> so that\n<span class=\"math-container\">\\begin{equation}\n\\Delta\\mathbf x'\\boldsymbol{=}\\mathbf x'_2\\boldsymbol{-}\\mathbf x'_1\\boldsymbol{=}\\mathbf r'\\,,\\qquad  \\Delta t'\\boldsymbol{=}t'_2\\boldsymbol{-}t'_1\\boldsymbol{=}0\n\\tag{C-02}\\label{C-02}\n\\end{equation}</span>\nInserting above <span class=\"math-container\">",
        "</span> in the Lorentz boost equations \\eqref{C-01a}-\\eqref{C-01b} the space and time separations <span class=\"math-container\">",
        "</span> of these events  in frame <span class=\"math-container\">",
        "</span> are\n<span class=\"math-container\">\\begin{align}                 \n\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\mathbf r^{\\boldsymbol{*}}& \\boldsymbol{=} \\Delta\\mathbf x \\boldsymbol{=}\\mathbf x_2\\boldsymbol{-}\\mathbf x_1 \\boldsymbol{=}  \\mathbf r'\\boldsymbol{+}\\left(\\gamma\\boldsymbol{-}1\\right)\\left(\\mathbf r'\\boldsymbol{\\cdot}\\mathbf n\\right)\\mathbf n \\boldsymbol{=}  \\mathbf r'\\boldsymbol{+}\\left(\\gamma\\boldsymbol{-}1\\right)\\mathbf r'_{\\boldsymbol{\\Vert}} \\boldsymbol{=}\\mathbf r'_{\\boldsymbol{\\perp}}\\boldsymbol{+}\\gamma\\mathbf r'_{\\boldsymbol{\\Vert}}\n\\tag{C-03a}\\label{C-03a}\\\\\n\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\! \\Delta t & \\boldsymbol{=}t_2\\boldsymbol{-}t_1\\boldsymbol{=}  \\gamma\\dfrac{\\boldsymbol{\\upsilon}\\boldsymbol{\\cdot} \\mathbf r'}{c^{2}}\n\\tag{C-03b}\\label{C-03b} \n\\end{align}</span>\nThe two events are not simultaneous in general in system <span class=\"math-container\">",
        "</span>. More exactly the charge <span class=\"math-container\">",
        "</span> positioned at <span class=\"math-container\">",
        "</span> with respect to <span class=\"math-container\">",
        "</span> is appeared there at a time <span class=\"math-container\">",
        "</span> where\n<span class=\"math-container\">\\begin{equation}\n\\left.\n\\begin{cases}\nt_2 \\boldsymbol{>}t_1  & \\texttt{if }\\left(\\boldsymbol{\\upsilon}\\boldsymbol{\\cdot} \\mathbf r'\\right)\\boldsymbol{>}0 \\\\\nt_2 \\boldsymbol{=}t_1  & \\texttt{if }\\left(\\boldsymbol{\\upsilon}\\boldsymbol{\\cdot} \\mathbf r'\\right)\\boldsymbol{=}0  \\\\\nt_2 \\boldsymbol{<}t_1 & \\texttt{if }\\left(\\boldsymbol{\\upsilon}\\boldsymbol{\\cdot} \\mathbf r'\\right)\\boldsymbol{<}0  \n\\end{cases}\\right\\}\n\\tag{C-04}\\label{C-04}\n\\end{equation}</span>\nTo see where the charge <span class=\"math-container\">",
        "</span> would be at the time moment <span class=\"math-container\">",
        "</span> in order to have a snapshot at this time moment in\n<span class=\"math-container\">",
        "</span>, we must translate it by\n<span class=\"math-container\">\\begin{equation}\n\\boldsymbol{\\upsilon}\\Delta t \\boldsymbol{=}  \\boldsymbol{\\upsilon}\\gamma\\dfrac{\\boldsymbol{\\upsilon}\\boldsymbol{\\cdot} \\mathbf r'}{c^{2}}\\boldsymbol{=}\\gamma\\dfrac{\\upsilon^2}{c^2}\\left(\\mathbf r'\\boldsymbol{\\cdot}\\mathbf n\\right)\\mathbf n \\boldsymbol{=}\\dfrac{\\gamma^2\\boldsymbol{-}1}{\\gamma}\\left(\\mathbf r'\\boldsymbol{\\cdot}\\mathbf n\\right)\\mathbf n\\boldsymbol{=}\\dfrac{\\gamma^2\\boldsymbol{-}1}{\\gamma}\\mathbf r'_{\\boldsymbol{\\Vert}}\n\\tag{C-05}\\label{C-05}\n\\end{equation}</span>\nmeaning that at the time moment <span class=\"math-container\">",
        "</span> the charge <span class=\"math-container\">",
        "</span> is positioned with respect to <span class=\"math-container\">",
        "</span> at\n<span class=\"math-container\">\\begin{equation}\n\\mathbf r \\boldsymbol{=} \\mathbf r^{\\boldsymbol{*}}\\boldsymbol{-}\\boldsymbol{\\upsilon}\\Delta t \\boldsymbol{=}   \\mathbf r'\\boldsymbol{+}\\left(\\gamma\\boldsymbol{-}1\\right)\\left(\\mathbf r'\\boldsymbol{\\cdot}\\mathbf n\\right)\\mathbf n \\boldsymbol{-}\\dfrac{\\gamma^2\\boldsymbol{-}1}{\\gamma}\\left(\\mathbf r'\\boldsymbol{\\cdot}\\mathbf n\\right)\\mathbf n\n\\tag{C-06}\\label{C-06}\n\\end{equation}</span>\nso\n<span class=\"math-container\">\\begin{equation}\n\\mathbf r  \\boldsymbol{=}\\mathbf r'\\boldsymbol{-}\\dfrac{\\gamma\\boldsymbol{-}1}{\\gamma}(\\mathbf r'\\boldsymbol{\\cdot} \\mathbf n)\\mathbf n \\boldsymbol{=}\\mathbf r'_{\\boldsymbol{\\perp}}\\boldsymbol{+}\\dfrac{1}{\\gamma}\\mathbf r'_{\\boldsymbol{\\Vert}} \n\\tag{C-07}\\label{C-07} \n\\end{equation}</span>\nThe vectors <span class=\"math-container\">",
        "</span> and their relations are shown in Figure-04. Each one is composed by two components, one along the velocity direction <span class=\"math-container\">",
        "</span> and the other normal to it\n<span class=\"math-container\">\\begin{equation}\n\\mathbf a \\boldsymbol{=}\\mathbf a_{\\boldsymbol{\\Vert}}\\boldsymbol{+}\\mathbf a_{\\boldsymbol{\\perp}}\\boldsymbol{=}\\underbrace{\\left(\\mathbf n\\boldsymbol{\\cdot}\\mathbf a\\right)\\mathbf n}_{\\mathbf a_{\\boldsymbol{\\Vert}}}\\boldsymbol{+}\\overbrace{\\underbrace{\\mathbf a\\boldsymbol{-}\\left(\\mathbf n\\boldsymbol{\\cdot}\\mathbf a\\right)\\mathbf n}_{\\mathbf a_{\\boldsymbol{\\perp}}}}^{\\left(\\mathbf n\\boldsymbol{\\times}\\mathbf a\\right)\\boldsymbol{\\times}\\mathbf n}\\;,\\qquad \\mathbf a \\boldsymbol{=}\\mathbf r',\\mathbf r^{\\boldsymbol{*}},\\mathbf r\n\\tag{C-08}\\label{C-08} \n\\end{equation}</span></p>\n<p><a href=\"https://i.stack.imgur.com/KwhWZ.png\" rel=\"nofollow noreferrer\"><img src=\"https://i.stack.imgur.com/KwhWZ.png\" alt=\"enter image description here\" /></a></p>\n<p>Let examine now how the electromagnetic fields and forces produced by the charges are Lorentz transformed from the rest frame <span class=\"math-container\">",
        "</span>  to the frame <span class=\"math-container\">",
        "</span>. It's sufficient to determine the field and the force from one particle, say <span class=\"math-container\">",
        "</span>, to the other <span class=\"math-container\">",
        "</span>.</p>\n<p>Under the Lorentz boost transformation expressed by equations \\eqref{C-01a}-\\eqref{C-01b} the electromagnetic field is transformed as follows\n<span class=\"math-container\">\\begin{align}\n\\mathbf E_1 & \\boldsymbol{=}\\gamma \\mathbf E'_1\\boldsymbol{-}\\left(\\gamma\\boldsymbol{-}1\\right)\\left( \\mathbf E'_1\\boldsymbol{\\cdot}\\mathbf{n}\\right)\\mathbf{n}\\boldsymbol{-}\\;\\gamma\\,\\left(\\boldsymbol{\\upsilon}\\boldsymbol{\\times} \\mathbf B'_1\\right)\n\\tag{C-09a}\\label{C-09a}\\\\\n \\mathbf B_1 & \\boldsymbol{=} \\gamma \\mathbf B'_1\\boldsymbol{-}\\left(\\gamma\\boldsymbol{-}1\\right)\\left(\\mathbf B'_1\\boldsymbol{\\cdot}\\mathbf{n}\\right)\\mathbf{n}\\boldsymbol{+}\\dfrac{\\gamma}{c^{2}}\\left(\\boldsymbol{\\upsilon}\\boldsymbol{\\times}\\mathbf E'_1\\right)\n\\tag{C-09b}\\label{C-09b}\n\\end{align}</span>\nSince <span class=\"math-container\">",
        "</span>, see equation \\eqref{A-01a}, we have\n<span class=\"math-container\">\\begin{align} \n\\mathbf E_1 & \\boldsymbol{=}\\gamma \\mathbf E'_1\\boldsymbol{-}\\left(\\gamma\\boldsymbol{-}1\\right)\\left( \\mathbf E'_1\\boldsymbol{\\cdot}\\mathbf{n}\\right)\\mathbf{n}\n\\tag{C-10a}\\label{C-10a}\\\\\n \\mathbf B_1 & \\boldsymbol{=} \\dfrac{\\gamma}{c^{2}}\\left(\\boldsymbol{\\upsilon}\\boldsymbol{\\times}\\mathbf E'_1\\right)\n\\tag{C-10b}\\label{C-10b}\n\\end{align}</span>\nNote that since <span class=\"math-container\">",
        "</span>\n<span class=\"math-container\">\\begin{equation}\n \\mathbf B_1 \\boldsymbol{=} \\dfrac{\\gamma}{c^{2}}\\left(\\boldsymbol{\\upsilon}\\boldsymbol{\\times}\\mathbf E'_1\\right)\\boldsymbol{=} \\dfrac{1}{c^{2}}\\left(\\boldsymbol{\\upsilon}\\boldsymbol{\\times}\\gamma\\mathbf E'_1\\right) \\boldsymbol{=}\\dfrac{1}{c^{2}}\\Bigl(\\boldsymbol{\\upsilon}\\boldsymbol{\\times}\\overbrace{\\bigl[\\gamma \\mathbf E'_1\\boldsymbol{-}\\left(\\gamma\\boldsymbol{-}1\\right)\\left( \\mathbf E'_1\\boldsymbol{\\cdot}\\mathbf{n}\\right)\\mathbf{n}\\bigr]}^{\\mathbf E_1}\\Bigr)\\boldsymbol{\\implies}\n\\nonumber\n\\end{equation}</span>\nso\n<span class=\"math-container\">\\begin{equation}\n \\boxed{\\:\\:\\mathbf B_1\\boldsymbol{=} \\dfrac{1}{c^{2}}\\left(\\boldsymbol{\\upsilon}\\boldsymbol{\\times}\\mathbf E_1\\right)\\:\\:}\n\\tag{C-11}\\label{C-11} \n\\end{equation}</span>\nThis is identical to the expression of <span class=\"math-container\">",
        "</span> in equation \\eqref{B-01a} derived by the Liénard–Wiechert potentials.</p>\n<p>Inserting in \\eqref{C-10a} the expression <span class=\"math-container\">",
        "</span>, see equation \\eqref{A-01a}, we have\n<span class=\"math-container\">\\begin{align}\n  \\mathbf E_1 & \\boldsymbol{=} \\dfrac{q_1}{4\\pi \\epsilon_0}\\biggl[\\gamma \\dfrac{\\mathbf r'}{\\Vert  \\mathbf r' \\Vert^3}\\boldsymbol{-}\\left(\\gamma\\boldsymbol{-}1\\right)\\left( \\dfrac{\\mathbf r'}{\\Vert  \\mathbf r' \\Vert^3}\\boldsymbol{\\cdot}\\mathbf{n}\\right)\\mathbf{n}\\biggr]\n\\nonumber\\\\\n & \\boldsymbol{=} \\dfrac{\\gamma}{\\Vert  \\mathbf r' \\Vert^3}\\underbrace{\\left[\\mathbf r'\\boldsymbol{-}\\dfrac{\\gamma\\boldsymbol{-}1}{\\gamma}(\\mathbf r'\\boldsymbol{\\cdot} \\mathbf n)\\mathbf n \\right]}_{\\mathbf r}\\boldsymbol{=} \\dfrac{\\gamma\\,\\mathbf r}{\\Vert  \\mathbf r' \\Vert^3} \\implies\n \\nonumber\\\\\n \\mathbf E_1 & \\boldsymbol{=} \\dfrac{q_1}{4\\pi \\epsilon_0}\\dfrac{\\gamma\\,\\mathbf r}{\\Vert  \\mathbf r' \\Vert^3}\n\\tag{C-12}\\label{C-12} \n\\end{align}</span>\nFrom the relation \\eqref{C-07} between <span class=\"math-container\">",
        "</span> and the help of Figure-04 we could prove<sup>(1)</sup> that\n<span class=\"math-container\">\\begin{equation}\n\\dfrac{\\gamma}{\\Vert  \\mathbf r' \\Vert^3}\\boldsymbol{=}  \\dfrac{\\left(1\\boldsymbol{-}\\beta^2\\right)}{\\left(1\\boldsymbol{-}\\beta^2\\sin^2\\!\\phi\\right)^{3/2}}\\dfrac{1}{\\Vert\\mathbf r\\Vert^3}\n\\tag{C-13}\\label{C-13} \n\\end{equation}</span>\nso \\eqref{C-12} yields\n<span class=\"math-container\">\\begin{equation}\n \\boxed{\\:\\:\\mathbf E_1 \\boldsymbol{=} \\dfrac{q_1}{4\\pi \\epsilon_0}\\dfrac{\\left(1\\boldsymbol{-}\\beta^2\\right)}{\\left(1\\boldsymbol{-}\\beta^2\\sin^2\\!\\phi\\right)^{3/2}}\\dfrac{\\mathbf r}{\\Vert\\mathbf r\\Vert^3}\\:\\:}\n\\tag{C-14}\\label{C-14}  \n\\end{equation}</span>\nThis is identical to the expression of <span class=\"math-container\">",
        "</span> in equation \\eqref{B-01a} derived by the Liénard–Wiechert potentials.</p>\n<p><span class=\"math-container\">",
        "</span></p>\n<p>Reference (1) :  <a href=\"https://physics.stackexchange.com/questions/412924/magnetic-field-due-to-a-single-moving-charge/413029#413029\">Magnetic field due to a single moving charge</a>.</p>\n<p>Reference (2):  <a href=\"https://physics.stackexchange.com/questions/296904/electric-field-associated-with-moving-charge/426795#426795\">Electric field associated with moving charge</a>.</p>\n<p><span class=\"math-container\">",
        "</span></p>\n<p><sup>(1)\n<strong>Proof of equation \\eqref{C-13}</strong>\n</sup></p>\n<sup>\nFrom equation \\eqref{C-07} and Figure-04 we have\n<span class=\"math-container\">\\begin{align}\n\\Vert\\mathbf r\\Vert^2 & \\boldsymbol{=}\\Vert\\mathbf r'\\Vert^2\\boldsymbol{+}\\left(\\dfrac{\\gamma\\boldsymbol{-}1}{\\gamma}\\right)^2\\underbrace{\\Vert\\left(\\mathbf r'\\boldsymbol{\\cdot} \\mathbf n\\right)\\mathbf n\\Vert^2}_{\\Vert\\mathbf r'\\Vert^2\\cos^2\\phi'} \\boldsymbol{-}2\\left(\\dfrac{\\gamma\\boldsymbol{-}1}{\\gamma}\\right)\\underbrace{\\left(\\mathbf r'\\boldsymbol{\\cdot} \\mathbf n\\right)^2}_{\\Vert\\mathbf r'\\Vert^2\\cos^2\\phi'} \\boldsymbol{\\implies}\n\\nonumber\\\\\n\\Vert\\mathbf r\\Vert^2 & \\boldsymbol{=}\\Vert\\mathbf r'\\Vert^2\\left[1\\boldsymbol{-}\\left(1\\boldsymbol{-}\\dfrac{1}{\\gamma^2}\\right)\\cos^2\\phi'\\right] \\boldsymbol{\\implies}\n\\nonumber\n\\end{align}</span>\n<span class=\"math-container\">\\begin{equation}\n\\Vert\\mathbf r\\Vert^2\\boldsymbol{=} \\Vert\\mathbf r'\\Vert^2\\left(1\\boldsymbol{-}\\beta^2\\cos^2\\phi'\\right)\n\\tag{Pr-01}\\label{Pr-01} \n\\end{equation}</span>\nFrom the normal components equality <span class=\"math-container\">",
        "</span> we have\n<span class=\"math-container\">\\begin{equation}\n\\Vert\\mathbf r\\Vert^2\\sin^2\\phi\\boldsymbol{=} \\Vert\\mathbf r'\\Vert^2\\sin^2\\phi'\n\\tag{Pr-02}\\label{Pr-02} \n\\end{equation}</span>\nthat is\n<span class=\"math-container\">\\begin{equation}\n\\cos^2\\phi'\\boldsymbol{=}1\\boldsymbol{-}\\dfrac{\\Vert\\mathbf r\\Vert^2}{\\Vert\\mathbf r'\\Vert^2}\\sin^2\\phi\n\\tag{Pr-03}\\label{Pr-03} \n\\end{equation}</span>\nInserting this expression of <span class=\"math-container\">",
        "</span> in equation \\eqref{Pr-01} and solving with respect to  <span class=\"math-container\">"
      ],
      "created": "2021-07-18T02:02:37.927",
      "golden_ner_terms": [
        "charge",
        "components",
        "coulomb's law",
        "equality",
        "equation",
        "expression",
        "field",
        "finite",
        "finite difference",
        "forces",
        "formula",
        "frame",
        "l system",
        "magnetic fields",
        "moment",
        "normal",
        "order",
        "relation",
        "represents",
        "right",
        "separation",
        "space",
        "sufficient",
        "time",
        "transformation",
        "translate",
        "vectors",
        "velocity"
      ],
      "golden_ner_count": 28,
      "golden_patterns": [
        {
          "pattern": "reduce-to-known-result",
          "score": 2.0,
          "hotwords": [
            "well-known"
          ]
        },
        {
          "pattern": "split-into-cases",
          "score": 2.0,
          "hotwords": [
            "two cases"
          ]
        }
      ],
      "golden_pattern_names": [
        "reduce-to-known-result",
        "split-into-cases"
      ],
      "golden_scopes": [
        {
          "type": "consider",
          "match": "Consider two electric charges "
        },
        {
          "type": "consider",
          "match": "Consider two electric charges "
        },
        {
          "type": "consider",
          "match": "Consider now that Figure-01 represents a snapshot of the charges in t"
        }
      ],
      "golden_scope_count": 3
    }
  ]
}